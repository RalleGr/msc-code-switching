<doc id="6469" url="https://es.wikipedia.org/wiki?curid=6469" title="La Palma">
La Palma

La Palma, cuyo nombre histórico es San Miguel de La Palma, es una isla del océano Atlántico perteneciente a la Comunidad Autónoma de Canarias (España). Junto a Tenerife, La Gomera y El Hierro conforma la provincia de Santa Cruz de Tenerife. Con una superficie de 708,32 km² y una población de 82 671 habitantes (INE 2019) ocupa el quinto lugar tanto en extensión como en población en el Archipiélago Canario. Además, es la segunda isla de Canarias en altitud, con los 2426 metros del Roque de los Muchachos.

La ciudad de Santa Cruz de La Palma es la capital de la isla, con un total de 17.716 habitantes (INE 2019) . Sin embargo, el municipio más poblado de la isla es Los Llanos de Aridane, con una población de 20.467 habitantes (INE 2019).

Desde 2002, toda la isla es Reserva de la Biosfera, siendo tras Lanzarote, y El Hierro la tercera isla canaria a la que la Unesco reconoce con esta protección. En el centro de la isla se ubica el parque nacional de la Caldera de Taburiente, donde se encuentra el mayor cráter volcánico emergido del mundo. La Caldera de Taburiente también es uno de los cuatro parques nacionales con los que cuenta las Islas Canarias.

La denominación ""la Palma"" (con la ""l"" en minúscula) aparece ya en los primeros escritos que los europeos realizaron de las Islas Canarias. En un texto de 1341 que relata la expedición de Niccoloso da Recco acompañado por tres naves pertenecientes a Alfonso IV de Portugal, menciona a cada una de las islas, y La Palma aparece mencionada ya con su nombre actual como "imponente y nublada". Aparentemente, el nombre actual de la isla se debe a las extensiones de palmeras canarias ("Phoenix canariensis") que posee. Sin embargo, esta explicación tiene ciertas incoherencias, pues la palmera no es el árbol más representativo de la flora de la isla, ni tampoco La Palma es la isla canaria que más palmeras tiene.

Una de las teorías que cita el investigador tinerfeño José de Viera y Clavijo es que el nombre de La Palma lo recibió de los navegantes mallorquines del siglo XIV, los cuales le dieron el nombre de la capital de la isla de la que procedían, es decir, de "Palma" o Palma de Mallorca, aunque añadiendo la sílaba ""la"" para diferenciarla de esta.

También recibe, tradicionalmente, el nombre de San Miguel de La Palma, el cuál es el nombre histórico de la isla.

A lo largo de la historia La Palma ha recibido numerosos nombres. Puede que la "Junonia Maior" que aparece en el texto de Plinio el Viejo haga referencia a La Palma, aunque algunos investigadores sostienen que el nombre refererido a La Palma sería "Ombrion". Los aborígenes la denominaban "Benahoare", que se ha traducido tanto como: ""Mi tierra"", o bien, ""lugar del ancestro"". Actualmente son muy populares los sobrenombres de: "La Isla Bonita", "La Isla Verde" o "La Isla Corazón".

La isla tiene una superficie de 708,32 km² (9,45% del territorio canario) y una población censada de 82 671 habitantes (INE, enero de 2019). Su territorio es muy abrupto, alcanzando los 2426 m en el Roque de los Muchachos, punto más elevado de la isla, que la convierte, tras Tenerife, en la segunda isla con mayor altitud de Canarias.

En el tercio norte de La Palma se encuentra una gran depresión de origen erosivo que forma la Caldera de Taburiente, declarada parque nacional en 1954. Desde el centro de la isla hasta el sur, en la llamada Cumbre Vieja, en ella se encuentra una serie de volcanes entre los que se destacan el Volcán de San Antonio, Volcán de San Juan y el Teneguía (última erupción volcánica terrestre de España en 1971). La Palma también posee el parque natural de Cumbre Vieja y el parque natural de Las Nieves, así como una serie de entidades protegidas de menor tamaño y grado de protección.

En 1983, la zona de "El Canal y Los Tilos" es declarada como Reserva de la biosfera por la Unesco. Esta área se amplió en 1997 para formar la "Reserva de la Biosfera de Los Tilos". Finalmente, en 2002, se extendió la reserva a toda la isla con la denominación de Reserva de la Biosfera de La Palma.

La Palma es una de las islas canarias con mayor superficie boscosa, tanto de pinos como de laurisilva. En cuanto a la agricultura, los cultivos principales son el plátano de Canarias y la vid.

En la actualidad, el municipio más poblado de la isla es Los Llanos de Aridane, que supera en este respecto a la capital insular Santa Cruz de La Palma, siendo por lo tanto, la única isla canaria cuyo municipio más poblado no es el de la capital insular.

La Palma está dividida en 14 municipios: 7 en su vertiente Oeste (Sotavento) y 7 en su vertiente Este (Barlovento) :
La Palma, como el resto de Canarias, es una isla de origen volcánico. Con una edad geológica estimada en 2 millones de años, es una de las más jóvenes del archipiélago. Surgió de un volcán submarino situado a 4000 metros bajo el nivel del mar. El edificio volcánico de la isla posee una altitud de 6500 m desde la plataforma abisal del Atlántico, encontrándose en él todos los tipos de rocas volcánicas.La isla se divide en dos zonas climáticas bien diferenciadas mediante una cadena de volcanes denominada Cumbre Vieja. En la zona sur existen volcanes todavía en activo. La última erupción tuvo lugar en 1971 en la punta meridional de la isla, en el municipio de Fuencaliente. De esa erupción surgió el volcán Teneguía, que sigue estando en el punto de mira de los científicos por seguir candente. La zona norte está dominada por la Caldera de Taburiente, una caldera submarina creada por erupciones y la erosión, que emergió hasta una altura de 3500 msnm. Esta caldera es el mayor cráter emergido del mundo. El interior de la caldera se vació en el pasado geológico por una rápida emisión de lava a través de una brecha que se abrió cerca del actual Balcón de Taburiente en lo que es hoy el Barranco de las Angustias. Las huellas de esta emisión de lava pueden verse en el interior de la caldera, ya que dichas huellas (barrancos en las paredes internas) están orientadas hacia el centro del cráter y no hacia el exterior, como hubiera sucedido en un cráter con erupciones explosivas (como sucedió en el Mount Saint Helens) (). La caldera mide 9 km de diámetro, 28 de circunferencia y 1500 metros de profundidad. La única salida que presenta es el "Barranco de las Angustias", lugar por el que sólo se puede acceder a pie. En ella sólo residen dos personas encargadas de las tomas de agua. En 1954 se creó el parque nacional de la Caldera de Taburiente.

Se encuentra rodeada por picos de entre 1700 y 2400 m de altitud, en los que está situada la mayor altitud de la isla, el Roque de los Muchachos, con 2426 msnm. En este pico se encuentra el Observatorio del Roque de los Muchachos.


Los datos de estas erupciones se han obtenido a través de los cráteres, los campos de cenizas y la longitud de las coladas de lava.

La actividad volcánica es un riesgo constante. Aunque está concentrada en la zona sur de la isla, existen teorías que predicen que una erupción podría volver inestable la zona occidental de la isla y caer al mar. Un estudio de los años 90 descubrió que la Cumbre Vieja se encuentra llena de agua, debido a la porosidad de la piedra. Pero en realidad, la característica explosiva de un volcán se encuentra en la mayor o menor temperatura de la lava: si esta es muy alta, la lava (como la que hay normalmente en una caldera), es muy líquida, lo que disminuye el carácter explosivo de sus erupciones y aumenta la posibilidad de los derrames, bien sea abriendo una brecha en la parte superior del cráter, o saliendo a través de una abertura en la pared del cráter.

Existe una teoría según la cual una erupción volcánica podría calentar el agua que se encuentra dentro de la Cumbre Vieja haciendo que esta colapsase. Afortunadamente, la explosión por vapor de agua en el interior de un volcán o caldera es más bien un fenómeno raro, ya que suele predominar la formación de géiseres en este caso. En algunos casos de las Islas Canarias (como sucede en Lanzarote) se da esta posibilidad, aunque por la sequedad del clima es necesario arrojar un balde de agua en una abertura para que se produzca la erupción de vapor.

En la erupción de 1949 se pudo comprobar cómo se abrió una falla, de forma que la parte sur de la isla se hundió cuatro metros en el Atlántico, lo que apoya esta teoría. En caso de cumplirse, es posible que se generase un megatsunami de dimensiones catastróficas.
Por otro lado, otros científicos estiman que lo que puede suceder es que la zona occidental de la isla se fragmente en partes pequeñas, como ocurrió en 1949, sin llegar a generar ningún tsunami o provocando una ola de menor intensidad.
En cualquier caso, la mayoría de los científicos aboga porque no hay ningún indicio actual que lleve a pensar que este hecho pueda ocurrir en las próximas décadas. La historia geológica fácilmente comprobable en la isla de La Palma sustenta esta idea, ya que es una isla extraordinariamente volcánica, con centenares de cráteres de todos los tipos y tamaños () y ello no justifica una explosión gigantesca en la isla por el hecho de que no existe una cámara magmática común a todos estos cráteres. Es decir, la erupción de un volcán en La Palma no suele afectar a otros volcanes aunque se encuentren muy cerca, lo que nos indica que la fuerza expansiva de esas erupciones se tendría que repartir en una gran cantidad de aberturas para afectar toda la isla.

En un documental de la serie "Horizon" de la "BBC" emitido el 12 de octubre de 2000, dos geólogos (Day and McGuire) citaron la brecha como prueba de que la mitad de la Cumbre Vieja se había deslizado hacia el océano Atlántico (Day et al., 1999; Ward y Day, 2001). Sugirieron que este proceso fue impulsado por la presión causada por el aumento del magma calentando el agua atrapada dentro de la estructura de la isla. La hipótesis sugerida establecía que en una futura erupción, el flanco occidental de la Cumbre Vieja, con una masa de aproximadamente 1,5 x10 kg, podría deslizarse hacia el océano. Esto podría generar una ola gigante, desencadenando un "mega-tsunami" de 900 m de altura en la región de las islas. La ola se desplazaría a través del Atlántico e inundaría la costa este de América del Norte como la de Estados Unidos de América, el Caribe y el norte de las costas de América del Sur alrededor de seis a ocho horas más tarde. Estimaron que el tsunami tendría posiblemente olas de 1000 pies o más alto y causaría una enorme devastación a lo largo de las costas. En modelos representados se indicó que el tsunami podría inundar hasta 25 km tierra adentro - dependiendo de la topografía. La teoría de Ward and Day (1999) dio como resultado el colapso de una porción mucho mayor del flanco occidental de la superficie fisuras visibles actualmente es inestable se sugieren pruebas de la cartografía geológica de Day et al. 1999. En este documento sostienen que una gran parte del flanco occidental se ha construido en la cicatriz de una caída anterior y, por tanto, se sienta sobre desechos inestables.

Esto también fue motivo de un docudrama de la BBC llamado "Fin de los Días" que pasó por varios escenarios hipotéticos de proporciones desastrosas.

Sin embargo, la Sociedad Tsunami (Pararas-Carayannis, 2002), publicó una declaración indicando que "..."Nos gustaría detener el alarmismo infundado de estos informes"..." Los principales puntos planteados en este informe incluyen:


Otros estudios también en desacuerdo con la hipótesis de Day "y otros"; (1999) y Ward y Day (2001).

Sin embargo, existe un consenso por los geólogos y vulcanólogos de que el ""edificio"" de una isla volcánica puede sufrir grandes modificaciones, levantamientos o hundimientos y que se pueden haber producido grandes tsunamis en el Atlántico en el pasado geológico. A pesar de ello todavía no hay evidencia confiable que demuestre una causa y efecto. Todos los documentos acerca de los tsunamis a gran escala en el Atlántico se han atribuido a los terremotos y no a los volcanes (el caso del hundimiento de Port Royal en Jamaica o el terremoto de Lisboa en el siglo XVIII, por ejemplo). ((Datos | fecha = agosto 2007)) La prueba de depósitos de tsunami se ha informado desde el Caribe y las Islas Canarias. Desde el decenio de 1990 la zona ha sido (y sigue siendo), el control y la circulación no se ha detectado. ((Datos | fecha = 2007)) agosto en curso y recientes (2008) el seguimiento demuestra que las dimensiones de acuerdo con los registrados en 1949. Lo que indica que el bloque no se ha movido desde 1949 ((Datos | fecha = agosto 2007)).

Datos para Santa-Cruz de La Palma

Datos para Los Gallegos (Barlovento)

Datos para Tazacorte

Debido a su formación y localización, La Palma presenta una gran variedad de paisajes, debido a la diversidad de ecosistemas que presenta, desde los áridos costeros hasta la muy húmeda formación boscosa de la laurisilva, además de bosques de pinares y un ecosistema de alta montaña. Toda esta diversidad le ha dado los sobrenombres de "La Isla Bonita" y "La Isla Verde".

La isla no sólo recibe agua a través de precipitaciones, sino que además y lo hace a través de la lluvia horizontal. Los vientos alisios traen nubes a una cota baja chocando con el relieve de forma constante durante casi todo el año, formando brumas que la vegetación, especialmente la laurisilva, condensa, produciendo este fenómeno conocido como lluvia horizontal. Un ejemplo de aprovechamiento de este hecho en las especies vegetales, es el caso del pino canario, que al tener sus hojas en forma de agujas actúan como filtro condensador y permiten que la bruma precipite sobre el pie del árbol.

Las formaciones boscosas de la palma se forman según su altitud y orientación que desde los campos de lava a los bosques de laurisilva, pasando por zonas de pinares, vegetación termófila, vegetación de cumbre y costera. Entre las plantas que crecen en la isla, 170 son endémicas de Canarias, siendo las más características el Drago, el Pino Canario y la Palmera Canaria.

La vegetación puede dividirse en una serie de pisos más o menos diferenciados, en torno a las dos vertientes de la isla, siendo por lo general más húmeda en la zona oriental que la occidental, y así mismo, más seca también en la meridional que en la septentrional.


Los Paisajes de la isla se componen de cuatro colores principales, el negro del basalto y la lava solidificada en forma de malpaís, el rojo de la toba volcánica, el verde de la densa vegetación y el eterno azul del océano Atlántico y del limpísimo cielo. De norte a sur de la isla, con mayor predominio en la mitad norte, existen barrancos profundos por donde discurre el agua depositada en las cumbres. En la zona norte y central se encuentran bosques de pinos y fayal-brezal, en la zona noreste laurisilva y en la zona sur-suroeste, tierras volcánicas debido a la reciente actividad volcánica. Sin embargo, esto es una leve aproximación de lo que puede dar de sí la isla, teniendo paisajes tan espectaculares como son el mar de nubes o las majestuosas laderas interiores de la Caldera de Taburiente, entre muchos otros.

Destaca por encima de todo el parque nacional de la Caldera de Taburiente, situado en el centro norte de la isla, coincidiendo con el accidente geográfico de La Caldera de Taburiente, de 7 km de eje máximo. Aparte de sus magníficas vistas, tiene aspectos realmente llamativos como son el Roque Idafe o el riachuelo de La Caldera, única corriente de agua continua de las islas Canarias. En cuanto a la vegetación, el Pino Canario es el rey casi absoluto en toda la Caldera.
La isla de La Palma concentra los únicos torrentes de caudal continuo de las Islas Canarias y se encuentran dentro de este parque nacional.

También es de gran atractivo turístico la Ruta de Los Volcanes, que recorre la mitad sur de la isla a través de la dorsal montañosa, atravesando enormes volcanes extintos rodeados de una serie de paisajes volcánicos espectaculares.

Destacan así mismo, unas piscinas naturales llamadas La Fajana, y situadas en el municipio de Barlovento, son de agua salada, procedente íntegramente del mar que está justo al lado. Este conjunto cuenta con 3 piscinas, situadas a distintos niveles, y con escaleras para facilitar el baño. Cuenta con una piscina para minusválidos pero actualmente no está en uso. Incluyendo esta serían cuatro piscinas, siendo esta última artificial.

Entre las especies endémicas de la isla se encuentra:


Según una ley del Gobierno de Canarias, los símbolos naturales de la isla desde 1991 son la graja y el pino canario.

Desde 1983, el bosque de laurisilva "Los Tilos" está catalogado como Reserva de la Biosfera por la Unesco. En 2002 se amplió esta declaración a toda la isla. La Palma fue la primera isla canaria en albergar un lugar de este tipo. Por otro lado se encuentra el parque nacional de la Caldera de Taburiente así como otros entornos sujetos a diferentes fórmulas de conservación según establece la Red Canaria de Espacios Naturales Protegidos.

EL Gobierno de Canarias aprobó el Plan Territorial Especial del Uso Turístico de La Palma (PTE), que incluye la construcción, en los próximos años, de 4 a 5 campos de golf de 18 hoyos con sus respectivos hoteles y villas de lujo. Uno de ellos, el Aridane Golf, invadiría el Paisaje Protegido de Tamanca, que además es Lugar de Interés Comunitario (LIC) con varias especies endémicas de fauna y flora en peligro de extinción. El proyecto tiene dos sentencias desfavorables del Tribunal Superior de Justicia de Canarias (TSJC). Los demás proyectos también afectan a LICs, Zonas de Especial Protección de las Aves (ZEPAs) y Parques naturales. El documento del PTE también permite la construcción de varios puertos deportivos, marinas y hoteles de turismo convencional en zonas vírgenes del litoral palmero. Se presentaron miles de alegaciones en contra de dichos planes, por ir en contra de los objetivos de la Reserva Mundial de la Biosfera de La Palma, de un turismo sostenible y respetuoso con la naturaleza y por perjudicar directamente a espacios naturales protegidos y sus especies endémicas de fauna y flora en peligro de extinción.

Más recientemente, el Gobierno de Canarias ha aprobado la ley de Medidas Urgentes en materia de Ordenación del Turismo cuyas enmiendas 39 y 40, permiten la construcción de infraestructuras turísticas en el interior de espacios naturales protegidos, contraviniendo las leyes medioambientales de Canarias, de España y comunitarias. Los biólogos de la Reserva Mundial de La Palma han advertido del mal estado de los fondos marinos, debido sobre todo a una sobrepesca, que sigue utilizando artes poco selectivas y agresivas como son las nasas. La consecuencia más directa de la falta de control de la pesca es la proliferación del erizo de Lima, plaga que destruye la cobertura algal dejando tras de sí un yermo blanquizal. La creación de la reserva marina de Fuencaliente ha servido para recuperar en esa franja litoral las poblaciones más importantes de peces, aunque los pescadores, al acecho en sus límites, no permiten la recuperación de otras zonas adyacentes.

Otras amenazas podrían empeorar la situación actual de las costas palmeras: proyectos de puertos deportivos y marinas; aumento de la urbanización del litoral o instalación de jaulas flotantes para acuicultura.

La isla tiene un total de 20 espacios con diversas categorías de protección:








Los primitivos habitantes de La Palma eran los benahoaritas, auaritas o awaras. En el momento de la conquista, estaba dividida en 12 cantones. Los primeros textos sobre La Palma datan de la Baja Edad Media (siglos XIV y XV). Aunque faltan datos concretos al respecto, se calcula que la población en ese momento, podía oscilar en torno a los 4000 habitantes. Los aborígenes vivían fundamentalmente del pastoreo de cabras, ovejas y cerdos y recolectaban frutos y raíces con los que elaboraban una especie de harina a la que llamaban "gofio", hecha con raíces de helecho y amagante, que tostaban y molían.

La hipótesis más aceptada sobre el origen de los aborígenes de la isla de la Palma los vincula a tribus bereberes provinentes del noroeste del continente africano. Se desconoce si llegaron a la isla por su propia voluntad o expulsados de sus lugares de origen por invasores como los (romanos o fenicios). Los restos hallados en los yacimientos muestran que la estatura media era de 1,70 metros para los hombres y de 1,65 metros para las mujeres.
Muchos historiadores han destacado la belicosidad de los aborígenes (como sucedía también con los guanches de Tenerife). Estos tenían con mucha frecuencia guerras civiles y todo tipo de enfrentamientos, que no se restringían a un cantón sino que con frecuencia afectaban a toda la isla. Un ejemplo de fuerte confrontación es el de la que tuvo lugar entre Atogamtoma (señor de Tijarafe) con Tanausú (Aceró) o Mayantigo (Aridane).

Los aborígenes palmeros también tenían un sistema de gobierno que aunque primitivo permitía discutir sin peleas muchos de los problemas existentes, esta institución era el "Tagoror". Asimismo, dentro de la comunidad se le daba mucha importancia a la familia y permitía unir a varios miembros en grupos por mismo linaje de sangre. Esta unión podría ser de primer orden o nuclear (padres e hijos), o también retrospectiva o extensiva (un antepasado común).

Se cree que el pueblo benahorita tenía una historia de alrededor de unos 2000 años, hasta que, en 1493, Alonso Fernández de Lugo desembarcó en la isla con la intención de conquistarla. La Palma fue la penúltima isla canaria en ser conquistada poco antes de Tenerife (1496). La conquista puso fin a las guerras intestinas de los aborígenes tanto en La Palma como en Tenerife.

Si bien, dependiendo de las fuentes los datos pueden cambiar, aparecen en ocasiones subdivisiones internas o distintos topónimos para designar un mismo territorio, suele aceptarse que los 12 cantones o segmentos en los que se dividía la isla y sus respectivos señores -señalados entre paréntesis- en el momento de la conquista eran:


A diferencia de Tenerife o Gran Canaria, en La Palma no existía ninguna superestructura por encima de estas unidades. De hecho este sistema de poder no es permanente y estas unidades o segmentos podían estar divididas en otras más pequeñas (en las propias fuentes del siglo XVI se hace referencia a otras unidades como el "bando de Gazmira").

A partir del siglo XVI, la colonización de La Palma ofrece a los nuevos pobladores posibilidades diversas de progreso económico: tierras de cultivo, entrada en el circuito comercial entre América y Europa y el abastecimiento de manufacturas a las islas. Junto a los pobladores españoles llegarán portugueses, genoveses, franceses y flamencos, que se mezclarán con los indígenas que quedaron tras la conquista. Se dedicarán principalmente a la agricultura, que va a girar en torno a la producción y comercio de monocultivos de exportación, beneficiados del clima canario y cuyo control generará grandes fortunas.

El primero de estos productos será la caña de azúcar, que a partir de la segunda mitad del siglo XVI será sustituido por los vinos canarios. Asimismo, llegarán grupos de población morisca y negros africanos, capturados para utilizarlos como esclavos en las plantaciones, o como mano de obra en el uso de maquinaria agrícola, a pesar de una carta papal de 1434, en la que Eugenio IV los declaraba "gente libre", prohibiendo el tráfico de hombres en la isla. En 1514, cuando se les equiparó en derechos, fueron siendo bautizados, mezclándose con los colonos europeos.

La caña de azúcar fue introducida por Alonso Fernández de Lugo. Los territorios de la isla fueron divididos entre mercaderes, agricultores y artesanos europeos. De esta forma, en 1508, Juan Fernández de Lugo vendió sus cultivos de caña de azúcar así como reservas de agua en Tazacorte y Argual un andaluz apellidado Dinarte; este los vendió un año más tarde a la Familia Welser, que los transmitiría al belga Jakob Groenenberch ("Jacobo Monteverde"), que terminaría por vendérselos a su compatriota Van de Valle.

A partir de 1553, el cultivo de la caña de azúcar dejó de ser rentable debido a la producción en masa proveniente de América Central y Sudamérica. Muchas de las plantaciones pasaron a dedicarse a la producción del vino. El vino de malvasía producido por los suelos volcánicos jóvenes del sur de la isla se convirtió en la principal exportación de la isla. El principal cliente de los vinos palmeros fue Inglaterra. El esplendor del vino palmero duró hasta el siglo XIX, cuando hubo un declive provocado por el cambio de gustos de los consumidores. Sin embargo, aún hoy en día se sigue cultivando y produciendo vino de malvasía aunque no sea el vino preferido por las masas de consumidores.

En el siglo XVI recibió La Palma, tras Amberes y Sevilla, el privilegio del comercio con América. El puerto de Santa Cruz de La Palma se convirtió enseguida en uno de los puertos más importantes del Imperio español. Esta nueva fuente de riqueza atrajo a su vez a los piratas que atacaban la isla para apropiarse de los tesoros llegados de las Indias. François Le Clerc y su grupo de piratas franceses tomaron la ciudad en 1553 robando todo lo transportable y quemando lo que no era posible transportar. Tras esa catástrofe hubo que reconstruir las casas, iglesias y conventos de la ciudad así como sus fuertes defensivos. Con las nuevas defensas, se pudo rechazar el ataque de Francis Drake de 1585, el cual no pudo llegar a desembarcar. 

El comercio con América también generó otra serie de actividades como los astilleros. Santa Cruz de La Palma atrajo a muchos comerciantes extranjeros (flamencos, franceses, castellanos, italianos, portugueses, etc.) dándole a la localidad un aire internacional. Las calles con nombres extranjeros son aún hoy testigos de esa época, como la calle O'Daly (irlandés) o la calle Vandale (flamenco). El declive comenzó a mediados del siglo XVII debido a una concesión de 1657 que obligaba a todos los barcos con destino América a registrarse en Tenerife. En 1778, Carlos III abrió todos los puertos de España al comercio con América, impidiendo que Santa Cruz de La Palma se recuperara de la crisis económica en la que se encontraba inmersa en aquellos momentos.

Sin la amenaza pirata, la vida en La Palma prosiguió su rumbo de forma tranquila. De cada crisis económica sufrida, se levantaba la isla, no por poseer riquezas minerales sino por la fertilidad de su tierra. Tras el cultivo de la caña de azúcar y de la vid, se pasó a la producción de miel, tabaco y seda. Desde principios del siglo XVI había comenzado la plantación de moreras, convirtiéndose La Palma en un foco de producción de seda. En 1830 se introdujo desde México el cultivo de la cochinilla, un parásito de las tuneras del que se extraía carmín. Con el desarrollo de los tintes sintéticos en 1880, el cultivo de la cochinilla dejó de ser rentable. Para salir de esta crisis se introdujo el cultivo del plátano impulsado por Elder y Fyffes, dos compañías británicas en 1878.

Mientras tanto, el pueblo llano apenas se veía beneficiado con las riquezas que producía la isla. Todavía en el siglo XIX, la mayoría de los habitantes de la isla vivían en casas de madera con techos de paja, debido a los altos costes que suponía erguir casas en piedra. Uno de los principales problemas era la falta de bienes de consumo. Debido al monocultivo practicado en la isla, faltaban tierras donde cultivar grano para alimentar a la población. Desde el siglo XVI se tenía que importar el grano, pagándose por él precios muy altos. El párroco de La Palma pagó sus impuestos con millo, lo que impulsó a la población a hacer lo mismo. La Inquisición dictó un Anatema sobre toda la isla provocando que durante varios años no se practicara ningún entierro cristiano. La pobreza en el campo era tan grande, que en muchas familias "los desnutridos y mal vestidos" hombres y mujeres, como relató el misionero Juan de Medinilla en 1758 en una carta al obispo, debían acudir por turnos a la misa de los domingos y festivos, debido a la falta de ropa.

Al producirse el levantamiento militar de 1936, que daría lugar a la Guerra Civil española, la isla de La Palma se resiste al golpe y mantiene la legalidad republicana entre los días 18 y 25 de julio, cuando llega a la ciudad de Santa Cruz de La Palma el cañonero Canalejas. Este periodo de tiempo será conocido como La semana roja.

El golpe militar fracasa en esta isla al ser interceptado por el jefe de telégrafos el mensaje dirigido por los golpistas al comandante militar Baltasar Gómez Navarro, que debía dirigir el golpe en La Palma. En esos momentos era Delegado del Gobierno en la isla Tomás Yanes Rodríguez, de Izquierda Republicana. Al llegar las noticias del golpe el Frente Popular declara la huelga general, y se forman las milicias populares pero la Delegación de Gobierno no autoriza la toma del cuartel militar y trata de evitar siempre que las organizaciones obreras tomen demasiado poder (en estos momentos destaca la figura del comunista José Miguel Pérez, y en algunos municipios como Tazacorte las organizaciones comunistas tienen una gran importancia). A la llegada del cañonero Canalejas la Delegación del Gobierno decide no ofrecer ningún tipo de resistencia armada y ordena desmovilizar a las milicias populares confiando en que el Gobierno de la República mande refuerzos, que el golpe fracase y que la legalidad se restablezca en toda la nación.

La Guerra Civil no se libró en las Canarias, pero pese a ello sí se sufrieron las consecuencias de la misma. El periodo de la posguerra unido a la crisis económica producida trajo años de penurias a la isla. Debido a la carencia de bienes de importación, los palmeros tuvieron que basar su alimentación en el plátano, generando una gran variedad de productos derivados del mismo como la harina de plátano. Una vez finalizada la posguerra La Palma fue desarrollando su economía e infraestructuras poco a poco. Se recuperaron las exportaciones del plátano y comenzó la construcción de carreteras y canales para transportar el agua de los riachuelos a los campos de cultivo. La obra más importante de la época fue la construcción de la carretera de la cumbre, que unía los municipios de Santa Cruz de La Palma y Los Llanos de Aridane a través de un túnel por debajo de las cumbres de la isla, acortando bastante la duración del recorrido unido a la puesta en funcionamiento del aeropuerto. Con la llegada de la democracia, la economía de la isla, fuertemente dependiente de la agricultura del plátano, se fue diversificando hacia otros sectores especialmente el turístico, que constituye hoy en día el principal motor de la economía canaria.

La Palma, como parte de la Comunidad Autónoma de Canarias, depende en función de las distintas competencias, del Gobierno de España, del Gobierno de Canarias y del Cabildo Insular de La Palma.

Esta institución es la encargada de representar al Gobierno de España en la isla y de gestionar todas aquellas competencias que no hayan sido transferidas al Gobierno de Canarias. La sede de la Dirección Insular se encuentra en la avenida marítima de Santa Cruz de La Palma. Desde julio de 2018 la "directora insular de la Administración General del Estado" en La Palma es Ana María de León Expósito. 

Los cabildos, formados a partir de la Ley de Cabildos de 1912, son las formas gubernativas y administrativas propias de las Islas Canarias y cumplen dos funciones principalmente. Por una parte, prestan servicios y ejercen competencias propias de la Comunidad Autónoma y por otra, son la entidad local que gobierna la isla. En las elecciones de 2003 fue elegido presidente José Luis Perestelo Rodríguez, de Coalición Canaria, agrupación que obtuvo el 49,7 % de los votos, seguida del PSOE, con un 22,6 %, y del PP, con un 21,6 % de los votos.

En las elecciones de 2019 y tras una moción de censura presentada por el PP y el PSOE, el presidente de la corporación es el popular Mariano Hernández Zapata. 

El Diputado del Común es el Defensor del Pueblo en Canarias. Lo designa el Parlamento de Canarias para la defensa de los derechos y libertades constitucionales en el ámbito autonómico. Su sede se encuentra en la calle O'Daly de Santa Cruz de La Palma, contando con oficinas en cada isla. No es un órgano administrativo de La Palma pues ejerce sus funciones en el ámbito autonómico.

La isla de La Palma es la quinta más poblada oficialmente del Archipiélago Canario y la séptima de España: a fecha de 1 de enero de 2019, y según fuentes del INE, tenía empadronados un total de 82 671 habitantes censados. Las otras islas del archipiélago que siguen a La Palma en población son La Gomera, con 21 503 habitantes, y El Hierro, con 10.968 habitantes en 2019. Históricamente, La Palma fue (tras Tenerife y Gran Canaria) la tercera isla más poblada de Canarias hasta 1998, desde entonces lo es Lanzarote.

Alrededor de un 25 % de la población total de la isla de La Palma (20 043 habitantes) reside en el municipio de Los Llanos de Aridane, y cerca del 40 % (34 651 personas), en el Valle de Aridane. La Palma tiene la población muy concentrada en dos ciudades: Santa Cruz de La Palma (12 783 habitantes) y Los Llanos de Aridane (3547 habitantes). Al municipio de Los Llanos de Aridane le siguen en población Santa Cruz de La Palma (15 711), El Paso (7457), Breña Alta (7086) y Breña Baja (5377) todos ellos con más de 5000 habitantes. El municipio de Garafía es el que cuenta con menor población de toda la isla (1607). Además, La Palma registra un nivel alto de población no censada, que pone de manifiesto el número de turistas que recibe anualmente y los crecientes fenómenos migratorios. Sin embargo, se considera que las cifras teóricas no reflejan la realidad, ya que gran parte de la población legal de la isla no reside realmente en la misma: se estima que se reduce a unos 55 000 habitantes reales como mucho (incluyendo a turistas y residentes no censados). 

En los últimos años, La Palma ha experimentado un notable estancamiento de la población. En 1990, un total de 82 131 habitantes estaban censados en la isla, cifra que aumentó hasta los 82 483 habitantes en el año 2000. Esos datos reflejan un incremento en 352 personas. Sin embargo, entre los años 2000 y 2010, la población aumentó en 4841 habitantes, hasta llegar a 87 324.

Como sucede en el resto del archipiélago y del país, la población de la isla de La Palma es mayoritariamente católica, si bien existen también minorías de otras religiones como pequeñas comunidades musulmanas. La isla tiene dos arciprestazgos pertenecientes a la Diócesis de San Cristóbal de La Laguna: el de "Santa Cruz de La Palma" y el de "Los Llanos de Aridane".

La isla se encuentra bajo el patronazgo de la Virgen de las Nieves y San Miguel Arcángel. Festivo insular en la isla es el 5 de agosto, festividad de la Virgen de las Nieves.

Actualmente se cultivan en la isla unas tres mil hectáreas de plátanos,: tras Tenerife, es la segunda isla de Canarias donde más se cultiva; además, existen plantaciones de cítricos, aguacates, verduras, papas y uvas (destinadas a la elaboración de vino). El traslado del agua de las cumbres a las huertas se hace a través de una red de galerías filtrantes y canales. La ganadería es principalmente caprina, destinada a la obtención de leche y la elaboración de quesos. Una creciente fuente de ingresos es el turismo, que se concentra en las zonas de Los Cancajos y Puerto Naos.

Al contrario de la agricultura, las manufacturas y la industria tienen una presencia escasa en La Palma. En la isla existen algunos establecimientos que transforman los productos de la tierra en productos de consumo o en obras de arte. También, gracias al turismo, la industria de la construcción tiene una presencia cada vez mayor en la isla. Solo existía una fábrica, la fábrica de puros de El Paso, con 300 trabajadores, que producía grandes cantidades de cigarros. El mercado principal es el alemán. También existen pequeños talleres de bordados y de sedas.

Las exportaciones principales de La Palma son las de productos agrícolas. Pese a ello, la balanza de importaciones y exportaciones sigue siendo negativa en la isla, es decir, se importa más de lo que se exporta. Entre los productos exportados se encuentra el plátano, naranjas, limones y productos agropecuarios. Las importaciones principales, generalmente de la España peninsular, son el petróleo, productos de consumo y productos mecánicos y eléctricos.

En 1890 existían más hoteles en La Palma que en la actualidad. A finales del siglo XIX y principios del siglo XX muchos ingleses convalecientes visitaban la isla en busca de curas. Unas décadas más tarde comenzó el turismo moderno, teniendo su cota más alta en los años 1960. En las décadas de los 70 y 80 se redujo el número de turistas, alejándose la isla del turismo de masas que se estaba desarrollando en la isla vecina de Tenerife. A finales de los años 1980, con la ampliación del aeropuerto comenzaron a llegar vuelos chárter procedentes de varias ciudades europeas.

Con una oferta de 7500 camas, en La Palma no se puede hablar de turismo de masas. Existen pocos hoteles grandes, puesto que normalmente los turistas alquilan apartamentos o casas. Los alemanes conforman el 80% de los visitantes de la isla. En la zona de Los Llanos de Aridane y El Paso existe una importante colonia de residentes alemanes que han escogido la isla como su lugar de residencia permanente.

Cerca del extremo sur de la isla hay un ‘cementerio’ submarino con 40 cruces de piedra erigidas sobre arena y roca. La inmersión a este lugar singular se conoce como Las Cruces de Malpique y es una de las más solicitadas de esta isla, declarada Reserva de la Biosfera por la Unesco. De corrientes ocasionalmente fuertes, excelente visibilidad, profundidad máxima de 25 metros y fácil acceso desde la costa.

Un soberbio ataque del pirata Jacques de Sores ocurrió en 1570 frente a las costas de Fuencaliente con la matanza de los mártires de Tazacorte, 39 frailes jesuitas portugueses y españoles que fueron arrojados por la borda de su buque. En homenaje a ellos, a 20 metros de profundidad, hoy yacen 39 cruces que reciben la visita regular de fotógrafos submarinos, amantes del buceo nocturno y submarinistas en general. La cercana Reserva Marina de La Palma propicia que el lugar cuente también con una biodiversidad marina notable.

No se puede establecer si La Palma seguirá siendo un lugar tranquilo con poco turismo o si sucumbirá al turismo de masas. Las Autoridades (tanto del cabildo como de los distintos ayuntamientos) y los grupos ecologistas, no se ponen de acuerdo sobre el número máximo de camas que podría soportar la isla. Algunos datos lo sitúan en 80 mil aunque los más moderados sólo estiman 20 mil. Aunque cada día se aprecia más la necesidad de evolucionar hacia un turismo sostenible y la mayoría de los turistas que visitan en la actualidad La Palma lo hacen buscando un destino diferenciado basado en pequeños hoteles (rurales) e infraestructuras de ocio integradas en el paisaje y respetuosas con el entorno, existen algunos proyectos urbanísticos y turísticos, promovidos por Ayuntamientos y Cabildo de la Isla, que amenazan seriamente la integridad de varios espacios naturales protegidos: Campos de golf de 18 hoyos y sus hoteles amenazan al Paisaje Protegido de Tamanca, un pinar de gran valor en Fuencaliente, el monteverde y una zona de especial protección de las aves (ZEPA) en La Pavona, etc. Desde mediados de los años 1990 visitan la isla unos 140 mil turistas cada año, de los cuales 100 mil son alemanes. La agricultura sigue siendo, sin embargo, la mayor fuente de riqueza de la isla. Las Playas de Los Cancajos y Puerto Naos ostentan la Bandera azul, lo que garantiza un alto nivel de calidad.

Desde hace algunos años, se ha implantado en la isla el denominado Turismo Rural. Esta modalidad turística consiste en la remodelación y modernización de casas antiguas para convertirlas en casas de huéspedes, respetando la arquitectura tanto interior como exterior. Este proyecto, inicialmente financiado por el proyecto LEADER de la Unión Europea, ayuda a preservar los paisajes de la isla pues sólo se pueden remodelar casas antiguas efectuando pequeñas ampliaciones. La primera entidad, constituida en 1992, para el impulso de este sector fue la Asociación de Turismo Rural Isla Bonita, que agrupa a los isleños propietarios de establecimientos.

La Asociación de Turismo Rural Isla Bonita es una organización que impulsa esta activiada.

Las forman una red de 510,06 kilómetros. Todas las carreteras están asfaltadas y en buen estado, aunque prácticamente todas presentan muchas curvas, algunas muy cerradas. Para acceder a algunos caseríos del norte hay que transitar por pistas de tierra. Existe una carretera de circunvalación de la isla, de unos 157,88 Kilómetros. Estrictamente, esta circunvalación está compuesta por dos carreteras, la Carretera General del Norte (LP-1) y la Carretera general del Sur (LP-2) . La LP-1, circunvalación norte de 102,430 km parte de Santa Cruz de La Palma y termina en Argual, pasando por (o cerca de) Puntallana, Los Sauces, Barlovento, Garafía, Puntagorda y Tijarafe. La LP-2, circunvalación sur, de 55,450 km parte de Santa Cruz de la Palma y termina en el Puerto de Tazacorte, pasando por (o cerca de) Breña Baja, Mazo, Fuencaliente, Los Llanos y Tazacorte. 

La LP-3, de 25,9 km, también conocida como "Carretera de la cumbre", es una carretera de montaña que atraviesa la isla de este a oeste pasando por dos túneles excavados bajo Cumbre Nueva. Su origen está en la LP-2, a 3 km de Santa Cruz y finaliza en el cruce de Tajuya (El Paso). La LP-4, de 47,840 km, carretera del Roque, sube al observatorio astrofísico del Roque de los Muchachos, bajando hasta Hoya Grande (Garafía) por la vertiente norte de la isla. La LP-5, carretera del Aeropuerto, de 3,8 km parte del barrio del Fuerte (Breña Baja), y acaba en el Aeropuerto de La Palma. La LP-20, "vía exterior" de Santa Cruz de La Palma, es una circunvalación de 3,7 km que evita el paso por el casco urbano de la capital, posee 5 túneles que con sus 1831 m de longitud constituyen el 49% del total de la vía. La red insular se completa con 47 carreteras más, de carácter secundario. El origen de todas las carreteras, punto kilométrico cero de la isla, se fija en la rotonda de acceso al puerto de Santa Cruz (Glorieta de Blas Pérez González).

Existen varias líneas de guaguas que unen las principales localidades de la isla a distintas horas. Para conocer detalles actuales sobre el servicio se puede acceder a la página oficial de Transportes Insulares de La Palma. Disponen de guaguas adaptadas para personas con movilidad reducida, como elevadores para sillas de ruedas. También poseen trenes turísticos para visitar la isla.

La bahía de la capital ha sido usada como puerto desde la conquista de la isla en 1493. Actualmente, parten ferris desde Santa Cruz de La Palma hacia las demás islas, sobre todo a Tenerife, donde operan las compañías Naviera Armas, Acciona Trasmediterránea y Fred. Olsen Express, con tiempos y horarios variables según el barco y compañía, que van desde las 2 horas hasta las 5 horas. También hay una línea que une una vez por semana Santa Cruz de La Palma con Cádiz. 

El nuevo puerto de Tazacorte poseía una conexión semanal con Tenerife, vía Santa Cruz de La Palma. Por otra parte, el municipio de Los Llanos de Aridane, situado junto al anteriormente mencionado Tazacorte, posee una población costera llamada Puerto Naos que pese a que su nombre podría prestar a confusión, no tiene ningún puerto al uso.

En 1950 entró en servicio el Aeropuerto de Buenavista, el primer aeropuerto de La Palma, que estaba emplazado en Breña Alta. Sin embargo, debido a los problemas meteorológicos y a la imposibilidad de ampliarlo para dar cabida a los nuevos aviones de reacción, dejó de utilizarse en 1970 ya que entró en funcionamiento un nuevo aeropuerto en la costa de Mazo. El 24 de febrero de 1970 aterrizó el primer avión en el aeropuerto, un DC-3 del Ejército del Aire. En 1987 el Aeropuerto de La Palma pasó al sexto lugar del archipiélago en número de operaciones. Actualmente, Binter Canarias y Canaryfly realizan las conexiones aéreas con las demás islas. Iberia ofrece conexiones con la península y Transavia y otros operadores chárter y de bajo coste unen la isla con diversas ciudades europeas.

Debido a la localización de la isla y a la altura que alcanza sobre el nivel del mar, han sido instalados varios telescopios en el Observatorio del Roque de los Muchachos. La ubicación geográfica, en medio del Atlántico, y el peculiar clima provocan la formación de nubes entre los 1000 y 2000 m de altura, que hacen de espejo e impiden que la contaminación lumínica de las poblaciones de la costa dificulte la observación de las estrellas. Con el fin de proteger la calidad del cielo, el Gobierno Español, a propuesta del parlamento de las Islas, aprobó el 31 de octubre de 1988 la Ley sobre Protección de la Calidad Astronómica y el 13 de marzo de 1992 el Reglamento que la regula. La Ley del Cielo protege a la isla de la contaminación lumínica, la contaminación radioeléctrica, la contaminación atmosférica y regula el tráfico aéreo sobre los observatorios evitando interferencias. A modo de ejemplo, gran parte de los municipios de la isla cuentan con alumbrado público de luz naranja y apuntado hacia el suelo.

La Ley del Cielo fue una norma pionera en Europa y tiene efectos favorables sobre la conservación del medioambiente, el ahorro energético y las preservación de las especies animales de la isla, (especialmente las nocturnas). Sin embargo, las restricciones a la actividad económica que implica la Ley del Cielo pueden tener efectos negativos sobre el desarrollo industrial en La Palma.


El DOT y el SST han sido construidos para estudiar el Sol.

El baloncesto se viene practicando de forma masiva en los colegios de la isla y ha alcanzado gran popularidad, especialmente entre la población joven. Actualmente el máximo representante de la isla es el C.B. Aridane milita en ligas regionales. Además hay otros equipos que juegan en las ligas regionales y locales.

El fútbol es el deporte por excelencia de La Palma, siendo además el que posee un mayor número de seguidores. Los tres equipos locales con más aficionados son el Club Deportivo Mensajero, la Sociedad Deportiva Tenisca en Santa Cruz de La Palma y la Unión Deportiva Los Llanos de Aridane en el municipio del mismo nombre. En total hay 19 clubes federados. Estos equipos militan en las categorías regionales de Canarias poseyendo la federación tinerfeña de fútbol una sede en la capital de la isla.

La Transvulcania es una ultramaratón de montaña, se basa en un recorrido muy exigente de algo más de 73 km. de distancia y 8500 m de desnivel acumulado. Desde 2012, puntúa para el Campeonato del Mundo de Carreras de Montaña.

En La Palma se practican numerosos juegos autóctonos. Algunos provienen de antiguos métodos de trabajo, como el Salto del Pastor, que la forma que tenían los pastores de descender desde las cumbres o el Calabazo, que era la forma de pasar agua de un canal a otro. Entre los deportes canarios practicados en la isla, cabe destacar los siguientes:

La lucha se desarrolla dentro de un círculo, generalmente de arena, denominado terrero. En él, dos luchadores se enfrentan agarrados intentando derribarse. El organismo de la isla que vela por este deporte es la Federación Insular de Lucha Canaria, y tiene su sede en Los Llanos de Aridane.

En La Palma, existen 10 terreros distribuidos por nueve municipios:

La Palma posee varios clubes que participan en la Liga Regional del Gobierno de Canarias. Estos clubes son el "Bediesta" (de San Andrés y Sauces), el "Candelaria-Mirca" y "Tedote" (de Santa Cruz de la Palma), el "Balta" (de Breña Alta), el "San Blas" (de Mazo), el "San Antonio" (de Fuencaliente), el "Las Manchas" (de El Paso), el "Aridane" (de Los Llanos), el "Tazacorte" (de Tazacorte) y el "Candelaria-Tijarafe" (de Tijarafe).

El juego del palo canario es un arte marcial que se práctica entre dos jugadores que, sin llegar a hacer contacto con el cuerpo del adversario, realizan un combate con palos. El juego del palo, en su origen, no tenía carácter lúdico, sino que era un método de combate que algunos creen ya utilizado por los canarios pre coloniales. En la isla de La Palma, existen dos clubes miembros de la "Federación del Juego del Palo", el "Club Escuela-El Paso" y el "Club Grupo Galguén". Estos clubes participan en la Liga del Juego del Palo, en la que compiten equipos de La Palma, Tenerife, Lanzarote, Gran Canaria y Fuerteventura. Pese a carecer de clubes, se sigue manteniendo la tradición en forma de exhibiciones, especialmente en las fiestas religiosas. El Estilo Vidal es originario de Garafía.

Similar al juego francés de la petanca, la bola canaria se práctica poco en la actualidad a nivel regional aunque en la isla existen varios equipos y canchas. Básicamente consiste en sumar equipos mediante el lanzamiento de unas bolas que hay que dejar lo más cerca posible de un objeto llamado mingue o boliche. Se juega en un terreno rectangular de arena o tierra de entre 18 y 25 m de largo y un ancho de entre 3,5 y 6 m. En La Palma hay afición a este juego, participando activamente en las competiciones celebradas tanto a nivel insular como regional.

Las características geográficas de los fondos marinos de la isla, junto con la gran calidad de sus aguas, hacen de La Palma un lugar especial para la práctica del submarinismo. Los fondos volcánicos de la palma presentan barrancos y arcos de lava subterráneos. En algunas zonas, debido a la gran profundidad, se práctica la "apnea", logrando Audrey Mestre el récord de profundidad (125 metros) frente a las costas de Puerto Naos.

Además de los citados, en la isla se practican otros deportes de los que la siguiente es una pequeña relación:

La celebración más destacada de La Palma tiene lugar en las denominadas Fiestas Lustrales de la Bajada de la Virgen de las Nieves, patrona de la isla que, cada cinco años —los terminados en -0 o -5—, se desplaza, el segundo sábado de julio, desde el Real Santuario Insular hacia la capital de la isla hasta el día de su onomástica, el 5 de agosto. Durante estas celebraciones, aparte de la romería que acompaña a la patrona hasta Santa Cruz y viceversa, se hace representaciones de la conquista de la Isla, simulaciones de rituales Benahoaritas y la Danza de los Enanos, el acto más destacado de la fiesta, en la cual unos danzarines disfrazados de enanos con trajes decimonónicos desfilan por las calles de la capital practicando una danza característica. La otra representación importante es el baile del Minué, el que se imita una danza decimonónica.

El carnaval es otra de las fiestas que más se celebran. A pesar de contar con todos los elementos característicos de los carnavales canarios ("reina del carnaval", "comparsas", "murgas" etc.) el Carnaval palmero destaca por la celebración de los Indianos. Esta fiesta, que tiene lugar el lunes de carnaval, es una burla a los indianos, es decir, a los palmeros retornados de las Américas. Para la ocasión, todo el mundo se disfraza con trajes de encaje y guayaberas de blanco impoluto de la misma forma que regresaban los acaudalados emigrantes. Tras una representación en la que un barco de época llega al puerto lleno de indianos, con sus loros, sirvientas (conocidas como la "negra Tomasa") y demás elementos característicos, comienza una batalla campal de polvos de talco por las calles capitalinas al ritmo del son cubano.

También tiene gran tradición la celebración del Día de la Cruz el 3 de mayo en los pueblos de la comarca este de la isla, en la que se conmemora doblemente la fundación de la ciudad de Santa Cruz de la Palma, acaecida el 3 de mayo de 1493 y la festividad de la cruz, para lo cual se enraman y se visten con joyas numerosas cruces repartidas por cada pueblo y barrio, y es costumbre visitar las diferentes cruces en la noche del día anterior.

Además cada municipio de La Palma posee sus fiestas patronales, habiendo incluso fiestas independientes en algunos barrios, como es el caso de Argual, en el municipio de Los Llanos. 

El folclore palmero es similar al del resto de las Canarias, con la excepción del baile del sirinoque que es oriundo de la isla.

El Festivalito, nombre por el que es más conocido el Festival Internacional de Cine Chico de Canarias-Isla de La Palma, es un festival de cine digital que se celebra cada verano en La Palma, desde 2002. Fue el primer certamen internacional en incorporar los rodajes a su programación aprovechando las nuevas tecnologías.

El Festivalito, desde su primera edición, suma a los apartados habituales en los festivales cinematográficos tradicionales —secciones oficiales e informativas, retrospectivas, mesas redondas...— un concurso que retaba a los participantes a escribir, rodar y estrenar un cortometraje en el marco del festival aprovechando las virtudes de la tecnología digital y el espacio natural de la isla. Es la sección La Palma Rueda, por la que han pasado cineastas de los cinco continentes. Desde 2002, se han producido más de 120 cortometrajes con el sello de La Palma Rueda, además de dos largometrajes experimentales. Las obras deben inspirarse en un lema que se hace público en la gala inaugural, y se estrenan en la ceremonia de clausura del certamen

Los galardones del Festivalito, que se conceden tanto en los apartados de exhibición como en la sección La Palma Rueda, son las Estrellas del Festivalito, que cada año diseña un artista diferente inspirándose en el límpido firmamento de La Palma.

En los últimos años, la isla ha sido receptora de rodajes de algunas producciones de cine, documentales, capítulos de series, cortometrajes o planos concretos tanto en largometrajes u otros contenidos audiovisuales. Algunos de los rodajes más importantes realizados en la isla son:


La gastronomía palmera destaca por ser una de las más elaboradas del archipiélago, especialmente en lo que se refiere a sus postres, presentes en la mayoría de las islas, y a los mojos.

Se trata de un tipo de salsa tradicional de las islas Canarias, acompañamiento imprescindible de algunas comidas típicas del archipiélago. Debido a su contenido en pimienta, muchos de los tipos de mojos son picantes, entre ellos se encuentran el mojo de cilantro, de perejil, de pimentón, etc. No obstante el abanico de estas salsas es muy amplio y permite el uso de distintos ingredientes en su elaboración como almendras, queso, azafrán, pan frito, entre otras posibilidades. 

Tanto los pescados como las carnes suelen acompañarse con papas arrugadas. Es este un plato típico del conjunto de Canarias que responde simplemente a la forma de cocinar las papas. Con agua, mucha sal, y sin pelar. En 2016 fueron proclamadas maravilla gastronómica de España en un concurso promovido por Allianz Global Assistance, al conseguir el primer puesto mediante voto popular a través de Internet.






</doc>
<doc id="6474" url="https://es.wikipedia.org/wiki?curid=6474" title="Tenerife">
Tenerife

Tenerife es una isla del océano Atlántico perteneciente a la comunidad autónoma española de Canarias. Junto a La Palma, La Gomera y El Hierro conforma la provincia de Santa Cruz de Tenerife. Con una superficie de 2034,38 km² y una población de 917 841 habitantes (2019) es la isla más extensa del archipiélago canario y la más poblada de España. Además, Tenerife es también la isla más extensa y poblada de la región de la Macaronesia.

La ciudad de Santa Cruz de Tenerife es la capital de la isla y de la provincia homónima, así como su municipio más poblado, con 207 312 habitantes (INE 2019). La ciudad es, además, capital de la Comunidad Autónoma de Canarias, compartiendo ese estatus con Las Palmas de Gran Canaria. A pesar de esto, entre 1833 y 1927 Santa Cruz de Tenerife fue oficialmente la única capital del archipiélago canario, hasta que en 1927 un decreto ordenó que la capitalidad de Canarias fuera compartida, que es como permanece actualmente. El segundo municipio por número de habitantes de la isla, y tercero de Canarias, con 155 549 habitantes (INE 2018), es San Cristóbal de La Laguna, cuyo casco histórico es Patrimonio de la Humanidad. El área metropolitana de Santa Cruz de Tenerife tiene una población de más de 400 000 habitantes.

La isla posee otro lugar catalogado por la UNESCO como Patrimonio de la Humanidad, el parque nacional del Teide, el cual es el más visitado de España y uno de los más visitados del mundo. En él se encuentra la máxima elevación de España y tercer volcán más grande del mundo desde su base en el lecho oceánico, el Teide. Por su parte, el Macizo de Anaga, por su riqueza natural y etnográfica, fue catalogado como Reserva de la Biosfera, también por la UNESCO, el 9 de junio de 2015. Se trata del paraje natural que mayor cantidad de endemismos tiene de Europa.

De gran importancia es el Carnaval de Santa Cruz de Tenerife, declarado y considerado como uno de los de mayor relevancia a nivel mundial. Además, la isla posee una variada arquitectura, destacando entre ella la colonial y la contemporánea, cuyo máximo exponente es el moderno edificio del Auditorio de Tenerife, situado en Santa Cruz de Tenerife. La isla también es conocida por ser un gran destino turístico, ya que recibe a más de cinco millones de turistas cada año, siendo, por lo tanto, el principal destino turístico del archipiélago canario, así como uno de los más importantes de España y del mundo.

Son diversos los nombres que las distintas culturas han atribuido a Tenerife a lo largo de la historia. Así por ejemplo, para los nativos guanches la isla recibía el nombre de Achined, Achinet o Chenet, aunque en función de la bibliografía que se consulte, la nómina puede adquirir diferentes variaciones ortográficas. Según el historiador Ignacio Reyes la forma primitiva sería "(w)a-zenzen" con el valor de 'resonancia, zumbido, retumbo', mientras que Álvarez Delgado indica que "Achinech" −"at-ti-ney"− es «una expresión cariñosa o afectiva» que traduce como 'he aquí la mía' o 'la mía', 'mi tierra'.

Las descripciones romanas de las islas Afortunadas (especialmente Plinio el Viejo en su obra "Naturalis Historia") incluían una llamada Nivaria o Ninguaria (del latín "nix", "nivis", "nieve"), que se cree que hace referencia a las nieves posadas sobre el volcán tinerfeño conocido como el Teide.

Los mapas portulanos de los siglos XIV y XV suelen designar a la isla como Insula del'inferno ("Isla del Infierno"), probablemente debido a los procesos eruptivos de los que el volcán era protagonista.

Se cree que el nombre actual de la isla se debe al aspecto del Teide ya que fue dado por los benahoaritas (aborígenes de La Palma) según las palabras Tener ("blanca") e ife ("montaña"). La mención escrita más antigua conocida de Tenerife es en la forma Tenerefiz y data de alrededor de 1350, en una obra literaria titulada "Libro del Conoscimiento". Sin embargo, a lo largo de la historia se han dado otras explicaciones para desvelar el origen del nombre de la isla. Así por ejemplo, los historiadores dieciochescos Juan Núñez de la Peña y Tomás Arias Marín de Cubas, entre otros, supusieron que el nombre de la isla podría provenir del legendario mencey guanche Tinerfe apodado —"el Grande"—, quien gobernó toda la isla en tiempos anteriores a la conquista de Canarias por parte de Castilla.

Tenerife es una isla en gran parte muy abrupta, de relieve formado por sucesivas erupciones volcánicas a lo largo de la historia, la más reciente de las cuales fue la del Chinyero en 1909.

La isla está situada entre los paralelos 28º y 29º N y los meridianos 16º y 17º O, ligeramente al norte del trópico de Cáncer, ocupando una posición central entre Gran Canaria, La Gomera y La Palma. Se encuentra a algo más de 300 km del continente africano, y a unos 1000 km de la península ibérica.

De forma triangular, Tenerife es la mayor isla del archipiélago canario, con una superficie de 2034,38 kilómetros cuadrados y la que más longitud de costas tiene con 342 kilómetros. Además, es la isla más alta de España y la : en su centro se alza el Pico del Teide, que con sus 3718 msnm representa a su vez el punto más elevado de toda España, de las islas del océano Atlántico y el tercer volcán más grande del mundo desde su base en el lecho oceánico, solo superado en este sentido por el Mauna Kea y el Mauna Loa (ambos en el Archipiélago de Hawái). Tiene hasta 200 pequeños islotes o roques a su alrededor, entre los que destacan los de Anaga, Garachico, Fasnia que suman un total de 213 835 metros cuadrados más. Tenerife es también la isla más grande y poblada de la región Macaronesia.

Tenerife es una isla de origen volcánico, cuya formación comenzó a gestarse en el fondo oceánico hace unos 20-50 millones de años.

Según una de las teorías más aceptadas actualmente por la comunidad científica ("Teoría de los bloques levantados"), el ascenso de magma procedente del manto terrestre se produce en periodos de actividad tectónica a partir de fallas o fracturas que existen en el fondo oceánico. Estas siguen los ejes estructurales de la isla, y se conformaron durante la orogenia Alpina de la Era Terciaria por el movimiento de la placa Africana. Estas erupciones de tipo fisural submarino originan las denominadas lavas almohadilladas o pillow-lavas, que se producen por el rápido enfriamiento que experimenta el magma al establecer contacto con el agua, obteniendo así una forma muy característica. Estos materiales se fueron acumulando y construyendo el edificio insular bajo el mar. A medida que este se aproximaba a la superficie, los gases, debido a la disminución de la presión circundante, se iban liberando del magma y los episodios vulcanológicos pasaban de ser tranquilos a tener un carácter marcadamente explosivo, formando materiales fragmentarios.

Tras largo tiempo de acumulación de materiales, el nacimiento de la isla se produjo a finales del Mioceno (Era Terciaria). Hace siete millones de años emergieron las zonas de Teno, Anaga y Macizo de Adeje, en la que se denomina "Serie Basáltica Antigua" o "Serie I". Se constituyeron de este modo tres islas cronológica y estratigráficamente distintas en los extremos oeste, este y sur de la actual Tenerife.

Hace aproximadamente 3 m a. comienza un segundo ciclo volcánico ("Formaciones Postmiocenas" o "Series Recientes II, III y IV"), mucho más intenso, que incorpora elementos en la zona central de la isla, la cual también emerge y unifica en uno solo a los tres edificios anteriormente descritos. La estructura conformada en ese momento recibe el nombre de "Edificio pre-Cañadas", sobre cuyos restos se erigiría más tarde el "Edificio Cañadas I". Este ultimó experimentó diversos colapsos y emitió una gran variedad de materiales explosivos que dieron lugar a las llamadas "Bandas del sur" (sur-sureste actual).

Posteriormente, sobre las ruinas de este complejo surgiría el "Edificio Cañadas II", ya por encima de los 2500 metros, también con intensos procesos explosivos. Hace alrededor de 1 m a. se inició la construcción de la "Cordillera Dorsal", con un vulcanismo de tipo fisural, a partir de los restos de los edificios ya parcialmente desmantelados de la "Serie I". La "Cordillera Dorsal" es la de mayor desarrollo altitudinal y longitudinal del Archipiélago Canario, con 1600 metros de altura y 25 kilómetros de longitud. En este mismo espacio cronológico (hace 800 000 años) tienen lugar dos deslizamientos gravitacionales que motivaron la aparición de los valles de La Orotava y Güímar.

Finalmente, ya en tiempos más próximos (200 000 años), comienzan las erupciones que levantarían el "Edificio Pico Viejo-Teide" en el centro de la isla, sobre la "Caldera de Las Cañadas".

La abrupta orografía isleña y su variedad de climas dan como resultado un territorio de múltiples paisajes y formas, desde el parque nacional del Teide con su amalgama de colores fruto de las sucesivas erupciones volcánicas, hasta los Acantilados de Los Gigantes con sus paredes verticales, pasando por zonas semidesérticas con plantas resistentes a la sequedad en el sur, o por ambientes de carácter meramente volcánico como es el Malpaís de Güímar o el Malpaís de La Rasca.

También cuenta con playas naturales como la de El Médano (con parajes protegidos en su entorno como Montaña Roja y Montaña Pelada) valles con cultivos tropicales y subtropicales, boscosos parajes de laurisilva en los macizos de Anaga y Teno (con profundos y escarpados barrancos) y extensos bosques de pinos por encima de esta última formación vegetal.

Las principales estructuras de Tenerife, que a continuación se describen, conforman el edificio central, con el complejo Teide-Pico Viejo y el circo de Las Cañadas. Se trata de una semicaldera de 130 kilómetros cuadrados, que ha sido originada por un conjunto de procesos geológicos explicados en el epígrafe "Origen y formación". El circo está parcialmente ocupado por el estratovolcán Teide-Pico Viejo y completado por los materiales que ha emitido en sus diferentes erupciones. Destacan en su interior los Roques de García, entre los que está el más conocido, el Roque Cinchado. Otra formación llamativa son Los Azulejos, compuesto por fonolitas de colores verdosos que se han creado por actividad hidrotermal.

Al sur de La Caldera destaca la Montaña de Guajara, que con 2718 metros es la de mayor altitud de las que constituyen el anfiteatro de Las Cañadas del Teide. Al pie de estas paredes se han creado llanos endorreicos de materiales sedimentarios muy finos, siendo el más conocido el Llano de Ucanca.
El Pico del Teide, con 3718 metros sobre el nivel del mar y más de 7000 sobre el fondo oceánico, es el punto más elevado de la isla, del territorio español y de todas las tierras emergidas del Atlántico. Este volcán, el tercero más grande del planeta desde su base, es el símbolo de Tenerife por antonomasia y el monumento natural más emblemático del archipiélago canario. Su situación central, sus importantes dimensiones, su silueta y su paisaje nevado lo dotan de una singular personalidad. Ya los aborígenes guanches lo consideraban lugar de culto y adoración.

Desde 1954, el Teide y todo el circo de su alrededor (aunque hubo una ampliación posterior de sus límites) está declarado como parque nacional. Además, desde junio de 2007 está incluido por la Unesco dentro de los espacios Patrimonio de la Humanidad como bien natural. Al oeste se encuentra el volcán Pico Viejo. En un lateral de este, se encuentra el Volcán de Chahorra o Narices del Teide, donde se produjo la última erupción que se ha dado en el entorno del Teide, en 1798.

El macizo de Anaga, en el extremo nororiental de la isla, posee un perfil topográfico irregular y escabroso donde a pesar de no presentar grandes cotas, destaca la Cruz de Taborno con 1024 metros. Debido a la antigüedad de sus materiales (5,7 m a.), a sus profundos procesos erosivos y a la densa red de diques que atraviesan el macizo, son numerosos los roques que aparecen en superficie, tanto de etiología fonolítica como traquítica. Existe una gran cantidad de barrancos escarpados y muy encajados en el terreno. En la costa de Anaga predominan los acantilados, por lo que existe un número escaso de playas; no obstante, las que hay suelen coincidir con zonas de desembocadura de barrancos, algunas de rocas y otras de arena negra.

El macizo de Teno se encuentra en el extremo noroccidental. Al igual que en Anaga, se trata de una zona de estructuras desmanteladas y hondos barrancos que se han originado por erosión. Sin embargo, aquí los materiales son más antiguos (aproximadamente 7,4 m. a.). Destacan la Montaña de Gala que con 1342 metros representa la mayor altitud. El paisaje más singular de este Macizo se encuentra en su costa sur. Se trata de los Acantilados de Los Gigantes, con paredes verticales que llegan a alcanzar en algunos puntos los 500 metros de altura.

El macizo de Adeje se sitúa en el extremo meridional de la isla, teniendo como mayor exponente al Roque del Conde, con 1001 metros de altitud. El macizo no es tan apreciable por su reducida estructura inicial, hecho que añadido a la historia geológica del lugar ha potenciado un intenso desmantelamiento de sus materiales, perdiendo de ese modo su aspecto y envergadura original.
La Cordillera Dorsal o dorsal de Pedro Gil abarca desde el principio del monte de La Esperanza, a unos 750 metros de altitud aproximadamente, hasta la zona central de la isla, en las inmediaciones de la Caldera de Las Cañadas, siendo Izaña, su punto más alto, con 2350 metros sobre el nivel del mar. Esta estructura se ha constituido a expensas de un vulcanismo fisural de tipo basáltico a través de uno de los ejes o directrices estructurales que han dado origen al vulcanismo de la isla.

La dorsal de Abeque se encuentra formada por una cadena de volcanes que unen el macizo de Teno con el edificio central insular Teide-Pico Viejo a partir de otro de los tres ejes o directrices estructurales de Tenerife. A esta dorsal pertenece el volcán histórico de Chinyero cuya última erupción se registró en 1909.

La dorsal Sur o dorsal de Adeje está al amparo del último de los ejes estructurales. Destacan los restos de su macizo como formación primigenia, así como las alineaciones de pequeños conos volcánicos y de roques esparcidos por toda esta zona del sur tinerfeño.

La dorsal de Anaga divide naturalmente la región del macizo de Anaga de este a oeste. Separa los valles de San Andrés (al sur) y Taganana (al norte).

Los valles son otra de las formas de relieve más destacadas. Los más importantes son el Valle de La Orotava y el Valle de Güímar que se han generado por el deslizamiento en masa de grandes cantidades de materiales hacia el mar, creando una hondonada en el terreno. Existen otros valles que se distribuyen por diversos enclaves de la geografía de Tenerife, aunque, en este caso, de diferente naturaleza. Suelen ser valles intercolinares que se han conformado tras el depósito de mayor cantidad de materiales geológicos en lomas laterales, o simplemente cauces amplios de determinados barrancos que en su evolución han tomado el aspecto de típicos valles.

Tenerife, debido principalmente a su gran altitud y a su silueta en semejanza a un tejado de dos aguas, está surcada por gran cantidad de barrancos. Estos constituyen uno de los elementos más característicos de su paisaje, originados por la erosión ejercida por la escorrentía superficial a lo largo de la historia. Destacan los barrancos de Ruiz, Fasnia y Güímar, el barranco del Infierno y Erques, todos ellos declarados espacios naturales protegidos por las instituciones canarias.

Las costas son, por lo general, accidentadas y abruptas, aunque lo son más en la zona norte que en la sur. No obstante 67,14 kilómetros de la costa tinerfeña lo representan playas, solo superada en este aspecto por la isla de Fuerteventura.
En el litoral septentrional son frecuentes las playas de cantos rodados o de arena negra, mientras que en la vertiente sur y suroeste de la isla predominan las playas con arenas más finas y de tonalidades más claras.

Los tubos de lava, o tubos volcánicos, son cuevas volcánicas, usualmente con forma de túneles, formados en el interior de coladas lávicas más o menos fluidas mientras dura la actividad reogenética. Entre los muchos tubos volcánicos existentes en la isla destaca la llamada Cueva del Viento, situada en el municipio norteño de Icod de los Vinos, que es el tubo volcánico más grande de Europa y uno de los más grande del mundo, aunque durante mucho tiempo fue considerado incluso el más grande del mundo.

La isla de Tenerife disfruta de una notable diversidad ecológica pese a su reducida superficie, lo que es consecuencia de unas condiciones ambientales especiales, ya que la accidentada orografía reinante modifica localmente las condiciones climáticas generales, originando una gran variedad de microclimas. Esta vasta existencia de microclimas y, por lo tanto, de hábitat naturales, se hace manifiesta en la vegetación insular, constituida por una flora rica y variada (1400 especies de plantas superiores), entre las que destacan numerosos endemismos canarios (200) y exclusivamente tinerfeños (140).

Al concentrar este patrimonio vegetal de unas 140 especies exclusivas, la isla de Tenerife muestra la mayor relación de endemismos florísticos de la denominada Macaronesia. Además, la diferente composición química de los diversos materiales volcánicos que han construido el edificio insular, siempre bajo la acción combinada de los factores climáticos, da lugar a una gran diversidad de suelos. La conjunción de estos agentes determina la presencia de múltiples hábitats que albergan numerosas comunidades de plantas y animales que constituyen los singulares ecosistemas de Tenerife.

El estudio de la flora y la fauna tinerfeña puede realizarse de un modo más ordenado si es clasificada según los diferentes pisos ecológicos en los que se divide el terreno de la isla. Dicha división atiende especialmente a la orientación norte o sur de las vertientes de la isla y, por supuesto, a la altitud:



Aún faltaría hablar de la extensa fauna marina de entre la que destacan viejas, meros, abades, salemas, samas, pargos, etc. Gran interés tienen también la tortuga boba y las colonias permanentes de ballenas y delfines que habitan el litoral sur de la isla. Tenerife posee un inventario faunístico que asciende a 56 especies de aves, 13 de mamíferos terrestres, 5 de reptiles, varios miles de invertebrados, 2 de anfibios y 400 de peces además de algunas especies de tortugas marinas y cetáceos.

Antes de la llegada de los aborígenes, las Islas Canarias y en especial la isla de Tenerife, estaba habitada por animales endémicos prehistóricos extintos en la actualidad en su mayoría. Estos especímenes alcanzaban tamaños superiores a lo habitual, esto es lo que se llama gigantismo insular.

Entre estas especies, las más conocidas que se encontraban en Tenerife eran;




Prácticamente la mitad de la isla (48,6%), se encuentra bajo las diferentes fórmulas de protección que atribuyen la Red Canaria de Espacios Naturales Protegidos. De los 146 espacios naturales recogidos por la citada red en el conjunto del archipiélago, un total de 43 se encuentran en Tenerife, siendo de este modo la isla que mayor número de espacios posee. Asimismo, atendiendo al porcentaje de territorio protegido con el que cada isla contribuye al total del archipiélago, hay que destacar que es Tenerife con un 37% la isla que encabeza la tabla. La Red contempla hasta ocho categorías de protección distintas, todas ellas representadas en la isla: aparte del parque nacional del Teide, cuenta con el mayor parque natural de Canarias (Corona Forestal), dos parques rurales (Anaga y Teno), cuatro reservas naturales integrales, seis reservas naturales especiales, un total de catorce monumentos naturales, nueve paisajes protegidos y hasta seis sitios de interés científico. El 9 de junio de 2015 el macizo de Anaga fue declarado reserva de la biosfera por la Unesco. El macizo de Anaga es el lugar que mayor cantidad de endemismos tiene en Europa.

El municipio de La Orotava, en gran parte a expensas del parque nacional del Teide, y el de Santa Cruz de Tenerife que hace lo propio con el parque rural de Anaga presentan, respectivamente, el 76% y el 74% de su extensión bajo protección. De la misma forma, el emplazamiento mayoritario del parque rural de Teno al amparo del municipio de Buenavista del Norte hace que éste disponga de una importante parte de su superficie protegida.

La relación completa de los espacios protegidos de Tenerife es la siguiente:









A Tenerife se la conoce internacionalmente como la "Isla de la Eterna Primavera". La atribución de esta denominación climática se produce en gran medida gracias a los vientos alisios, cuya humedad, principalmente, se condensa en las zonas de medianías del norte y nordeste insular, constituyendo amplios mares de nubes que se disponen preferentemente entre los 600 y 1800 metros de altura.

Otro factor que influye en la suavidad del clima de las Islas con respecto al que por latitud correspondería (desierto del Sahara), es la corriente marina fría de Canarias, que enfría la temperatura de las aguas que bañan las costas y playas isleñas con respecto a la ambiental. Por último, la propia orografía tinerfeña también habría que tenerla en cuenta en esta terna de agentes encargados de hacer realidad el anteriormente citado eslogan.

A grandes rasgos, el clima de Tenerife es moderado, templado y muy suave en cualquier estación del año. No hay períodos de frío pero tampoco los hay de calor asfixiante. Las temperaturas medias son de 18 °C en invierno y 25 °C en verano, aunque estos sean valores relativos y generales. Evidentemente se producen importantes contrastes, como el que se produce durante los meses de invierno, en los cuales es posible disfrutar del sol en zonas de costa y, sin embargo, 3000 metros por encima poder contemplar la blanca estampa nevada del Teide, lugar en el que nieva todos los años.
Otro ejemplo de contraste climático lo encontraríamos en la ciudad de Santa Cruz con respecto a la ciudad de La Laguna. Municipios unidos físicamente pero distanciados en cuanto a condiciones climáticas. Generalmente Santa Cruz tiene durante todo el año un clima cálido con temperaturas sensiblemente superiores a las que se disfrutan en la aledaña La Laguna, donde frecuentemente hace un poco más de frío y existe mayor probabilidad de precipitaciones, y cuyo clima es similar a las medianías del norte de la península.

El norte y el sur de Tenerife poseen igualmente diferentes características climáticas. En barlovento se registra un 73% de las precipitaciones totales además, la humedad relativa del aire es superior y la insolación inferior. Los máximos pluviométricos se registran en barlovento a una altitud media entre 1000-1200 m, casi exclusivamente en los montes de La Orotava.

Pero quizás sea más significativo que todo el norte de la isla carezca de un espacio en el que la pluviosidad media sea inferior a los 250 mm anuales. En cambio, en la vertiente sur de la isla los valores pluviales son significativamente menores. Los únicos reductos sureños que se salvan de esta situación son Masca y Güímar, probablemente debido a sus características físicas que posibilitan una mayor presencia del alisio.

A modo de anécdota es interesante saber que los médicos europeos, sobre todo ingleses y holandeses, del pasado siglo XIX elogiaban el clima del norte de Tenerife, y lo recomendaban a sus pacientes para aliviar dolencias de la edad y del aparato circulatorio.

El suelo volcánico de Tenerife, generalmente de carácter poroso y permeable es motivo para que una considerable fracción del agua procedente de la lluvia, unida a aquella producto de condensaciones en zonas boscosas y a la proveniente del deshielo de las cumbres más elevadas de la isla, se infiltre en el subsuelo.

La construcción de embalses y presas como principales métodos de obtención de agua está desaconsejada debido a las mencionadas condiciones geológicas, que no permiten el almacenamiento del preciado líquido en superficie, así como a la irregularidad de las precipitaciones.

De este modo, la mayor parte del agua (90%) procede de pozos y principalmente de galerías, importantes sistemas que sirven para extraer el recurso hídrico del acuífero. Tenerife dispone en la actualidad de más de un millar de galerías perforadas.

La compleja formación y evolución geológica de Tenerife, en conjunción con las distintas características orográficas y bioclimáticas ofrecen como resultado una amplia gama de suelos atendiendo principalmente a su mayor o menor grado de evolución. En este sentido, se comportan como menos desarrollados, los suelos de las zonas expuestas al sur; sin embargo, las zonas al norte, afectadas por los vientos alisios, que aportan una gran humedad, muestran mayores niveles de evolución. Al ahondar en este capítulo resulta importante establecer una clasificación de los mismos en función de su capacidad de uso agrícola o no, así como sus limitaciones y los riesgos que implica.





Los guanches debieron llegar a Tenerife en un período comprendido desde antes del siglo V a.C. hasta el comienzo de la Era Cristiana. Durante casi dos mil años, poblaron la isla y trataron de adaptarse a sus particularidades medioambientales hasta que en 1496 fueron sometidos por las tropas españolas.

La zona arqueológica de la Cueva de los Guanches en el municipio de Icod de los Vinos, ha proporcionado las cronologías más antiguas de Canarias, con dataciones en torno al siglo a. C.

Respecto al nivel tecnológico, los guanches pueden ser encuadrados entre los pueblos de la Edad de Piedra, si bien esta terminología es rechazada debido a la ambigüedad que presenta. La cultura guanche se caracteriza por un desarrollo cultural avanzado, que posiblemente está en relación con los rasgos culturales bereberes importados desde el norte de África y un desarrollo tecnológico pobre, determinado por la escasez de materias primas, sobre todo de minerales que permitan la extracción de metales. Su actividad principal era el pastoreo, pero también se dedicaban a la agricultura, la recolección, la pesca, marisqueo de orilla o la artesanía.

En cuanto a las creencias, la religión guanche era politeísta aunque el culto astral estaba generalizado. Junto a él había una religiosidad animista que sacralizaba ciertos lugares, fundamentalmente roques y montañas. Entre los principales dioses guanches se podrían destacar; Achamán (dios del cielo y supremo creador), Chaxiraxi (diosa madre identificada más tarde con la Virgen de Candelaria), Magec (dios del sol) y Guayota (el demonio) entre otros muchos dioses y espíritus ancestrales. Especialmente singular era el culto a los muertos, practicándose la momificación de cadáveres. Además, en la isla han aparecido pequeñas figurillas líticas y de arcilla de tipo antropomorfo y zoomorfo asociadas a rituales, interpretados como ídolos. Destaca entre estos el llamado Ídolo de Guatimac, el cual se cree que representa a un genio o espíritu protector.

La sociedad guanche estaba dividida en estratos definidos por la riqueza, en cabezas de ganado especialmente, diferenciándose por un lado la nobleza y por otro el pueblo. La isla se dividía en territorios cuyo rey era el mencey (nombre dado al monarca de los guanches de Tenerife, que regía un menceyato o territorio). Unos cien años antes de la conquista, existía un mencey llamado Tinerfe el Grande, hijo del Mencey Sunta. Tinerfe tenía su corte en Adeje desde donde gobernaba toda la isla. A su muerte, sus nueve hijos se rebelaron y se repartieron la isla en nueve menceyatos y dos achimenceyatos independientes (llamados capitanías por los conquistadores). Los menceyatos y sus menceyes (por orden de descendencia) fueron los siguientes:

También se encontraba el Achimenceyato de Punta del Hidalgo gobernado por Aguahuco (el "Hidalgo pobre", hijo ilegítimo del Gran Tinerfe) y Zebenzui.

A pesar del tiempo transcurrido desde la conquista del Archipiélago aún hoy se mantienen muchos topónimos que nos recuerdan la lengua de los guanches. Son muchos los lugares que, aunque con importantes variaciones, conservan su denominación prehispánica.

La isla cuenta además con varias zonas arqueológicas de esta época anterior a la conquista. Por lo general, este patrimonio lo representan cuevas rupestres que si bien están repartidas por toda la geografía insular, la mayoría se encuentran en la vertiente meridional.

Dos de los más importantes yacimientos arqueológicos de la isla son: la zona arqueológica de la Cueva de los Guanches, donde se han encontrado los asentamientos más antiguos de Tenerife y ha proporcionado las cronologías más antiguas del archipiélago, con dataciones en torno al siglo a. C. y las llamadas Cuevas de Don Gaspar, por el hallazgo de restos vegetales en forma de semillas carbonizadas, la cual constata la práctica de la agricultura en la isla de Tenerife en tiempos de los guanches. Ambos yacimientos se encuentran en el municipio de Icod de los Vinos. Destaca también en la isla, la Estación solar de Masca la cuál fue un santuario aborigen destinado a las celebraciones de ritos relacionados con la fertilidad y la petición de agua de lluvia. Esta se encuentra en el municipio de Buenavista del Norte.

Otros yacimientos importantes a destacar son la "zona Arqueológica Los Cambados" y la "zona Arqueológica de El Barranco del Rey" ambas en el municipio de Arona. También podríamos destacar la Cueva de Achbinico (primer santuario mariano de Canarias, de época guanche-castellana) y lugar este donde han aparecido diversos utensilios arqueológicos de época guanche muy anteriores a la conquista. Otro lugar de gran interés arqueológico es el Macizo de Anaga, esta zona de la isla es uno de los lugares más ricos en hallazgos arqueológicos de Canarias. Se han hallado gran cantidad de momias guanches en este lugar así como cuevas con algunos restos de animales momificados, y piedras con inscripciones como la llamada "Piedra de Anaga". Al otro lado de la isla en el municipio de El Tanque se encontró otra piedra con inscripciones la "Piedra Zanata" que parece haber estado relacionada con el mundo mágico-religioso de los guanches. Además en la isla se encuentran las controvertidas Pirámides de Güímar, de las cuales hay muchas hipótesis sobre su construcción, aunque aún no se ha dado una definición oficial sobre su origen.

Existen además en Tenerife vestigios que revelan la presencia púnica en la isla, como sucede en la estela llamada comúnmente ""Piedra de los Guanches"" en la localidad de Taganana. Este yacimiento arqueológico se compone de una estructura formada por un bloque de piedra de grandes dimensiones, al aire libre, que presenta grabados rupestres en su superficie. Entre estos, destaca la presencia de una representación de la diosa cartaginesa Tanit, representada mediante un símbolo en forma de "botella" rodeada de motivos cruciformes. Se piensa que el monumento se trataba originalmente de un ara de sacrificio vinculada a las que se encuentran en el ámbito semita y posteriormente reutilizada para el ritual aborigen de la momificación.

Tenerife fue la última isla de Canarias en ser conquistada y la que más tiempo tardó en someterse a las tropas castellanas. Aunque las fechas tradicionales de conquista de Tenerife se establecen entre 1494 (desembarco de Alonso Fernández de Lugo) y 1496 (conquista de la isla), hay que tener en cuenta que los intentos de anexionar la isla de Tenerife a la Corona de Castilla se remontan al menos a 1464. Desde el primer intento de conquistar la isla en 1464, hasta que se conquista definitivamente en 1496 transcurren treinta y dos años.

Ese año de 1464, tiene lugar en el barranco del Bufadero la toma de posesión simbólica de la isla por el señor de las Canarias Diego García de Herrera. Este firma un tratado de paz con los menceyes, permitiéndole poco después el mencey de Anaga construir una torre en sus tierras, donde guanches y europeos tienen tratos hasta que es demolida hacia 1472 por los mismos guanches.

En 1492 el gobernador de Gran Canaria Francisco Maldonado organiza una razia que termina en desastre para los europeos, pues son derrotados por los guanches de Anaga.

En diciembre de 1493, Alonso Fernández de Lugo obtuvo de los Reyes Católicos la confirmación de sus derechos de conquista sobre la isla de Tenerife. En abril de 1494, y procedente de Gran Canaria, desembarcó el conquistador en la costa de la actual Santa Cruz de Tenerife con una tropa de peninsulares y canarios (gomeros y grancanarios, sobre todo) formada por unos dos mil hombres de a pie y 200 a caballo. Tras levantar un fortín se dispuso a adentrarse hacia el interior de la isla.

Los menceyes de la isla de Tenerife tomaron distintas posturas en el momento de la conquista. Se constituyeron así el "bando de paz" y el "bando de guerra", integrado el primero por los menceyatos de, Güímar, Abona y Adeje, y el segundo por Anaga, Tegueste, Tacoronte, Taoro, Icoden y Daute. El bando opositor se enfrentó tenazmente a los castellanos de modo que la conquista tinerfeña se prolongó durante dos años. Las tropas castellanas sufrieron una derrota a manos de los guanches en la Primera Batalla de Acentejo en 1494. Sin embargo, los guanches, superados por la tecnología y por las nuevas enfermedades a las cuales no eran inmunes, cayeron frente a las tropas de la Corona de Castilla en la batalla de Aguere y en la segunda batalla de Acentejo culminando la conquista en septiembre de 1496.

Como en el resto de las islas, muchos de los aborígenes fueron esclavizados, especialmente los pertenecientes al "bando de guerra", mientras que buena parte de la población indígena sucumbió a enfermedades importadas como la gripe y, probablemente, la viruela, enfermedades infecciosas para las que aquella sociedad neolítica, debido a su aislamiento, no había desarrollado su sistema inmune. Tras la conquista, y especialmente durante el siglo posterior a ella, se fue produciendo una repoblación y colonización paulatina de la isla con la llegada de inmigrantes provenientes de diversos territorios pertenecientes al incipiente Imperio Español (Portugal, Flandes, Italia, Alemania).

Los bosques de Tenerife se vieron gradualmente afectados por el crecimiento poblacional y por la consecuente necesidad obtener terrenos despejados que permitieran la explotación agrícola para consumo propio y para la exportación. Así fue el caso de la introducción del cultivo de la caña de azúcar a principios del siglo XVI mientras que, en siglos sucesivos, la economía de la isla se centró en el aprovechamiento de otros cultivos tales como la vid y la cochinilla para fabricar tintes, así como el plátano.

Durante el siglo XVIII tras la Guerra de Sucesión española se desarrolla en Canarias el llamado ""comercio canario-americano"", el cual estaba regulado por una Real Cédula de 1697. Esta concedía el comercio canario con América por espacio de ocho años y lo limitaba a 1000 toneladas. A Tenerife le correspondían 600, a La Palma 300 y 100 a Gran Canaria.

Tenerife era en este sentido la isla hegemónica, pues superaba el 50% del número de navíos y el 60% del tonelaje. En las islas de La Palma y Gran Canaria el porcentaje giraba en torno al 19% para la primera y el 7% en la segunda. Se desconoce el volumen del tráfico entre las Indias y Canarias, pero debió de ser muy importante y estaba concentrado casi exclusivamente en Tenerife.

Entre los productos que se exportaban destacaban: la cochinilla, el ron, y la caña de azúcar, los cuales eran desembarcados principalmente en los puertos americanos de La Guaira, La Habana, Campeche y Veracruz. La importancia de Tenerife supuso el impulso decisivo del puerto de Santa Cruz de Tenerife, que se convirtió en el único puerto comercial de la isla, superando en importancia a los de Garachico y Puerto de la Cruz en el comercio indiano.

Muchos navegantes tinerfeños se sumaron a este comercio marítimo transcontinental, entre los que destacan el corsario Amaro Rodríguez Felipe, más comúnmente conocido como "Amaro Pargo". Su participación en la carrera de Indias comenzó en el bienio 1703-1705. Fue capitán de la fragata "El Ave María y las Ánimas", navío con el que navegó desde el puerto de Santa Cruz de Tenerife hasta el de La Habana. Entre otros importantes marinos tinerfeños de la época destacan también, Juan Pedro Dujardín y Bernardo de Espinosa, ambos compañeros de Amaro Pargo.

Con la aplicación general del libre comercio en 1778 y la Guerra de Independencia de los Estados Unidos, supuso la paralización total del tráfico, hasta la paz de 1783. A partir de aquí esta ruta comercial comenzó a decaer. A finales del siglo XVIII y principios del siglo XIX, el comercio canario-americano tomó su fin debido a una etapa de gran conflictividad bélica entre países que paralizó por varios años el comercio.

Tenerife fue atacada, como las otras islas, por corsarios de varias nacionalidades (franceses, ingleses, holandeses y berberiscos) varias veces a lo largo de su historia, según el devenir de las alianzas y guerras de España. De entre estos ataques destaca por su lugar en la historia el ataque de los británicos de 1797.

El 25 de julio, el almirante Horatio Nelson atacó Santa Cruz de Tenerife, capital de la isla y Jefatura de la Capitanía General. Tras un feroz ataque, la defensa organizada por el general Gutiérrez repelió a los británicos. Nelson perdió su brazo derecho por una bala de cañón (dice la leyenda que del "cañón Tigre") mientras intentaba desembarcar en la orilla de la costa de la zona de Paso Alto.

El 5 de septiembre, otro intento de desembarco en la región de Puerto Santiago fue repelido por los habitantes del Valle de Santiago del Teide, que lanzaron piedras a los británicos desde lo alto de los acantilados de Los Gigantes.

Otros corsarios, principalmente ingleses, también atacaron la isla de Tenerife con mayor o menor suerte, Robert Blake (1656), Walter Raleigh, John Hawkins, John Genings (1706), Woodes Rogers, entre otros.

Tenerife, del mismo modo que otras islas, ha guardado una estrecha relación con América. Desde los inicios del proceso de colonización del nuevo mundo, fueron varias las expediciones que antes de surcar el Atlántico hicieron escala en la isla y sumaron al pasaje numerosos tinerfeños que formaron parte integrante de las expediciones de conquista o que simplemente partieron en busca de mejores garantías de futuro rumbo al continente americano. A su vez, independientemente del tránsito humano fue importante el intercambio de especies animales y vegetales que se estableció entre las dos tierras.

Tras un siglo y medio de relativo crecimiento alrededor del año 1670 el complicado comercio exterior del sector vitivinícola propició la emigración de muchas familias especialmente hacia Venezuela y Cuba. Además por esas fechas surgió el interés por parte de la Corona de poblar aquellas zonas vacías de América a fin de evitar su ocupación por otras potencias, como había ocurrido en el caso de los ingleses con Jamaica o los franceses con las Guayanas o el oeste de La Española, de manera que también importantes remesas de canarios y entre ellos tinerfeños partieron hacia el nuevo destino colombino. La creciente agricultura cacaotera en Venezuela y tabaquera en Cuba, de finales del siglo XVII y principios del XVIII, contribuyó a la despoblación casi íntegra de localidades como Buenavista del Norte, Vilaflor o El Sauzal. Testimonio de la historia emigrante de la isla fue la fundación en las afueras de Santo Domingo del poblado de San Carlos de Tenerife en 1684. Este poblado fundado esencialmente por tinerfeños se creó con un claro objetivo estratégico ya que permitía preservar la ciudad del asedio de los franceses establecidos en la parte occidental de la isla de La Española. Entre 1720 y 1730 fueron trasladadas por la Corona 176 familias canarias, entre ellas numerosas tinerfeñas a la isla caribeña de Puerto Rico. En 1726, en torno a 25 familias isleñas emigraron a América para terminar fundando la ciudad de Montevideo. Cuatro años más tarde, en 1730, partió otro grupo que, al año siguiente, fundó la ciudad de San Antonio de Texas, en Estados Unidos. Luego, entre 1777 y 1783, el puerto de Santa Cruz de Tenerife despidió a los fundadores de San Bernardo, en el estado de Luisiana, y también a algunas remesas con rumbo a Florida.

Debido a los problemas económicos derivados de la escasez de materias primas y de la lejanía con respecto a Europa, la emigración al continente americano, eminentemente a Cuba y Venezuela, continuó en los siglos XIX y principios del XX. Desde hace décadas, con las nuevas políticas de protección de la economía canaria y con el auge de la industria turística la dinámica migratoria se ha invertido, y hoy es Tenerife la que atiende el retorno de estos isleños, sus descendientes y otros inmigrantes perdurando así el influjo que germinó cinco siglos atrás.

Las erupciones volcánicas acontecidas en Tenerife de las que se tiene indudable constancia histórica se limitan a cinco. La primera de ellas fue en 1492 en el volcán Boca Cangrejo que fue observada por Cristóbal Colón. La siguiente ocurrió en el año 1704, cuando entraron en erupción, de forma sincrónica, los volcanes de Arafo, Fasnia y Siete Fuentes. Dos años más tarde, en 1706, tuvo lugar la erupción de mayor magnitud de las históricas al entrar en erupción el volcán de Trevejo. Este arrojó grandes cantidades de lava que sepultaron la ciudad y puerto de Garachico, en aquel entonces el más importante de la isla. La última erupción volcánica del siglo XVIII se produjo en 1798 en las Cañadas de Teide, concretamente en Chahorra. Finalmente, en 1909 la actividad eruptiva irrumpió en el volcán de Chinyero, en el municipio de Santiago del Teide. Posteriormente a esa fecha y hasta la actualidad no se han producido nuevas erupciones en la isla. Además, a pesar de la naturaleza absolutamente volcánica de Tenerife, los cinco episodios eruptivos históricos no han ocasionado víctima mortal alguna.

Otros visitantes menos hostiles llegarían a la isla en siglos sucesivos. En 1799 el naturalista Alexander von Humboldt ascendió el pico del Teide y comentó la belleza de la isla.

Numerosos turistas comenzaron a visitar Tenerife a partir de la década de 1890, especialmente las ciudades norteñas de Puerto de la Cruz (primer municipio turístico de Canarias mediante orden ministerial del 13 de octubre de 1955 que lo declaró ’Lugar de Interés Turístico’) y Santa Cruz de Tenerife.

En marzo de 1936, el general Francisco Franco fue destinado a Tenerife por el gobierno republicano, temeroso de su influencia militar y política, con el fin de alejarlo de los centros de poder.

La colisión entre dos aviones ocurrida el 27 de marzo de 1977 en el aeropuerto de Tenerife Norte, al norte de la isla, sigue siendo el accidente con mayor número de muertos de la historia de la aviación. Los aviones implicados en la tragedia tenían como destino Gran Canaria, pero habían sido desviados a Tenerife debido a la explosión de una bomba (colocada por el grupo terrorista separatista MPAIAC) en el aeropuerto grancanario.

Comenzando el siglo XXI tiene lugar la riada de Tenerife de 2002, el 31 de marzo de ese año. Se trató de un fenómeno de gota fría caracterizado por la caída reiterada de lluvias torrenciales acompañadas de aparato eléctrico, afectando al área metropolitana de Santa Cruz de Tenerife y extendiéndose en dirección NE hacia la zona de San Andrés. Las lluvias ocasionaron 8 muertos, 12 desaparecidos y decenas de heridos. Además de las pérdidas humanas la riada causó cuantiosos daños materiales, 70 000 personas sin luz así como la destrucción total o parcial de al menos 400 viviendas. Las pérdidas se calcularon en 90 millones de euros.

Por otro lado en noviembre de 2005, Tenerife fue la isla canaria más afectada por la tormenta tropical Delta. Se llegaron a registrar vientos de 140 km/h en la costa y casi los 250 km/h en el Teide, cumbre de Tenerife.

El gentilicio formal es "tinerfeño/a", aunque también de manera coloquial se utiliza la denominación "chicharrero/a". Sin embargo, este último se reserva en la propia isla para los habitantes de la capital, Santa Cruz.

El gentilicio "chicharrero" procede de un término despectivo empleado por los habitantes de la cercana ciudad de La Laguna, entonces capital de la isla, para los habitantes del entonces pobre y pequeño puerto de pescadores. Justamente por dicha pobreza, los habitantes de Santa Cruz debían conformarse con comer chicharros, un pescado pequeño y barato de relativa baja calidad; de donde procede el término. Con el tiempo y el crecimiento de Santa Cruz, hasta conseguir el traslado de la capitalidad desde La Laguna, bajo el reinado de Fernando VII (siglo XIX), sus ciudadanos tomaron el insulto a honra y asumieron como propio el gentilicio.

La bandera de Tenerife fue adoptada originariamente en 1845 a modo de distintivo o bandera de matrícula de la que en aquel entonces se denominaba provincia marítima de Canarias con base en el Puerto de Santa Cruz de Tenerife. En la actualidad, esta enseña representa a toda la isla de Tenerife. Fue aprobada a instancia del Cabildo Insular por Orden del Gobierno de Canarias el 9 de mayo de 1989 y publicada el 22 de mayo de 1989 en el "Boletín Oficial de Canarias".

El escudo heráldico de Tenerife fue otorgado mediante diploma real el 23 de marzo de 1510, concedido por el rey Fernando V "El Católico", fue expedido en Madrid a nombre de su hija Juana, reina de Castilla. El escudo se describe en campo de oro, con San Miguel Arcángel (patrono de la isla) armado superando a una montaña de su color natural de la que brotan llamas, y que representa al pico del Teide. Bajo esta montaña la isla de sinople sobre ondas azul y plata. A la derecha se observa un castillo de gules, y a la izquierda un león rampante de gules. El escudo que usa el Cabildo Insular se diferencia del que usa el Ayuntamiento de La Laguna en el lema que aparece en la bordura y en el añadido de unas ramas de palma.

Según una ley del Gobierno de Canarias los símbolos naturales de la isla desde 1991 son el pinzón azul y el drago.

El órgano de gobierno de la isla de Tenerife es el Cabildo Insular de Tenerife con sede en la plaza de España de la capital tinerfeña (Palacio Insular de Tenerife). La organización política de Canarias se caracteriza porque no posee órgano político provincial sino que cada isla posee un cabildo insular propio. En Tenerife, su presidente en la actualidad es Pedro Manuel Martín Domínguez (PSOE). Desde que se constituyó en marzo de 1913 dispone de una amplia serie de competencias propias, hoy recogidas en el Estatuto de Autonomía de Canarias y reguladas por la Ley 14/1990, de 26 de julio, de Régimen Jurídico de las Administraciones Públicas de Canarias.

El Cabildo se compone de los siguientes órganos:


La isla de Tenerife está dividida en 31 municipios, por lo que es la isla canaria que más municipios posee.

De todos ellos, sólo tres no tienen costa: Tegueste, El Tanque y Vilaflor. La Orotava es el municipio de mayor altitud de España al albergar el pico del Teide, mientras que Vilaflor o Chasna tiene la capital municipal más alta de toda Canarias al estar a 1400 metros de altitud.

El municipio más extenso con 207,31 km² es el de La Orotava, que abarca gran parte del parque nacional del Teide. El municipio más pequeño de la isla y del archipiélago es Puerto de la Cruz, con una superficie inferior a los 9 km².

La gran mayoría de estos municipios confluyen en la zona de cumbre central de la isla y a partir de ahí se extienden hacia la costa, orientándose unos hacia el norte y otros hacia el sur.

A su vez, es frecuente encontrar otro tipo de división insular, es aquella que establece el territorio según una Zona Metropolitana, alrededor del área de influencia de las ciudades de Santa Cruz y La Laguna ("véase" Área metropolitana de Tenerife), Zona Norte (aquellos municipios que se abren al océano por el norte) y Zona Sur (aquellos que lo hacen hacia el sur). Esta división junto con la municipal se puede observar en el mapa de la derecha.

A continuación se muestra la relación de todos los municipios tinerfeños ordenados alfabéticamente:

Además, todos estos municipios se han agrupado tradicionalmente en 8 comarcas. Sin embargo, en 2011 el Cabildo de la isla, a través del Plan Insular de Ordenación de Tenerife, define hasta 11 modelos comarcales, cuyas líneas básicas coinciden en gran medida con las tradicionalmente consideradas en los trabajos que se han realizado sobre la isla desde diversos enfoques. De esta manera, la isla queda dividida en las once comarcas siguientes:


Tenerife es la isla más poblada de Canarias y de España, fruto del crecimiento de la natalidad y de la gran inmigración extranjera que ha recibido la isla en la última década (200 mil inmigrantes residen en Tenerife). La isla albergaba a fecha de 1 de enero de 2019 y según fuentes del INE un total de 917 841 habitantes empadronados.

Se sabe que tras la conquista de Canarias, Tenerife se convirtió rápidamente en la isla más poblada del archipiélago canario, distinción que mantuvo hasta bien entrado el siglo XX. Desde el Censo de Aranda de 1768, Tenerife y Gran Canaria fueron siempre las dos islas más pobladas del Archipiélago, ocupando Tenerife el primer lugar hasta 1940. En el último siglo, Gran Canaria ha sido la más poblada de Canarias, hasta 2002 cuando Tenerife la supera de nuevo.

Actualmente las otras grandes islas españolas que siguen a Tenerife en población son Mallorca con 880 113 habitantes y Gran Canaria con 846 717 habitantes en 2018. Alrededor de un 25% de la población total de la isla de Tenerife (222 643 habitantes) lo están en su municipio capital, Santa Cruz de Tenerife, y cerca del 45% (403 013 personas) en su área metropolitana. A diferencia de otras islas de Canarias, Tenerife tiene la población muy dispersa por diferentes ciudades y municipios. Las ciudades de Santa Cruz de Tenerife y San Cristóbal de La Laguna se encuentran urbanísticamente unidas mediante los núcleos poblacionales de La Cuesta y Taco, albergando juntas más de 359 000 habitantes, conformando el Área metropolitana de Santa Cruz de Tenerife, la segunda más poblada de Canarias tras la de Las Palmas de Gran Canaria, siendo la de Santa Cruz de Tenerife además, la decimocuarta en población del país.

La isla presenta unos altos índices de población residente no registrada en los censos poblacionales. Esto ha hecho que diversas fuentes señalen que más de un millón de habitantes de hecho viven en la actualidad en la isla de Tenerife. La isla es además, la más multicultural del archipiélago al concentrar el mayor número de extranjeros empadronados -44,9% de los foráneos inscritos en Canarias-, los cuáles representan el 14% de la población total de la isla. Tenerife destaca también en el contexto del archipiélago, al concentrar además la mayor presencia de población extranjera no comunitaria -procedente del exterior de la Unión Europea-.

Tenerife presenta tres grandes áreas poblaciones bien diferencias y repartidas: La Zona Metropolitana, la Zona Sur y la Zona Norte. Con varios parques naturales protegidos –el 48,6% del territorio- y un enjambre urbanístico alrededor de la isla, en el último medio siglo la plataforma costera insular a pasado a convertirse en un sistema metropolitano altamente urbanizado. El alto nivel de población en un territorio relativamente pequeño -más de 900 000 habitantes en poco más de 2000 km²- y la fuerte urbanización han convertido a la isla de Tenerife, en palabras del arquitecto Federico García Barba; en una «isla-ciudad» o «isla-anular».

- Municipios con más de 25 000 habitantes:

- El municipio de Vilaflor es el que cuenta con menor población de toda la isla (1645).

En los últimos años Tenerife ha experimentado un notable crecimiento de la población muy por encima de la media estatal. En el año 1990 un total de 663 306 habitantes estaban censados en la isla, cifra que aumentó hasta los 709 365 habitantes en el año 2000. Esos datos reflejan un incremento en 46 059 personas o lo que es lo mismo, un crecimiento del 0,69% anual en el decenio 1990-2000. Sin embargo, en el intervalo comprendido entre el año 2000 y 2007, la tasa de crecimiento se multiplicó por 4 o por 5 hasta llegar al 3,14% anual. La población ha aumentado en este intervalo de tiempo en un total de 155 705 personas hasta alcanzar la cifra de 865 070 habitantes.

Esos resultados reafirman la dinámica actual de poblaciones en España, donde desde finales del siglo pasado el importante número de inmigrantes llegados ha permitido invertir el panorama que, el hundimiento de la tasa de fertilidad, había dibujado desde 1976. Desde 2001 la tasa de crecimiento en España se ha situado en torno al 1,7% anual contrastando con el 3,14% que ha experimentado la isla de Tenerife, uno de los territorios del país que mayor incremento ha sufrido en tal periodo.

Actualmente, Tenerife es la isla que tiene el PIB más alto de Canarias. El presupuesto del Cabildo de Tenerife para 2008 ascendió a 906 millones de euros. A pesar de que la economía tinerfeña está altamente especializada en el sector servicios, que integra un 78,08% de su capacidad de producción total, la importancia del resto de sectores es clave para un desarrollo armónico de su tejido productivo. En este sentido, el sector primario, que solamente representa el 1,98% del producto total, aglutina actividades de especial sensibilidad y para el desarrollo sostenible del territorio insular. El sector energético que contribuye con un 2,85% ejerce un papel primordial en la implantación de energías renovables. El sector industrial que participa en un 5,80% se configura como una actividad de interés creciente para la isla, a la vista de las nuevas posibilidades que generan los avances tecnológicos. Finalmente, el sector de la construcción con un 11,29% del producto total tiene un carácter estratégico prioritario, por cuanto es un sector con relativa estabilidad que permite múltiples posibilidades de desarrollo. La isla fue sede de la primera entidad financiera del Archipiélago Canario, la Caja General de Ahorros de Canarias (CajaCanarias), que contaba con más de 1600 empleados directos y una red de más de dos centenares de oficinas en todas las islas. En 2012 esta caja de ahorros fue absorbida por CaixaBank. Cuenta también con la sede de la cooperativa de crédito Cajasiete, actualmente única entidad financiera netamente canaria. También hay que desatacar que en la isla hay oficina de todos los bancos nacionales y de muchos internacionales debido a su economía fuertemente internacionalizada.

Tenerife se ha convertido en los últimos años en sede de grandes empresas internacionales como , , Traveltool, Globalia, Expedia, Japan Tobacco International, Be Live Hotels, Schreiber o Klingele, entre otras muchas. 

Es un importante polo de atracción de empresas de animación internacionales, y es que en la isla han abierto sede empresas importantes del sector como: Tomavisión, El Ranchito, o 3 Doubles Producciones, por nombrar algunas. 

También es destacable las grandes empresas con sede central en la isla, como Hospiten, Disa, Binter (Sede dividida entre Telde y La Laguna), Encuentro Moda, Cofarte, Jesumán, Grupo Loro Parque, Tubillete, Grupo Fedola, Grupo CIO, Grupo Número Uno, Grupo Montesano, Fred Olsen, Treexor o Coplaca, por nombrar a algunas.

La isla se ha convertido en la última década en un importante nodo de comunicaciones a nivel mundial gracias al D-ALIX, que cuenta también con el superordenador TEIDE-HPC gestionado por la multinacional francesa Atos y que es el segundo más potente de España. Ambos dentro del Instituto Tecnológico y de Energías Renovables.

Como se indicaba en el párrafo anterior, la economía de Tenerife, al igual que la de otras islas de Canarias, se basa fundamentalmente en el turismo (60% directo del PIB). Ya en el siglo XIX y gran parte del XX destacaba la afluencia de turismo extranjero —sobre todo del inglés— debido a los intereses agrarios que poseía en esta isla. Tenerife atrae gran cantidad de turismo nacional e internacional (es de hecho la isla canaria más visitada turísticamente), los turistas llegan a la isla en busca de sus playas, su variada oferta cultural y su animada vida nocturna.

Más tarde con las guerras mundiales este sector se resiente, pero entrada la segunda mitad del pasado siglo comienza a evolucionar de un modo muy notable. En un principio destaca el Puerto de la Cruz por su bondadoso clima y por todos los atractivos que el valle norteño de La Orotava concentraba, pero persiguiendo captar el turismo de sol y playa, alrededor de 1980 surge el boom turístico del sur de Tenerife, donde destacan ciudades como Arona o Adeje, en torno a núcleos turísticos como Los Cristianos, Playa de Las Américas y Costa Adeje, que hoy albergan más del 65% de las plazas hoteleras de toda la isla. Tenerife recibe cada año más de 5 millones de turistas, siendo de este modo, de entre todo el archipiélago canario, la isla preferida a este respecto. Sin embargo, este dato también pone de manifiesto la gran cantidad de recursos que esta actividad consume (espacio, energía, agua, etc.).

En la actualidad, el municipio de Adeje en el sur de la isla cuenta con la mayor concentración de hoteles 5 estrellas de Europa y además tiene el que es considerado el mejor hotel de lujo de España según "World Travel Awards".

Gran importancia turística tiene también el parque nacional del Teide pues es de hecho el segundo parque nacional más visitado del mundo.

Otro atractivo de la isla (con certificación internacional Starlight) que la sitúa entre los destinos más atractivos del planeta es la belleza de su cielo estrellado. Precisamente en el parque nacional del Teide se encuentra el telescopio Gregor, el más grande de Europa, con el que se puede disfrutar de algunas de las imágenes más espectaculares de las estrellas.

El turismo de negocios y reuniones, como actividad económica, tiene sus orígenes en los últimos años del siglo XIX y en los primeros del siglo XX. No obstante, no fue hasta bien entrada la segunda mitad de ese siglo cuando la denominada “industria de reuniones” se definió y reconoció como tal. Desde entonces las actividades relacionadas con la realización de eventos, de reuniones y congresos, se han ido posicionando en muchos países como uno de los sectores turísticos en alza.

Según la Asociación Internacional de Congresos y Convenciones (ICCA), España es el tercer país del mundo en este tipo de turismo. Actualmente, el turismo de negocio es un valor imprescindible para el desarrollo de la industria turística en general. El 10% de los más de 56 millones de turistas internacionales que llegan anualmente a España lo hacen por motivos de negocio. Estos movimientos generan más de 3000 millones de euros al año. Es conveniente tener en cuenta que el gasto medio por turista de negocios y día, es de cinco veces mayor que el de un turista vacacional. Sin duda, el elevado poder adquisitivo de este tipo de visitantes es una de las claves de la importancia de este segmento del turismo.

Tenerife cuenta con una infraestructura capaz de albergar grandes y pequeños eventos de prestigio. Dispone de cinco centros de congresos multifuncionales, localizados en diferentes zonas de la Isla. Se trata de edificios emblemáticos, con carácter propio, y con capacidad para reunir hasta 4000 delegados.

Para ofrecer información y asesoramiento sobre la organización de congresos y convenciones en Tenerife, Turismo de Tenerife cuenta con un departamento especializado, el Tenerife convention bureau.





A pesar de la intensa participación del turismo en el PIB tinerfeño, y en consecuencia el sector servicios, el sector primario, la industria y el comercio son responsables del 40% restante. En concreto el sector primario ha perdido su tradicional importancia en la renta insular en beneficio de la industria y los servicios.

La contribución del sector agrario en el PIB no llega al 10%, si bien su aportación a la isla es vital por cuanto genera beneficios difícilmente mensurables, que se relacionan con el sostenimiento de la estampa rural y el mantenimiento de valores culturales del tinerfeño. El sector agrario se desarrolla en la vertiente septentrional, lugar en el que los cultivos se distribuyen con base en la altitud: en la zona costera se cultivan principalmente tomates y plátanos, productos ambos de elevada rentabilidad dado que se exportan a la Península y al resto de Europa; en la zona intermedia proliferan los cultivos de secano, sobre todo papa, tabaco y maíz; en la zona meridional tiene relevancia el cultivo de la cebolla.

Particularmente, el cultivo del plátano figura en primer lugar en cuanto a producción se refiere, siendo Tenerife la isla que más plátanos manufactura en Canarias. La producción anual de la isla se ha consolidado en torno a unas 150 000 toneladas en estos últimos años, tras haber alcanzado un máximo de 200 000 toneladas en 1986. Algo más del 90% del total se destina al mercado nacional, ocupando este cultivo una superficie de 4200 hectáreas. Detrás del plátano destacan los cultivos de tomates, vides, papas y flores. La pesca supone también gran parte de la economía tinerfeña (Canarias es la segunda región pesquera de España).

El comercio posee un destacado peso en la economía tinerfeña, pues representa casi el 20% del PIB, cuyo mayor baluarte lo supone el Puerto de Santa Cruz de Tenerife. Ya finalmente, y a pesar de los diversos polígonos industriales que existen en el territorio insular, la actividad industrial (10 % del PIB) de mayor importancia es la refinería de petróleos de Santa Cruz de Tenerife, la cual suministra productos petrolíferos no solo al archipiélago canario sino también al mercado peninsular, africano y americano. La Refinería de Santa Cruz de Tenerife es la industria más grande de Canarias. Históricamente, esta refinería ha garantizado el suministro energético del Archipiélago, ha contribuido de manera importante a la actividad de los puertos canarios, como punto idóneo de repostaje para el tráfico marítimo del Atlántico.

En los siglos XVI y XVII destaca, en el campo de la poesía épica, Antonio de Viana. Este escritor que nace en La Laguna compuso el poema "Antigüedades de las Islas Afortunadas", un material de gran valor antropológico para entender las formas de vida de aquel entonces.

Ya posteriormente, en el llamado Siglo de las Luces (siglo XVIII) aparecen figuras relevantes de la Ilustración en Tenerife como José de Viera y Clavijo y Tomás de Iriarte.

En el siglo XIX hay que situar a Aurelio Pérez Zamora y en la poesía a Ángel Guimerá y José Plácido Sansón.

En el siglo XX pueden señalarse, entre otros, los siguientes autores: Agustín Espinosa, Domingo López Torres, Emeterio Gutiérrez Albelo, Mercedes Pinto, Isaac de Vega, Rafael Arozarena, Antonio Bermejo, Luis Feria, Félix Francisco Casanova, Manuel Padorno, Alfonso García-Ramos, Alberto Omar Walls, Luis Alemany, Juan Manuel García Ramos, Juan Cruz Ruiz, Sabas Martín... Por otra parte, como ensayistas hay que destacar a Domingo Pérez Minik, María Rosa Alonso, Juan Manuel Trujillo... Pero es necesario reflejar, que fue la poestisa cubana Dulce María Loynaz la primera en escribir un libro dedicado a esta isla a la cual se sentía unidad por estrechos lazos al ser su esposo natural de ella, titulado "Un verano en Tenerife".
El ámbito musical tiene en la figura de Teobaldo Power y Lugo Viña uno de sus exponentes más claros. Natural de Santa Cruz, se trata de un pianista y compositor, autor de los Cantos Canarios. En concreto, los arreglos de la melodía del arrorró de estos Cantos Canarios constituyen el Himno de la Comunidad Autónoma. En este campo también destaca el folclore. Similar al del resto de las islas, se caracteriza por la participación de timples, guitarras, bandurrias, laúdes y distintos tipos de instrumentos de percusión.

Son numerosos los grupos folclóricos que se reparten por la geografía isleña y que suelen aparecer en distintas celebraciones populares como las romerías. En este aspecto habría que citar a Los Sabandeños, quienes conforman un importante símbolo de la cultura canaria. Este grupo folclórico rescató la idiosincrasia del pueblo isleño en un momento en el que el carácter uniformador de la cultura española de los años setenta hace caer prácticamente en la decadencia y el olvido diferentes elementos de la música canaria.
Las canciones típicas de las islas: isa, folía, tajaraste, malagueña... se configuran como melodías mestizas entre la música ancestral de los guanches con distintos enlaces entre lo andaluz e hispanoamericano.

El primer núcleo de arte pictórico en Tenerife se distingue en la ciudad de La Laguna, donde en el transcurso del siglo XVI aparecen algunos pintores de renombre. Más adelante se suman artistas de otros lugares como Garachico, Santa Cruz, La Orotava y Puerto de la Cruz. Originarios de La Orotava son dos de los mejores pintores del archipiélago del siglo XVII: Cristóbal Hernández de Quintana y Gaspar de Quevedo, con numerosas obras distribuidas por iglesias de la isla.

En el Puerto de la Cruz, concretamente en la iglesia de Nuestra Señora de la Peña de Francia, se puede contemplar la aportación realizada por Luis de la Cruz y Ríos. Nacido en 1775, el que fuera pintor de cámara del rey Fernando VII de España y miniaturista, obtiene un reconocido prestigio en la Corte, donde se le conoce como "El Canario".

En el año 1849 nace en Santa Cruz de Tenerife el paisajista Valentín Sanz. El Museo Municipal de Bellas Artes de Santa Cruz cuenta con una abundante muestra de su quehacer. También en este museo capitalino se pueden observar cuadros de Juan Rodríguez Botas (1880-1917), quien es considerado el primer impresionista canario.

Del mismo modo cabe citar, dentro del grupo expresionista, a Mariano de Cossío. A este autor hay que atribuirle los frescos de la iglesia de Santo Domingo, en San Cristóbal de La Laguna. Por otro lado, en 1874 nace Francisco Bonnín Guerín, acuarelista de Santa Cruz que formó una escuela para promover su labor pictórica. Por último, en 1906 nace en Tacoronte uno de los pintores canarios más universales: Óscar Domínguez. Perteneciente a la corriente modernista denominada surrealismo, inventó la técnica de la decalcomanía y contribuyó con una obra pictórica de internacional reconocimiento, gracias, fundamentalmente, a los viajes que realizó a París.

Entre los artistas actuales cabe citar, entre otros, al reconocido Cristino de Vera (Santa Cruz de Tenerife, 1931) quien ha recibido el Premio Nacional de Artes Plásticas en 1998. Recibió también la Medalla de Oro al Mérito en las Bellas Artes en 2001 y el Premio Canarias de Arte en su edición de 2005. Por otro lado se debe señalar a Pedro González (San Cristóbal de La Laguna, 1927) pintor que ejerce labor docente en la Facultad de Bellas Artes y que también resultó galardonado con el Premio Canarias de Arte en 1988.

Se podría considerar que la práctica escultórica comienza en Tenerife a partir del siglo XVII, momento en el cual llega a la isla el arquitecto y escultor Martín de Andújar Cantos desde Sevilla, donde había recibido instrucciones del maestro Juan Martínez Montañés. Con él arribaron nuevas técnicas y planteamientos de la escuela hispalense que transmitió a sus discípulos, entre los que destaca el garachiquense Blas García Ravelo.

Otros escultores que, en esta época y en el posterior siglo XVIII, irrumpen a la escena son Sebastián Fernández Méndez, Lázaro González de Ocampo, José Rodríguez de la Oliva, y principalmente el orotavense Fernando Estévez, alumno de Luján Pérez, quien contribuye con una extensa colección de imágenes religiosas y tallas repartidas por diversas iglesias de Tenerife, como por ejemplo, en la Parroquia Matriz del Apóstol Santiago de Los Realejos; en la Catedral de La Laguna, la Iglesia de la Concepción también en La Laguna, la Basílica de Candelaria, el Real Santuario del Cristo de La Laguna y en distintos lugares de culto de La Orotava.

Actualmente, el ámbito escultórico tinerfeño se encuentra representado entre otros por José Abad, Fernando Garcíarramos y José Luis Fajardo.

Al igual que la que predomina en las otras islas, en la arquitectura tinerfeña sobresalen las directrices de las casonas señoriales y las de las casas más humildes y populares. Este tipo arquitectónico, que tiene notables influencias de Andalucía y Portugal, presenta, no obstante una fuerte personalidad propia.

De las casas señoriales hay que subrayar los ejemplos que existen en La Orotava y en La Laguna. Estas edificaciones se caracterizan por sus balcones típicos y por la presencia de patios interiores. La madera, especialmente la tea (pino), cobra un gran protagonismo en estas construcciones. Estas casas presentan fachadas no demasiado complejas con poca ornamentación.

Son típicos los grandes balcones de madera y el uso de celosías. Las ventanas cierran en guillotina y son habituales los asientos interiores adosados a ellas. Los patios interiores funcionan como verdaderos jardines que sirven para dar iluminación a las habitaciones. Estas se comunican con el patio por medio de galerías rematadas frecuentemente en piedra y madera. Artilugios como las destiladeras, las bombas de agua, los bancos y mesones son elementos que muchas veces forman parte de estos patios interiores.

En cuanto a las casas tradicionales, éstas se caracterizan por ser edificios de escasa altura, con toscas paredes de colores variopintos. En ocasiones la continuidad de estas paredes se ve interrumpida por la presencia de bloques de piedra que asoman a la superficie de forma ornamental. A lo largo de toda la isla son muchos los ejemplos a contemplar de esta arquitectura.

Los edificios oficiales o de carácter religioso se han ido conformando según las distintas corrientes arquitectónicas que en cada momento han imperado. Los núcleos urbanos de las ciudades de La Laguna y La Orotava están declarados como monumentos histórico-artístico nacionales.

Otra muestra arquitectónica presente en la isla la representan los edificios defensivos que se erigieron a modo de torres o pequeños castillos tras la conquista castellana. Estas construcciones tuvieron la finalidad de proteger a la isla de ataques piráticos, o de incursiones foráneas, como la del almirante inglés Horatio Nelson en julio de 1797. Entre estos fortines se podrían citar en la capital tinerfeña el castillo de San Cristóbal, el castillo de San Andrés o el castillo de San Juan o Castillo Negro. En el norte de la isla también se emplazan algunas de estas edificaciones como el castillo de San Miguel en Garachico o el castillo de San Felipe en el Puerto de la Cruz, entre otros.

En los últimos años, por parte de los diferentes gobiernos, ha predominado el concepto de llevar a cabo grandes proyectos, en ocasiones ostentosos, diseñados por reconocidos arquitectos. Entre ellos se podría incluir por ejemplo, la remodelación de la plaza de España por los arquitectos suizos Herzog & De Meuron, el nuevo proyecto del francés Dominique Perrault de la Playa de Las Teresitas, el centro Magma Arte & Congresos, las Torres de Santa Cruz o el Auditorio de Tenerife. Este último edificio, obra del arquitecto español Santiago Calatrava, se alza al noreste del Parque Marítimo en Santa Cruz de Tenerife. Uno de sus elementos más destacables es la estampa de su vela alada simulando un barco, que lo ha convertido en el emblema arquitectónico de la ciudad de Santa Cruz.

El calado es una labor de bordado fundamentada en una técnica consistente en ir deshilando un paño tensamente sujeto a un bastidor por lo general de madera. El resultado final suele aplicarse, sobre todo, a la mantelería u otros elementos decorativos. La roseta se confecciona substancialmente en el municipio de Vilaflor, y consiste en crear dibujos con hilos que son cruzados entre fijadores. Estas pequeñas piezas así elaboradas son unidas posteriormente obteniéndose paños individuales y composiciones.

En este ámbito hay que destacar igualmente la ebanistería. El norte de Tenerife ha proporcionado a la historia varios maestros en la talla que han contribuido con elementos que van desde balcones, celosías, puertas y ventanas hasta un original mobiliario cargado de objetos elaborados en madera fina. La cestería también es una labor de cierto peso en la artesanía tinerfeña donde sus artesanos trabajan desde hojas de palma y varas de castaño a la fibra de la platanera, conocida por el sector como la badana, que conlleva una producción igualmente diversa y heterogénea.

Existe, como en el resto de las Islas Canarias, toda una tradición artesana alrededor de la alfarería. El uso del barro procede de la primitiva cerámica llevada a cabo por los antiguos guanches, quienes desconocían el uso del torno. Los alfareros de la isla trabajan la arcilla con las manos, lo que imprime una gran autenticidad a sus obras.

El nacimiento de la educación se debe en la isla a las órdenes religiosas. En el año 1530, Tenerife accede a la cultura de la mano de la cátedra de filosofía que, poseen los dominicos en el convento de La Concepción de La Laguna. A pesar de ello, hasta bien avanzado el siglo XVIII no comienzan a funcionar las pocas escuelas que por aquel entonces existían.

En este sentido, hay que recalcar el trabajo desempeñado por la Real Sociedad Económica de Amigos del País, que creó diversas escuelas en San Cristóbal de La Laguna. Fue en 1846 cuando se instaura el primer instituto de enseñanza secundaria con el fin de suplir el cierre de la Universidad de San Fernando ("véase" Universidad de La Laguna). Anexa a este edificio se fundó en 1850 la primera Escuela Normal Elemental del archipiélago que pasaría a denominarse Escuela Normal Superior de Magisterio en 1866. Así se mantiene esta situación ya que a pesar de que el dictador Miguel Primo de Rivera crease algunos centros, el punto de inflexión lo supone la política educativa que desarrolló la Segunda República, de modo que en apenas cuatro años (1929-1933) casi se dobla el número de escuelas existentes.

Posteriormente, el inicio de la Guerra Civil y la ulterior dictadura de Francisco Franco constituyeron un considerable retroceso. La educación en manos de órdenes religiosas tuvo cierta importancia en el devenir de los tinerfeños hasta que en 1970 la Ley General de Educación resta peso a estas instituciones religiosas en favor de los centros públicos. Estos últimos, y ya en menor grado los primeros, comienzan a multiplicarse desde entonces y son impulsados con la instauración de la democracia. Tenerife cuenta a día de hoy con 301 centros de educación infantil, 297 colegios de primaria, 140 de secundaria y 86 institutos de bachiller. Además, en la isla existen varios centros de estudios universitarios o de postgrado: Universidad de La Laguna, Universidad Nacional de Educación a Distancia, Universidad Internacional Menéndez Pelayo, Universidad del Atlántico Medio y la Universidad Europea de Canarias. Hay dos escuelas de turismo adscritas a la Universidad de La Laguna: Escuela Universitaria Iriarte y la Escuela Universitaria de Turismo de Santa Cruz. También hay una sede en la isla de la escuela de turismo Vatel Tourism School, considerada la mejor escuela hostelera del mundo. Por último, hay tres escuelas de negocios privadas: Escuela Canaria de Negocios, la European School of Management -Tenerife y la International Maritime Business School (IMBS), que es una escuela de negocio especializada en el sector marítimo.

El campo de la investigación, históricamente, no se ha desarrollado de un modo especialmente relevante. No obstante, entre los centros que se dedican a esta labor destaca sobre todo el Instituto de Astrofísica de Canarias que tiene sede en esta isla.

Asimismo cabría citar el Instituto de Bio-Orgánica Antonio González, vinculado a la Universidad de La Laguna. También adheridos a esta universidad se encuentran el Instituto de Lingüística Andrés Bello, el Centro de Estudios Medievales y Renacentistas, el Instituto Universitario de la Empresa, el Instituto de Derecho Regional y el Instituto Universitario de Ciencias Políticas y Sociales al igual que el Instituto de Enfermedades Tropicales (perteneciente a la Red de Investigación de Centros de Enfermedades Tropicales, que dispone de siete nodos extendidos a lo largo del país, uno de ellos en Canarias).

Con sede en la ciudad del Puerto de la Cruz se encuentra el Instituto de Estudios Hispánicos de Canarias, adscrito al Instituto de Cultura Hispánica de Madrid. En la ciudad de La Laguna se encuentra la delegación canaria del Consejo Superior de Investigaciones Científicas (CSIC), el Instituto Canario de Investigaciones Agrarias, el Instituto de Estudios Canarios y el Centro Internacional para la Conservación del Patrimonio.

Otros organismos que trabajan en el ámbito de la investigación que tienen sede en Tenerife son el Instituto Tecnológico de Canarias, el Instituto Vulcanológico de Canarias, la Asociación Industrial de Canarias, el Instituto Tecnológico de Energías Renovables y el Instituto Oceanográfico de Canarias emplazado en la ciudad de Santa Cruz de Tenerife.

La isla cuenta con diversos recintos museísticos de diferente naturaleza que están bajo el dominio de distintas instituciones. Quizás los más destacados sean los pertenecientes al Organismo Autónomo de Museos y Centros, que dispone de los siguientes espacios:







Desligados del Organismo Autónomo de Museos y Centros destacan:










Tenerife tiene un amplio calendario festivo en el que destaca principalmente el Carnaval de Santa Cruz de Tenerife, el más importante del país y uno de los más importantes del mundo. El día oficial de la isla es el 2 de febrero en honor de la Virgen de Candelaria (Patrona de Canarias), celebrada en ese día como fiesta insular en Tenerife, mientras que la festividad del 15 de agosto (día de la Asunción de María a los Cielos), es fiesta nacional en España, si bien en Canarias es el día en que se celebra a la Virgen de Candelaria como patrona de este archipiélago. Otras fiestas son sus romerías, el Corpus Christi, la Semana Santa y la fiesta del Santísimo Cristo de La Laguna el 14 de septiembre.

Quizás la fiesta tinerfeña de mayor repercusión nacional e internacional sea el Carnaval de Santa Cruz de Tenerife, no en vano declarado . Aparte de la capital, el carnaval se celebra en múltiples localidades del norte y sur de la isla, pero es en la primera donde tiene mayor envergadura. Con la elección de la reina adulta se pone fin a éstos y comienza lo que los tinerfeños denominan carnaval en la calle con importantes concentraciones de carnavaleros en el centro de Santa Cruz, que se prolongan durante diez días de fiesta.

Las fiestas populares más tradicionales y extendidas en Tenerife son quizás las romerías. Éstas, a caballo entre lo pagano y lo religioso son manifestaciones multitudinarias con carrozas o carretas, aperos y ganado en honor al patrón o patrona del lugar. Es frecuente en estos festejos la reunión de marcados factores identitarios de la etnografía isleña: folclore, danza, artesanía, comida típica, deportes autóctonos, donde se puede observar a gran parte de los asistentes ataviados con los diferentes trajes de mago típicos de las islas.

En origen las romerías encarnaban fiestas de las clases más adineradas de la sociedad, que se congregaban en veneración de los santos a los que atribuían buenas cosechas, tierras fértiles, copiosidad de lluvias, exoneración de determinadas enfermedades y un largo etcétera. En consecuencia, los allí reunidos degustaban los alimentos y vinos de la tierra y, brindaban y compartían sus bienes rindiendo así pleitesía. Estas celebraciones se fueron popularizando paulatinamente y dieron paso a una de las fiestas más emblemáticas de la actualidad. Dentro de las grandes romerías de la isla cabe señalar las romerías de San Marcos en Tegueste, donde las carretas son decoradas con productos del campo (semillas, cereales, flores, etc), San Isidro Labrador en Los Realejos, San Isidro Labrador y Santa María de la Cabeza en La Orotava, la Romería de la Virgen de Candelaria en la Villa Mariana de Candelaria, El Socorro de Güímar, San Benito Abad en La Laguna, San Isidro Labrador en Tacoronte, San Roque en Garachico o la de San Agustín en Arafo.

La Virgen de Candelaria es la Patrona de Canarias. Su fiesta es celebrada dos veces al año, en febrero y en agosto. La Romería-Ofrenda a la Virgen de Candelaria se celebra cada 15 de agosto. En este acto es tradición que representaciones de todas las islas del Archipiélago Canario acudan a ofrendar a su Patrona y también están presentes representaciones de los municipios de Tenerife. Otro acto significativo de la fiesta de la Virgen de Candelaria es la peregrinación a la Villa Mariana realizada en la noche del 14 al 15 de agosto, en la cual los fieles recorren andando multitud de kilómetros desde diferentes partes de la isla hasta llegar a la Villa Mariana de Candelaria, lugar donde se encuentra la venerada imagen de la Virgen. Es habitual recibir peregrinos de otras islas e incluso de otras partes de España. El día 15 tiene lugar los actos principales con la solemne eucaristía presidida por el obispo de Tenerife y posterior procesión con presencia de las máximas autoridades religiosas, políticas y militares de Canarias. La Fiesta de la Virgen de Candelaria del 15 de agosto tiene la consideración de "Fiesta de Interés Turístico Nacional de España".

El 2 de febrero se celebra la Fiesta Litúrgica de La Candelaria. También en este día se acercan a la villa muchos fieles de la "Virgen Morenita". Durante las fiestas de febrero destaca la llamada "Procesión de Las Candelas", en la cual los fieles acompañan a la Virgen en la oscuridad de la noche iluminados solo por candelas y rezando el rosario. Al día siguiente, tiene lugar la eucaristía solemne y posterior procesión presididas ambas por el obispo de Tenerife y de nuevo con presencia de las máximas autoridades religiosas, políticas y militares de Canarias. Es también tradición que cada siete años la imagen de la Virgen sea trasladada alternativamente por dos semanas a las ciudades de Santa Cruz de Tenerife (capital) y San Cristóbal de La Laguna (sede de la diócesis). La última visita o traslado de la Virgen se produjo en 2018 de manera excepcional a ambas ciudades en coincidencia con las celebraciones con motivo de los actos conmemorativos de los 200 años de la creación de la Diócesis de Tenerife. Anteriores traslados fueron en octubre de 2002 a Santa Cruz y en mayo de 2009 a La Laguna.

Esta Festividad Litúrgica que, tiene varios siglos de historia, es celebrada cada 14 de septiembre en la ciudad de San Cristóbal de La Laguna y gira en torno al Santísimo Cristo de La Laguna. El Cristo de La Laguna es una de las imágenes más veneradas de las Islas Canarias y especialmente en la isla de Tenerife junto con la Virgen de Candelaria. Es la advocación de Cristo más venerada de Canarias, y una de las imágenes religiosas más antiguas del archipiélago.

Cada 9 de septiembre la venerada imagen del Cristo es bajada en público del altar mayor de su Real Santuario, para el rito del besapiés y para más tarde ser colocada en el trono procesional, para sus fiestas mayores de septiembre. La imagen permanece en su trono procesional hasta el 21 de septiembre, día en que la sagrada imagen es subida de nuevo a su altar. Durante este tiempo la imagen es solemnemente trasladada hasta la Catedral de La Laguna (9 de septiembre), en la cual procesiona en una cruz repujada en plata. En dicha catedral permanece durante varios días, hasta el día 14 de septiembre, cuando se procede al traslado de vuelta a su Real Santuario.

Con marcado carácter religioso se encuentra la festividad del Corpus Christi, en la que es habitual la confección de alfombras florales en las calles. A título especial se pueden incluir las realizadas en La Orotava, donde se puede contemplar un tapiz de considerables dimensiones confeccionado en la plaza del ayuntamiento mediante tierras volcánicas de diversas tonalidades, extraídas del parque nacional del Teide que, tras la celebración son devueltas a fin de respetar el entorno del Parque. La festividad del Corpus Christi de La Orotava está declarada Bien de Interés Cultural en la categoría de Actividad Tradicional de Ámbito Insular. En La Laguna la festividad del Corpus Christi tiene también una especial significación, pues es considerada como la procesión más antigua de Canarias.

En el capítulo de celebraciones a reseñar de la isla de Tenerife habría que contar con la Semana Santa. Esta se celebra en todos los municipios pero probablemente sea en San Cristóbal de La Laguna (que de hecho esta Semana Santa ostenta el honorable título de ser la más antigua e importante del Archipiélago Canario), Santa Cruz de Tenerife, La Orotava y Los Realejos donde adquiera especial significado. En este sentido destacan principalmente las procesiones que se desarrollan durante el Jueves Santo, Viernes Santo y Domingo de Resurrección.

Tenerife posee comunicaciones por tierra, mar y aire. La isla posee dos grandes autopistas; la Autopista del Sur y la Autopista del Norte, dos aeropuertos internacionales; el Aeropuerto de Tenerife Sur y el Aeropuerto de Tenerife Norte y dos grandes puertos; el de Santa Cruz de Tenerife y el de Los Cristianos. Lo que la convierten en la isla canaria que más pasajeros registra, tanto por mar como por aire, con más de cinco millones de pasajeros anualmente.

Las principales comunicaciones que se producen en Tenerife se establecen por carretera. Las más importantes son la Autopista del Sur y la Autopista del Norte, que parten desde la zona metropolitana hacia las zonas sur y norte respectivamente. Estas dos autopistas están conectadas a través de la Autovía de Interconexión Norte-Sur también en las afueras del área metropolitana. Dentro de la red de carreteras de la isla existen otras de menor importancia que las anteriores pero cabe destacar la Autovía de San Andrés y la Autovía de Penetración de Santa Cruz de Tenerife, ambas en Santa Cruz de Tenerife.

Asimismo está previsto la construcción de una autovía de circunvalación norte del área metropolitana de Santa Cruz de Tenerife-La Laguna. Esta autovía pretende comunicar los núcleos de Guamasa y Acorán, a través de Los Baldíos, Centenero, Llano del Moro, El Sobradillo, El Tablero, El Chorrillo, entre otros barrios. La vía tendrá aproximadamente 20 km y un coste estimado de 190 millones de euros.

El principal medio para llegar a Tenerife es el avión. Existen dos aeropuertos en la isla: el Aeropuerto de Tenerife Sur y el Aeropuerto de Tenerife Norte, el primero situado en el sur de la isla y con un tráfico turístico dominante, mientras que el segundo está situado en el área metropolitana y tiene un tráfico principalmente regional y doméstico. A pesar de que el aeropuerto de Tenerife Sur es el que recibe mayor número de pasajeros, ambos son aeropuertos internacionales que disponen de vuelos regulares tanto con otras islas como con diferentes puntos de la península ibérica (Madrid, Barcelona, Sevilla, Valencia, Málaga, Bilbao, etc.) y del extranjero, especialmente con otros países de la Unión Europea y Venezuela. Teniendo en cuenta los dos aeropuertos, Tenerife es la isla canaria que anualmente recibe mayor número de pasajeros.

Además del avión, Tenerife tiene dos puertos marítimos principales que le sirven de conexión. El Puerto de Santa Cruz que conecta con las capitales de cada isla, y en particular con aquellas de la provincia oriental, y el Puerto de Los Cristianos que se centra en mayor medida en las comunicaciones con las capitales de la provincia de Santa Cruz de Tenerife. Además es posible el tráfico de pasajeros entre los Puertos de Santa Cruz de Tenerife y Cádiz y viceversa. En 2017 abrió un gran puerto de importancia en el sur de la Isla, el de Granadilla, y está previsto la construcción de otro en la parte oeste, en Fonsalía.

El Puerto de Los Cristianos y el Puerto de Santa Cruz de Tenerife son, respectivamente, el primero y el segundo en tráfico de pasajeros de Canarias, de los puertos que dependen del Estado. Asimismo, el Puerto de Santa Cruz de Tenerife es el que más pasajeros de crucero y vehículos en régimen de pasaje registra del conjunto del archipiélago.

La isla cuenta también con una extensa red de guaguas tanto urbanas como interurbanas que conectan la gran mayoría de los núcleos de población. Para ello cuenta con estaciones de guaguas en todas las ciudades, como el Intercambiador de Transportes de Santa Cruz de Tenerife. El servicio regular de viajeros por carretera lo cubre la compañía Transportes Interurbanos de Tenerife, S.A.U.

Con la inauguración de la línea 1 del Tranvía de Tenerife, que une destacados sectores de la conurbación Santa Cruz-La Laguna, se puso en marcha la red tranviaria de Tenerife. Esta primera línea tiene como cabeceras el Intercambiador de Transportes de Santa Cruz de Tenerife y la lagunera Avenida de La Trinidad y conecta puntos como los dos centros hospitalarios de referencia de la isla o los campus universitarios. La segunda fase une los barrios de Tíncer (perteneciente a Santa Cruz) y La Cuesta (La Laguna), por medio de la línea 2, que se inauguró el día 30 de mayo de 2009. Esta línea tiene dos paradas de trasbordo con la línea 1: Hospital Universitario de Canarias y El Cardonal. Es el único tranvía existente en Canarias.

Perteneciente a la misma empresa que explota el Tranvía de Tenerife, en torno a 2017 se espera que comience, tras la aprobación de su construcción por el pleno del Cabildo Insular de Tenerife el 27 de abril de 2007, los trabajos para habilitar un tren que unirá Santa Cruz de Tenerife con el sur de la isla. El recorrido total será de 80 km y tiene previsto realizar su trayecto completo en 35 minutos y si tuviera que parar en todas las estaciones, lo haría en 45 minutos.

En Tenerife se pueden practicar gran cantidad de deportes, tanto al aire libre como en las distintas instalaciones disponibles en toda la isla. Como en el resto de España el deporte más prácticado y de moda es el fútbol, con su principal equipo representativo el Club Deportivo Tenerife.

Entre los deportes canarios practicados en la isla, cabe destacar los siguientes:

La lucha se desarrolla dentro de un círculo, generalmente de arena, denominado terrero. En él, dos luchadores se enfrentan agarrados intentando derribarse. En Tenerife hay 26 terreros distribuidos por algunos municipios de la isla, utilizados por los 26 equipos masculinos federados y los dos equipos femeninos. La isla cuenta además con una liga escolar organizada por el cabildo y con un programa de promoción de este deporte puesto en marcha por instituciones, federaciones y clubes en el que participan 24 escuelas de lucha.

El juego del palo canario es un arte marcial que se practica entre dos jugadores que, sin llegar a hacer contacto con el cuerpo del adversario, realizan un combate con palos. El juego del palo, en su origen, no tenía carácter lúdico, sino que era un método de combate utilizado por los canarios precoloniales.

En Tenerife existen los siguientes clubes federados:

Similar al juego francés de la petanca, la bola canaria es un deporte que básicamente consiste en sumar puntos mediante el lanzamiento de unas bolas que hay que dejar lo más cerca posible de un objeto llamado mingue o boliche. Se juega en un terreno rectangular de arena o tierra de entre 18 y 25 m de largo y un ancho de entre 3,5 y 6 m. En Tenerife se compite a nivel federado existiendo una treintena de equipos que se organizan en tres categorías (primera, segunda y segunda B). En Tegueste existe una federación interna e independiente que funciona sólo en ese municipio y los equipos pueden ser mixtos.

En la isla se practican otras manifestaciones deportivas relacionadas con el ámbito rural, como el levantamiento de piedras y el arrastre de ganado, esta última con un creciente arraigo popular al disponer de un campeonato que organiza la Asociación Canaria de Arrastre. En abril suele celebrarse en Tegueste una exhibición de deportes rurales de Canarias.

Las condiciones de mar y el clima hacen que la isla sea idónea para la práctica de una amplia variedad de deportes acuáticos.

En la isla se practican tanto el surf tradicional, deslizándose sobre las olas encima de una tabla, como el windsurf en el que la tabla se desplaza gracias a una vela y como el más reciente kitesurf, en el que la fuerza necesaria para la navegación se obtiene de una cometa. La isla cuenta con diez escuelas, una de ellas participada por el cabildo, y diversos cursos dedicados al aprendizaje de estos deportes. Las principales zonas para la práctica de estas disciplinas son El Médano, Playa de Las Américas, la costa de Santa Cruz de Tenerife, Güímar y las costas del Valle de La Orotava, principalmente en la Playa del Socorro en Los Realejos. En algunas de ellas se han celebrado varias pruebas del Grand Slam puntuables para la World Cup en las disciplinas de "olas", "slalom" y "course race".
Al igual que ocurre con el surf y el windsurf, se pueden encontrar escuelas de submarinismo por toda la costa de Tenerife. En la isla existen hasta treinta puntos de inmersión repartidos por su litoral donde no solo es posible descubrir una interesante flora y fauna marinas, sino también restos de barcos hundidos. Dentro de los mejores sitios para el buceo se encuentran, entre otros, Las Galletas, Playa Paraíso y la Punta de la Rasca al Sur, así como Garachico, Puerto de la Cruz o la Punta de Teno al Norte. En Tenerife concurren un gran número de clubes de buceo, tanto para la práctica como para el aprendizaje. Uno de ellos, también como en el caso del windsurf bajo las directrices del Cabildo Insular en el Centro Insular de Deportes Marinos (CIDEMAT). Destaca la presencia de especies como la tortuga boba, y de una colonia permanente de ballenas piloto, también bajo el nombre de calderones tropicales frente a las costas del Sur, avistándose igualmente con frecuencia el delfín mular. Estas dos especies de cetáceos viven de forma permanente en el canal existente entre Tenerife y La Gomera.

La natación es uno de los deportes más practicados ya sea en piscinas o en las costas de la isla. De este modo no solo la natación, sino el waterpolo, con el Club Natación Martiánez en la División de Honor de la Liga Nacional Española o la Agrupación Deportiva Santa Cruz en la liga Regional Canaria, la natación sincronizada, con varias integrantes en el combinado nacional, o las pruebas de larga distancia (Copa Cabildo de Aguas Abiertas) son modalidades destacadas. En Tenerife existen dieciséis equipos federados de natación:

Como es lógico, también se practican distintas modalidades de deportes acuáticos desarrolladas a bordo de embarcaciones. De estos deportes, se celebran en la isla distintas competiciones de Vela latina, Laser, Snipe, Crucero o de Optimist, por ejemplo.

En la isla tienen lugar eventos de índole insular, regional, nacional e incluso internacional.

Otro deporte que podríamos englobar en esta categoría es la pesca deportiva, existiendo clubes en la isla para ello. Aparte también es posible desarrollar excursiones marítimas con este fin. Además existe un campeonato insular de pesca de altura.

En otro sentido, los deportes aéreos como el paracaidismo y sobre todo el parapente juegan un papel importante. En Los Realejos se celebra el Festival Internacional de Parapente (Flypa). Existen en la isla más de 40 despegues frecuentados de los que los más señalados son:


El deporte del motor ocupa un lugar significativo en Tenerife. El motocross, el karting y los rallies son tres variedades que encontramos en la isla. Durante todo el año se llevan a cabo competiciones de rally, en sus diferentes especialidades, valederas para el campeonato regional de Canarias. En cuanto a la modalidad de rallyes de "asfalto" se realizan varias pruebas. Estas son el Rally Norte, Rally Isla de Tenerife, Rally de Granadilla y Rally Villa de Adeje. La categoría de "montaña" suma siete pruebas (Subida a Los Loros, Subida a Guía de Isora, Subida a Güímar, Subida Arona-La Escalona, Subida a Tamaimo, Subida a San Miguel y Subida a La Guancha) en esta isla. Por otro lado, en la categoría de "tierra", en los municipios de Arico y Granadilla se realizan dos rallyes. Finalmente, en la modalidad de "slalom", destaca la prueba de Arico.

Está prevista la construcción de un circuito homologado para entrenamiento de la F1 en Atogo. Este circuito contaría con una distancia total de 4068 metros en su mayor variante, con una recta de 819 metros. El ancho de la pista sería de 15m en la recta de meta, y 12m en el resto del circuito. Contará con 15 curvas (10 de izquierda y 5 de derecha) y una inclinación máxima del 5%. Tendrá 10 posibles variantes, y entre ellas una que permite 2 trazados simultáneos. El proyecto se encuentra actualmente adjudicado a falta de que comiencen las obras.

Además de los citados, en la isla se practican otros deportes de los que la siguiente es una pequeña relación:

El voleibol es el deporte más laureado de los existentes en la isla. Dentro de esta disciplina deportiva destaca en la categoría femenina, de manera clara, el campeón europeo Club Voleibol Tenerife que participa en la Superliga femenina española. Otros dos combinados a subrayar en este aspecto son el femenino Club Voleibol Aguere y el masculino Arona Playa de las Américas que compite en la Superliga masculina de voleibol. Fruto del éxito de los diferentes clubes de voleibol se observa un creciente interés por esta práctica deportiva en los escolares de Tenerife.
Como ocurre en gran parte del resto de España el fútbol es el deporte más practicado y seguido por el conjunto de los tinerfeños. La Federación Tinerfeña de Fútbol cuenta con un total de 305 equipos federados que compiten en las diferentes categorías en el conjunto de campos de fútbol que se reparten en el territorio isleño. Por otro lado, el máximo exponente tinerfeño en cuanto a fútbol se refiere es el Club Deportivo Tenerife, equipo que ha jugado trece temporadas en Primera División. Sus grandes hitos deportivos son dos quintos puestos en la Liga (1993 y 1996), la disputa de unas semifinales de la Copa del Rey contra el Celta de Vigo y la disputa de unas semifinales de la Copa de la UEFA contra el FC Schalke 04 (1997). Actualmente forma parte de la Segunda División española.

En Tenerife también existe la posibilidad de practicar el baloncesto. Al igual que en el caso del fútbol, voleibol, e incluso balonmano, karate, etc. muchos colegios e institutos ofrecen la posibilidad de aprender y profundizar en la práctica de este deporte. Además se hallan en la isla diversos equipos federados. En concreto, en la liga LEB se integran dos equipos de esta isla: el Tenerife Rural y el Club Baloncesto Canarias. En el verano de 2010, el Club Baloncesto Canarias realizó un nuevo proceso de fusión con el CB Tenerife, pasando a denominarse Isla de Tenerife Socas Canarias y trasladando la sede del club al Pabellón Santiago Martín.

La extensa red de caminos existentes en la isla permite la práctica del senderismo. Son numerosas las asociaciones o compañías que organizan estas excursiones por los montes, montañas o caminos rurales de la isla. En esta línea se encuentra el descenso de barrancos, una especialidad que permite recorrer lugares inaccesibles y de gran valor natural y paisajístico. Lo puede practicar cualquier persona ya que hay barrancos para cualquier nivel. Básicamente consiste en recorrer los caminos que ha abierto el agua entre las montañas a lo largo de miles de años. Atendiendo a las características y singularidades de cada barranco se requieren cuerdas y material de escalada para practicar con la técnica del rappel si fuera necesario. A pesar de que hay barrancos en los que esta práctica está prohibida son cuatro los barrancos especialmente habilitados para el descenso:

El golf es un deporte que en Tenerife cuenta con unas buenas y extensas instalaciones para su práctica. No obstante, es una disciplina orientada principalmente al turismo que deja unas importantes cantidades de dinero en la isla pero que como contrapartida consume una gran cantidad de los recursos hídricos limitados disponibles. Existen en total nueve campos de golf:

El Tenerife Ladies Open, torneo de golf femenino que se disputa en el Golf Costa Adeje, es un evento integrado dentro del circuito europeo femenino. Habiendo disputado ocho ediciones, cuenta con prestigio dentro del calendario del circuito europeo y está considerado como el referente en cuanto a torneos profesionales femeninos en el ámbito nacional.

En la isla también se practican deportes como el ciclismo, el pádel, el tenis, el squash, la hípica, el judo, el frontenis, el atletismo con el Club Atletismo Tenerife CajaCanarias como referente y la ultramaratón CajaMar Tenerife Bluetrail, la carrera más alta de España y segunda de Europa.

Los principales centros sanitarios de la Isla son el Hospital Universitario de Canarias y el Hospital Universitario Nuestra Señora de Candelaria. Ambos son hospitales de tercer nivel, es decir, de atención especializada y de referencia en algunas especialidades para toda Canarias e incluso España. Están incorporados a la red docente de la Universidad de La Laguna. Ambos complejos hospitalarios son gestionados por el Servicio Canario de Salud. El Hospital Universitario Nuestra Señora de Candelaria es el complejo hospitalario más grande de las Islas Canarias.

En 2012 fue abierto en el municipio de Icod de los Vinos el nuevo Hospital del Norte de Tenerife con cobertura para los municipios norteños de la isla y desde 2015 entró en funcionamiento el Hospital del Sur de Tenerife situado en el municipio de Arona con cobertura para los municipios sureños. Estos son dos nuevos hospitales periféricos en las zonas norte y sur de la isla que cuentan de acuerdo a su clasificación como hospitales de segundo nivel, con servicios de hospitalización, diagnóstico avanzado, urgencias, cirugía mayor ambulatoria, rehabilitación, etc. Los hospitales de tercer nivel y los de segundo nivel junto con los 39 centros de atención primaria y los múltiples centros de atención especializada completan las infraestructuras sanitarias de Tenerife.

Al igual que ocurre en el resto de España, y según las encuestas, la sociedad tinerfeña se declara mayoritariamente católica, aunque la mayoría de esta tampoco es practicante. No obstante, las crecientes corrientes migratorias (turismo, inmigración, etc.) están incrementando el número de fieles de otras religiones que se dan cita en la isla, como el Islam, el Hinduismo, el Budismo, confesiones cristianas evangélicas, el Judaísmo y Religiones afroamericanas. Confesiones minoritarias destacadas en la isla son: las Religiones chinas, el Bahaísmo y una forma de neopaganismo autóctono, la Iglesia del Pueblo Guanche, entre otras.

Son numerosas las advocaciones católicas que existen, sin embargo, aquí tiene lugar cada año la peregrinación más importante del archipiélago, debido a la celebración de la festividad de la Virgen de Candelaria (Patrona de Canarias), quien representa la unión de las culturas guanche y española. Los guanches tomaron como propia la imagen que los misioneros de Lanzarote y Fuerteventura dejaron en una playa próxima a la actual Villa Mariana de Candelaria. A partir de ahí, la historia y la leyenda de esta imagen, se entrelazan y dan paso al culto y peregrinación que hasta nuestros días mantienen los habitantes de las islas y de Tenerife en particular. Por otro lado, hay que nombrar también a la Virgen de los Remedios, patrona de la isla de Tenerife y de la Diócesis de San Cristóbal de La Laguna (la cual engloba a toda la provincia), la festividad de la Virgen de los Remedios es el 8 de septiembre. Otra imagen venerada en la isla y en Canarias es el Santísimo Cristo de La Laguna celebrado el 14 de septiembre. De la misma manera, recibe especial veneración el Santísimo Cristo de los Dolores de Tacoronte.

En Tenerife nacieron dos de los más grandes misioneros que han existido en el continente americano y que a su vez son venerados como santos de la Iglesia católica: Pedro de San José Betancur y José de Anchieta. El primero nació en Vilaflor en el sur de la isla, fue misionero en Guatemala y fundador de la Orden de los Betlemitas (la primera orden religiosa nacida en el continente americano). El segundo nacido en San Cristóbal de La Laguna fue misionero en Brasil, y fue el fundador de São Paulo y uno de los fundadores de Río de Janeiro. Otra personalidad religiosa destacada es la monja María de León Bello y Delgado ("La Siervita"), nacida en El Sauzal y cuyo cuerpo permanece incorrupto. Esta religiosa con fama de santidad, es muy venerada en todo el archipiélago.

El 2 de febrero el día litúrgico de la Virgen de Candelaria (Patrona del Archipiélago Canario) es festivo en la isla. San Miguel Arcángel es el Santo Patrono de la isla de Tenerife y del Ayuntamiento de La Laguna. De hecho su imagen aparece en el escudo insular, que fue concedido por Fernando el Católico, el 23 de marzo de 1510, en nombre de su hija la reina Doña Juana.

La isla de Tenerife se divide en trece arciprestazgos pertenecientes a la Diócesis de San Cristóbal de La Laguna, diócesis que tiene su sede en la isla. Estos son los de: "Icod", "Isora", "Güímar", "Granadilla", "La Cuesta", "La Laguna", "La Orotava", "La Salud", "Ofra", "Taco", "Tacoronte", "Tegueste" y "Santa Cruz de Tenerife".

Entre los seguidores del Islam, en Tenerife se encuentra la sede de la Federación Islámica de Canarias, que es el organismo que agrupa a las asociaciones y comunidades de religión musulmana del archipiélago canario.

En cuanto a los principales núcleos o templos religiosos destacan:



Hay otros edificios religiosos de cierta relevancia, como puede ser la Iglesia de la Concepción, la de San Agustín y Santo Domingo en La Orotava, el Templo de Nuestra Señora de la Peña de Francia, en el Puerto de la Cruz, el de San Marcos Evangelista en Icod de los Vinos o el de Santa Ana en Garachico, así como la Iglesia de la Concepción de Santa Cruz de Tenerife, la Cueva-Santuario del Santo Hermano Pedro, al sur de la isla, que es lugar de peregrinación, entre otros.

Otro edificio religioso importante de la isla es el Templo Masónico de Santa Cruz de Tenerife, el cual está considerado como el más bello ejemplo de templo masónico en España, y fue de hecho, el mayor centro masónico de España hasta la ocupación por los militares del régimen franquista.

Como resulta lógico por la influencia marina, los productos del mar gozan aquí de cierta abundancia tanto en cantidad como en variedad. Entre las especies más apreciadas están las viejas, y también, entre otros, la sama, el bocinegro, la salema, el cherne, el mero... Destacan asimismo los diversos tipos de túnidos que abundan en sus costas. Las caballas, sardinas y chicharros también deben ser citadas entre los pescados más consumidos. Otra especie que disfruta de cierta fama es la morena, que se suele servir frita. Estas variedades marinas se suelen preparar simplemente cocidas, o a la espalda, a la sal, etc. Es frecuente que se acompañen con mojo y papas arrugadas.

En el apartado de carnes, es un plato muy popular la típica carne de fiesta (tacos de cerdo adobados) que se prepara para los festejos de los pueblos en ventorrillos (puestos de feria), bares y casas particulares. El conejo en salmorejo, el cabrito, y por supuesto el vacuno, el porcino y las carnes de ave son también consumidas habitualmente. Otro plato típico de la gastronomía tinerfeña es el puchero canario que, a semejanza con otros cocidos españoles, representa una de las ollas más completas de la culinaria nacional. El contenido del puchero puede, evidentemente, variar pero es rico en verduras, hortalizas, legumbres y carnes.

Tanto los pescados como las carnes suelen acompañarse con papas arrugadas. Es éste un plato típico del conjunto de Canarias que responde simplemente a la forma de cocinar las papas. Con agua, mucha sal, y sin pelar. En 2016 fueron proclamadas maravilla gastronómica de España en un concurso promovido por Allianz Global Assistance, consiguiendo el primer puesto mediante voto popular a través de internet.

Una salsa típica canaria que da un cierto sabor picante a las comidas. Se designan las salsas típicas de las islas. Los mojos suelen llevar cilantro, perejil, pimentón o pimienta. No obstante el abanico de estas salsas es muy amplio y permite el uso de distintos ingredientes en su elaboración como almendras, queso, azafrán, pan frito, entre otras posibilidades.

En cuanto finalizó la conquista de las islas, una de las primeras actividades económicas de las que se pusieron en marcha inmediatamente fue la elaboración y el mercado del queso. Era una forma racional de rentabilizar la pequeña ganadería existente. Como anécdota, podemos apuntar que el queso fue incluso utilizado como moneda de cambio y compraventa. Desde entonces este es un alimento fundamental en las zonas agrarias.

Viene a ser otro de los platos más comúnmente elaborados y consumidos. Destacan los producidos en granjas de Arico, La Orotava o Teno. A su vez, son diversas variedades las que existen: quesos tiernos, curados, semicurados, ahumados… y son en su mayoría artesanales. Hoy en día predominan los quesos de cabra, aunque en ocasiones se confeccionan con ciertas cantidades de leche de oveja o de vaca. Suelen servirse a modo de entrante o simplemente de tentempié. Los quesos canarios gozan de una buena crítica internacional, entre otras cosas, por su suavidad y por su sabor, dotándolos de una personalidad que los diferencia de otros quesos europeos. En concreto, el queso tinerfeño curado de cabra con coberturas de pimentón y gofio de la Quesería de Arico ha resultado premiado en su categoría como mejor queso del mundo en la final de los "World Cheese Awards 2008" celebrados en Dublín.

Uno de los últimos estudios realizados revela que en Tenerife se producen aproximadamente 3400 toneladas al año, lo que supone el 50% de la producción de la provincia y un 25% de todo el Archipiélago. A día de hoy existen 75 queserías artesanales, según el Registro General Sanitario de Alimentos.

Actualmente los quesos de Tenerife disponen de una marca de garantía potenciada por la Fundación Tenerife Rural para homologar su calidad. Con esta marca de garantía, se intenta dar a conocer las principales cualidades de los quesos, valorizar el producto y mejorar su comercialización.

El gofio es uno más de los elementos tradicionales de la cocina canaria y particularmente de Tenerife. Se realiza con granos de cereales que son tostados y posteriormente molidos. El género de mayor consumo en la isla es el de trigo, aunque existen otros tipos como el de millo o en menor medida el de garbanzo. Es también relativamente frecuente aquel de tipo mixto, trigo-millo. Desde incluso antes de la conquista de Canarias ya servía de sustento para los guanches. En posteriores tiempos de hambruna y escasez de alimentos formó parte de la dieta popular canaria. Hoy en día se utiliza como plato único (gofio escaldado) o como complemento en platos de distinta índole: carnes, pescados, potajes, postres. También son típicas las denominadas pellas de gofio, en las que este ingrediente principal se amasa junto a otros (miel, azúcar, agua, almendras, pasas, queso, etc.) y son servidas a modo de pequeñas formas redondeadas. Las pellas de gofio son frecuentemente degustadas en romerías, ferias tradicionales y vendimias. Incluso algún cocinero de prestigio ha confeccionado helados de gofio recibiendo buena crítica al respecto.
La repostería en Tenerife se encuentra representada y fuertemente influenciada por la repostería palmera, con exquisiteces como el bienmesabe, la leche asada, el Príncipe Alberto, el frangollo, los huevos moles, el quesillo y un largo etcétera. Del mismo modo, los rosquetes, las truchas, y diversos tipos de pasteles, entre los que se encuentran los laguneros y los singulares rosquetes de Guía de Isora, forman parte de este capítulo del recetario.

El cultivo de la vid en el archipiélago y especialmente en Tenerife nace tras la conquista, cuando los colonizadores traen variedades de viñas y comprueban la nobleza que adquirían los caldos canarios. En los siglos XVI y XVII, el vino alcanza un gran peso en la economía tinerfeña pues son muchas las familias que se dedicaban a su cultivo y posterior negocio. Especial mención merece el malvasía canario, que llegó a ser considerado el mejor vino del mundo y era ansiado por las cortes europeas y las mayores bodegas de Europa y América. Escritores como William Shakespeare o Walter Scott hacen referencia en algunas de sus obras a estos vinos. La isla presenta actualmente cinco denominaciones de origen: Abona, Valle de Güímar, Valle de La Orotava, Tacoronte-Acentejo e Ycoden-Daute-Isora.

Canarias, y por ende Tenerife, no tienen grandes niveles de contaminación atmosférica gracias, por un lado, a la escasez de fábricas e industrias y, por otro, al régimen de los vientos alisios que alejan las masas de aire contaminadas de las islas. Según datos oficiales ofrecidos por las consejerías de Sanidad y de Industria, Tenerife es uno de los lugares de España con menor índice de contaminación atmosférica con un promedio medio-bajo. No obstante, las principales fuentes contaminantes de la isla son las centrales térmicas de Las Caletillas y Granadilla, el tráfico rodado y la refinería de Santa Cruz de Tenerife.

Además, en la isla de Tenerife al igual que en la de La Palma debe controlarse la contaminación lumínica, por su afección a los observatorios astrofísicos situados en las cumbres de estas islas.
En cuanto a lo concerniente a la calidad de las aguas, aquellas de consumo se encuentran todas calificadas como aptas, aunque en algunos puntos tiene un sabor considerado desagradable, por tener origen en acuíferos salinizados o en agua marina desalada. Algo similar ocurre en referencia a las aguas de baño, ya que todas las playas de la isla de Tenerife han sido catalogadas por el Ministerio de Sanidad y Consumo como aguas aptas para el baño de muy buena calidad.


En los últimos años, la isla ha sido receptora de rodajes de superproducciones cinematográficas. Algunos de los rodajes más importantes realizados en la isla son:







Igualmente, la isla ha sido escogida para grabaciones musicales y como escenario paisajístico para videoclips:










Entre las obras literarias que tienen por trasfondo la isla o se hace alusión a ella, destacan: "La señorita de compañía" y "El hombre del mar", ambos de Agatha Christie, "La cueva de las mil momias" de Alberto Vázquez-Figueroa, "El picnic de los ladrones" de Leslie Charteris, "El Sarcófago de las tres llaves" de Pompeyo Reina Moreno y "Atentado" de Mariano Gambín, entre otras.

Existen diferentes lugares en el mundo que comparten con esta isla el topónimo de Tenerife, en su mayoría por la presencia de isleños en esos lugares. También la isla ha dado nombre a lugares situados incluso fuera de la Tierra:




</doc>
<doc id="6475" url="https://es.wikipedia.org/wiki?curid=6475" title="Provincia de Santa Cruz de Tenerife">
Provincia de Santa Cruz de Tenerife

La provincia de Santa Cruz de Tenerife, también llamada solo provincia de Santa Cruz, es una provincia española de la comunidad autónoma de Canarias, formada por las islas de La Palma, La Gomera, El Hierro y Tenerife, así como por una serie de roques adyacentes (como los de Salmor, Fasnia, Bonanza, Garachico y Anaga). La provincia de Santa Cruz de Tenerife es la provincia española más occidental y meridional. Además es la tercera provincia española más montañosa por desnivel del terreno, aunque la primera en altitud máxima, por encontrarse en ella la máxima altitud del país, el pico Teide (3718 m s. n. m.). 

En 2019, la provincia contaba con 1 032 983 habitantes. Es la segunda provincia canaria en población, aunque la primera en densidad del archipiélago. Además, es la provincia cuya población más crece porcentualmente de Canarias. La capital de la provincia es la ciudad de Santa Cruz de Tenerife, situada en la isla de Tenerife, la isla más poblada de Canarias y la más poblada de España. También es la capital de la comunidad autónoma de Canarias junto a Las Palmas de Gran Canaria.

La provincia surgió en 1833 con la división de España en 49 provincias, en la que Canarias constaba de una única provincia con capital en Santa Cruz de Tenerife.

No obstante, en 1927 la mitad oriental se separó para formar la provincia de Las Palmas y la Provincia de Canarias se la denominó Provincia de Santa Cruz de Tenerife, pues dejó de englobar a todo el archipiélago. A partir de ese año, fue emitido un decreto por el cual la ciudad de Santa Cruz de Tenerife debía compartir la capitalidad del archipiélago con Las Palmas de Gran Canaria, que es como permanece en la actualidad.

Actualmente fuera de Canarias, esta provincia es también conocida con la denominación de Tenerife, que en realidad es su isla más poblada y extensa.

A los naturales de la isla de Tenerife se les denomina "tinerfeños" o "chicharreros", aunque esta última no es del todo correcta, ya que realmente los chicharreros son solamente los naturales de la ciudad de Santa Cruz de Tenerife; a los de La Palma, palmeros; a los de El Hierro, herreños y a los de La Gomera, gomeros o colombinos.

Las islas que integran la provincia de Santa Cruz de Tenerife suelen ser comúnmente denominadas ""islas occidentales"", para diferenciarlas de las islas de la provincia de Las Palmas, denominadas ""islas orientales"" por su situación geográfica al este de esta provincia. En ocasiones, también se ha utilizado para la provincia de Santa Cruz de Tenerife la denominación de ""Islas Nivarienses"", aunque esta denominación surge por influencia de la diócesis católica de esta provincia, llamada "Diócesis Nivariense". En este sentido, la provincia de Las Palmas por su parte se denomina como ""Islas Canarienses"", también por el nombre de su diócesis.

A diferencia de la mayoría de las provincias de España, la provincia de Santa Cruz de Tenerife (al igual que la de Las Palmas) carece de un órgano administrativo común para toda la provincia. Las competencias que normalmente ostentan las Diputaciones Provinciales se las reparten entre el Gobierno de Canarias y los Cabildos Insulares. El "Boletín Oficial de la Provincia de Santa Cruz de Tenerife" es editado por el Gobierno de Canarias.

La Provincia de Santa Cruz de Tenerife está formada por cuatro islas mayores; Tenerife, La Palma, La Gomera y El Hierro. Y una serie de roques e islotes deshabitados; Salmor, Fasnia, Bonanza, Garachico y Anaga.

Las islas mayores son:

Tenerife es la isla más extensa de Canarias, con una superficie de 2034,38 km². Además, es la más poblada del archipiélago, con 917 841 habitantes (2019) y una densidad de población de 445 hab./km², y la isla más poblada de España. Los municipios más poblados de la isla son Santa Cruz de Tenerife (222 643 habitantes), San Cristóbal de La Laguna (152 222 habitantes) y Arona (79 377 habitantes). La ciudad de Santa Cruz de Tenerife (206 325 habitantes) es sede del Parlamento de Canarias, de la Capitanía General de Canarias y del Cabildo de Tenerife. Es además la ciudad más poblada del municipio y de la provincia y capital insular, provincial y de la Comunidad Autónoma de Canarias conjuntamente con Las Palmas de Gran Canaria. La ciudad de La Laguna (32 078 habitantes) es la segunda más poblada de la isla y tercera del archipiélago, está declarada Patrimonio de la Humanidad por la Unesco, y en ella tiene sede la Universidad de La Laguna. Por su parte, también en esta ciudad se encuentra la sede del Consejo Consultivo de Canarias, que es el supremo órgano consultivo de la Comunidad Autónoma de Canarias. Destacan también, por su importancia turística, otros cuatro municipios: La Orotava, Puerto de la Cruz en el norte, y Arona y Adeje en el sur. Hay que citar además la Villa Mariana de Candelaria, donde se encuentra la imagen de Virgen de Candelaria, Patrona del Archipiélago Canario. La patrona de la Diócesis Nivariense (que engloba la provincia de Santa Cruz de Tenerife) es la Virgen de los Remedios, que se venera en La Laguna. Tenerife es conocida, en virtud de su clima, como "la isla de la eterna primavera", y cuenta con diversas playas de arena fina oscura volcánica y diversos parques naturales. Entre otros espacios naturales protegidos, alberga el parque nacional del Teide, también declarado Patrimonio de la Humanidad por la Unesco: es uno de los parques nacionales más visitados del mundo y en él se encuentra el pico del Teide, que con sus 3718 m s. n. m. representa el techo de España y el tercer volcán más grande del mundo desde su base. Además destacan también en la isla otros espacios naturales de gran valor ecológico, como el parque natural de la Corona Forestal, es cuál es el mayor espacio natural protegido de las Islas Canarias, y los parques rurales de Anaga (en el este) y de Teno (en el oeste de la isla). El Macizo de Anaga es el lugar que cuenta con mayor cantidad de endemismos de Europa y que es Reserva de la Biosfera por la UNESCO desde 2015. Geológicamente es importante también destacar la Cueva del Viento, la cual es el tubo volcánico más grande de la Unión Europea y uno de los más grandes del mundo, de hecho es el quinto. La isla es conocida internacionalmente por su carnaval considerado el segundo más importante del mundo.

La Palma, con 82 671 habitantes (2019), sus 708,32 km² son en su totalidad Reserva de la Biosfera. Ha tenido actividad volcánica reciente, apreciable en el volcán Teneguía, que entró en erupción por última vez en 1971. Además, es la segunda isla más alta de Canarias, con el Roque de los Muchachos (2426 metros) como punto más elevado. Este pico se halla en los límites del parque nacional de la Caldera de Taburiente, y en sus inmediaciones está emplazado el Observatorio del Roque de los Muchachos del Instituto de Astrofísica de Canarias: en él se encuentra el Gran Telescopio Canarias, que con su espejo principal de 10,40 m de diámetro se cuenta entre los telescopios ópticos más grandes del planeta. Por su exuberante vegetación, La Palma es conocida también como la "Isla Bonita". Su capital es Santa Cruz de La Palma (13 842 en la capital, 17 084 habitantes en el municipio), ciudad donde tiene su sede el Diputado del Común del Parlamento de Canarias (cargo equivalente al Defensor del Pueblo, pero a nivel autonómico), y el municipio más poblado es Los Llanos de Aridane (20 766 habitantes).

La Gomera tiene una superficie de 369,76 km² y es la segunda isla menos poblada de las siete mayores, con 21 503 habitantes (2019). Geológicamente es una de las más antiguas del archipiélago. La capital insular es San Sebastián de La Gomera (8965 habitantes). En La Gomera se encuentra el parque nacional de Garajonay, declarado por la Unesco en 1986 Patrimonio de la Humanidad, que representa un buen ejemplo de bosque de laurisilva. Por su parte el Silbo gomero (lenguaje silbado practicado por algunos habitantes de la isla), también es Patrimonio de la Humanidad desde 2009. La isla fue el último territorio que tocó Cristóbal Colón antes de llegar a América en su viaje de descubrimiento de 1492: por ello es también conocida como la "Isla Colombina".

El Hierro es la isla más occidental del archipiélago, y entre las islas principales, es la más pequeña, con 268,71 km², y la menos poblada, con 10 968 habitantes (2019). Su capital es Valverde (4995 habitantes). Toda la isla fue declarada Reserva de la Biosfera en 2000. Es conocida por sus ejemplares de sabina inclinados por el viento; por el antiguo Garoé o Árbol Santo; por sus lagartos gigantes, y porque en el pasado el meridiano 0º tomaba como referencia la Punta de Orchilla, situada en el oeste de la isla. Desde el siglo XVIII, se viajaba a esta isla para tomar las aguas curativas del Pozo de Sabinosa, o Pozo de la Salud. Aquí nació la "cantadora" y tamborilera Valentina la de Sabinosa, figura del folclore canario. La isla también destaca por sus fondos marinos y sus centros de buceo. Entre octubre de 2011 y marzo de 2012 tuvo lugar una erupción submarina, ya concluida. El cono volcánico submarino se encuentra a 88 metros de profundidad, cerca de la localidad de La Restinga, en el Mar de las Calmas. La erupción de El Hierro de 2011 fue noticia en todos los medios de comunicación.

El crecimiento de la población en las últimas décadas se centra fundamentalmente en la isla de Tenerife, sobre todo en la costa sur y el área metropolitana de Santa Cruz-La Laguna, mientras que la costa norte de Tenerife pierde población en los últimos años. el resto de las islas se puede decir que sufren un estancamiento de la población, con crecimientos o pérdidas muy leves.
La provincia de Santa Cruz de Tenerife es la 43.ª de España en que existe un mayor porcentaje de habitantes concentrados en su capital (20,11 %, frente a 31,96 % del conjunto de España).

La provincia de Santa Cruz de Tenerife cuenta con 3 parques nacionales, siendo la provincia española que más parques nacionales tiene: (Las Cañadas del Teide, La Caldera de Taburiente y Garajonay). Tres lugares en Canarias han sido declarados Patrimonio de la Humanidad por la UNESCO, y todos ellos se encuentran en esta provincia, estos son: parque nacional de Garajonay en 1986 y el parque nacional del Teide en 2007. También la ciudad de San Cristóbal de La Laguna fue declarada por la Unesco en 1999, Patrimonio de la Humanidad. Además son declaradas en su totalidad Reservas de la Biosfera las islas de La Palma en el año 1983, aunque posteriormente en 1997 y 2002 se le han añadido terrenos a la reserva hasta alcanzar la totalidad de la isla, El Hierro en el año 2000 y La Gomera en 2012. Además, desde 2015 el Macizo de Anaga en Tenerife también tiene la consideración de Reserva de la Biosfera.


</doc>
<doc id="6477" url="https://es.wikipedia.org/wiki?curid=6477" title="Iberia (aerolínea)">
Iberia (aerolínea)

Iberia (IATA: IB, OACI: IBE) es la aerolínea bandera de España, fundada en 1927 con el nombre de Iberia, Compañía Aérea de Transporte. Su denominación social actual es Iberia Líneas Aéreas de España, S. A. Operadora Unipersonal. Actualmente es una de las compañías aéreas más antiguas del mundo. Tiene su sede social en Madrid y cotizó en la Bolsa de Madrid desde abril de 2001 hasta enero de 2011, cuando fue sustituida por su matriz International Airlines Group, producto de la fusión con British Airways en 2011.

Su principal base es el Aeropuerto Adolfo Suárez Madrid-Barajas. En 2010, la aerolínea obtuvo 89 millones de euros de beneficio (frente a las pérdidas netas de 273 millones de € de 2009, los 32 millones de € de 2008 y los 327 millones de € de 2007, los 56,7 millones de € de beneficios en 2006 y los 395 millones de € de beneficios en 2005) y transportó 24,3 millones pasajeros. El Grupo Iberia vuela a 108 destinos en 42 países. El 12 de noviembre de 2009, Iberia confirmó que había llegado a un acuerdo preliminar de entendimiento con British Airways para fusionarse con ella con la firma de un acuerdo vinculante, que solo se dio paso cuando el valor de las acciones de British Airways superaron a las de Iberia, siendo en abril de 2010 la firma del contrato de fusión. En julio de 2010 la Comisión Europea aprobó la operación. Para su culminación, en noviembre de 2010, los accionistas de Iberia y British Airways dan el visto bueno a la fusión de las dos compañías, último requisito necesario para llevar a cabo la fusión de uno de los grupos más potentes del sector.

La nueva sociedad "holding" resultante, llamada International Airlines Group (IAG), es la sexta aerolínea del mundo por ingresos y tercera de Europa. Actualmente Iberia posee una filial (Iberia Express) y tiene una compañía franquiciada (Air Nostrum).
Fue fundada el 28 de junio de 1927 por el empresario vizcaíno Horacio Echevarrieta y la alemana Lufthansa durante la dictadura de Primo de Rivera como monopolio del transporte aéreo español. Estaba previsto que el primer viaje comercial de Iberia fuese el 14 de diciembre de 1927 entre Madrid y Barcelona, con Alfonso XIII como pasajero de excepción. Sin embargo, fue un vuelo Barcelona-Madrid el primero que operó Iberia al salir dos horas antes que el vuelo oficial. La intención era que Alfonso XIII pudiese presenciar el primer aterrizaje en el Aeropuerto de Carabanchel, actual aeropuerto de Cuatro Vientos, pero cuestiones meteorológicas obligaron a este avión a llegar más tarde de lo previsto. En 1928 había crecido, con tres aviones Rohrbach Ro VIII Roland, que eran unos trimotores con capacidad para diez pasajeros.

En 1929 fue forzada a aportar sus rutas y aviones a la recién creada CLASSA, a instancias del dictamen del Directorio Militar para formar un monopolio con una sola compañía que agrupara todas las existentes por entonces en España. Iberia retenía parte del accionariado de CLASSA. Tras la proclamación de la Segunda República, CLASSA fue disuelta y se creó así LAPE, que absorbió todas las rutas y bienes de CLASSA. Todos los accionistas de CLASSA recibieron una indemnización por ello. Durante todos los años de existencia de CLASSA y LAPE, Iberia fue una sociedad durmiente sin actividad real, pero pese a ello, presentaba anualmente su balance de cuentas en el registro mercantil.

En 1937, durante la Guerra Civil Española Iberia fue reactivada y se convirtió en la línea aérea del bando sublevado, con sede en Salamanca , volando algunos Dragon Rapide y Junkers 52. Hasta 1939, en que realizó su primer vuelo entre Madrid y Lisboa, fue una aerolínea de ámbito exclusivamente nacional. A Lisboa le seguirían Londres y París. Iberia fue nacionalizada en 1944, pasando a formar parte del Instituto Nacional de Industria. El 22 de septiembre de 1946 se convierte en la primera aerolínea en volar entre Europa y América del Sur, mediante el establecimiento del itinerario entre Madrid y Buenos Aires, con escalas en Villa Cisneros, Natal y Río de Janeiro. El avión usado para la ocasión fue un Douglas DC-4, y en él también volaron las primeras azafatas. En 1954 se inauguró el vuelo entre Madrid y Nueva York. Para ello se utilizó un aparato Lockheed Constellation.

En el segundo semestre de 1961 Iberia empezó a utilizar aviones de reacción. Así, se fueron incorporando tres aparatos Douglas DC-8 para ser explotados en los itinerarios de largo radio. Poco después, en 1962, se adquirieron aparatos franceses Caravelle, fabricados por Sud Aviation, para cubrir las líneas europeas. La flota de Iberia a partir de entonces fue creciendo, impulsada por el fuerte auge del aumento de pasaje turístico y por el crecimiento de la renta disponible en España y en el conjunto de la Europa occidental.

El trayecto escogido fue el de Madrid a Barcelona. En esta época no existía la actual congestión de tráfico, con lo que el servicio consistía en un avión en Madrid y otro en Barcelona que tan pronto se llenaban, se ponían rumbo al otro punto de la ruta. Sin embargo, con el aumento del tráfico en los aeropuertos hubo que desechar este funcionamiento y poner unas frecuencias fijas de salida en horas puntas, política que inauguraba a la Monarquía Española. La transición política hacia el sistema democrático convenció a los nuevos responsables del INI de que eran necesarios llevar a cabo una nueva identidad corporativa en la compañía aérea que impulsara la nueva imagen de España en los mercados mundiales.

El 10 de septiembre de 1981 llegó en un avión de la compañía el Guernica de Picasso.
En 1987 se estrenó el sistema informático Amadeus, el sistema de reserva informatizado más grande del mundo hasta la actualidad. La iniciativa correspondió a Iberia junto a sus socios en este negocio, Deutsche Lufthansa y Air France.

Entre los últimos años de la década de 1980 y principios de la década de 1990, Iberia realizó una enorme modernización en su flota. Los nuevos McDonnell Douglas MD-87, Airbus 320, Airbus 340 y Boeing 757 reemplazaron a los antiguos Douglas DC-9, Douglas DC-10 y Boeing 727. En 1991 se creó el primer programa de carácter internacional que se implantó en Europa. En 1992, Iberia se convirtió en patrocinador oficial de los Juegos Olímpicos de Barcelona y de la Exposición Universal de Sevilla.

A comienzos de la década de 1990 los directivos del Instituto Nacional de Industria (INI), propietario de la mayoría del capital de Iberia, plantearon una estrategia de crecimiento de la compañía, con el objetivo de preparar la liberalización de los mercados aéreos en la Unión Europea, prevista para 1994. La expansión de la compañía se hizo en el mercado latinoamericano. Se llevó a cabo la adquisición de una parte de la propiedad de Aerolíneas Argentinas, de la venezolana Viasa y el 35% de la chilena Ladeco, Líneas Aéreas del Cobre. El resultado de esta estrategia, que preveía dotar de activos a Iberia frente a un movimiento de fusiones entre las líneas de bandera europea, fue un fracaso. La pésima gestión de Aerolíneas, las dificultades para poner en marcha una gestión de la compañía desde Iberia, además del pésimo entorno del mercado creado por la Guerra del Golfo en 1991, abocaron a una situación insostenible para la aerolínea española. Las pérdidas fueron cuantiosas para Iberia. El INI, a través del Gobierno español, tuvo que acometer dos ampliaciones de capital. La autorización de la Comisión Europea para que el Estado español aportara capitales logró sacar a Iberia de la bancarrota técnica en la que estuvo inmersa en 1994. Poco después, se puso en marcha el proceso de reajuste de Iberia, que pasó por el desarrollo de cuatro puntos: cambio de directivos y gerentes, salida ordenada de las participadas latinoamericanas, profunda reducción de costes y reajuste interno e inicio del proceso de privatización definitiva de la aerolínea española. En 1999 pasó a formar parte de la alianza Oneworld, junto con las aerolíneas British Airways y American Airlines, entre otras.

El año 2001 marcó también un antes y un después en la historia de la compañía. Con su salida a bolsa en abril de ese año culminaba el proceso de privatización de la compañía y volvía al ámbito privado, en el que nació, aunque fuera pública la mayor parte de su historia. Al año siguiente pasa a formar parte del Ibex 35, en el que cotizó hasta 2011 por su fusión con British Airways. En 2002 la compañía vendió su filial Binter Canarias.

El 25 de octubre de 2008 Iberia retiró de su flota a las aeronaves de la compañía MD, en concreto el modelo MD-88, en dicho día se realizó el último vuelo del mencionado avión. La ruta seguida fue Alicante-Madrid.

Iberia tiene una elevada cuota de mercado en las rutas españolas y entre Europa y Latinoamérica.

En la actualidad, Iberia LAE Operadora vuela a 76 destinos en 47 países desde su base en el Aeropuerto Adolfo Suárez Madrid-Barajas. El Grupo Iberia vuela a 126 destinos de 47 países y en código compartido con otras aerolíneas vuela a 224 destinos en 50 países. Con una flota de 148 aviones, realiza unos 1000 vuelos diarios. En el año 2010 transportó 24,4 millones de pasajeros y 262 402 toneladas de carga. En 2005 introdujo la nueva clase Business Plus en sus aviones A340, que fue mejorada en 2009-2010 con unos asientos que se sitúan en posición totalmente horizontal y menús diseñados por reputados chef españoles.

Iberia realiza mantenimiento aeronáutico, actividad que presta a su propia flota y a la de otras 100 empresas más, incluidas algunas de las más importantes de Europa, en su base de mantenimiento de La Muñoza, en Barajas. Iberia es el primer operador de "handling" (asistencia a aviones y pasajeros) en todos los aeropuertos de España; tiene entre sus clientes a más de 200 compañías aéreas.

Por otro lado, Iberia fue fundadora y tiene todavía un porcentaje de la propiedad de Amadeus, un sistema de reservas informáticas. Además, junto con Swissair creó Iberswiss, empresa que producía más de 14 millones de bandejas de comida al año y que posteriormente vendió a Gate Gourmet, una empresa en el sector del cáterin aéreo. Participa en el negocio de los viajes turísticos a través de los turoperadores Viva Tours y en el transporte urgente con Cacesa.

La alianza alcanzada con American Airlines y con British Airways, junto con la entrada en Oneworld, sitúan a Iberia en uno de los grandes grupos que se están formando de cara a competir en un mercado global.

Iberia franquicia su marca a otra aerolínea independiente, Air Nostrum, que es propiedad de Nefinsa y Caja Duero, y que opera una red propia de vuelos regionales que son comercializados por Iberia.

En 2006 Iberia cerró su base de operaciones de Barcelona y creó una línea aérea de bajo coste denominada Clickair, con base en el aeropuerto de Barcelona. Un año después de su primer vuelo, Clickair se convirtió en la aerolínea líder en el aeropuerto de Barcelona. En 2009 Clickair se fusionó con su rival Vueling. Iberia tiene el 45 % del accionariado de la sociedad resultante, que recientemente ha sido ampliado a más del 90 %.

EL 15 de octubre de 2013, Iberia presentó su nuevo logotipo y eslogan. El primer avión con el nuevo logotipo entró en servicio a finales del 2013.

En mayo de 2017, Iberia implementará la nueva cabina Turista Premium para vuelos intercontinentales. La aerolínea se convierte en la primera compañía aérea española en ofrecer este modelo de cabina, producto intermedio entre la cabina económica y ejecutiva Business Plus.

En 2020 su presidente Luis Gallego afirmó que la flota podría reducirse debido a la menor demanda, fruto de los efectos de la pandemia COVID-19 .

El 9 de Septiembre de 2020 sucede en el cargo de Presidente a Luis Gallego, Javier Sánchez-Prieto.

En julio de 2009, Iberia y la aerolínea británica British Airways comenzaron negociaciones para fusionarse. Se trataba fundamentalmente de una fusión a nivel económico ya que ambas empresas mantendrían sus propias marcas dentro de la nueva aerolínea, que se convertiría en uno de los grupos aéreos internacionales más grandes del sector: la tercera compañía aérea a nivel mundial. Se mantendrán ambas marcas según las conocidas estructuras de nacionalidad de ambas compañías, publicadas en el Documento de Registro de IAG S.A, para evitar la pérdida de derechos de vuelo, sobre todo hacia países de Latinoamérica, ya que éstos son acuerdos bilaterales entre estados.

En julio de 2009, Antonio Vázquez Romero relevó a Fernando Conte como presidente de Iberia.

El 12 de noviembre de 2009 los consejos de administración de Iberia y British Airways dieron por fin el visto bueno a la fusión. Se creó una nueva sociedad, "International Airlines Group", propietaria de British Airways e Iberia. La sede social y fiscal del nuevo "holding" se sitúo en Madrid, mientras que la financiera estaba en Londres. A mediados de noviembre de 2010 Iberia sometió el acuerdo a sus respectivos accionistas para su aprobación, ejecutándose la operación aproximadamente un mes después de su aprobación. El 20 de enero de 2011 Iberia y British Airways dejaron de cotizar en bolsa para ser sustituidas por IAG desde el día 24.
Tras la fusión entre Iberia y British Airways en enero de 2011, ambas aerolíneas pertenecen al grupo aéreo International Airlines Group (IAG), que cotiza en la bolsa de Londres y en el Ibex 35. Su sede social se encuentra en Madrid y la financiera y operativa en Londres.

El 6 de octubre de 2010 Iberia, British Airways y American Airlines anunciaron oficialmente, tras recibir la aprobación de la Unión Europea, su Acuerdo de Negocio Conjunto en la explotación de las rutas aéreas del Atlántico Norte.
Durante 2010 Iberia volvió a los beneficios, después de que en el año 2009 se registraran pérdidas tras 13 años consecutivos de resultados positivos. En concreto, el beneficio neto ascendió a 89 millones de euros, frente a los 273 millones de pérdidas de 2009. Sin embargo, el resultado de explotación fue ligeramente negativo – tres millones de euros –.

En noviembre de 2010, Iberia anunció la reapertura de la base en el aeropuerto de Barcelona basando un Airbus A340 para iniciar operaciones hacia América. El 29 de marzo de 2011 se abrió oficialmente la base en Barcelona operando la ruta Barcelona-Miami tres veces por semana, la aerolínea también anunció la apertura el 19 de junio de la ruta Barcelona-São Paulo, la cual se inauguró con un 100 % de ocupación.

En octubre de 2011 el consejo de administración de Iberia aprobó la creación de una filial para los vuelos de corto y medio radio, que se denominaba Iberia Express. El capital de la nueva filial era al 100 % de Iberia. Inició sus operaciones en el mes de abril de 2012. Según la compañía, el objetivo de la nueva filial es conseguir unos costes de producción más bajos en el negocio de corto y medio radio, que es deficitario en la actualidad.

Diversos sindicatos han recogido que desde que se hizo efectivo el proceso de integración se ha favorecido a British Airways, la cual aumentó su oferta en España en un 23 %, aunque en realidad lo único que ha hecho es poner vuelos desde Londres a destinos de costa españoles. Iberia ha reducido su oferta en un 15 %, siendo la caída del 4,6 % si se suman los datos de la franquicia Iberia Regional-Air Nostrum y de la aerolínea participada Vueling. Lo cierto es que British Airways ha aumentado su oferta desde su hub de Londres Heathrow por el buen comportamiento de la demanda desde y hacia Londres, mientras que la oferta de Iberia ha tenido que adaptarse al descenso de la demanda en su hub de Madrid por la crisis. Paralelamente, en la Sentencia de la Audiencia Nacional ""los demandantes no han demostrado de ningún modo que se hayan cedido rutas rentables a Iberia Express o a British Airways [...]. Se ha demostrado, por el contrario, que la empresa demandada ha dejado de operar únicamente rutas que no eran rentables"". Iberia y British Airways tienen mercados diferentes, de modo que el que uno crezca en su mercado nunca puede ser en detrimento del otro. Además, Iberia tiene que reducir su oferta para eliminar frecuencias o rutas no rentables y crecerá cuando su cuenta de resultados sea positiva. La fusión ha reportado a Iberia en poco más de un año y medio 109 millones de euros en sinergias.

Los mismos sindicatos dicen que la fórmula de crecimiento de las dos compañías es distinta, ya que mientras la aerolínea británica tiene una previsión de incorporar 39 nuevas aeronaves (de los cuales: 12 Airbus A380, 3 Boeing 777 y 24 Boeing 787) y 800 pilotos. La realidad es que, desde principios de 2013, Iberia está incorporando aviones Airbus A330-300 hasta un total de 8 aeronaves y está renovando los interiores de todos los A-340/600, además de tener la opción de compra para otros 44 aviones.

Asimismo, poco después del anuncio de creación de Iberia Express, desde el Sindicato Español de Pilotos de Líneas Aéreas (SEPLA), criticaron la medida ya que consideran que no es nueva. Citan que anteriores iniciativas similares como Viva Air, Binter Mediterráneo o Clickair hicieron perder cerca de 200 millones de euros a Iberia.

Sin embargo, Clickair, convertida en Vueling, es una compañía de éxito que ahora es participada de Iberia, y fue galardonada con cuatro de los ocho premios que otorga la World Low Cost Airlines Congress el pasado año, además de la distinción principal como mejor aerolínea del año. Siendo 2012 el cuarto año consecutivo que Vueling recibe el reconocimiento del sector.

Asimismo, el SEPLA afirmaba que el objetivo final que existe detrás de la creación de esta aerolínea era la de entregar y regalar la matriz Iberia a la aerolínea británica British Airways. La Audiencia Nacional en su Sentencia del 4 de julio de 2013 reconoce que la situación de la compañía es comprometida, por la crisis económica, los altos precios del combustible y los altos costes laborales, por lo que considera imprescindible «la adecuación de sus costes y plantilla, así como el incremento de productividad». Además avala las decisiones que se han ido tomando por la compañía.


La flota de Iberia —sin contar las filiales Iberia Express, Iberia Regional o Level— promedia una edad de 9.3 años. A septiembre de 2020, está compuesta por las siguientes aeronaves: 

La flota total, contando Iberia Express (24) e Iberia Regional (51), es de 148 aeronaves.

El Grupo Iberia «opera 126 destinos en 47 países diferentes, a los que hay que añadir 224 adicionales en 50 países gracias a acuerdos de código compartido con otras compañías aéreas». Mientras que en los vuelos hacia América tiene una importante presencia, la principal debilidad de Iberia son las rutas a Asia ya que cuenta con pocos destinos propios, siendo la mayoría en código compartido.

Iberia tiene firmados acuerdos de vuelo con código compartido con las siguientes aerolíneas:

El peor accidente de Iberia fue el Vuelo 610 del 19 de febrero de 1985. Un Boeing 727 apodado "Alhambra de Granada", que viajaba de Madrid a Bilbao, chocó contra una antena de televisión instalada en el monte Oiz durante las maniobras de aterrizaje. Fallecieron los 148 ocupantes que viajaban a bordo.






</doc>
<doc id="6480" url="https://es.wikipedia.org/wiki?curid=6480" title="Clítoris">
Clítoris

El clítoris es un órgano del aparato genital femenino cuya función es la de proporcionar placer sexual a la mujer. Su punta o "glande" asoma en la parte superior de la vulva, pero se extiende por el interior de los labios mayores, del perineo y rodea el tercio inferior de la vagina.

El clítoris tiene cinco partes, de las cuales cuatro son bilaterales y simétricas. El "glande", la única parte visible, es el elemento impar y se encuentra cubierto por el prepucio. Los "bulbos vestibulares" se encuentran bajo la piel de los labios mayores y se unen por debajo de la vagina y rodean su entrada y su tercio inferior. Externamente a ellos se localizan los "cuerpos pareados" o "crura", unidos en la línea media y separados solo por un septo fibroso. Todas sus partes componentes incluyen tejido nervioso y tejido eréctil. Los tejidos eréctiles se llenan de sangre durante la respuesta al estímulo sexual, lo que hace aumentar su tamaño.

El glande del clítoris asoma en la parte superior de la vulva. Está parcial o totalmente recubierto, dependiendo del tamaño y el estado de erección o relajación, por un pliegue cutáneo llamado capuchón o prepucio clitorial, formado por la unión, en su parte superior, de los dos labios menores. Es solamente el extremo visible del órgano y se corresponde, ontogénica y morfológicamente al glande del pene masculino.

En él se concentran los nervios que producen placer sexual en la mujer.

El cuerpo del clítoris puede llegar a tener un tamaño de 15 a 23mm en toda su longitud, mientras que el glande clitoridiano mide entre 3 y 4mm de ancho, y entre 4 y 5mm de largo, en estado de reposo; en erección puede alcanzar de 17 a 25mm de longitud. 

En el desarrollo embrionario, hay una fase en que ambos sexos son indistinguibles. El clítoris se desarrolla al mismo tiempo que los demás órganos sexuales externos de la mujer, desde la séptima semana de la vida embrionaria, a partir del tubérculo genital.

La estimulación del clítoris tiene lugar durante la masturbación o el acto sexual. Dependiendo de la situación, como por ejemplo, una pareja heterosexual u homosexual, la postura, etc., se puede estimular de distintas formas, más o menos localizadas o indirectas. Para la masturbación del clítoris por el glande o parte visible, es recomendable hacerlo indirectamente y con lubricación, ya sea producida o artificial. El clítoris al extenderse por dentro puede llevar al orgasmo o clímax también por estimulación interna, es decir, todos los orgasmos son clitorianos. 

En algunos países se practica la infibulación o la ablación —mutilación total o parcial— del glande del clítoris y de otras partes del aparato genital femenino, desde una edad muy temprana, con el fin de imposibilitar el placer sexual de la mujer. Esta práctica, realizada por motivos culturales, religiosos, o por otros no médicos, además de implicar riesgos sanitarios potenciales, no tiene ningún beneficio para la salud y es considerada por las Naciones Unidas como una violación de los derechos humanos de las mujeres y de las niñas. 

Su principal motivación podría encontrarse en el control del deseo y las conductas sexuales de las mujeres, por creer que disminuye la posibilidad de relaciones pre o extramatrimoniales.

El término "clítoris" procede del griego antiguo κλειτορίς ("kleitorís"), que fue reintroducido sin cambios en el Renacimiento. El primer médico antiguo en haberlo descrito y nombrado fue Rufo de Éfeso (siglos - d. C.). Este autor señala que en griego existía un verbo derivado, κλειτοριάζω ("kleitoriázō"), que significaba «acariciar(se) el clítoris para producir placer».

La literatura médica moderna menciona por primera vez la existencia del clítoris hacia el , aunque hay discusiones sobre el momento exacto. Renaldo Columbus, también conocido como Mateo Realdo Colombo, fue un profesor de cirugía en la Universidad de Padua, en Italia, y publicó en 1559 un libro, llamado "De re anatomica", en el que describió «la sede del placer femenino». Columbus concluyó que «como nadie ha descubierto estos detalles y su propósito, si se permite que le dé nombres a cosas que descubro, debería ser llamado “el amor o dulzura de Venus”».

La aseveración de Columbus fue rechazada por su sucesor en la universidad, Gabriele Falloppio, quien describió por primera vez las trompas de Falopio, también denominadas "tubas uterinas", y se adjudicó el haber sido el primero en describir el clítoris. En el , el anatomista holandés Caspar Bartholin –véase glándulas de Bartolino– rechazó ambas pretensiones, diciendo que el clítoris ya era ampliamente conocido por la ciencia médica desde el .

Durante la época victoriana del , las mujeres que padecían problemas uterinos, hormonales o emocionales eran diagnosticadas con una supuesta enfermedad llamada histeria femenina, la cual no tenía remedio y solo podía ser aminorada por medio de masajes de clítoris, equivalentes a lo que hoy en día reconocemos como masturbación. Los médicos manipulaban la vulva de la paciente hasta que esta alcanzaba el orgasmo, momento en que se aplacaban los síntomas de su mal. La lista de síntomas asociados con este mal era tan larga que llegó un momento en que se convirtió en una epidemia; casi cualquier dolencia leve podía servir para diagnosticar histeria. 

El ginecólogo William Masters y la sexóloga Virginia Johnson, conocidos popularmente por sus dos apellidos juntos, Masters y Johnson, y pioneros del estudio de la respuesta sexual humana, efectuaron estudios sobre el clítoris.

En 1998, la uróloga australiana Helen O’Connell describió por primera vez la anatomía completa del clítoris con todos sus elementos. En esa descripción se incluyen los bulbos cavernosos, se menciona su relación con la uretra y la vagina y se detalla su vascularización.

Existen debates sobre si es un órgano vestigial, una adaptación o si tiene funciones reproductivas. Geoffrey Miller ha dicho que el clítoris humano «no muestra indicios de haber evolucionado por selección preferente, es decir, por la elección directa de los machos sobre hembras con un carácter determinado. No es especialmente grande, ni tiene colores brillantes, o una forma específica, ni está exhibido en la selección durante el cortejo».



</doc>
<doc id="6481" url="https://es.wikipedia.org/wiki?curid=6481" title="1701">
1701

1701 (MDCCI) fue un año común comenzado en sábado según el calendario gregoriano. Fue el primer año del siglo XVIII y el segundo año de la década de 1700.






</doc>
<doc id="6484" url="https://es.wikipedia.org/wiki?curid=6484" title="Tim Berners-Lee">
Tim Berners-Lee

Timothy "Tim" John Berners-Lee (Londres, Reino Unido, 8 de junio de 1955), conocido como Tim Berners-Lee, es un científico de la computación británica, conocido por ser el padre de la World Wide Web. Estableció la primera comunicación entre un cliente y un servidor usando el protocolo HTTP en noviembre de 1989. En octubre de 1994 fundó el Consorcio de la World Wide Web (W3C) con sede en el MIT, para supervisar y estandarizar el desarrollo de las tecnologías sobre las que se fundamenta la Web y que permiten el funcionamiento de Internet.

Ante la necesidad de distribuir e intercambiar información acerca de sus investigaciones de una manera más efectiva, Berners-Lee desarrolló las ideas fundamentales que estructuran la web. Él y su grupo crearon lo que por sus siglas en inglés se denomina Lenguaje HTML (HyperText Markup Language") o lenguaje de etiquetas de hipertexto, el protocolo HTTP (HyperText Transfer Protocol") y el sistema de localización de objetos en la web URL ("Uniform Resource Locator").

Es posible encontrar muchas de las ideas plasmadas por Berners-Lee en el proyecto Xanadú (que propuso Ted Nelson) y el memex (de Vannevar Bush).

Tim Berners-Lee nació en el sudoeste de Londres, Reino Unido, el 8 de junio de 1955. Sus padres eran Conway Berners-Lee y Mary Lee Woods. Sus padres eran matemáticos británicos y . Por ello, la orientación profesional le venía de familia, ya que sus padres se habían conocido en el proyecto de desarrollo del Ferranti Mark I, el primer ordenador comercial con programa almacenado desarrollado por la empresa Ferranti en marzo de 1951. Su padre, Conway Berners-Lee, y su madre, Mary Lee Woods, también fueron especialistas en computación; trabajaron con el equipo que construyó el Ferranti Mark l.

Comenzó en la escuela primaria Sheen Mont y luego pasó al Emanuel School ambas en Londres, de 1969 a 1973. Estudió en Queen's College, de la Universidad de Oxford, de 1973 a 1976, donde recibió un título de primera clase de Física. Conoció a su primera esposa en este tiempo. En 1978, trabajó en D.G. Nash Limited (también en Poole) donde escribió un sistema operativo.

Berners-Lee trabajó en el CERN desde junio hasta diciembre de 1980. Durante ese tiempo, propuso un proyecto basado en el hipertexto para facilitar la forma de compartir y la puesta al día de la información entre investigadores. En este periodo también construyó un programa llamado ENQUIRE que no llegó a ver la luz.

Después de dejar el CERN, en 1980, se fue a trabajar a la empresa de John Poole Image Computer Systems Ltd., pero regresó al CERN otra vez en 1984. 

En 1989, el CERN era el nodo de Internet más grande de Europa y Berners-Lee vio la oportunidad de unir Internet y el hipertexto (HTTP y HTML), de lo que surgiría la World Wide Web. Desarrolló su primera propuesta de la Web el 12 de marzo de 1989, pero no tuvo mucho eco, por lo que en 1990 y con la ayuda de Robert Cailliau, hicieron una revisión que fue aceptada por su gerente, Mike Sendall. Usó ideas similares a las que había usado en el sistema Enquire, para crear la World Wide Web, para esto diseñó y construyó el primer navegador (llamado WorldWideWeb y desarrollado con NEXTSTEP) y el primer servidor Web al que llamó httpd (HyperText Transfer Protocol daemon).

El primer servidor Web se encontraba en el CERN y fue puesto en línea el 6 de agosto de 1991. Esto proporcionó una explicación sobre lo que era el World Wide Web, cómo uno podría tener un navegador y cómo establecer un servidor Web. Este fue también el primer directorio Web del mundo, ya que Berners-Lee mantuvo una lista de otros sitios Web aparte del suyo. Debido a que tanto el software del servidor como del cliente fue liberado de forma gratuita desde el CERN, el corazón de Internet Europeo en esa época, su difusión fue muy rápida. El número de servidores Web pasó de veintiséis en 1992 a doscientos en octubre de 1995 lo que refleja cual fue la velocidad de la difusión de internet.

En 1994 entró en el Laboratorio de Ciencias de la Computación e Inteligencia Artificial del Massachusetts Institute of Technology. Se trasladó a EE. UU. y puso en marcha el W3C, que dirige actualmente. El W3C es un organismo internacional de estandarización de tecnologías Web dirigido conjuntamente por el Instituto Tecnológico de Massachusetts, el ERCIM francés y la Universidad de Keiō en Japón. Este organismo decidió que todos sus estándares fuesen libres, es decir, que los pudiese utilizar todo el mundo libremente sin coste alguno, lo que sin lugar a dudas fue una de las grandes razones para que la Web haya llegado a tener la importancia que tiene hoy en día.

En su libro "Tejiendo la red", publicado en 1999, Berners-Lee explica por qué la tecnología web es libre y gratis. .

En el pasado, Berners-Lee se opuso a la creación de nombres de dominio nuevos como el '.mobi'. De hecho, cuando el '.mobi ' nació, él era uno de sus detractores. Él argumentó que todo el mundo debería acceder a las mismas web, independientemente de si usase un ordenador o un móvil. Básicamente lo que no le gustaba a Berners-Lee del '.mobi' es que este sería para que se accediese únicamente con los móviles, ya que él desarrolló la web como una forma de comunicación universal y no veía necesario el desarrollo del '.mobi' únicamente para el uso en móviles.

También hubo una pelea entre diferentes gobiernos y el ICANN sobre la propiedad de los nombres de los dominios, sobre todo con el ".com". Berners-Lee apoya que nadie tenga los nombres de los dominios, sino que estos sean un recurso público.
Berners-Lee también dejó claro que el nombre o la propiedad de los dominios no era el aspecto más importante en el proceso de estandarización, sino que eran más importantes los estándares de vídeo, codificación, estándares abiertos de comunicación de datos, subida de datos científicos y clínicos o la propagación de información entre países.





















Las personas más importantes de la historia de la informática. 


</doc>
<doc id="6485" url="https://es.wikipedia.org/wiki?curid=6485" title="NSFNet">
NSFNet

Acrónimo inglés de National Science Foundation's Network. La NSFNET comenzó con una serie de redes dedicadas a la comunicación de la investigación y de la educación. Fue creada por el gobierno de los Estados Unidos (a través de la National Science Foundation), y fue reemplazo de ARPANET como backbone de Internet. Desde entonces ha sido reemplazada por las redes comerciales.

Computer Science Network (CSNET) era una red de servicios para departamentos académicos, y en 1981 la National Science Foundation (NSF) se fijó en ella para crear una red académica para las supercomputadoras de la NSF. Esta red se estableció en 1985 con los siguientes cinco nodos:


NSFNET fue una red de propósito general para conectar las supercomputadoras mencionadas además de redes y campus regionales usando el ya existente TCP/IP en 1986, desarrollado y probado en ARPANET. En la implementación se utilizaron computadoras PDP-11/73 como routers las cuales se llamadon "Fuzzballs" bajo la supervisión de Ed Krol de la Universidad de Illinois. La velocidad máxima del backbone era de 56 kb/s

Soporte técnico fue proporcionado por NSF Network Service Center (NNSC) de BBN Technologies el cual publicaba el "Internet Manager´s Phonebook" con información de cada dominio y dirección IP en 1990. Krol creó el Hitchhiker Guide to The Internet como uno de los primeros manuales destinado al uso de la red. Conforme la red creció los 56 kb/s quedaron rebasados, en junio de 1987 NSF inició los procesos necesarios para mejorar NSFNET.

En noviembre de 1987 NSF comisionó a Merit Network, un consortium de universidades públicas de Michigan para mejorar la red original de 56 kb/s a 13 nodos de 1.5 Mb/s (T-1) en julio de 1988. Los nodos usaban como routers nueve sistemas IBM RT con software AOS, una versión modificada de Berkeley UNIX 



</doc>
<doc id="6488" url="https://es.wikipedia.org/wiki?curid=6488" title="Protocolo de internet">
Protocolo de internet

El Protocolo de Internet (en inglés Internet protocol o IP) es un protocolo de comunicación de datos digitales clasificado funcionalmente en la capa de red según el modelo internacional OSI.

Su función principal es el uso bidireccional en origen o destino de comunicación para transmitir datos mediante un protocolo no orientado a conexión que transfiere paquetes conmutados a través de distintas redes físicas previamente enlazadas según la norma OSI de enlace de datos.

El diseño del protocolo IP se realizó presuponiendo que la entrega de los paquetes de datos sería no confiable. Por ello, IP tratará de realizarla del mejor modo posible, mediante técnicas de enrutamiento, sin garantías de alcanzar el destino final pero tratando de buscar la mejor ruta entre las conocidas por la máquina que esté usando IP.

Los datos en una red basada en IP son enviados en bloques conocidos como paquetes o datagramas (en el protocolo IP estos términos se suelen usar indistintamente). En particular, en IP no se necesita ningún intercambio de información de control previa a la carga útil (datos), como sí que ocurre, por ejemplo, con TCP.

IP provee un servicio de datagramas no fiable (también llamado del "mejor esfuerzo": lo hará lo mejor posible, pero garantizando poco). IP no provee ningún mecanismo para determinar si un paquete alcanza o no su destino y únicamente proporciona seguridad (mediante "checksums" o sumas de comprobación) de sus cabeceras y no de los datos transmitidos. Por ejemplo, al no garantizar nada sobre la recepción del paquete, este podría llegar dañado, en otro orden con respecto a otros paquetes, duplicado o simplemente no llegar. Si se necesita fiabilidad, ésta es proporcionada por los protocolos de la capa de transporte, como TCP.
Las cabeceras IP contienen las direcciones de las máquinas de origen y destino (direcciones IP), direcciones que serán usadas por los enrutadores (routers) para decidir el tramo de red por el que reenviarán los paquetes.

El IP es el elemento común en el Internet de hoy. El actual y más popular protocolo de red es IPv4. IPv6 es el sucesor propuesto de IPv4; poco a poco Internet está agotando las direcciones disponibles por lo que IPv6 utiliza direcciones de fuente y destino de 128 bits, muchas más direcciones que las que provee IPv4 con 32 bits. Las versiones de la 0 a la 3 están reservadas o no fueron usadas. La versión 5 fue usada para un protocolo experimental. Otros números han sido asignados, usualmente para protocolos experimentales, pero no han sido muy extendidos.

Si la información a transmitir ("datagramas") supera el tamaño máximo "negociado" (MTU) en el tramo de red por el que va a circular podrá ser dividida en paquetes más pequeños, y reensamblada luego cuando sea necesario. Estos fragmentos podrán ir cada uno por un camino diferente dependiendo de como estén de congestionadas las rutas en cada momento.

Quizás los aspectos más complejos de IP son el direccionamiento y el enrutamiento. El direccionamiento se refiere a la forma como se asigna una dirección IP y cómo se dividen y se agrupan subredes de equipos. 

El enrutamiento consiste en encontrar un camino que conecte una red con otra y, aunque es llevado a cabo por todos los equipos, es realizado principalmente por routers, que no son más que computadoras especializadas en recibir y enviar paquetes por diferentes interfaces de red, así como proporcionar opciones de seguridad, redundancia de caminos y eficiencia en la utilización de los recursos.

Una dirección IP es un número que identifica de manera lógica y jerárquicamente a una interfaz de un dispositivo (habitualmente una computadora) dentro de una red que utilice el protocolo de Internet ("Internet Protocol"), que corresponde al nivel de red o nivel 3 del modelo de referencia OSI. Por ejemplo, si un equipo dispone de una tarjeta de red Ethernet y otra WiFi, tendrá una dirección IP asignada a cada una si las está usando. Dicho número no se ha de confundir con la dirección MAC que es un número físico que es asignado a la tarjeta o dispositivo de red (viene impuesta por el fabricante y no varía en toda su vida útil), mientras que la dirección IP puede cambiarse, por ejemplo, cambiando la red a la cual está conectado el equipo.

El usuario al conectarse desde su hogar a Internet utiliza una dirección IP. Esta dirección puede cambiar al reconectar. A la posibilidad de cambio de dirección de la IP se denomina "dirección IP dinámica". Existe un protocolo para asignar direcciones IP dinámicas llamado DHCP ("Dynamic Host Configuration Protocol").

Los sitios de Internet que por su naturaleza necesitan estar permanentemente conectados, generalmente tienen una "dirección IP fija" ("IP fija" o "IP estática"); es decir, no cambia con el tiempo. Los servidores de correo, dns, ftp públicos, servidores web, conviene que tengan una dirección IP fija o estática, ya que de esta forma se facilita su ubicación. 

Las máquinas manipulan y jerarquizan la información de forma numérica, y son altamente eficientes para hacerlo y ubicar direcciones IP. Sin embargo, los seres humanos debemos utilizar otra notación más fácil de recordar y utilizar, por ello las direcciones IP pueden utilizar un sinónimo, llamado nombre de dominio (Domain Name), para convertir los nombres de dominio en direcciones IP, se utiliza la resolución de nombres de dominio DNS.

En comunicaciones, el encaminamiento (a veces conocido por el anglicismo ruteo o enrutamiento) es el mecanismo por el que en una red los paquetes de información se hacen llegar desde su origen a su destino final, siguiendo un camino o ruta a través de la red. En una red grande o en un conjunto de redes interconectadas el camino a seguir hasta llegar al destino final puede suponer transitar por muchos nodos intermedios. 

Asociado al encaminamiento existe el concepto de métrica, que es una medida de lo "bueno" que es usar un camino determinado. La métrica puede estar asociada a distintas magnitudes: distancia, coste, retardo de transmisión, número de saltos, etc., o incluso a una combinación de varias magnitudes. Si la métrica es el retardo, es mejor un camino cuyo retardo total sea menor que el de otro. Lo ideal en una red es conseguir el encaminamiento óptimo: tener caminos de distancia (o coste, o retardo, o la magnitud que sea, según la métrica) mínimos. Típicamente el encaminamiento es una función implantada en la capa 3 (capa de red) del modelo de referencia OSI.




</doc>
<doc id="6489" url="https://es.wikipedia.org/wiki?curid=6489" title="ICQ">
ICQ

ICQ (""I seek you"", en castellano "te busco") es un cliente de mensajería instantánea y el primero de su tipo en ser ampliamente utilizado en Internet, mediante el cual es posible chatear y enviar mensajes instantáneos a otros usuarios conectados a la red de ICQ. También permite el envío de archivos, videoconferencias y charlas de voz.

ICQ fue creado por 2 jóvenes israelíes en 1996 con el nombre de Mirabilis con el propósito de introducir una nueva forma de comunicación sobre la Internet. El 8 de junio de 1998 la compañía fue adquirida por AOL por 287 millones de dólares. Hoy en día ICQ es usado por más de 38 millones de usuarios por todo el mundo. Según Time Warner, ICQ tiene más de 50 millones de cuentas registradas.

Desde abril de 2010 es propiedad de Mail.ru Group.

El protocolo de comunicaciones utilizado por ICQ es conocido como OSCAR, utilizado también por AIM. Los usuarios de la red ICQ son identificados con un número, el cual es asignado al momento de registrar un nuevo usuario, llamado UIN (""Universal Internet Number"" o "número universal de Internet"). 

IRC vería el ocaso de su época dorada con el lanzamiento de ICQ. Las cosas cambiarían radicalmente y la carrera de la mensajería instantánea no se detendría jamás, ya que se sumarían al mercado nuevos servicios de mensajería, que coparon el mercado. El crecimiento de ICQ fue exponencial desde su lanzamiento, tanto, que ya "para 1998, la compañía fue adquirida por AOL por 287 millones de dólares (430 millones actuales aproximadamente)". En su pico más alto de popularidad ICQ alcanzó los 100 millones de cuentas registradas.

Con el aumento de usuarios de ICQ los ID tuvieron más y más dígitos, curiosamente los ID cortos son puestos a la venta, entre menos dígitos, más caros se cotizaban trayendo grandes beneficiosos a la compañía.

AOL vendió ICQ a Digital Sky Technologies en 2010. Fue vendida por 187,5 millones de dólares (212 millones con el ajuste de inflación), una suma mucho menor de lo que pagó por ella en 1998. Esta compra fue criticada porque presuntamente comprometía la seguridad de los usuarios debido a que Digital Sky Technologies (ahora conocida como Mail.ru) es una compañía rusa. 

En Rusia y Europa del este ICQ sigue siendo un servicio popular, mucho más que en países occidentales.

ICQ fue concebida como un servicio de mensajería como tal, por tanto, "tiene una serie de características irresistibles": un perfil que se podía personalizar, sonidos, estados de conexión, emoticons, transferencia de contactos y archivos, envío de SMS, ecards (!) juegos, chats grupales y video llamadas.

Debido al gran número de usuarios de ICQ, las identificaciones de usuario más recientes se encuentran por encima del número 100.000.000. En algunos casos, los números más simples y fáciles de recordar son vendidos en subastas por Internet o incluso secuestrados por otros usuarios.




</doc>
<doc id="6493" url="https://es.wikipedia.org/wiki?curid=6493" title="Estación de trabajo">
Estación de trabajo

En informática una estación de trabajo (en inglés "workstation") es un computador de altas prestaciones destinado para trabajo técnico o científico. En una red de computadoras, es una computadora que facilita a las personas el acceso a los servidores y periféricos de la red. A diferencia de una computadora aislada, tiene una tarjeta de red y está físicamente conectada por medio de cables u otros medios no guiados con los servidores. Los componentes para servidores y estaciones de trabajo alcanzan nuevos niveles de rendimiento informático, al tiempo que ofrecen fiabilidad, compatibilidad, escalabilidad y arquitectura avanzada ideales para entornos multiproceso.

Lo de las computadoras en general, las computadoras promedio de hoy en día son más poderosas que las mejores estaciones de trabajo de una generación atrás. Como resultado, el mercado de las estaciones de trabajo se está volviendo cada vez más especializado, ya que muchas operaciones complejas que antes requerían sistemas de alto rendimiento pueden ser ahora dirigidas a computadores de propósito general. Sin embargo, el "hardware" de las estaciones de trabajo está optimizado para situaciones que requieren un alto rendimiento y fiabilidad, donde generalmente se mantienen operativas en situaciones en las cuales cualquier computadora personal tradicional dejaría rápidamente de responder. 

Actualmente las estaciones de trabajo suelen ser vendidas por grandes fabricantes de ordenadores como LENOVO, HP o Dell y utilizan CPU x86-64 como Intel Xeon o AMD Opteron ejecutando Microsoft Windows o GNU/Linux. Apple Inc. y Sun Microsystems comercializan también su propio sistema operativo tipo UNIX para sus estaciones de trabajo.

Las estaciones de trabajo fueron un tipo popular de computadoras para ingeniería, ciencia y gráficos durante las décadas de 1980 y 1990. Últimamente se las asocia con CPU RISC, pero inicialmente estaban basadas casi exclusivamente en la serie de procesadores Motorola 68000.

Las estaciones de trabajo han seguido un camino de evolución diferente al de las computadoras personales o PC. Fueron versiones de bajo costo de minicomputadoras como son las de la línea VAX, la cual había sido diseñada para sacar datos de tareas de cómputos más pequeñas de la muy cara computadora mainframe de la época. Rápidamente adoptaron un solo chip micropocesador de 32 bits, en oposición a los más costosos procesadores de multi-chip prevalecientes en aquel entonces. Posteriormente, las generaciones de estaciones de trabajo usaron procesadores RISC de 32 bits y 64 bits, que ofrecían un rendimiento más alto que los procesadores CISC usados en los computadoras personales.

Las estaciones de trabajo también corrían el mismo sistema operativo multi-usuario/multi-tarea que las microcomputadoras usaban, comúnmente Unix. También usaban redes para conectarse a computadoras más potentes para análisis de ingeniería y visualización de diseños. El bajo costo relativo a minicomputadoras y mainframes permitió una productividad total mayor a muchas compañías que usaban computadoras poderosas para el trabajo de cómputo técnico, ya que ahora cada usuario individual contaba con una máquina para tareas pequeñas y medianas, liberando así a las computadoras más grandes para los tratamientos por lotes. 

Las Computadoras personales, en contraste con las estaciones de trabajo, no fueron diseñadas para traer el rendimiento de la minicomputadora al escritorio de un ingeniero, sino que fueron previstas originalmente para el uso en casa o la productividad de oficina, la sensibilidad al precio fue un aspecto de consideración primaria. La primera computadora personal usaba un chip de procesador de 8 bits, especialmente los procesadores MOS Technology 6502 y Zilog Z80, en los días de Apple II, Atari 800, Commodore 64 y TRS-80. La introducción del IBM PC en 1981, basado en el diseño de procesador Intel x86, finalmente cambió la industria.

Los primeros sistemas operativos de PC fueron diseñados para ser de una sola tarea (MS DOS), luego incluyeron una limitada multitarea cooperativa (Windows 3.1) y últimamente han incluido multitarea con prioridad (Windows 95, Windows XP, GNU/Linux). Cada uno de estos diferentes tipos de sistemas operativos varía en la habilidad para utilizar la potencia total inherente del "hardware" para realizar múltiples tareas simultáneamente.

Tal vez la primera computadora que podría ser calificada como estación de trabajo fue la IBM 1620, una pequeña computadora científica diseñada para ser usada interactivamente por una sola persona sentada en la consola. Fue introducida en 1959. Una característica peculiar de la máquina era que carecía de cualquier tipo de circuito aritmético real. Para realizar la adición, requería una tabla almacenada en la memoria central con reglas decimales de la adición. Lo que permitía ahorrar en costos de circuitos lógicos, permitiendo a IBM hacerlo más económica. El nombre código de la máquina fue CADET, el cual algunas personas decían que significaba "No puede sumar, ni siquiera lo intenta ("Can't Add, Doesn't Even Try")". No obstante, se alquiló inicialmente por unos $1000 por mes. 

Posteriormente llegaron el IBM 1130 (sucesor del 1620 en 1965), y el minicomputador PDP-8 de Digital Equipment Corporation.

Las primeras estaciones de trabajo basadas en microordenadores destinados a ser utilizados por un único usuario fueron máquina Lisp del MIT a comienzos de la década de 1970, seguidas de los Xerox Alto (1973), PERQ (1979) y Xerox Star (1981).

En los años 1980 se utilizaron estaciones de trabajo basadas en CPU Motorola 68000 comercializadas por nuevas empresas como Apollo Computer, Sun Microsystems y SGI. Posteriormente llegarían NeXT y otras.

Desde finales de los años 1980 se fueron sustituyendo por equipos generalmente con CPU RISC diseñada por el fabricante del ordenador, con su sistema operativo propietario, casi siempre una variante de UNIX (con excepciones no basadas en UNIX, como OpenVMS o las versiones de Windows NT para plataformas RISC). Aunque también hubo estaciones de trabajo con CPU Intel x86 ejecutando Windows NT como las Intergraph "ViZual Workstation Zx" y varios modelos Compaq y Dell.

Lista no exhaustiva de las estaciones de trabajo RISC más famosas de los años 90:

En la actualidad se ha pasado de las arquitecturas RISC de IBM POWER, MIPS, SPARC, PA-RISC o DEC Alpha a la plataforma x86-64 con CPU Intel y AMD. Tras ser retiradas del mercado las Sun Ultra 25/45 en julio de 2008 y las IBM IntelliStation Power en enero de 2009, ya no se comercializan modelos con CPU RISC que tan comunes fueron en la década de 1990.

Así pues actualmente se utiliza normalmente CPU Intel Xeon o AMD Opteron, pudiendo usarse otras CPU x86-64 más comunes (como intel Core 2 o Core i5) en modelos más asequibles. Son comunes las GPU profesionales NVIDIA Quadro FX y ATI FireGL.

Lista de algunos fabricantes y modelos:







</doc>
<doc id="6495" url="https://es.wikipedia.org/wiki?curid=6495" title="1908">
1908

1908 () fue un año bisiesto comenzando en miércoles en el calendario gregoriano.

































</doc>
<doc id="6498" url="https://es.wikipedia.org/wiki?curid=6498" title="Institute of Electrical and Electronics Engineers">
Institute of Electrical and Electronics Engineers

El Instituto de Ingeniería Eléctrica y Electrónica (conocido por sus siglas IEEE, leído "i-triple-e" en Latinoamérica o "i-e-cubo" en España; en inglés "Institute of Electrical and Electronics Engineers") es una asociación mundial de ingenieros dedicada a la normalización y el desarrollo en áreas técnicas. Con cerca de 425 000 miembros y voluntarios en 160 países, es la mayor asociación internacional sin ánimo de lucro formada por profesionales de las nuevas tecnologías, como ingenieros electricistas, ingenieros electrónicos, ingenieros de sistemas, ingenieros en computación, matemáticos aplicados, ingenieros en biomedicina, ingenieros en telecomunicación, ingenieros en mecatrónica, ingenieros en telemática, ingenieros sociales, cibernéticos, ingenieros en software, ingenieros industriales, etc.

Fue creado en 1884 por Thomas Alva Edison, Alexander Graham Bell, Franklin Leonard Pope y otros ingenieros. En 1963 adoptó el nombre de IEEE al fusionarse asociaciones con el AIEE ("American Institute of Electrical Engineers") y el IRE ("Institute of Radio Engineers").

Según el mismo IEEE, su trabajo es promover la creatividad, el desarrollo y la integración, compartir y aplicar los avances en las tecnologías de la información, electrónica y ciencias en general para beneficio de la humanidad y de los mismos profesionales. Algunos de sus estándares son:



Mediante sus actividades de publicación técnica, conferencias y estándares basados en consenso, el IEEE produce más del 30 % de la literatura publicada en el mundo sobre ingeniería eléctrica de potencia, electrónica, en computación, telecomunicaciones, telemática, mecatrónica y tecnología de control y robótica, biomédica y biónica, procesamiento digital de señales, sistemas energéticos, entre otras ramas derivadas y correspondientes a la Ingeniería Eléctrica; organiza más de 1000 conferencias al año en todo el mundo, y posee cerca de 900 estándares activos, con otros 700 más bajo desarrollo.

El IEEE se encuentra agrupado en treinta y ocho sociedades enfocadas en un área de trabajo específica.
Estas sociedades proveen publicaciones especializadas, conferencias y redes de negocio, entre otros servicios.

IEEE es una de las organizaciones líderes en la creación de estándares en el mundo. IEEE realiza sus estándares y mantiene las funciones a través de la Asociación de estándares IEEE . Estándares IEEE afectan a una amplia gama de industrias, incluyendo: potencia y energía, biomedicina y salud, tecnología de la información, las telecomunicaciones, el transporte, la nanotecnología, la seguridad de la información, y muchos más. En 2013, la IEEE tenía más de 900 estándares activos, con más de 500 normas en elaboración. Uno de los más notables estándares IEEE es la IEEE 802 LAN/MAN grupo de normas que incluye el estándar IEEE 802.3 Ethernet y el estándar IEEE 802.11 de red inalámbrica.




</doc>
<doc id="6506" url="https://es.wikipedia.org/wiki?curid=6506" title="Scott Joplin">
Scott Joplin

Scott Joplin (Texarkana, Texas; 24 de noviembre de 1868-Manhattan, Nueva York; 1 de abril de 1917) fue un compositor y pianista afroestadounidense, una de las figuras más importantes en el desarrollo del ragtime clásico, para el que deseaba un estatus similar al de la música seria proveniente de Europa y la posibilidad de admitir composiciones extensas como óperas y sinfonías. 

Joplin nació en una familia de obreros músicos en el noreste de Texas, y desarrolló su conocimiento musical con la ayuda de maestros locales. Creció en Texarkana, donde formó un cuarteto vocal, y enseñó mandolina y guitarra. Luego de viajar por el sur de EE.UU. fue a Chicago para la Exposición Mundial Colombina de 1893, que desempeñó un papel importante en popularizar el ragtime para 1897.

En 1894 se muda a Sedalia (Misuri), donde se gana la vida como profesor de piano; ahí les enseñó a los futuros compositores de "ragtime" Arthur Marshall, Scott Hayden y Brun Campbell. Joplin comenzó a publicar música en 1895, y la publicación del "Maple Leaf Rag" le dio fama en 1899. En 1901 se mudó a Saint Louis, donde continuó componiendo y publicando, tocando con frecuencia en esa comunidad. Le confiscan la partitura de su primera ópera, "Un invitado de honor", por no pagar las cuentas, y ahora se considera perdida. En 1907 se muda a Nueva York para buscar un productor para una nueva ópera. Su segunda ópera, "Treemonisha", no fue bien recibida en su puesta en escena parcial de 1915.

En 1916 Joplin sufre de demencia como consecuencia de la sífilis. Se interna en un psiquiátrico en 1917 donde muere tres meses después, a los 48 años. Se considera ampliamente que la muerte de Joplin marcó el fin del "ragtime" como formato de música de tendencia, para evolucionar en los siguientes años hacia otros estilos y convertirse en "stride", "jazz", y eventualmente "swing" de Big Band. 

Su música fue redescubierta y volvió a ser popular a principios de la década de 1970, con el lanzamiento del álbum de Joshua Rifkin que vendió un millón de copias. Esto fue seguido de la película "El Golpe", ganadora del Óscar en 1973, que destacaba varias de sus composiciones, destacando "The Entertainer". La ópera "Treemonisha" fue finalmente producida por completo en 1972. En 1976 se le concedió a Joplin un premio Pulitzer póstumo.

Scott Joplin, a diferencia de otros músicos contemporáneos, tuvo una formación musical clásica muy sólida, lo que se materializó en su tendencia a obtener un equilibrio formal basándose en el uso de tonalidades muy próximas entre sí.

Sus "rags" se valen de distintos ritmos e incluyen, generalmente, cuatro temas de 16 compases repetidos, con introducción y modulación antes del tercer tema; tras la profusión de sonidos irregulares y arpegiados siempre se encuentra en sus "rags" una melodía de carácter pegadizo que sigue el clásico patrón de frase antecedente-consecuente, dividiéndose así la melodía de ocho compases en dos partes relacionadas entre sí. Por lo demás, sus "rags" carecen de pasajes de desarrollo.

Scott Joplin no grabó nunca discos, aunque sí algunos rollos de pianola a finales de 1915 o principios de 1916; su legado, por tanto, se centra casi exclusivamente en sus partituras, diseñadas "para una ejecución milimétrica y minuciosa por parte del artista".

Joplin nació en Linden (Texas), quizás a finales de 1867 o principios de 1868. Aunque durante muchos años se aceptó su fecha de nacimiento como 24 de noviembre de 1868, la investigación ha revelado que es casi sin duda inexacta (la fecha aproximada más probable sería el segundo semestre de 1867). Fue el segundo de seis hijos (los otros son Monroe, Robert, William, Myrtle y Ossie) nacidos de Giles Joplin, un exesclavo de Carolina del Norte, y de Florece Givens, una afroamericana nacida libre de Kentucky. Los Joplin posteriormente se trasladaron a Texarkana (Texas) donde Giles trabajó como peón para el ferrocarril mientras que Florence era mujer de la limpieza. El padre de Joplin había tocado el violín para las fiestas de la plantación en Carolina del Norte, y su madre cantaba y tocaba el banjo. Joplin recibió una rudimentaria educación musical de su familia y a la edad de siete años le fue permitido tocar el piano mientras su madre limpiaba.

En algún momento de principios de la década de 1880, Giles Joplin abandonó a la familia por otra mujer, dejando a Florence mantener a sus hijos con su trabajo doméstico. La biógrafa Susan Curtis especuló que el apoyo de su madre a la educación musical de Joplin fue un importante factor causal en esta separación. Su padre argumentaba que esto lo alejó de un empleo práctico que complementaría los ingresos familiares.

Según un amigo de la familia, el joven Joplin era serio y ambicioso, estudiaba música y tocaba el piano después de salir de la escuela. Mientras que algunos profesores locales le ayudaban, recibió mucha de su educación musical de Julius Weiss, un profesor de música judío-alemán que había inmigrado a los EE. UU. desde Alemania. Weiss había estudiado música en la universidad en Alemania y figuraba en los registros de la ciudad como "Profesor de música". Impresionado por el talento de Joplin, y dándose cuenta de la difícil situación de su familia, Weiss le enseñó música sin cobrarle nada. Tutorizó a Joplin desde los 11 hasta los 16 años, durante ese tiempo Weiss le introdujo en la música popular y en la clásica, incluyendo la ópera. Weiss ayudaba a Joplin a apreciar la música como un "arte y también un entretenimiento", y ayudó a su madre a adquirir un piano de segunda mano. De acuerdo con su esposa Lottie, Joplin nunca olvidó a Weiss y en sus últimos años, cuando él alcanzó fama como compositor, le enviaba a su antiguo profesor "... regalos de dinero cuando él estaba viejo y enfermo", hasta que Weiss murió. A la edad de 16 años Joplin actuaba en un cuarteto vocal con otros tres chicos en Texarkana y sus alrededores, tocando el piano. Además enseñó guitarra y mandolina.

Al final de la década de 1880, habiendo actuado en varios eventos locales como adolescente, Joplin optó por dejar el trabajo como empleado en el ferrocarril y abandonó Texarkana para convertirse en un músico itinerante. Poco se sabe sobre sus movimientos en este momento, a pesar de que se registra en Texarkana en julio de 1891 como miembro de los "Trovadores Texarkana" en una actuación que con el fin de recaudar fondos para un monumento a Jefferson Davis, Presidente de la Confederación del Sur. Él descubrió pronto, sin embargo, que había pocas oportunidades allí para los pianistas negros. Las iglesias y los burdeles estaban entre las pocas opciones de trabajo estable. Joplin tocó pre-ragtime "jig-piano" en varias zonas rojas, en distritos a lo largo del medio sur, y algunos reivindican que estuvo en Sedalia (Misuri) y Saint Louis durante este tiempo.

En 1893 Joplin estuvo en Chicago para la Feria Mundial. Durante su estancia en esa ciudad formó su primera banda tocando la corneta y empezó con arreglos musicales para que el grupo los interpretara. Aunque la Feria Mundial minimizó la participación de los afroamericanos, aun así los artistas negros actuaban en los bares, cafés y burdeles que rodeaban la feria. A la exposición asistieron 27 millones de americanos y tuvo un profundo efecto en muchas áreas de la vida cultural americana, incluyendo el "ragtime". Aunque la información específica es escasa, numerosas fuentes han asociado a la Feria Mundial de Chicago con la difusión de la popularidad del "ragtime". Joplin encontró que su música, así como la de otros artistas negros, era muy popular entre los visitantes. Para 1897 el ragtime se había convertido en una moda en las ciudades americanas y fue descrito por el "St. Louis Dispatch" como "...un verdadero llamado de la naturaleza, que agitó poderosamente el pulso de la gente de ciudad".

En 1894 Joplin arribó a Sedalia (Misuri). Al principio, Joplin se quedó con la familia de Arthur Marshall, en el momento un muchacho de trece años de edad, pero más tarde uno de los estudiantes de Joplin y compositor de "ragtime" por derecho propio. No hay registro de que Joplin tuviera una residencia permanente en la ciudad hasta 1904, porque hasta entonces llevaba una vida de músico itinerante.

Hay poca evidencia precisa conocida sobre las actividades de Joplin en este tiempo, a pesar de que actuó como solista en los bailes y en los principales clubes de negros en Sedalia, el club "Negro 400" y el "Club Hoja de Arce (Maple Leaf Club)". Actuó en la banda Queen City Cornet, y su propia orquesta de baile de seis piezas. Una gira con su propio grupo de canto, Texas Medley Quartet, le dio su primera oportunidad de publicar sus propias composiciones y se sabe que fue a Siracusa, Nueva York y Texas. Dos hombres de negocios de Nueva York publican los primeros dos trabajos de Joplin, las canciones "Please Say You Will" y "A Picture of her Face", en 1895. La visita de Joplin a Temple, Texas le permitió tener tres piezas publicadas allí en 1896, incluyendo "Great Crush Collision March", que conmemora un choque de trenes programado como espectáculo del ferrocarril Misuri-Kansas-Texas el 15 de septiembre de 1896 del que puede haber sido testigo. La marcha fue descrita por uno de los biógrafos de Joplin como un "especial...temprano ensayo de ragtime." Estando en Sedalia enseñaba piano a estudiantes entre quienes había futuros compositores de "ragtime" como el mencionado Arthur Marshall, Brun Campbel y Scott Hayden. De vuelta, Joplin se inscribió en el George R. Smith College, donde aparentemente estudió "...armonía avanzada y composición." Los registros de la universidad fueron destruidos en un incendio en 1925, y su biógrafo Edward A. Berlin señala que es poco probable que un pequeño colegio para afroamericanos pudiese ofrecer tal curso.

En 1899, Joplin se casa con Belle, la cuñada del colaborador Scott Hayden. Aunque había cientos de "rags" impresos en el momento en el que fue publicado el "Maple Leaf Rag", Joplin no se quedaba muy atrás. Su primer "rag" publicado, "Original Rags", se había completado en 1897, el mismo año del primer trabajo de "ragtime" impreso, el "Mississippi Rag", de William Krell. "Maple Leaf Rag" pudo probablemente haber sido conocido en Sedalia antes de su publicación en 1899; Brun Campbell reivindica haber visto el manuscrito del trabajo en aproximadamente 1898. Las circunstancias exactas que llevaron a la publicación del "Maple Leaf Rag" son desconocidas, y algunas versiones del evento se contradicen entre sí. Después de varias aproximaciones infructuosas a los editores, Joplin firmó un contrato el 10 de agosto de 1899 con John Stillwell Stark, un minorista de instrumentos musicales quien posteriormente se convirtió en su editor más importante. El contrato estipulaba que Joplin recibiría un 1% de derechos de autor sobre todas las ventas del rag, con un precio de venta mínimo de 25 centavos de dólar. Con la inscripción "Para el Maple Leaf Club" muy a la vista en la parte superior de al menos algunas ediciones, es probable que el "rag" fuera llamado así por la hoja de arce emblema del Club, aunque no hay evidencia directa que demostrara el vínculo, y había muchas otras fuentes posibles para el nombre en y alrededor de Sedalia en el momento.
Ha habido muchas aseveraciones respecto a las ventas del "Maple Leaf Rag", por ejemplo, que Joplin fue el primer músico en vender un millón de copias de una pieza de música instrumental. El primer biógrafo de Joplin, Rudi Blesh escribió que durante sus primeros seis meses, la pieza vendió 75.000 copias, y se convirtió en "... el primer gran éxito de una obra instrumental en América". Sin embargo, la investigación por el posterior biógrafo de Joplin, Edward A. Berlín demostró que este no fue el caso; a la tirada inicial de 400 le llevó un año venderse, y en los términos del contrato de Joplin con una regalía de 1% habría dado Joplin un ingreso de 4 dólares (o aproximadamente $ dólares al cambio actual). Más tarde las ventas se estabilizaron, y le podrían haber dado a Joplin unos ingresos que habrían cubierto sus gastos. En 1909, las ventas estimadas le habrían dado un ingreso de 600 dólares anualmente (aproximadamente $ dólares al cambio actual).

El "Maple Leaf Rag" sirvió de modelo para los cientos de "rags" por venir en futuras composiciones, especialmente en el desarrollo del "ragtime" clásico. Después de la publicación del "Maple Leaf Rag", Joplin fue pronto descrito como el "Rey de los escritores de Ragtime (King of rag time writers)", y nada menos que por él mismo en las portadas de su propio trabajo, tales como "The Easy Winners (Los ganadores fáciles)" y "Elite Syncopations (Élite sin igual)".

Después que los Joplin se trasladaron a Saint Louis a principios de 1900, tuvieron una hijita que murió a los pocos meses de nacer. La relación de Joplin con su mujer fue difícil porque ella no tenía interés por la música. Se separaron finalmente y luego se divorciaron. Por ese tiempo, Joplin colaboró con Scott Hayden en la composición de cuatro "rags". Fue en San Luis donde Joplin produjo algunos de sus más conocidos trabajos, incluyendo "The Entertainer (El Animador)", "Marzo Majestuoso" ("March Majestic)" y la breve obra teatral "The Ragtime Dance" ("La danza Rag"). 

En junio de 1904, Joplin se casa con Freddie Alexander de Little Rock (Arkansas), la joven a quien dedicó "The Chrysanthemum" ("El Crisantemo"). Ella murió el 10 de septiembre de ese mismo año por las complicaciones resultantes de un resfriado, diez semanas después de su boda. El primer trabajo de Joplin con derechos de autor después de la muerte de Freddie, "Bethena", fue descrito por un biógrafo como "una encantadoramente maravillosa pieza que está entre las más grandes de los vals del "ragtime"."

Durante este tiempo, Joplin creó una compañía de ópera de 30 personas y produjo su primera ópera "Un invitado de honor" ("A Guest of Honor") para una gira nacional. No es seguro cuantas producciones fueron puestas en escena, o incluso si este fue un espectáculo de negros o una producción de razas mezcladas. Durante la gira, ya sea en Springfield (Illinois), o Pittsburg (Kansas), alguien relacionado con la empresa robó los ingresos de la caja. Joplin no podría pagar la nómina de sueldos de su compañía o pagar por su alojamiento en una casa de hospedaje teatral. Se cree que la partitura de "Un invitado de honor" ("A Guest of Honor") se perdió, quizás destruida debido a la falta de pago de la factura de la casa de hospedaje de la compañía.

En 1907, Joplin se trasladó a la ciudad de Nueva York, la cual creía que era el mejor lugar para encontrar un productor para una nueva ópera. Después de su mudanza a Nueva York, Joplin conoció a Lottie Stokes, con quien se casó en 1909. En 1911, incapaz de encontrar un editor, Joplin emprendió la carga financiera de publicar "Treemonisha" él mismo en formato piano-vocal. En 1915, como un último esfuerzo para verla realizada, invitó a una pequeña audiencia para oírla en una sala de ensayo en Harlem. Pobremente escenificado y con sólo Joplin en acompañamiento de piano, fue "un fracaso miserable" para un público que no está listo para formas de música negra "crudas" tan diferentes de la europea y de la gran ópera de la época. El público, incluyendo patrocinadores potenciales, fue indiferente y se marchó. Scott escribe que "luego de una desastrosa actuación individual... Joplin sufrió una desilusión. Él estaba en bancarrota, desanimado y agotado." Llega a la conclusión de que pocos artistas americanos de su generación se enfrentaron a estos obstáculos: "Treemonisha" pasó inadvertida y sin crítica, en gran parte porque Joplin había abandonado la música comercial en favor de la música artística, un campo cerrado para los afroamericanos. De hecho, no fue hasta la década de 1970 que esta ópera recibió una puesta en escena teatral completa.

En 1914, Joplin y Lottie autopublicaron su "Magnetic Rag" como la "Compañía de Música Scott Joplin (Scott Joplin Music Company)", que se había formado el diciembre anterior. La biógrafa Vera Brodsky Lawrence especula que Joplin fue consciente de su avanzado deterioro debido a la sífilis y estaba "...deliberadamente corriendo contra el reloj." En sus notas de contratapa del lanzamiento de "Treemonisha" en 1992 por Deutsche Grammophon, señala que "... se hundió febrilmente en la tarea de orquestar su ópera, día y noche, con su amigo Sam Patterson preparado para copiar las partes, página por página, a medida que cada página de la partitura entera se completaba".

Para 1916, Joplin estaba padeciendo de la fase final de sífilis y su consecuente caída en la demencia. En enero de 1917 fue admitido en el Hospital Estatal de Manhattan, un psiquiátrico. Muere allí el 1 de abril de demencia sifilítica, a la edad de 48 años y es enterrado en una fosa común que quedaría sin marcar por 57 años. Se le obsequió su tumba con nombre en el Cementerio San Miguel, en el este de Elmhurs, finalmente en 1974.

Por orden alfabético (entre paréntesis, el año del "copyright"):


La combinación de la música clásica, el ambiente musical presente alrededor de Texarkana —incluyendo canciones de trabajo, góspel, espiritual y música de baile— y la capacidad natural de Joplin ha sido citada como una contribución significativa a la invención de un nuevo estilo que mezcló los estilos musicales afroamericanos con formas y melodías europeas, y se hizo famoso por primera vez en la década de 1890: el ragtime.

Cuando Joplin estaba aprendiendo piano, los círculos musicales formales condenaron el ragtime debido a su asociación con canciones vulgares y tontas "... estampados por los fabricantes de melodías de Tin Pan Alley." Como compositor Joplin refinó el ragtime, elevándolo por encima de la forma baja y basta tocada por los "... vagantes pianistas de burdel... que tocaban mera música de baile" de la imaginación popular. Esta nueva forma de arte, el rag clásico, combinó la síncopa de la música popular afroamericana y el romanticismo europeo del siglo XIX, con sus esquemas armónicos y sus tempos como de marcha. En palabras de un crítico, "el ragtime era básicamente ... una versión afroamericana de la polca, o su análoga, la marcha de estilo Sousa." Con esto como base, Joplin destinó sus composiciones a ser reproducidas exactamente como él las escribió  –sin improvisación. Joplin escribe sus rags como música "clásica" en versión reducida, con el fin de elevar el ragtime por encima de sus orígenes de "burdel barato" y crear una obra que la historiadora de ópera Elise Kirk describió como "... más melódica, a contrapunto, pegadiza y armónicamente colorida que ninguna otra de su época."

Algunos especulan con que los logros de Joplin estuvieron influenciados por su profesor de música alemán Julius Weiss, formado en clásica, que le pudo haber traído una sensibilidad rítmica de polca del viejo país al Joplin de 11 años. Como Curtis dijo, "el alemán educado podría abrir la puerta a un mundo de aprendizaje y música de la cual el joven Joplin fue en gran parte inconsciente."

El primer y más significativo éxito de Joplin, el Maple Leaf Rag, fue descrito como el arquetipo del rag clásico, e influyó en compositores de rag posteriores durante al menos 12 años después de su publicación inicial, gracias a sus patrones rítmicos, líneas melódicas y armonía, aunque con la excepción de Joseph Lamb, generalmente fallaban en ampliarlo.

La escena de la ópera es una antigua comunidad de esclavos en un bosque aislado cerca de Texarkana, la ciudad de la infancia de Joplin, en septiembre de 1884. La trama se centra en una joven de 18 años, Treemonisha, a quien le enseña a leer una mujer blanca, y que luego lidera su comunidad contra la influencia de charlatanes que se aprovechan de la ignorancia y la superstición. Treemonisha es secuestrada y está a punto de ser arrojada en un nido de avispas cuando su amigo Remus la rescata. La comunidad se da cuenta del valor de la educación y la responsabilidad de la ignorancia de ellos para luego elegirla como su maestra y líder.

Joplin escribió tanto la partitura como el libreto para la ópera, que en gran medida imita el tipo de ópera europeo, con muchas arias tradicionales, ensembles y coros. Además, los temas de superstición y misticismo evidente en "Treemonisha" son comunes en la tradición operística, y ciertos aspectos se hacen eco de formas del trabajo del compositor alemán Richard Wagner (de lo cual Joplin era consciente). Un árbol sagrado bajo el que Treemonisha se sienta recuerda el árbol del que Siegmund saca su espada encantada en "Die Walküre", y el recuento del origen de la protagonista se hace eco de aspectos de la ópera Sigfrido. Además, los cuentos del folclore afroamericano influyen en la historia -el incidente del avispero es similar al relato de El Hermano Conejo y El Hermano Zorro y el matorral.

"Treemonisha" no es una ópera ragtime porque Joplin empleó los estilos del ragtime y otras músicas negras frugalmente, usándolas para transmitir un "carácter racial", y celebrar la música de su infancia en los finales del siglo XIX. La ópera se ha visto como un valioso registro de la música rural negra del tardío siglo XIX recreada por un "hábil y sensitivo participante".

Berlín especula sobre paralelismos entre el argumento y la propia vida de Joplin. Él observa que Lottie Joplin (la tercera esposa del compositor) vio una conexión entre los deseos de Treemonisha de sacar a su pueblo de la ignorancia y un deseo similar en el compositor. Además se ha especulado con que Treemonisha representa a Freddie, la segunda esposa de Joplin, porque la fecha del desarrollo de la ópera era probablemente la fecha del mes de su nacimiento.

Al tiempo de la publicación de la ópera en 1911, el "American Musician and Art Journal" lo elogió como "...una forma completamente nueva de arte operístico". Después los críticos han elogiado la ópera como ocupando un lugar especial en la historia americana, con su heroína, "...una sorprendentemente temprana voz para las causas modernas de los derechos civiles, notablemente la importancia de la educación y conocimiento del ascenso afro-americano." La conclusión de Curtis es similar: "En el final, "Treemonisha" ofrecía una celebración de literatura, aprendizaje, trabajo duro y solidaridad comunitaria como la mejor fórmula para el avance de la raza". Berlín la describe como "... una ópera fina, ciertamente más interesante que muchas óperas siendo escritas en los Estados Unidos," pero más tarde señala que el propio libreto de Joplin mostró al compositor, "... no era un dramaturgo competente," con el libro no llega a la calidad de la música.

Las habilidades de Joplin como pianista fueron descritas en términos brillantes por un periódico de Sedalia en 1898, y los compañeros compositores de ragtime, Arthur Marshall y Joe Jordan dijeron que tocaba bien el instrumento. Sin embargo el hijo del editor John Stark afirmaba que Joplin era un pianista más bien mediocre y que compuso sobre papel, en vez de en el piano. Artie Matthews recordó el deleite que los músicos de St. Louis sentían por tocar mejor que Joplin.

Como Joplin nunca hizo una grabación de sonido, su música es preservada solo en siete rollos de piano para uso en pianolas. Los siete fueron hechos en 1916. De éstos, los seis lanzados bajo el sello Connorized muestran evidencias de edición significativa, probablemente por William Axtmann, el arreglador del personal de Connorizad. Berlín teoriza que al momento en que Joplin llegó a St. Louis, él pudo haber experimentado descoordinación de los dedos, temblores y una dificultad para hablar, claramente todos síntomas de la sífilis que cobró su vida en 1917. El biógrafo Blesh describió la segunda grabación de rollo de "Maple Leaf Rag" para el sello UniRecord de junio de 1916 como "...chocante... desorganizada y completamente dolorosa de oír." Mientras hay desacuerdo entre los expertos en rollos para pianola respecto de la precisión en la reproducción de lo que ejecuta el músico, Berlin nota que el rollo del Maple Leaf Rag era, "dolorosamente malo," y que probablemente sería el más fiel registro de como Joplin tocaba en esa época. El rollo, pues, no refleja las habilidades que pudo tener más temprano en su vida.

Joplin y sus compañeros compositores de ragtime rejuvenecieron la música popular estadounidense, estimulando la apreciación de la música Afroamericana entre los americanos de origen europeo, creando melodías de baile estimulantes y liberadoras, cambiando el gusto musical americano. "Su síncopa y manejo rítmico, le dieron una vitalidad y frescura atractiva a las jóvenes audiencias urbanas, indiferentes al decoro Victoriano... el ragtime de Joplin expresaba la intensidad y energía de un moderno Estados Unidos urbano."

Joshua Rifkin, un artista destacado por sus grabaciones de Joplin, escribió: "Una penetrante sensación generalizada de lirismo infunde su trabajo, y aún en su más alto espíritu, no puede reprimir un toque de melancolía o adversidad ... él tenía muy poco en común con la rápida y llamativa escuela de ragtime que creció después de él". El historiador de Joplin Bill Ryerson agrega que "en las manos de un auténtico profesional como Joplin, el ragtime fue una forma disciplinada capaz de asombrosa variedad y sutileza... Joplin hizo por el rag lo que Chopin hizo por la mazurka. Su estilo abarcaba desde tonos de tormento a maravillosas serenatas que incorporaron el bolero y el tango." La biógrafa Susan Curtis escribió que la música de Joplin había ayudado a "revolucionar la música y la cultura americana" eliminando la compostura Victoriana.

Al compositor y actor Max Morath le resultó llamativo que la inmensa mayoría de la obra de Joplin no disfrutó de la popularidad del Maple Leaf Rag, porque mientras las composiciones eran de creciente belleza lírica y delicadez de síncopa quedaron a la sombra sin ser oídas durante su vida. Joplin aparentemente se dio cuenta de que su música estaba adelantada a su tiempo: como menciona el historiador Ian Whitcomb que Joplin "opinaba que el Maple Leaf Rag lo haría el 'Rey de los Compositores de Ragtime' pero que también sabía que no sería un héroe de la música popular en su propia vida. "Cuando lleve veinticinco años muerto, la gente me va a reconocer,' le dijo a un amigo." Justo treinta años después fue reconocido, y luego el historiador Rudi Blesh escribió un gran libro acerca del ragtime, el cual dedicó a la memoria de Joplin.

Aunque no tenía un centavo y decepcionado al final de su vida, Joplin estableció el estándar para las composiciones del ragtime y desempeñó un papel clave en el desarrollo del rag. Y como compositor e intérprete pionero, ayudó a abrir el camino para los jóvenes artistas negros para llegar a las audiencias americanas de ambas razas. Después de su muerte, el historiador de jazz Floyd Levin observó: "Esos pocos que se dieron cuenta de su grandeza inclinaron sus cabezas con dolor. Este fue el fallecimiento del rey de todos los escritores de ragtime, el hombre que dio a América una auténtica música nativa.

Después de su muerte en 1917, la música y el ragtime de Joplin en general disminuyeron en popularidad a medida que surgieron nuevas formas de estilos musicales, como el jazz y el piano novedad. Aun así, las bandas de jazz y artistas como Tommy Dorsey en 1936, Jelly Roll Morton en 1939 y J. Russell Robinson en 1947 relanzaron grabaciones de composiciones de Joplin. Maple Leaf Rag fue la pieza de Joplin que se encuentra con mayor frecuencia en discos de 78 rpm.

En la década de 1960, un despertar a pequeña escala del interés por el ragtime clásico estaba en marcha entre algunos estudiosos de la música estadounidense como Trebor Tichenor, William Bolcom, William Albright y Rudi Blesh. Audiophile Récords lanzó un conjunto de dos discos, "The Complete Piano Works of Scott Joplin, The Greatest of Ragtime Composers", interpretados por Knocky Parker, en 1970.

En 1968, Bolcom y Albright le hicieron interesarse a Joshua Rifkin, un joven musicólogo, en el cuerpo del trabajo de Joplin. Juntos presentaron una ocasional noche de ragtime y jazz temprano en la radio WBAI. En noviembre de 1970, Rifkin lanzó una grabación llamada "Scott Joplin: Piano Rags" bajo el clásico sello Nonesuch, se vendieron 100.000 copias en su primer año y eventualmente se convirtió en el primer millón de ventas de discos de Nonesuch. La lista de la cartelera de los "LP clásicos más vendidos" del 28 de septiembre de 1974 lo tiene registrado en el número 5, con el que le sigue, "Volumen 2", en el número 4, y el combinado de los dos volúmenes en el número 3. Separadamente, ambos volúmenes han estado en la lista durante 64 semanas. Al tope de los mejores siete de esa lista, seis de los registros eran grabaciones de las obras de Joplin, tres de las cuales eran de Rifkin. Las discográficas se vieron por primera vez poniendo rag en la sección de música clásica. El álbum fue nominado en 1971 en dos categorías de los premios Grammy: Mejores Notas de Álbum y Mejor Performance de Solista Instrumental (sin orquesta). A Rifkin también se lo consideró para un tercer Grammy por una grabación no relacionada con Joplin, pero en la ceremonia del 14 de marzo de 1972 Rifkin no logró ganar en ninguna categoría. Hizo una gira en 1974, la cual incluyó apariciones en la BBC TV y terminó el concierto en el London´s Royal Festival Hall. En 1979 Alan Rich escribió en la revista "New York Magazine" que por darle a artistas como Rifkin la oportunidad de poner la música de Joplin en un disco Nonesuch Récords "...creó casi solo, el renacimiento de Scott Joplin.

En enero de 1971, Harold C. Schonberg, el crítico musical del New York Times, habiendo recién escuchado el álbum de Rifkin, escribió un artículo destacado de la edición del domingo titulado: "¡Estudiantes, ocúpense de Scott Joplin!" El llamado a la acción de Schonberg ha sido descrito como el catalizador para los estudiosos de música clásica, el tipo de gente con la que Joplin había batallado toda su vida, para llegar a la conclusión de que Joplin fue un genio. Vera Brodsky Lawrence de la biblioteca pública de Nueva York, publicó en dos volúmenes conjuntos el trabajo de Joplin en junio de 1971, titulado "The Collected Works of Scott Joplin", estimulando un interés más amplio en la interpretación musical de Joplin.

A mediados de febrero de 1973 bajo la dirección de Gunther Schuller, The New England Conservatory Ragtime Ensemble grabó un álbum de los rags de Joplin, tomados del período de "Standard High-Class Rags" llamado "". El álbum ganó un premio Grammy como Mejor Interpretación de Música de Cámara de ese año, y siguió hasta convertirse en el Mejor Álbum de Clásica de 1974 según la revista "Billboard". El grupo posteriormente grabó dos álbumes más para Golden Crest Récords: "More Scott Joplin Rags" en 1974 y "The Road From Rags To Jazz" en 1975.

En 1973, el productor cinematográfico George Roy Hill contactó con Schuller y Rifkin separadamente, pidiéndole a cada uno escribir la partitura para un proyecto de película sobre la que estaba trabajando: "El golpe (película de 1973)". Ambos personajes rechazaron la solicitud por compromisos previos. En cambio Hill encontró a Marvin Hamlisch disponible y lo trajo al proyecto como compositor. Hamlisch adaptó ligeramente la música de Joplin para El Golpe, por la cual ganó un premio Óscar a la mejor banda sonora y mejor adaptación musical el 2 de abril de 1974. Su versión de "The Entertainer" alcanzó el número 3 en el Billboard Hot 100 y la American Top 40 de música el 18 de mayo de 1974, provocando que el "The New York Times" escriba: "La nación entera ha comenzado a comprender". Gracias a la película y su partitura, el trabajo de Joplin comenzó a apreciarse tanto en el mundo de la música popular como el de la clásica mundial, favoreciendo (en las palabras de la revista musical Record Word), el "fenómeno clásico de la década". Rifkin luego dijo de la banda sonora de la película, que Hamlish tomó sus adaptaciones de piano directamente del estilo de Rifkin y sus adaptaciones de banda del estilo de Schuller. Schuller dijo de Hamlisch,"... consiguió el Óscar por la música que él no escribió (porque es de Joplin) y los arreglos que no escribió y las 'ediciones' que no hizo. Mucha gente se ofendió por eso, pero ¡así es el negocio del "espectáculo"!.

El 22 de octubre de 1971, se presentaron extractos de "Treemonisha" en forma de concierto en el Lincoln Center con actuaciones musicales de Bolcom, Rifkin y Mary Lou Williams, apoyando a un grupo de cantantes. Por último, el 28 de enero de 1972, la orquestación de Treemonisha de T. J. Anderson fue puesta en escena en una producción de ópera completa por dos noches consecutivas, auspiciada por el Taller de Música Afroamericana del Morehouse College en Atlanta, con cantantes acompañados por la Orquesta Sinfónica de Atlanta bajo la dirección de Robert Shaw, y coreografía de Katherine Dunham. Schonberg observó en febrero de 1972 que el "Renacimiento de Scott Joplin" contaba con pleno envión y aún creciendo. En mayo de 1975, "Treemonisha" fue puesta en escena en una producción de ópera completa por la Houston Grand Opera. La compañía hizo una gira rápida y luego se quedó ocho semanas seguidas en Broadway, Nueva York, en el Palace Theater entre octubre y noviembre. Estas apariciones fueron dirigidas por Gunther Schuller y la soprano Carmen Balthrop alternó con Kathleen Battle como el personaje del título. Se produjo una grabación del "reparto original de Broadway". Debido a la falta de exposición nacional dada a la breve puesta en escena del Morehouse College en 1972, muchos estudiosos de Joplin escribieron que la función de la Houston Grand Opera de 1975 fue la primera gran producción de la obra.

1974 vio al Royal Ballet, bajo la dirección de Kenneth MacMillan, crear la Elite Syncopations, un ballet basado en canciones de Joplin y otros compositores de la época. Ese año también trajo el estreno del Ballet de Los Ángeles de "Red Back Book", coreografiada por John Clifford para rags Joplin de la colección del mismo nombre, que incluye tanto actuaciones de piano solista como actuaciones y arreglos para orquesta completa.

1970: Joplin ha sido incluido dentro de los compositores del Salón de la Fama de la Academia Nacional de la Música Popular.

1976: a Joplin se le dio un Premio Pulitzer Especial..." otorgado póstumamente en el Año del Bicentenario de la nación, por su contribución en la Música Americana.

1977: Producciones Motown, produjo Scott Joplin, una biografía fílmica con el actor Billy Dee Williams como Joplin, lanzado por Universal Pictures.

1983: El Servicio Postal de los Estados Unidos emitió un sello conmemorativo del compositor como parte de su herencia negra.

1989: Joplin recibió una estrella en el Corredor de la Fama de San Luis.

2002: una colección de las propias actuaciones de Joplin registradas en los rollos de pianola en la década de 1900 fue incluida por la Junta Nacional de Preservación de Grabaciones en el Registro de Grabaciones de la Biblioteca del Congreso Nacional. La Junta anualmente selecciona canciones que son "... cultural, histórica o estéticamente significativas".




</doc>
<doc id="6507" url="https://es.wikipedia.org/wiki?curid=6507" title="Marcación decádica por pulsos">
Marcación decádica por pulsos

La marcación decádica por pulsos es una tecnología de señalización en telecomunicaciones en la cual un circuito de bucle local es interrumpido de acuerdo a una codificación definida, usualmente un dígito. Cada uno de los diez dígitos es codificado en secuencias de hasta diez pulsos y de ahí su nombre. Históricamente, el dispositivo más común para producir estos pulsos es el disco de marcar del teléfono. Otro término, el de discado por desconexión de bucle, surge del método que se emplea que es el de la interrupción del circuito local.

La velocidad de generación de los pulsos, fue históricamente determinada basándose en el tiempo de respuesta necesario para que los sistemas de conmutación electromecánica operaran confiablemente. La mayoría de los sistemas telefónicos usó una velocidad de 10 pulsos/segundo, pero el discado por operador dentro y entre las centrales telefónicas a veces usaban velocidades de 20 pulsos/segundo.

Los sistemas de centrales telefónicas automáticas fueron desarrollados al final del siglo XIX e inicios del siglo 20. Para ser identificados, a los suscriptores del servicio les fue asignado un número telefónico único por cada circuito. La primera central telefónica automática diseñada por el empresario estadounidense y su sobrino Walter S. Strowger fue abierta en La Porte (Indiana), Estados Unidos el 3 de noviembre de 1892, con equipos fabricados por la compañía "Strowger Automatic Telephone Exchange" y utilizó aparatos telefónicos que tenían tres manipuladores telegráficos, que debían ser pulsados en un número adecuado de veces para controlar indicar al equipo de la central el número del receptor y un cuarto manipulador era usado, si el usuario se equivocaba al marcar.Sin embargo, el uso de ese sistema resultaba impráctico. El sistema más común de señalización fue el de la interrupción del bucle local mediante un tren de pulsos de corriente directa generado en los teléfonos de los suscriptores...

En vista de lo inconveniente del sistema de pulsadores, el 11 de enero de 1898, a tres de los socios de la compañía de Almon Strowger, Alexander Keith, John y Charles Erickson se les concedió la patente 597.062 de "un dispositivo de llamada para centrales telefónicas" en el cual se incluía un disco rotatorio para el marcado del teléfono de destino, pero este disco no tenía huecos sino bordes similares a los de una rueda dentada. Los pulsos eran enviados cuando el usuario giraba el disco hasta una posición de tope diferente para cada dígito transmitido. La operación libre de errores del disco requería de un movimiento de rotación suave por parte del usuario, pero este sistema se consideró poco fiable. Este mecanismo fue pronto refinado para incluir un resorte recuperador y un regulador centrífugo para controlar la velocidad de retroceso, innovación que ya aparecía en 1905. El usuario seleccionaba un número mediante la inserción de un dedo en el orificio del disco giratorio correspondiente hasta un tope. Cuando se liberaba de esta posición, los contactos de marcación se abrían y cerraban varias veces, interrumpiendo así la corriente de bucle. La central telefónica decodificaba el patrón de cada dígito transmitido de este modo de transmisión mediante relés de paso o por acumulación en los llamados registradores de dígitos. Inicialmente, los discos de marcar se fabricaban de metal, hasta que el 3 de junio de 1941 la Oficina de Patentes de Estados Unidos concedió al estadounidense Frank A. Cosgrove, empleado de AT&T, la patente 2.244.609 por inventar un disco de plástico, que se convirtió en estándar en lo sucesivo. 

Cuando el sistema de conmutación electromecánica aún estaba en uso, los pulsos de corriente generados por el disco giratorio activaban relés en los interruptores o conmutadores de la central telefónica. La naturaleza mecánica de estos relés y la capacitancia del bucle, afectaban la forma del pulso, limitando por lo general la velocidad de la operación a diez pulsos por segundo.

Las especificaciones de Bell Systems en los EE.UU. requerían que el personal de servicio ajustara los discos de marcar en las equipos de los clientes con una precisión de 9,5 a 10,5 impulsos por segundo, pero la tolerancia de los equipos de conmutación era generalmente de entre 8 y 11 puntos porcentuales.

En algunos teléfonos, los pulsos se pueden escuchar en el receptor como chasquidos. Cada dígito está representado por un número diferente de pulsos. En la mayoría de los países, un pulso identifica al dígito 1, dos pulsos de 2, y así sucesivamente, con diez pulsos para el dígito 0; lo que hace que el código sea unario, excepto para el dígito 0. Excepciones a esto, ocurrieron en Suecia, con un solo pulso para 0, dos pulsos para 1, y así sucesivamente; y en Nueva Zelanda, con diez pulsos para 0, nueve pulsos para 1, etc. En Oslo, la capital de Noruega, se usó el sistema de marcación del sistema de Nueva Zelanda, pero el resto del país no lo hizo. Los sistemas que utilizan esta codificación de los 10 dígitos en una secuencia de hasta 10 pulsos, se conocen a veces como sistemas de marcación decádicos.

Más tarde, algunos sistemas de conmutación utilizaron registros de dígitos que duplicaron la tasa de pulsos permisible a 20 pulsos por segundo, y que reducían la pausa entre dígitos ya que la selección del interruptor no tenía que ser completada durante la pausa. Algunos de estos sistemas incluyeron centrales de barras cruzadas, la versión 7A2 del sistema de conmutación Rotary, y las primeras centrales telefónicas de la década de 1970 de control por programa almacenado.

En 1963, Bell System introdujo la tecnología de marcación por tonos conocida también como DTMF ("Dual-Tone Multifrequency", en idioma inglés) bajo la marca registrada "Touch-Tone" utilizando teléfonos con teclado. En las décadas siguientes, la marcación por pulsos se ha ido suprimiendo gradualmente como método de señalización primaria a la central telefónica, pero muchos sistemas siguen apoyando teléfonos de disco para ofrecer compatibilidad. Algunos modelos de teléfonos de teclado tienen un interruptor para seleccionar la marcación por tonos o pulsos.

En los sistemas de voz sobre Protocolo de Internet (VoIP), la marcación de dígitos se puede sustituir por completo al iniciar una llamada telefónica mediante la especificación del identificador de recursos uniforme del receptor.


</doc>
<doc id="6509" url="https://es.wikipedia.org/wiki?curid=6509" title="Darwinismo">
Darwinismo

El darwinismo es un término con el que se describen las ideas de Charles Darwin, especialmente en relación a la evolución biológica por selección natural.

El darwinismo no es sinónimo de evolucionismo, dado que este último es anterior a Charles Darwin: las teorías darwinistas son evolucionistas, pero su aportación clave es el concepto de selección natural considerado determinante para explicar la causa de la evolución y que en su posterior desarrollo, con numerosas aportaciones y correcciones, permitirá la formulación de la teoría de la evolución actual o síntesis evolutiva moderna. Por tanto es igualmente equivocado usar el término «darwinismo» para referir la actual teoría de la evolución, ya que esta no se reduce solo a las ideas postuladas por Charles Darwin.

Para el biólogo evolutivo Ernst Mayr el término «darwinismo» tiene a lo largo de la historia y desde 1859 (año de publicación de la obra de Darwin "El origen de las especies") al menos nueve usos diferentes. Al principio el darwinismo solo significaba anticreacionismo.
Si alguien explicaba el cambio evolutivo acudiendo a causas naturales y no divinas era tachado de «darwinista» (por ejemplo, Thomas Henry Huxley y Charles Lyell).

El uso del término variará conforme las diversas teorías y subteorías que contenían los postulados los cuales fueron poco a poco siendo aceptados, para después ser matizados, corregidos y completados hasta la formulación, en la década de 1940 a 1950, de la síntesis evolutiva moderna. Desde entonces puede decirse que el paradigma darwinista resiste frente a los ataques sufridos y el reduccionismo, su formulación básica está vigente y parece que puede durar: la evolución es el resultado de la variación genética y de su ordenamiento mediante la eliminación y la selección.

Las concepciones evolucionistas de Darwin constituyen un complejo sistema teórico, un conjunto de teorías relacionadas, más que una teoría singular. El núcleo de esas concepciones sigue conservando toda su validez, a pesar de su natural insuficiencia y de algún error significativo, sobre todo en su explicación de la herencia a través de pangénesis. En el darwinismo hay tres ejes teóricos que explican distintos aspectos de la realidad biológica.


La teoría propuesta por Darwin de la evolución de las especies por medio de la selección natural de las variaciones genéticas lleva implícita una visión de los seres vivos que se puede clasificar como materialista.
El ser humano no ocupa ningún lugar privilegiado dentro del mundo vivo. Las causas finales no encuentran acomodo en el mecanicismo darwiniano. No hay lugar en la teoría evolutiva para la emergencia de una «mente» en el sentido dualista, pues la generación y evolución de los sistemas nerviosos son procesos estrictamente biológicos y, por ende, físicos.

Las formulaciones que Darwin hace de sus teorías fueron influidas en un alto grado por un lenguaje aprendido de sociólogos o publicistas (politólogos), como Thomas Malthus y Herbert Spencer. Como el propio Alfred Russel Wallace reconoció, la lectura de Malthus fue decisiva para la formulación de la teoría de la selección natural. Las ideas malthusianas se conocían y discutían en los ambientes intelectuales de la época. Conceptos como competencia, lucha por la vida y sobrepoblación, que aparecen en "Ensayo sobre el principio de la población" de Thomas Malthus, sirvieron tanto a Alfred Russel Wallace como a Darwin para dar forma a sus teorías.

En pleno auge de la teoría de la selección natural propuesta por Charles Darwin, y tras las controversias iniciales, el concepto de la selección natural y las relaciones interespecíficas fueron trasladadas a las relaciones sociales; sin embargo, no existe un método claro de aplicar el uno a las otras y, así, bajo el término peyorativo de "darwinismo social" se han calificado ideologías, muchas veces contrapuestas que, lo mismo podían defender el "laissez faire" que el socialismo de estado, el imperialismo o la eugenesia a escala local. Muchas de estas tendencias tienen poco que ver con las ideas de Darwin quien, ciertamente, defendió la eugenesia voluntaria en su libro "La herencia del hombre y la selección en relación con el sexo"; pero no por imposición.






</doc>
<doc id="6516" url="https://es.wikipedia.org/wiki?curid=6516" title="Literatura de México">
Literatura de México

La literatura de México es una de las más prolíficas de la lengua española cuyo antecedente radica en la literatura de los pueblos indígenas de Mesoamérica . Entre los escritores más importantes, notorios y reconocidos a nivel internacional están José Joaquín Fernández de Lizardi, Sor Juana Inés de la Cruz, Juan Rulfo, Juan José Arreola, Elena Garro, Octavio Paz, Rosario Castellanos, José Gorostiza, Carlos Fuentes, Amado Nervo, Jaime Sabines, Federico Gamboa, José Emilio Pacheco, Alfonso Reyes, Fernando del Paso y Ramón López Velarde por mencionar solo algunos.
Históricamente, con la llegada de los españoles se produjo un proceso de mestizaje que luego dio paso a una época de criollización de la literatura producida en la Nueva España. El mestizaje de la literatura novohispana es evidente en la incorporación de numerosos términos de uso corriente en el habla local del Virreinato y en algunos de los temas que se tocaron en las obras del periodo. Durante la época Virreinal, la Nueva España albergó a escritores barrocos como Bernardo de Balbuena, Carlos de Sigüenza y Góngora, Juan Ruiz de Alarcón, Francisco de Castro, Luis de Sandoval y Zapata, Sor Juana Inés de la Cruz, llamada "La décima musa". Muy destacados todos, y que dieron la lucha inicial por la emancipación de la literatura nacional de la literatura de la península: Diego José Abad, Francisco Javier Alegre y fray Servando Teresa de Mier.

Hacia el final del régimen colonial, en Nueva España surgieron figuras como José Joaquín Fernández de Lizardi cuya obra, "El Periquillo Sarniento", es considerada el emblema de la picaresca mexicana y la primera novela moderna escrita en el continente americano. Hacia la segunda mitad de ese siglo, surgen obras como "Los mexicanos pintados por sí mismos", libro costumbrista que nos da una idea aproximada de cómo veían los intelectuales de la época al resto de sus coterráneos. Hacia el final del siglo, durante el Porfiriato, los escritores mexicanos se inclinaron hacia las tendencias dominantes de la época. Para celebrar el centenario de la Independencia de México, se preparó la llamada "Antología del Centenario", que pretendía recopilar autores de los primeros cien años de México, pero quedó trunca y se publicó solo el primer tomo en dos volúmenes que, sin embargo, recogen la poesía. Los grandes poetas de la época son fray Manuel de Navarrete, Fernando Calderón e Ignacio Rodríguez Galván. Destacan la pléyade de poetas modernistas como Amado Nervo y Manuel Gutiérrez Nájera. De la misma época y que recopiló la "Antología del Centenario", Luis G. Urbina. De reconocido prestigio, Efrén Rebolledo, José Juan Tablada, Enrique González Martínez y Ramón López Velarde.

La irrupción de la Revolución mexicana favorece el desarrollo del género periodístico. Una vez concluido el conflicto civil, la revolución se convirtió en un tema recurrente en novelas, cuentos y obras teatrales en las plumas de Mariano Azuela o Rodolfo Usigli. Esta tendencia sería antecedente del florecimiento de una literatura nacionalista, que tomó cuerpo en la obra de escritores como Rosario Castellanos o Juan Rulfo. También aparece en escena una literatura de corte indigenista, que pretende retratar el pensamiento y la vida de los pueblos indígenas de México, aunque irónicamente, ninguno de los autores fuera indígena. Entre ellos hay que señalar a Miguel Ángel Menéndez Reyes, a Ricardo Pozas y a Francisco Rojas González. De modo alterno a estas corrientes dominantes, se desarrollaron en el país otros movimientos menos conocidos por estar fuera del foco principal. Entre ellos hay que señalar a los "estridentistas" (década de 1920), como Arqueles Vela y Manuel Maples Arce. 

Otro movimiento de gran relevancia para la historia literaria del país lo constituyó el grupo de "Los Contemporáneos," quienes aparecieron durante la década de los años 30, el cual fue conformado por el periodista Salvador Novo y los poetas Xavier Villaurrutia y José Gorostiza. Ya hacia la segunda mitad del siglo XX, la literatura mexicana se había diversificado en temáticas, estilos y géneros. Surgen nuevos grupos, como "La onda" (década de 1960), que apostaba por una literatura urbana, satírica y contestataria; entre los autores destacados están Parménides García Saldaña y José Agustín; "Los infrarrealistas" (década de 1970), que pretendía "volarle la tapa de los sesos a la cultura oficial"; "La mafia" (década de 1960), conformada por Carlos Fuentes, Salvador Elizondo, José Emilio Pacheco, Carlos Monsiváis, Inés Arredondo, Fernando Benítez y otros. En 1990, Octavio Paz se convirtió en el único mexicano hasta la fecha que ha ganado el Premio Nobel de Literatura. En la década de 2010, surge un nuevo movimiento literario, a partir de la revista literaria El Comité 1973, movimiento que tiene lugar en el centro del país, entre Ciudad de México y la Ciudad de Pachuca, Hidalgo. En donde, entre los escritores que se reúnen, se encuentran: Guadalupe Flores Liera, Claudia Hernández de Valle Arizpe, Meneses Monroy, Agustín Cadena y Juan Antonio Rosado.

A pesar de que los pueblos de Mesoamérica desarrollaron sistemas de escritura, estos no fueron empleados para conservar la literatura de esos pueblos. La mayor parte de los mitos y obras literarias de los pueblos de Mesoamérica se transmitieron por tradición oral. Se sabe, por ejemplo, que entre las actividades que tenían que dominar los novicios de sacerdotes entre los mexicas se encontraba la memorización de obras líricas o de la mitología de su pueblo. Algunas de estas producciones fueron fijadas para siempre por medio del alfabeto latino que los misioneros de Indias emplearon en el siglo XVI para transcribir la información que recibían de los indígenas. Especialistas modernos como Ángel María Garibay K. y Miguel León-Portilla, han traducido estas obras que se encontraban desperdigadas en varios textos y las han reunido o reseñado en obras como "Visión de los vencidos", "Poesía indígena de la Altiplanicie" o "Historia de la literatura náhuatl".

La obra de los misioneros en el centro de México permitió conservar más fielmente la tradición oral de los pueblos de habla náhuatl, en comparación con los habitantes de otras zonas de Mesoamérica. En ese sentido resulta especial el conjunto de obras líricas atribuidas a Acolmiztli Nezahualcóyotl (1402–1472), "tlatoani" de Tetzcuco, que pasó a la posteridad con el título de "Rey Poeta""". Sus obras, junto con las de otros nobles de los pueblos nahuatlacas del Eje Neovolcánico como Ayocuan (de Chalco-Atenco) y Tecayehuatzin (de Huexotzinco), constituyen la muestra más amplia de obras líricas y filosóficas precolombinas recuperadas para la posteridad. De menores dimensiones es el acervo literario recuperado entre otros pueblos del Posclásico, como los purépechas, los zapotecos y los mixtecos. El caso de los mixtecos es especial, puesto que se conservan cuatro códices que han permitido hacer una aproximación a la historia de ese pueblo bajo la impronta de Ocho Venado, "yya" (Señor) de Tilantongo y Tututepec. Por otra parte, en el Área Maya, se conservaron fragmentos de los llamados "Libros de Chilam Balam". Bien conocida es, por otra parte, la literatura precolombina de los quichés, pueblo mayance que sin embargo no habitó en el actual territorio mexicano, sino en lo que hoy es Guatemala. En idioma quiché se escribió el "Popol Wuh" o "Libro del Consejo", que incorpora dos mitos cosmogónicos mayas: la creación del mundo y el descenso de Hunahpú e Ixbalanqué a Xibalbá, el inframundo de los mayas.

Por último, fuera de Mesoamérica, Arturo Warman adelanta como hipótesis que las coplas interpretadas por los músicos yaquis y mayos durante la ejecución de la Danza del Venado tengan su origen en la época precolombina, y que hayan llegado hasta nuestros días con pocos cambios desde entonces.

Entre los pueblos prehispánicos floreció:

También fueron reconocidos por sus hermosas obras como el sentimiento musical

En la literatura virreinal de México podemos distinguir varios periodos. En el primero la literatura está vinculada con el momento histórico de la conquista, en él abundan las cartas y crónicas.

Obras y escritores:


En este periodo floreció el arte barroco. Muchos de los autores conocidos del siglo incursionaron con mayor o menor éxito en el terreno de los juegos literarios, con obras como anagramas, emblemas y laberintos. Hubo autores notables en la poesía, la lírica, la narrativa y la dramaturgia. Subgéneros: soneto, décima, octava real, romance, epigrama, glosa, centón, quintilla, vaya, redondilla, redondilla de pie quebrado, romance con asonantes forzosos.

Autores:

Surgieron escritores ilustrados y clasicistas como:


Durante el siglo XIX hubo tres grandes corrientes literarias: el romanticismo, el realismo-naturalismo y el modernismo.

Los escritores románticos se agruparon en torno a cientos de asociaciones; entre las más importantes la Academia de Letrán, fundada en 1836 (José María Lacunza, Guillermo Prieto, Manuel Carpio, Andrés Quintana Roo, José Joaquín Pesado, Ignacio Rodríguez Galván, Ignacio Ramírez), y el Liceo Hidalgo, fundado en 1850 (Ignacio Manuel Altamirano, Manuel Acuña, Manuel M. Flores). A quienes se etiquetó como neoclásicos o académicos, en oposición a la categoría de "románticos" que se les daba a los primeros. A este primer grupo también pertenecen José Manuel Martínez de Navarrete, Vicente Riva Palacio, Joaquín Arcadio Pagaza, Justo Sierra y Manuel José Othón.

Más tarde, durante el auge del positivismo el gusto estético cambió. Entre los escritores mexicanos realistas y naturalistas tenemos a Luis G. Inclán, Rafael Delgado, Emilio Rabasa, José Tomás de Cuéllar, Federico Gamboa y Ángel de Campo "Micrós".

Dentro de la corriente modernista, revolución literaria originaria de América Latina, hubo numerosas innovaciones métricas y de rima, resurgimiento de formas en desuso y, principalmente, hallazgos simbólicos. Entre 1895 y 1910 México se volvió un núcleo de actividad modernista; entre los escritores tenemos a Manuel Gutiérrez Nájera, Enrique González Martínez, Salvador Díaz Mirón y Amado Nervo.




En los años que van de 1900 a 1914, siguió predominando en la poesía el modernismo y en la prosa el realismo y naturalismo. Durante este periodo, convivieron los representantes de la literatura decimonónica con los integrantes del Ateneo de la juventud.
De 1915 a 1930 hubo tres corrientes: una renovación estilística que incorporaba influencias de las vanguardias europeas (el estridentismo (Manuel Maples Arce, Germán List Arzubide, Arqueles Vela) y los Contemporáneos), un grupo de escritores retomaba temas coloniales (Xavier Villaurrutia, Jaime Torres Bodet, Jorge Cuesta, José Gorostiza, Salvador Novo), y otros que comenzaron a publicar las llamadas «novelas de la Revolución» (la más conocida es "Los de abajo" de Mariano Azuela): Martín Luis Guzmán, Rafael F. Muñoz, Heriberto Frías, Jorge Ferretis, Nellie Campobello, Francisco L. Urquizo.

Hasta mediados de la década de 1940 hubo autores que continuaron con narrativa realista, pero también conocieron su auge la novela indigenista y las reflexiones en torno al ser y la cultura nacional. Surgieron dos nuevas generaciones poéticas, agrupadas en torno a las revistas "Taller" y "Tierra Nueva".

Con la publicación de "Al filo del agua" de Agustín Yáñez en 1947 comenzó lo que llamamos «novela mexicana contemporánea», que incorporó técnicas entonces novedosas, influencias de escritores estadounidenses (William Faulkner y John Dos Passos), e influencia europea (James Joyce y Franz Kafka), y en 1963, la hasta entonces conocida por sus artículos en periódicos y revistas y su hermoso teatro, Elena Garro, publica la novela "Los recuerdos del porvenir". Si bien durante el periodo que va de 1947 a 1961 predominaron los narradores (Arreola, Rulfo, Fuentes), surgieron entonces poetas de valía como Rubén Bonifaz Nuño y Rosario Castellanos (también narradora).

En 1960 se editó la antología "La espiga amotinada", que agrupó al importante grupo de poetas: Juan Bañuelos, Oscar Oliva, Jaime Augusto Shelley, Eraclio Zepeda y Jaime Labastida. Las revistas literarias fueron uno de los principales vehículos de difusión de los escritores, de manera que se tiende a agrupar a muchos de ellos bajo el nombre de las revistas en las que participaron. "El Hijo pródigo" fue dirigida por Xavier Villaurrutia, del grupo "Los Contemporáneos", quien tuvo como colaborador a Octavio Paz. Octavio Paz fundó, tras su salida del periódico "Excélsior", la revista "Vuelta", que encabezó durante muchos años la cultura nacional, fundamentalmente tras la muerte de Martín Luis Guzmán en 1976. Tras la muerte de Octavio Paz, un grupo de sus colaboradores trató de fundar una revista que ocupara su lugar, pero la revista naciente, "Letras libres", no logró tener la aceptación que tuvo "Vuelta".

En 1979, Gabriel Zaid hace un censo de poetas que publica en su antología "Asamblea de poetas jóvenes de México"; entre quienes han destacado de los incluidos, como poetas, Eduardo Hurtado, Alberto Blanco, Coral Bracho, Eduardo Casar, Eduardo Langagne, Manuel Ulacia, Vicente Quirarte, Víctor Manuel Mendiola, Dante Medina, Verónica Volkow, Perla Schwartz, Jaime Moreno Villarreal y Francisco Segovia. Estos y el resto de los incluidos son quienes conforman actualmente el grupo de autores en la cúspide de sus carreras literarias. La mayoría colaboró en "Vuelta".
Tal vez los poetas actuales de mayor envergadura sean Elsa Cross y Efraín Bartolomé, cuyas voces se hacen escuchar con gran fuerza en los grandes medios.






Le llamaron Generación de la Casa del Lago o de la "Revista Mexicana de Literatura", y, al igual que el movimiento de "Ruptura", un objetivo primordial de estos escritores fue dejar de lados los sentimientos nacionalistas y la literatura indigenista para lanzarse a una expresión literaria mucho más universal, siendo sus principales modelos diversos artistas mexicanos que de igual forma propusieron una expresión artística más amplia, tales como los integrantes del llamado "Ateneo de la Juventud", los autores contemporáneos y la llamada "Generación Taller".
No hay una fecha precisa o año exacto en el que pueda definirse el comienzo de esta corriente; sin embargo, el año 1956 fue crucial para este grupo de artistas, por ser el año en que Octavio Paz publicó su ensayo "El arco y la lira", donde hacía referencia a las características de la escritura, de la poesía y la novela, de la vocación mística de la literatura, de lo sagrado y el misterio del arte. Esta obra fue de suma importancia para los escritores de la Generación de la Casa del Lago, ya les marcó la pauta acerca de las características que anhelaban transmitir en sus obras.
José Emilio Pacheco consideraba que 1958 fue otro año esencial para estos intelectuales, marcado por la publicación de la famosa obra del aclamado escritor Carlos Fuentes, "La región más transparente", considerada la primera novela urbana por excelencia.
Un factor que favoreció la cohesión de los autores de esta corriente, fue su integración en diversas instancias culturales con la ayuda de Jaime García Terrés, quien entre 1953 y 1965 ocupó el cargo de Director de Difusión Cultural de la Universidad Nacional Autónoma de México.
Papel fundamental desempeña en la unión de esta generación de escritores, la fundación de "La Casa del Lago", de cuyo nombre se adquiere el mote de dicha generación, ubicada en el Bosque de Chapultepec en la Ciudad de México y cuyo primer director fue el talentoso Juan José Arreola quien se encargó de reunir a una serie de artistas que sacudieron el panorama cultural de la época. En los diversos espacios se experimentó, se rompió con las formas dominantes del arte en las diversas disciplinas y con frecuencia se arriesgó hasta nuevos límites, convirtiendo al foro universitario de Chapultepec en un referente del arte emergente y un espacio de formación e información de lo que sucede en el arte en otros países.
La literatura de los escritores de la Generación de la Casa del Lago tiene un fin en sí misma y está llena de claves secretas, de influencias de escritores de otras latitudes que solo ellos conocían en México, siempre hay mucho más que leerles entre líneas, pudiendo decirse que en forma esencial se agrupó bajo las siguientes características:









Hasta la fecha no ha habido otra generación que los supere en talento y productividad. Ningún otro grupo en México ha alcanzado tal calidad literaria y nivel de compromiso con el arte. Los más jóvenes, Carlos Monsiváis (1938-2010) y José Emilio Pacheco (1939-2014), se convirtieron en íconos de la cultura nacional.

Los autores más representativos de esta corriente literaria son:













</doc>
<doc id="6522" url="https://es.wikipedia.org/wiki?curid=6522" title="Ley de Ohm">
Ley de Ohm

La ley de Ohm, "postulada por" el físico y matemático alemán Georg Simon Ohm, es una ley básica de los circuitos eléctricos. Establece que la diferencia de potencial formula_1 que aplicamos entre los extremos de un conductor determinado es directamente proporcional a la intensidad de la corriente formula_2 que circula por el citado conductor. Ohm completó la ley introduciendo la noción de resistencia eléctrica formula_3; que es el factor de proporcionalidad que aparece en la relación entre formula_1 e formula_2:

La fórmula anterior se conoce como "fórmula general de la ley de Ohm", y en la misma, formula_1 corresponde a la diferencia de potencial, formula_3 a la resistencia e formula_2 a la intensidad de la corriente. Las unidades de esas tres magnitudes en el sistema internacional de unidades son, respectivamente, voltios (V), ohmios (Ω) y amperios (A).
En física, el término "ley de Ohm" se usa para referirse a varias generalizaciones de la ley originalmente formulada por Ohm. El ejemplo más simple es:

donde J es la densidad de corriente en una localización dada en el material resistivo, E es el campo eléctrico en esa localización, y "σ" (sigma) es un parámetro dependiente del material llamado conductividad. Esta reformulación de la ley de Ohm se debe a Gustav Kirchhoff.

Georg Simon Ohm nació en Erlangen (Alemania) el 16 de marzo de 1789 en el seno de una familia protestante, y desde muy joven trabajó en la cerrajería de su padre, el cual también hacía las veces de profesor de su hijo. Tras su paso por la universidad dirigió el Instituto Politécnico de Núremberg y dio clases de física experimental en la Universidad de Múnich hasta el final de su vida. Falleció en esta última ciudad el 6 de julio de 1854.

Poniendo a prueba su intuición en la física experimental consiguió introducir y cuantificar la resistencia eléctrica. Su formulación de la relación entre intensidad de corriente, diferencia de potencial y resistencia constituye la ley de Ohm, por ello la unidad de resistencia eléctrica se denominó ohmio en su honor.

Sufrió durante mucho tiempo la reticencia de los medios científicos europeos para aceptar sus ideas pero finalmente la Real Sociedad de Londres lo premió con la Medalla Copley en 1841 y la Universidad de Múnich le otorgó la cátedra de Física en 1849.

En 1840 estudió las perturbaciones sonoras en el campo de la acústica fisiológica (ley de Ohm-Helmholtz) y a partir de 1852 centró su actividad en los estudios de carácter óptico, en especial en los fenómenos de interferencia.
Años antes de que Ohm enunciara su ley, otros científicos habían realizado experimentos con la corriente eléctrica y la tensión. Destaca el caso del británico Henry Cavendish, que experimentó con la botella de Leyden en 1781 pero no llegó a publicar sus conclusiones, hasta que casi 100 años después, en 1879, James Clerk Maxwell las publicó.

En la actualidad disponemos de muchos instrumentos que nos permiten medir con precisión la tensión (voltaje) y la corriente eléctrica pero en el muchos dispositivos, tales como la pila Daniell y la pila de artesa, no estaban disponibles. Los aparatos que medían la tensión y la corriente de la época no eran suficientes para obtener lecturas precisas para el desarrollo de la fórmula que George S. Ohm quería obtener.

Es por ello por lo que Ohm, mediante los descubrimientos que otros investigadores realizaron anteriormente, creó y modificó dispositivos ya fabricados para llevar a cabo sus experimentos. La balanza de torsión de Coulomb es uno de estos aparatos; fue descrito por Ohm en su artículo «"Vorläufige Anzeige des Gesetzes, nach welchem Metalle die Contactelectricität leiten"», publicado en 1825 en los "Anales de la Física". Ohm incluyó en la balanza una barra magnética gracias a los avances de Hans Christian Ørsted, que en 1819 descubrió que un cable conductor por el que fluía una corriente eléctrica desviaba una aguja magnética situada en sus proximidades. Con esto y varios cables de distintas longitudes y grosor, una pila voltaica y recipientes de mercurio, pudo crear un circuito en el que buscaba relacionar matemáticamente la disminución de la fuerza electromagnética creada por una corriente que fluye por un cable y la longitud de dicho cable.

Mediante este circuito llegó a encontrar una expresión que representaba correctamente todo los datos obtenidos:

Esta relación la puso en entredicho el propio Georg Ohm; sin embargo fue la primera expresión documentada que le llevó a su relación entre la corriente formula_2, la tensión formula_1 y la resistencia formula_11 de un circuito: la ley de Ohm, publicada en 1827 en su artículo «El circuito galvánico, analizado matemáticamente» («Die galvanische Kette, mathematisch bearbeitet»): 

Este último artículo recibió una acogida tan fría que lo impulsó a presentar la renuncia a su cargo de profesor de matemáticas en el colegio jesuita de Colonia. Finalmente, en 1833 aceptó una plaza en la Escuela Politécnica de Núremberg en la que siguió investigando.

La importancia de esta ley reside en que verifica la relación entre la diferencia de potencial en bornes de una resistencia o impedancia, en general, y la intensidad de corriente que circula a su través. Con ella se resuelven numerosos problemas eléctricos no solo de la física y de la industria sino también de la vida diaria como son los consumos o las pérdidas en las instalaciones eléctricas de las empresas y de los hogares. También introduce una nueva forma para obtener la potencia eléctrica, y para calcular la energía eléctrica utilizada en cualquier suministro eléctrico desde las centrales eléctricas a los consumidores. La ley es necesaria, por ejemplo, para determinar qué valor debe tener una resistencia a incorporar en un circuito eléctrico con el fin de que este funcione con el mejor rendimiento.

En un diagrama se muestran las tres formas de relacionar las magnitudes físicas que intervienen en la ley de Ohm, formula_1, formula_11 e formula_2.

La elección de la fórmula a utilizar dependerá del contexto en el que se aplique. Por ejemplo, si se trata de la curva característica I-V de un dispositivo eléctrico como un calefactor, se escribiría como: I = V/R. Si se trata de calcular la tensión V en bornes de una resistencia R por la que circula una corriente I, la aplicación de la ley sería: V= R I. También es posible calcular la resistencia R que ofrece un conductor que tiene una tensión V entre sus bornes y por el que circula una corriente I, aplicando la fórmula R = V/ I.

Una forma mnemotécnica más sencilla de recordar las relaciones entre las magnitudes que intervienen en la ley de Ohm es el llamado "triángulo de la ley de Ohm": para conocer el valor de una de estas magnitudes, se tapa la letra correspondiente en el triángulo y las dos letras que quedan indican su relación (teniendo en cuenta que las que están una al lado de otra se multiplican, y cuando quedan una encima de la otra se dividen como en un operador matemático común).

En los circuitos de alterna senoidal, a partir del concepto de impedancia, se ha generalizado esta ley, dando lugar a la llamada ley de Ohm para circuitos recorridos por corriente alterna, que indica:

siendo formula_2 corresponde al fasor corriente, formula_1 al fasor tensión y formula_17 a la impedancia.

Algunas partículas presentan una propiedad fundamental de la materia llamada carga eléctrica. Para estudiar la corriente eléctrica interesa ver cómo se desplazan esas cargas, es decir cómo se mueven las partículas elementales con una carga asociada como los electrones o los iones. La corriente se define como la carga neta que fluye a través de un área transversal formula_18 por unidad de tiempo.

Su unidad en el SI es el amperio (A). Un amperio es un culombio por segundo (electrones/segundo). Dado que en el movimiento de las cargas pueden intervenir tanto cargas positivas como negativas, por definición se adopta el criterio de que la corriente eléctrica tiene el sentido del movimiento de cargas positivo.

Tal y como está definida la corriente, parece que la velocidad a la que se desplazan los electrones es constante. Sin embargo, para conseguir una corriente eléctrica es necesario que las cargas estén sometidas a un campo eléctrico formula_19. El campo eléctrico es la fuerza por unidad de carga. Por tanto, al establecer una corriente eléctrica se ejerce sobre las cargas una fuerza eléctrica formula_20 y sobre las partículas cargadas se producirá, por tanto, una aceleración, tal y como señala la primera ley de Newton. Cada electrón experimenta una fuerza formula_20; por tanto, la aceleración es

siendo formula_22 la masa de la partícula cargada. Como formula_19 es constante y la masa y la carga también, entonces formula_24 también es constante.
El razonamiento anterior es válido cuando las cargas se mueven en el vacío y, por tanto, sin encontrar ningún obstáculo a su movimiento. Sin embargo, al desplazarse las cargas (electrones) por el interior de un material, por ejemplo en un metal, chocan reiteradamente con los iones de la estructura del metal, de forma que la velocidad definitiva con la que se mueven las cargas es constante. A esta velocidad (formula_25) se le llama velocidad de arrastre o de deriva. 

El fenómeno de los choques se puede interpretar como una fuerza de rozamiento o resistiva que se opone a formula_26 hasta el punto de anularla, y entonces la velocidad neta de las cargas es constante. En cierta manera el fenómeno es similar al de las gotas de lluvia que en lugar de caer con una aceleración constante ( formula_27 ), alcanzan una velocidad límite constante en su caída debido a la presencia de aire.

La densidad de corriente formula_28 es un vector que lleva la dirección de la corriente y el sentido del campo eléctrico que acelera las cargas (si el material es lineal) como se explica en la Ley de Ohm en forma local. El vector formula_28 establece, además, una relación directa entre la corriente eléctrica y la velocidad de arrastre formula_30 de las partículas cargadas que la forman. Se supone que hay formula_31 partículas cargadas por unidad de volumen. Se tiene en cuenta también que la formula_32 es igual para todas las partículas. En estas condiciones se tiene que en un tiempo formula_33 una partícula se desplazará una distancia formula_34.
Se elige un volumen elemental tomado a lo largo del conductor por donde circula la corriente y se amplía para observarlo mejor. Por ejemplo, el volumen de un cilindro es igual a formula_35. El número de partículas dentro del cilindro es formula_36. Si cada partícula posee una carga formula_37, la carga formula_38 que fluye fuera del cilindro durante el tiempo formula_33 es formula_40.

La corriente por unidad de área trasversal se conoce como densidad de corriente formula_41.
La densidad de corriente, y por tanto el sentido de circulación de la corriente, lleva el signo de las cargas positivas, por ello sustituimos en la expresión anterior formula_37 por formula_43 y se obtiene, finalmente, lo siguiente:
La densidad de corriente se expresa como un vector cuyo sentido es el del campo eléctrico aplicado al conductor. Su expresión vectorial es:

Si por ejemplo se tratara de electrones, su carga formula_37 es negativa y el sentido de su velocidad de arrastre formula_30 también negativo; el resultado sería, finalmente, positivo.

Las aplicaciones más generales sobre la corriente eléctrica se realizan en conductores eléctricos, siendo los metales los más básicos. En un metal los electrones de valencia siguen el llamado modelo de electrón libre, según el cual los electrones de valencia de un metal tienen libertad para moverse y están deslocalizados, es decir, no se pueden asociar a ningún ion de la estructura porque están continuamente moviéndose al azar, de forma similar a las moléculas de un gas. Las velocidades de los electrones dependen de la temperatura del material conductor; a la temperatura ambiente estas velocidades térmicas son elevadas, pudiendo alcanzar valores de formula_46. Ahora bien, el hecho de que se desplacen no quiere decir que haya una corriente eléctrica: el movimiento que llevan a cabo es desordenado y al azar, de forma que en conjunto el desplazamiento de unos electrones se compensa con el de otros y el resultado es que el movimiento neto de cargas es prácticamente nulo.

Cuando se aplica un campo eléctrico formula_19 a un metal los electrones modifican su movimiento aleatorio de tal manera que se arrastran lentamente en sentido opuesto al del campo eléctrico. De esta forma la velocidad total de un electrón pasa a ser la velocidad que tenía en ausencia de campo eléctrico más la provocada por el campo eléctrico. Así, la trayectoria de este electrón se vería modificada. Aparece, pues, una velocidad neta de los electrones en un sentido que recibe el nombre de velocidad de arrastre formula_48. Los valores numéricos de esta velocidad son bajos pues se encuentran en torno a los formula_49.
Si se toma como tiempo τ el tiempo promediado entre colisiones del electrón con los iones atómicos, usando la expresión de la aceleración que provoca un campo eléctrico sobre una carga, se obtiene la velocidad de arrastre formula_50. Sustituyendo en la ecuación anterior para la densidad de corriente formula_28, se llega a "la ley de Ohm microscópica o en forma local".

donde σ es la llamada conductividad eléctrica que relaciona directamente la densidad de corriente formula_28 en un conductor y el campo eléctrico aplicado al mismo formula_53. En materiales lineales u óhmicos esta relación es lineal y a mayor campo eléctrico aplicado, mayor será la densidad de corriente generada, con su misma dirección y sentido ya que es una ley vectorial.

A partir de la ley de Ohm en forma local se puede obtener la ley de Ohm macroscópica, generalmente usada. Para ello se parte de un conductor metálico de sección formula_54 por donde circula una corriente formula_2 y se toma una longitud formula_56 del mismo. Entre los dos extremos del tramo se aplica una diferencia de potencial formula_57. Por tanto, si se sustituye en la expresión anterior sucede que 

Por definición, la relación entre la densidad J y la intensidad I de la corriente eléctrica que circula a través del conductor es formula_58 y formula_11 es una propiedad importante del material conductor que se llama resistencia eléctrica, que es inversamente proporcional a la conductividad del material y que representa una medida de la oposición del conductor a la conducción eléctrica.

La ley de Ohm determina que para algunos materiales —como la mayoría de los conductores metálicos— la densidad de corriente formula_41 y el campo eléctrico formula_61 se relacionan a través de una constante formula_62llamada conductividad, característica de cada sustancia. Es decir:

Esta es la ley de Ohm en forma local, obtenida a partir de la noción del campo eléctrico que acelera a los electrones que se desplazan libremente por el metal conductor. Gracias a ella se ha obtenido la ley clásica o macroscópica:

Para los metales y casi todos los otros conductores, R es constante; esto es, no depende de la cantidad de corriente. En algunos materiales, y notablemente en los materiales semiconductores, R no es constante y este hecho es muy útil en rectificadores, amplificadores y otros aparatos.

Aquellos materiales cuya resistencia es constante se conocen como lineales u óhmicos, mientras que aquellos donde no es constante se los denomina no lineales o no óhmicos. En ciertos materiales no lineales, la relación formula_63 o curva característica Volt-Ampere, tiene algunos tramos lineales donde puede suponerse que R es constante. Además, los elementos no lineales pueden clasificarse en simétricos y asimétricos; siendo los primeros aquellos cuyas características formula_63 no dependen de los sentidos de las corrientes ni de las tensiones en sus extremos, y los segundos resultan aquellos cuyas características formula_63 son diferentes para distintos sentidos de las corrientes y de las tensiones.

Esta ley contiene menos información, al ser escalar, que la ley para la densidad de corriente (que incluye módulo, dirección y sentido por su naturaleza vectorial).

No se puede considerar la ley de Ohm como una ley fundamental de la naturaleza ya que solo la cumplen ciertos materiales por lo que se considera una relación empírica. Sin embargo, esta ley tiene aplicación práctica para una gran variedad de materiales, en especial los metales.

El inverso de la conductividad es la resistividad; que es la resistencia eléctrica específica de un determinado material, se simboliza con la letra griega rho minúscula (ρ) y se mide en ohmios metro.

Una diferencia de potencial formula_66 mantenida a través de un conductor establece un campo eléctrico formula_67 y este campo produce una corriente formula_68 que es proporcional a la diferencia de potencial. Si el campo se considera uniforme, la diferencia de potencial formula_69 se puede relacionar con el campo eléctrico formula_67 de la siguiente forma:

Por tanto, la magnitud de la densidad de corriente en el cable formula_71 se puede expresar como:

Puesto que formula_72, la diferencia de potencial puede escribirse como:

La cantidad formula_73 se denomina resistencia formula_11 del conductor. La resistencia es la razón entre la diferencia de potencial aplicada a un conductor formula_75 y la corriente que pasa por el mismo formula_68:

Dicha igualdad representa un caso particular de la ecuación formula_77, donde la sección del conductor es uniforme y el campo eléctrico creado también, lo que permite expresar el ohmio (formula_78) como unidad de la resistencia de la siguiente manera:

Dado que formula_11 es igual a formula_80, la resistencia de un conductor cilíndrico determinado es proporcional a su longitud e inversamente proporcional al área de su sección transversal.

La resistividad formula_81 es una propiedad de una sustancia, en tanto que la resistencia es la propiedad de un objeto constituido por una sustancia y con una forma determinada. Las sustancias con resistividades grandes son malos conductores o buenos aislantes, e inversamente, las sustancias de pequeña resistividad son buenos conductores.

La resistividad de cada material óhmico depende de las propiedades de dicho material y de la temperatura y, por otro lado, la resistencia de una sustancia depende de la forma del material y de la resistividad. En general, la relación funcional entre la temperatura y la resistividad de un metal puede calcularse a partir de la relación polinómica:

En el rango de temperaturas de 0ºC a 200ºC, la resistividad de un metal varía aproximadamente de manera lineal con la temperatura de acuerdo con la expresión:

Donde formula_82 es la resistividad a cierta temperatura formula_83 (en grados Celsius), formula_84 es la resistividad a determinada temperatura de referencia formula_85 (que suele considerarse igual a 20º C) y formula_86 es el coeficiente de temperatura de resistividad.

Nótese que los valores de formula_86 son en general positivos, salvo para el carbono, el germanio y el silicio.

Dado que en un objeto determinando, la resistencia es proporcional a la resistividad, se puede denotar la variación en su resistencia como:

A partir de la fórmula anterior se pueden realizar determinaciones de temperatura, a partir de la medición de la resistencia de un objeto.

Para los metales la resistividad es casi proporcional a la temperatura, aunque siempre hay una zona no lineal a muy bajas temperaturas donde resistividad suele acercarse a un determinado valor finito según la temperatura se acerca al cero absoluto. Esta resistividad cerca del cero absoluto se debe, sobre todo, a choques de electrones con impurezas e imperfecciones en el metal. En contraposición, la resistividad de alta temperatura (la zona lineal) se caracteriza, principalmente, por choques entre electrones y átomos metálicos.

La disminución de la resistividad a causa a la temperatura, con valores de formula_86 negativos, es debida al incremento en la densidad de portadores de carga a muy altas temperaturas. En vista de que los portadores de carga en un semiconductor a menudo se asocian con átomos de impurezas, la resistividad de estos materiales es muy sensible al tipo y concentración de dichas impurezas.


Los metales son materiales que conducen bien el calor y la electricidad. Cuando una corriente eléctrica circula por un hilo conductor, este se calienta. Dicho fenómeno se conoce como efecto Joule, se debe a que los metales presentan cierta resistencia al paso de la corriente eléctrica por su interior, ya que cuando se mueven sufren colisiones con los átomos del material. Sin embargo, en un material superconductor esto no ocurre; estos materiales no ofrecen ninguna resistencia al paso de la corriente eléctrica por debajo de una cierta temperatura formula_90, llamada temperatura crítica.
Los electrones se agrupan en parejas interaccionando con los átomos del material de manera que logran sintonizar su movimiento con el de los átomos, desplazándose sin sufrir colisiones con ellos. Esto significa que no se calientan, por lo que no hay pérdida de energía al transportar la corriente eléctrica debido al efecto Joule. La teoría básica que explica su comportamiento microscópico se llama 'teoría BCS' porque fue publicada por Bardeen, Cooper y Schrieffer en 1957. Sin embargo, en sentido estricto, no hay una única teoría CBS sino que agrupa a un cierto número de ellas, que son en parte fenomenológicas.

El valor de formula_90 depende de la composición química, la presión y la estructura molecular. Algunos elementos como el cobre, la plata o el oro, excelentes conductores, no presentan superconductividad.

La gráfica resistencia-temperatura para un superconductor sigue la de un metal normal a temperaturas por encima de formula_90.

Cuando la temperatura alcanza el valor de formula_90, la resistividad cae repentinamente hasta cero. Este fenómeno fue descubierto en 1911 por el físico neerlandés Heike Kamerlingh Onnes, de la Universidad de Leiden. Onnes estudió a principios del las propiedades de la materia a bajas temperaturas. Su trabajo le llevó al descubrimiento de la superconductividad en el mercurio al ser enfriado a –269 °C. Sus esfuerzos se vieron recompensados en 1913 cuando se le concedió el .

Recientes mediciones han demostrado que las resistividades de superconductores por debajo de sus valores de temperaturas críticas son inferiores a formula_94 —aproximadamente formula_95 veces más pequeños que la resistividad del cobre— y en la práctica se consideran iguales a cero. Actualmente se conocen miles de superconductores y las temperaturas críticas de los superconductores son bastante más elevadas de lo que en principio se pudo suponer.

En 1986 Johannes Georg Bednorz y Karl Alexander Müller (ganadores del Premio Nobel en 1987), en unos laboratorios de IBM en Suiza, descubrieron los materiales superconductores cerámicos. Estos materiales han revolucionado el mundo de la superconductividad al poder trabajar a temperaturas por encima de la de ebullición del nitrógeno líquido (–169 °C), lo que permite enfriarlos con mucha facilidad y de forma barata. Dichos materiales superconductores han logrado que aumente el interés tecnológico para desarrollar un gran número de aplicaciones.

Una de las características más importantes de los superconductores es que una vez que se ha establecido en ellos una corriente, esta persiste sin necesidad de una fuerza electromotriz aplicada debido a la práctica ausencia de resistencia. Se han observado corrientes estables que persisten en circuitos superconductores durante varios años sin un decaimiento aparente.

En 1933 Walter Meissner y Robert Ochsenfeld descubrieron que un material superconductor no solamente no presenta resistencia al paso de corriente, sino que también cuenta entre sus propiedades la capacidad para apantallar un campo magnético. Si enfriamos el superconductor por debajo de su temperatura crítica y lo colocamos en presencia de un campo magnético, este crea corrientes de apantallamiento capaces de generar un campo magnético opuesto al aplicado. Esto ocurre hasta que el campo magnético alcanza un valor, llamado campo magnético crítico, momento en el que el superconductor deja de apantallar el campo magnético y el material recupera su estado normal.

El hecho de que el superconductor pueda apantallar totalmente el campo magnético de su interior se conoce como superconductividad tipo I. Los superconductores tipo II permiten que el campo magnético pueda penetrar en su interior sin dejar de ser superconductores. Este comportamiento se mantiene para campos magnéticos cuyo valor puede ser hasta varios millones de veces el campo magnético terrestre. Mientras que los superconductores tipo I siempre intentan expulsar el campo magnético de su interior, los de tipo II se oponen a que este cambie.

Llamamos efecto Joule al fenómeno irreversible por el cual si en un conductor circula corriente eléctrica, parte de la energía cinética de los electrones se transforma en calor debido a los choques que sufren con los átomos del material conductor por el que circulan, elevando la temperatura del mismo. Llega un momento en el que la temperatura del conductor alcanza el equilibrio térmico con el exterior, comenzando entonces a disipar energía en forma de calor. El nombre es en honor a su descubridor, el físico británico James Prescott Joule.

El movimiento de los electrones en un conductor es desordenado; esto provoca continuos choques entre los electrones y los átomos móviles de la red y como consecuencia aparece un aumento de la temperatura en el propio conductor pues transforma energía cinética en calorífica de acuerdo con la siguiente ecuación y tomando como unidades [P]=W=vatios, [V]=V=voltios, [I]=A=amperios, [E]=J=julios, [t]=s=segundos,

para la potencia disipada en un tramo conductor que tiene una tensión V entre sus extremos y circula a través una corriente I. Además, la energía que habrá disipado al cabo de un tiempo t será:

De las dos ecuaciones se deduce: formula_96 

Según Joule, «la cantidad de energía calorífica producida por una corriente eléctrica depende directamente del cuadrado de la intensidad de la corriente, del tiempo que esta circula por el conductor y de la resistencia que opone el mismo al paso de la corriente». Con [R]=Ω=ohmios. Si sustituimos en esta ecuación, la ley de Ohm clásica formula_97, se obtiene la ley de Joule en su forma más clásica: 

Asimismo, ya que la potencia disipada es la energía perdida por unidad de tiempo, podemos calcular la potencia disipada en un conductor o en una resistencia de las siguientes tres maneras:

El funcionamiento eléctrico y las aplicaciones de numerosos electrodomésticos se fundamentan primero en la ley de Ohm, y en segundo lugar, sus implicaciones energéticas, en la ley de Joule. En algunos de estos aparatos eléctricos como los hornos, las tostadoras, las calefacciones eléctricas y otros empleados industrialmente, el efecto útil buscado es precisamente el calor que desprende el conductor por el paso de la corriente. En la mayoría de las aplicaciones, sin embargo, es un efecto indeseado y la razón por la que los aparatos eléctricos y electrónicos (como el ordenador) necesitan un ventilador que disipe el calor generado y evite el calentamiento excesivo de los diferentes dispositivos.

Como explica la ley de Ohm, para que circule corriente por un circuito es necesario aportar una energía para mantener una diferencia de potencial y crear el campo eléctrico que acelera las cargas. Se denomina fuerza electromotriz formula_98 (FEM) a la energía necesaria para transportar la unidad de carga positiva a través de un circuito cerrado. Esta energía proviene de cualquier fuente, medio o dispositivo que suministre la energía eléctrica, como puede ser una pila o una batería. Para ello se necesita mantener una diferencia de potencial formula_99 entre dos puntos o polos de dicha fuente que sea capaz de impulsar las cargas eléctricas a través de un circuito cerrado. En el caso de pilas o baterías la energía inicial es de origen químico que se transforma en energía eléctrica para disiparse posteriormente en el conductor por efecto Joule.

La energía suministrada al circuito puede expresarse como:

La potencia que suministra generador es: 

Comparando ambas expresiones se obtiene una posible justificación de fuerza electromotriz. Los generadores reales se caracterizan por su fuerza electromotriz y por su resistencia interna, es decir, un generador transforma en energía eléctrica otras formas de energía y cuando es recorrido por una corriente, se calienta. Esto representa una pérdida de potencia suministrada al circuito exterior. Expresión de la potencia suministrada al circuito por un generador real: 

Este balance de energías se puede analizar en un circuito cerrado básico con una batería de fem formula_98 y de resistencia interna formula_102 por el que circula una corriente formula_2 y alimenta una resistencia formula_11. Además, formula_99 es la diferencia de potencial que se aplica en los bornes del generador que por la ley de Ohm será igual a formula_106. Este balance se puede expresar como:

Significa que la potencia suministrada por el generador es igual a la suministrada al circuito exterior formula_108, más la consumida internamente formula_109.

Dividiendo la expresión anterior por la corriente eléctrica resulta lo siguiente:

Cuando un generador suministra una energía al circuito, este es recorrido por una intensidad de corriente, los electrones del circuito son acelerados por el campo eléctrico "E" y la diferencia de potencial entre las bornes del generador se reduce en el valor de la caída de potencial que se produce en su resistencia interna. La diferencia de potencial entre los bornes del generador de una corriente eléctrica I a través del circuito es: 

Si no circula corriente por el circuito (circuito abierto), al ser la intensidad nula la fuerza electromotriz coincidirá con la diferencia de potencial entre los bornes del generador.




</doc>
<doc id="6524" url="https://es.wikipedia.org/wiki?curid=6524" title="Contabilidad">
Contabilidad

La contabilidad es una disciplina que se encarga de estudiar, medir y analizar el patrimonio y la situación económica financiera de una empresa u organización, con el fin de facilitar la toma de decisiones en el seno de la misma y el control externo, presentando la información, previamente registrada, de manera sistemática y útil para las distintas partes interesadas.

La contabilidad es una disciplina técnica que a partir del procesamiento de datos sobre la composición y evolución del patrimonio de un ente, los bienes de propiedad de terceros en su poder y ciertas contingencias produce información para la toma de decisiones de administradores y terceros interesados y para la vigilancia sobre los recursos y obligaciones del ente. 

La finalidad de la contabilidad es suministrar información en un momento dado de los resultados obtenidos durante un período de tiempo, que resulta de utilidad a la toma de decisiones, tanto para el control de la gestión pasada, como para las estimaciones de los resultados futuros, dotando tales decisiones de racionalidad y eficiencia.

Actualmente existen discusiones sobre la mesa que intentan definir un estado ontológico de la contabilidad, en las que hay grandes desacuerdos sobre si esta ha logrado un estatus científico, es decir si puede ser denominada como ciencia o en su defecto, arte, lenguaje, o simplemente técnica. Para evadir esta discusión, muchas veces se opta por hacer referencia a la contabilidad como "disciplina", sin embargo, algunos autores se han esforzado por argumentar su posición referente a este tema.

Una de las definiciones ontológicas que parece tener mayor capacidad argumentada desde las concepciones de la historia de la ciencia y la epistemología, es la que la denomina como una tecnología de carácter inmaterial o "tecnología blanda". Esto no significa abandonar el carácter científico de la profesión contable, por el contrario, desplaza el centro de la atención sobre el punto de definir cierta cientificidad de la profesión contable, de la herramienta (contabilidad) al actor y operario de dicha herramienta (Contador). En este sentido, el contador sería el sujeto, que puede adquirir el estatus científico haciendo uso de una herramienta desarrollada socialmente (contabilidad), capaz de ser calibrada bajo las reglas de diferentes "normas contables" para medir de manera razonable los intercambios de recursos en una organización, bien sea una empresa privada, pública, un departamento o un país. 

Asimismo, debe reconocerse que en la mayoría de las publicaciones científicas, la palabra "ciencia" es la más utilizada para hacer referencia a la contabilidad, no obstante, adoleciendo de argumentos que sustenten el uso de la palabra. Asimismo, el uso de la palabra "disciplina" constituye una opción neutral en esta discusión ya que reúne los aspectos relativos al ejercicio contable sin tener primero que definir su estatus científico.

La historia de la contabilidad y de su técnica está ligada al desarrollo del comercio, la agricultura y la industrialización como actividades económicas. Desde su comienzo, se buscó la manera de conservar el registro de las transacciones y de los resultados obtenidos en la actividad comercial e incluso se dice que la contabilidad apareció en la historia cuando las operaciones se hacían a través del trueque y no eran liquidadas en el momento de entregar o recibir la mercancía. 

Los arqueólogos han encontrado en las civilizaciones del Imperio inca, del Antiguo Egipto y de Roma variadas manifestaciones de registros contables, que de una manera básica constituyen un registro de las entradas y salidas de productos comercializados, así como del dinero. La utilización de la moneda fue importante para el desarrollo de la contabilidad, ya que no cabía una evolución semejante en una economía de trueque.

Existe dificultad para proporcionar datos objetivos sobre el desarrollo de la contabilidad en el "Mundo Antiguo", especialmente en Roma, por la escasez de documentos conservados sobre la materia y por su desconocimiento formal sobre ésta. Sin embargo, se conoce que gozaba de un papel relevante, ya que la inscripción de préstamos en el libro contable del acreedor ("Codex rationum") y el libro de ingresos y gastos, ("codees acceti et expensi") se admitían como medio jurídico de prueba. 

Catón el Viejo, en su obra "De re rustica" (o "Res rustica"), incluye los datos fundamentales que se requerían para la contabilidad y su utilización como herramienta para evaluar la gestión de los negocios por los "factores" frente a los propietarios agrícolas que solían residir en las ciudades.

Algunos historiadores han creído observar en los fragmentos incompletos que se conservan de contabilidad, un primer desarrollo del principio de la partida doble y aunque existe mucha diversidad de opiniones sobre esta tesis, hay algunas citas de grandes autores, como Cicerón, que parecen sustentar tal hecho, pero son demasiado confusas como para establecer la tesis de que el método de la partida doble era conocido en la Antigüedad.

Las prácticas contables más o menos evolucionadas, desaparecieron en el mundo antiguo debido a la disminución del comercio en Europa durante los siglos posteriores a la caída del Imperio Romano, por lo que la contabilidad tuvo que desarrollarse partiendo de cero, especialmente al compás del auge comercial que tuvo su primer gran impulso con las cruzadas.

Dos grandes órdenes militares, la de los templarios y la de los caballeros teutónicos, durante los siglos XII y XIII desarrollaron sistemas de registros contables más o menos perfeccionados, influidos probablemente por las prácticas de los comerciantes libaneses con los que ambas órdenes tuvieron contacto en sus inicios.

Los comerciantes de la Liga Hanseática desarrollaron la «"contabilidad de factor"», es decir, la del comisionista que debe rendir cuentas a su comitente. En tanto que los mercaderes italianos prestaron mayor atención a una contabilidad de carácter patrimonial, es decir, más adaptada al contrato del comerciante sobre sus empleados.

Durante los últimos siglos de la Edad Media, las repúblicas comerciales italianas y los Países Bajos serían las regiones europeas en donde la vida comercial iba a ser más intensa. Como consecuencia natural, en estos países la práctica contable iría desarrollando nuevos métodos y por lo tanto, sería en todas estas repúblicas italianas donde surgiría la contabilidad moderna.

De los memoriales primitivos en los que los comerciantes anotaban sin ningún orden particular las diversas operaciones que precisaban recordar, fue evolucionando poco a poco hacia un sistema contable de partida simple. Debido al gran número de anotaciones necesarias, los comerciantes y prestamistas comenzaron a desglosar diversas cuentas del memorial, en las que anotaban grupos de operaciones poseedoras de alguna característica común, tales como ir referenciadas a una determinada mercadería o bien a una misma persona. 

El desarrollo progresivo de ciertas reglas prácticas y el modo de realizar los registros fue perfeccionándose cada vez más hasta que en un momento no determinado con exactitud por los historiadores, se descubrió el método de la partida doble.

La partida doble tuvo su origen probablemente en la región de la Toscana antes de finales del siglo XIII, el ejemplo más antiguo de su uso son las cuentas públicas de la ciudad de Génova del año 1340. En el siglo XV, parece ser que los banqueros y comerciantes toscanos disponían de una técnica contable tan desarrollada o más que la empleada por los venecianos, y diferente en algunos puntos importantes de la de estos. Sin embargo fue la contabilidad "a la veneciana" la que se impuso, gracias a la imprenta, que permitió su difusión antes que ninguna otra.

En el Renacimiento, la aparición del concepto de capital productivo y el desarrollo del crédito sentaron los fundamentos necesarios para la elaboración de un sistema contable. Surge en primer lugar las cuentas que reflejaban los créditos y los débitos de las personas. Por extensión, se pensó en llevar una cuenta para el conjunto de los bienes poseídos y otra que presentara las ganancias o las pérdidas. Este conjunto de cuentas condujo a la elaboración del sistema contable de partida doble.

Los historiadores estiman que la contabilidad por partida doble apareció hacia 1340 en Génova (Italia). La invención de la imprenta generalizó este método, en particular desde la publicación de los tratados de Luca Pacioli, cuya primera obra, editada en 1494 bajo el título "Summa de Arithmetica, Geometría, Proportioni et Proportionalitá", enuncia los principios fundamentales en el capítulo relativo a las cuentas y libros.

El primer autor del que tenemos noticia que estableció claramente el uso del método de la partida doble fue Benedetto Cotrugli (en eslavo, Kotruljević), nacido en la actual Dubrovnik en Croacia, entonces una ciudad comercial adriática del área de influencia veneciana, llamada Ragusa.

Cotrugli residió en Nápoles gran parte de su vida, y fue comerciante y consejero del rey Fernando I de Aragón. Su obra "Della Mercature e del Mercante Perffeto" fue escrita en 1458. De forma casi incidental, dedica uno de sus capítulos al modo de llevar las cuentas mencionando distintos libros: "El Memorial, el Diario y el Mayor", al que denomina "Quaderno". Enumera también algunas reglas generales para contabilizar las operaciones comerciales, pero en conjunto las referencias de Benedetto a la contabilidad del comerciante son incompletas.

El libro de Cotrugli tardó casi ciento quince años en ser llevado a la imprenta, lo que, unido al carácter incompleto de su exposición, impide que pueda adjudicarse a su autor en la historia de la contabilidad un papel comparable al de Luca Pacioli. El trabajo de este último fue impreso y conocido muchos años antes, aunque escrito con posterioridad al de Benedetto.

Luca Pacioli, o Luca de Borgo Sancti Sepulchri, nació en el pueblo Umbría, provincia italiana en el año de 1455. Estudió en Venecia, donde fue preceptor de los hijos de un rico mercader de la ciudad, del que probablemente aprendió los procedimientos contables que luego expuso en su magna obra "Summa de Arithmetica, Geometría, Proportioni et Proportionalità", impresa en Venecia en 1494, por lo que los ejemplares de esta edición son libros incunables. Pacioli, que parece no ingresó en la orden de San Francisco hasta edad madura, fue un gran matemático, un auténtico humanista del Renacimiento, amigo de Leonardo da Vinci, del cual se dice que incluso ilustró algunos de los textos y de otros grandes pensadores de la época, que impartió enseñanza en varias universidades italianas.

Pacioli dedicó treinta y seis capítulos del "Summa de Arithmetica, Geometría, Proportioni et Proportionalità" a la descripción de los métodos contables empleados por los principales comerciantes venecianos. El autor destinó, además, parte de sus trabajos a la descripción de otros usos mercantiles, tales como contratos de sociedad, el cobro de intereses y el empleo de las letras de cambio.

Según Pacioli las anotaciones en el libro diario constan de dos partes claramente diferenciadas: una comenzando con la palabra Por (el Debe del asiento contable) y la otra con la palabra A (el Haber del asiento contable), antecedente del modelo de asiento contable tradicional. Dado que en aquella época no era costumbre la utilización del balance de situación, sólo describe los usos en la elaboración del balance de comprobación de sumas y saldos, que era utilizado al agotarse las páginas del libro mayor.

Estas anotaciones eran efectuadas bajo las normas de la partida doble, la cual, Pacioli aseguraba que él sólo enseñaba, lo cual ya se ejecutaba mucho antes por los mercaderes. La partida doble asegura que por cada aumento del activo (en el "debe") hay un aumento en las cuentas del pasivo y capital (dentro del "haber"). Asimismo, habiendo una disminución en las cuentas del activo (dentro del debe), hay igualmente una disminución en las cuentas del pasivo y capital (dentro del haber), así efectuándose las normas de la partida doble.

La traducción en inglés fue publicada en Londres por John Gouge o Gough en 1543. Se describe como "Un Tratado Provechoso" ("A profitable treatyce"), también denominado "El Instrumento o Libro para aprender el buen orden de llevanza del famoso conocimiento llamado en Latín Dare y Habere, es decir, Debe y Haber.

Algunos de los aspectos en los que colaboró Pacioli para la contabilidad son:





En 1588, se publicó un pequeño libro de instrucción por John Mellis de Southwark, en el que dice: «Soy el renovador y revividor de una antigua copia publicada aquí en Londres, el 14 de agosto de 1543». John Mellis se refiere al hecho de que los principios de contabilidad que explica (que es un sistema de partida doble) siguen «la forma de Venecia».

Desde el nacimiento de la partida doble en el Renacimiento, la contabilidad, si bien ha sido enriquecida con desarrollos técnicos, no ha sufrido cambios fundamentales. El estudio sistemático de la historia de la contabilidad moderna comienza a mediados del siglo XIX, los italianos fueron los pioneros en la elaboración de teorías más o menos científicas basadas en la relación entre contabilidad y administración empresarial. Las principales escuelas, iniciadas a principios del fueron las siguientes:

La escuela lombarda de Francesco Villa, que aborda la elaboración de teorías más o menos científicas, distinguiendo entre la técnica y la ciencia y elaborando un conjunto de principios económico-administrativos.

Francesco Villa. Nacido en Milán en 1801, ha sido considerado el padre de la contabilidad moderna italiana. Efectivamente, su magna obra "Elementi di amministrazione e contabilitá", aparecida en Pavía en 1850, puede considerarse el punto de partida de una nueva concepción de la contabilidad, sobre bases completamente distintas a las anteriores. La mecánica de la teneduría de libros es, para este autor, un simple instrumento utilizado por la contabilidad, ciencia de contenido y ambiciones mucho más amplios, que se integra como parte fundamental en el complejo organizativo de la empresa. Los "Elementi" de Villa se dividen en tres partes, cuya enumeración ya nos permite calibrar la “modernidad” de su autor: Conceptos económico-administrativos, De la Teneduría de libros y de sus aplicaciones más usuales, y finalmente Organización administrativa y revisión de cuentas. En sus "Elementi", Villa desarrolló un estudio sistemático y profundo de la empresa desde el punto de vista de la organización, la división del trabajo, los objetivos perseguidos y los principios administrativos que deben orientar la manera de llevar los libros. Además, el autor milanés publicó muchas otras obras, no solamente sobre temas contables hasta que finalmente murió en el año 1884.

En el año 1867 apareció en Prato una obra que había de hacer célebre a su autor, Francesco Marchi (1822-1917). En ella se atacaba duramente a la doctrina de la escuela cincocuentista, seguidora del método de Degranges, que durante más de medio siglo había dominado la escena de los estudios de teoría contable en Europa.

Desde el punto de vista de Marchi son cuatro las clases de personas interesadas en la vida de la empresa:

Las cuentas se dividen en dos grupos:
Después de Marchi apareció un gran número de autores que configuraron la escuela toscana. Entre todos ellos descolló Giuseppe Cerboni.

En su obra "Primi saggi di logismografia", proponía un nuevo sistema contable. La logismografía está emparentada con la doctrina de la personificación de las cuentas, pero visto desde un punto de vista jurídico, en la que el hecho contable crea una relación contable entre personas, naturales o jurídicas que se anota, utilizando una cuenta para cada una de las personas implicadas. Según Cerboni, la contabilidad debe contemplar, antes que la actividad económica de la empresa, los actos de sus órganos administrativos, con el objeto de ejercer un control sobre ellos.

El impulsor de la escuela veneciana, Fabio Besta (escuela controlista), define el patrimonio como un conjunto de bienes o fondo de valores, analizándolo desde el punto de vista económico y las cuentas son los medios de representación de los elementos que componen el patrimonio.

Para Besta, la contabilidad aspira a ser la ciencia del control financiero o económico. La contabilidad debe encaminarse a la medición del patrimonio económico que a su vez no tiene porqué coincidir con el concepto jurídico del patrimonio.

Zappa defiende que la finalidad de la unidad económica es obtener crédito y que toda la problemática contable debe quedar subordinada a la determinación del mismo.

Sostiene que el objeto de la investigación contable es el patrimonio considerado en su aspecto estático y dinámico, cualitativo y cuantitativo y que su fin es el gobierno oportuno, prudente y conveniente de tal patrimonio.

Creador de la escuela patrimonialista fue quien configuró finalmente los alcances del paradigma de beneficio económico, al señalar que “la contabilidad tiene por objeto el estudio de los fenómenos patrimoniales, sus manifestaciones y su comportamiento trata de disciplinarlos con relación a determinado patrimonio de la empresa. De acuerdo con este paradigma las generalizaciones simbólicas se basan en los conceptos de renta y valor para la medición del patrimonio, la partida doble evolucionada a una dualidad de la empresa en marcha se constituye en el patrón metodológico de medición, las técnicas y procedimientos se seleccionan en función a su correlación y uniformidad con los conceptos fundamentales, y el sistema contable refleja adecuadamente la realidad económica (verdad económica) y suministra uni-direccionalmente la información suficiente a los usuarios potenciales. Como valores compartidos se encuentran la búsqueda de la verdad económica: el cálculo del beneficio y de la situación patrimonial, sin importar quién la recibe y por qué. Ejemplares: aparece un nuevo conformante financiero, el patrimonio, por tanto la mejor medición y representación posible de la situación patrimonial y del beneficio fueron fines de la regulación contable.

La contabilidad puede ser clasificada en dos ramas, dependiendo del criterio de división utilizado. De acuerdo con el tipo de unidad económica a la que se refiere la información contable generada se puede hacer la siguiente clasificación:

Es una disciplina que sigue el método para generar y después aplicar cierta teoría y también procesos, los cuales son:



La contabilidad nacional ofrece la representación numérica sistemática de la actividad económica de un país, durante un periodo determinado. Es elaborada por los Estados, suministra información útil que orienta la política económica del país.

Es la contabilidad de las pequeñas unidades económicas. Su objetivo es suministrar información que se utilizará en la toma de decisiones. Dentro de la microcontabilidad se distingue una contabilidad pública, ejecutada por las distintas administraciones públicas y una contabilidad privada, orientada a la empresa.

Dentro de la contabilidad empresarial, los usuarios de la información contable pueden ser divididos en dos usuarios, internos y externos. El grupo de usuarios internos comprende a todas aquellas personas u órganos que utilizan la información desde dentro de la empresa para la toma de decisiones adecuada en la dirección de la misma. Por otro lado, los usuarios externos utilizan la contabilidad para la gestión de la empresa objeto de la información, y comprenden a todos aquellos entes que no participan en la gestión, como accionistas, acreedores, prestamistas, clientes, inversores, empleados y la administración pública, especialmente la administración tributaria, y que necesitan básicamente de la información contable para tomar también decisiones y controlar la empresa desde múltiples puntos de vista. En función de los usuarios de la contabilidad se distingue entre contabilidad financiera y contabilidad directiva o de gestión:
Son cada uno de los bienes, derechos y obligaciones que forman parte del patrimonio de las empresas. El marco conceptual del Comité de Normas Internacionales de Contabilidad (International Accounting Standards Board, IASB) define cinco elementos básicos que componen la contabilidad:

Todos éstos deben seguir un itinerario lógico para su adecuada contabilización, cuyos pasos quedan reflejados en los estados financieros:


El patrimonio de una empresa es el conjunto de bienes, derechos y obligaciones relativos a una empresa que constituyen los medios económicos y financieros a través de los cuales puede cumplir sus objetivos.

El patrimonio está formado por multitud de elementos de carácter muy dispar, por lo que se denomina, como ya se mencionó, elemento patrimonial a cada uno de los bienes, derechos y obligaciones que forman parte de la empresa.

A efectos de su valoración, el patrimonio está formado por una parte positiva (activo), constituida por los bienes (elementos materiales), así como de los derechos (elementos intangibles), derivados de relaciones jurídicas de la empresa y una parte negativa (pasivo), formada por las obligaciones. La suma algebraica del valor positivo de los bienes y derechos, junto con el valor negativo de las obligaciones daría como resultado el valor del patrimonio neto.

La ecuación fundamental del patrimonio expone que se cumple cuando la suma del valor de los activos (bienes y derechos) es igual a la suma del valor de los pasivos y del patrimonio neto (capital).

ECUACIÓN CONTABLE CON EJEMPLO NUMÉRICO
véase también ecuación contable 

El activo es el conjunto de bienes (elementos materiales) y derechos (elementos intangibles) controlados económicamente por la empresa, derivados de relaciones jurídicas de propiedad, posesión, uso, crédito, etc. El cual se divide en "circulante", "fijo" y "diferido".






Se puede calcular esta partida como la diferencia entre el activo y el pasivo. Se cumple así la ecuación contable: "Activo total = Pasivo total + Patrimonio neto (Capital)"; o lo que es lo mismo, "Activo total − Pasivo total = Patrimonio neto (Capital)", siendo entonces "Activo total − Pasivo corriente − Pasivo no corriente = Patrimonio neto contable". También puede ser calculada por la agrupación o suma directa de los elementos que la componen básicamente capital más reservas más resultados del ejercicio.

La contabilidad, como ciencia, utiliza un método denominado contable, que se compone de cuatro pasos:

Las cuentas son los instrumentos de representación y medida de cada elemento patrimonial. Cada una consta de una denominación y un código numérico, que la identifican de manera única. Estos elementos identificativos son la representación de la realidad de los elementos del patrimonio, escritos en un papel o en un registro electrónico. Por tanto, hay tantas cuentas como elementos patrimoniales tenga la empresa. La regulación contable suele establecer la libertad para que cada entidad disponga las cuentas que va a utilizar en su proceso contable y el grado de detalle de su información, aunque hay legislaciones (como por ejemplo Francia, México, España o Perú) que establecen planes o manuales de cuentas orientativos para que sean utilizados por las empresas y aunque la legislación contable en materia de número y nombre de cuentas no suele ser obligatoria, si es utilizado habitualmente de forma homogénea por las empresas de un mismo país. El plan contable de una empresa es la codificación del conjunto de cuentas que utiliza una empresa, incluye todas las cuentas y las agrupaciones de las mismas.

Es en donde se registran los aumentos o disminuciones de cada partida generados por una transacción de negocios y todo el sistema contable tiene una cuenta por separado por cada clase de activo, pasivo, capital, ingresos y gastos. Cada cuenta tiene una sección para anotar los aumentos y otra para las disminuciones. La cuenta tiene dos columnas básicas para registrar las operaciones de negocios. En estas columnas se registran los aumentos y las disminuciones, que se llaman movimientos. Las columnas se identifican con el nombre de debe y haber, o bien cargo y abono.

De acuerdo con esto, por ejemplo es habitual, que existan cuentas para los inmuebles, el mobiliario y el conjunto de inmovilizado de una empresa, las mercancías, las materias primas, las deudas de clientes y los créditos con proveedores, las cuentas y préstamos bancarios, así como cuentas para los distintos gastos e ingresos existentes como pueden ser gastos de personal, financieros, de servicios recibidos. Cada empresa también dispone del grado de desarrollo que quiere utilizar en su sistema contable y las cuentas suelen agruparse en distintas partidas o grupos que reflejan los mismos conceptos de bienes o gastos.

Gráficamente se dibujan como una letra "T", donde a la parte izquierda se llama "débito" o "debe" y a la parte derecha "crédito"o "haber", sin que estos términos tengan ningún otro significado más que el indicar una mera situación física dentro de la cuenta (el debe es la parte izquierda de la cuenta y el haber es la parte derecha de la cuenta, y no representan otra cosa como lo pueden indicar las diferentes acepciones de estas palabras). Hay dos tipos de cuenta: de patrimonio y de gestión o de resultados. Las cuentas de patrimonio aparecerán en el balance y pueden formar parte del activo o del pasivo (y dentro de este, del pasivo exigible o del capital, también llamado fondos propios o patrimonio neto). Las cuentas de gestión o de resultados son las que reflejan ingresos o gastos y aparecerán en la cuenta de "Pérdidas y Ganancias".

Independientemente de si las cuentas son de patrimonio o de resultados, también se dice que por su naturaleza son deudoras o acreedoras. Las cuentas son "deudoras" cuando, siendo de patrimonio, se refieren a un activo o, siendo de gestión, se refieren a un gasto; y son "acreedoras" cuando, siendo de patrimonio, se refieren a un pasivo o a una cuenta de capital, o cuando, siendo de gestión, se refieren a un ingreso.

"Cargar" o "debitar" una cuenta es hacer una anotación en el debe, mientras que "Abonar" o "acreditar" una cuenta es hacer una anotación en el haber. En las cuentas de activo, cuando este aumenta, se cargan, y cuando disminuye, se abonan. En las cuentas de pasivo y de capital, cuando este aumenta, se abonan, y cuando disminuye, se cargan.

Por lo que se refiere al efecto que tienen las transacciones comerciales sin alterar la ecuación patrimonial, aunque cada transacción afecta el balance, cambia los valores en el patrimonio, pero sin alterar la igualdad de la ecuación. En cada una de esas transacciones, actúan por lo menos dos cuentas. Existen siete tipos de transacciones que siguen la teoría del cargo y del abono.

El sistema de partida doble consiste en que, en cada hecho contable, se ha de producir al menos un "cargo" en una cuenta y un "abono" en otra, y la suma de los cargos debe ser igual a la suma de los abonos efectuados; en otras palabras, todos los recursos que existen en una empresa son el resultado de la aplicación de recursos que tuvieron una fuente definida.

La "partida doble" como técnica contable obedece a los siguientes criterios:

Se llama saldo de una cuenta a la diferencia entre los débitos (anotaciones realizadas en el debe de una cuenta) y créditos (anotaciones realizadas en el haber de una cuenta). Cuando los débitos sean mayores que los créditos será saldo deudor, sin embargo cuando los créditos sean mayores que los débitos será saldo acreedor. Cuando los débitos sean iguales a los créditos, se entenderá que la cuenta está saldada, balanceada o sin saldo.

Cuando se han realizado todas las anotaciones contables en el libro diario se calcula el saldo de cada una de las cuentas y se elabora un estado transitorio denominado "balance de comprobación o de saldos", que es un listado de todas las cuentas abiertas con su saldo. La suma de los saldos acreedores debe ser igual a la suma de los saldos deudores, debido al sistema mencionado de partida doble.

Partiendo del balance de sumas y saldos se realiza el denominado asiento de regularización en el que se regularizan todas las cuentas de ingresos y gastos y aparece la cuenta de pérdidas y ganancias. El balance de situación se obtiene así después de regularizar el balance de comprobación.

Los libros de contabilidad son los documentos que soportan y reflejan los hechos con trascendencia en la realidad económica de la empresa a lo largo de un período de tiempo. La legislación mercantil establece cuáles son los libros contables obligatorios para las empresas. Los libros de contabilidad principales son:

El libro diario (en inglés "journal") es uno de los principales libros contables, donde se recogen, por orden cronológico, todas las operaciones de la actividad económico empresarial, según se van produciendo en el tiempo. La anotación de un hecho económico en el libro diario se denomina "asiento". Cada asiento debe reflejar la información referida a un hecho económico completo y debe estar compuesto al menos por dos apuntes o anotaciones en dos cuentas diferentes. Los asientos por definición deben estar cuadrados, lo que significa que la suma de las cantidades anotadas en un asiento en el debe han de ser iguales a las cantidades anotadas en el haber de ese mismo asiento. El que un asiento esté cuadrado manifiesta que se han tenido en cuenta todas las consecuencias del hecho económico.

Este libro (en inglés "ledger") recoge la información ya incluida en el diario, pero reordenada por cuentas, en él se recogen para cada cuenta, de acuerdo con el principio de partida doble, todos los cargos y abonos realizados en las mismas y es más fácil de llevar.

Los libros de balances (en inglés "balance sheet") reflejan la situación del patrimonio de la empresa en una fecha determinada. Los balances se crean cuando hemos pasado las cantidades de las cuentas de los asientos a su libro mayor.

Desde el punto de vista legal, la ley suele marcar el plazo durante el cual los empresarios deben conservar sus libros obligatorios (diario, inventarios y cuentas anuales) como los no obligatorios (mayor, registros de impuesto sobre el valor añadido, auxiliares, etc), así como la documentación y justificantes que sirven de soporte a las anotaciones registradas en los libros. En España, este plazo de conservación está fijado en seis años. Los libros obligatorios, deben conservarse en soporte material de papel y en una adecuada encuadernación.

El funcionamiento de la contabilidad es regulado por las normas contables, que debido a diferencias de carácter fiscal, cultural, económicas y políticas, presentan diferencias entre los países, lo que dificulta la comparabilidad de la información publicada por las empresas en distintos países. Estas normas pueden ser aprobadas de forma legal o pueden estar reguladas por entidades privadas de carácter profesional. Su contenido incluye los principios, reglas y prácticas necesarias para preparar los estados financieros.

Los denominados estados financieros o cuentas anuales son los informes que muestran de forma sintetizada, los datos fundamentales del proceso contable de un ejercicio, su formulación se realiza una vez al año, después de la terminación del ejercicio económico. Los documentos que los componen deben ser claros y expresar la imagen fiel del patrimonio, de la situación financiera y de los resultados de la empresa a la que se refieran.

Aunque cada país regula el contenido obligatorio de los estados financieros, suele estar formados por los siguientes elementos:

Los estados financieros suministran informes que pueden utilizar las instituciones para reportar la situación económica y financiera y los cambios que experimenta la misma a una fecha o periodo determinado. Esta información resulta útil para los administradores, gestores, reguladores y otros tipos de interesados como los accionistas, acreedores o propietarios.

Según el marco conceptual para la preparación y presentación de los estados financieros, existen cuatro criterios de medición:
Los activos se registran por el monto de efectivo o equivalentes de efectivo pagados, o por el valor justo del activo entregado a
cambio en el momento de la adquisición. Los pasivos se registran por el valor del producto recibido a cambio de incurrir en la obligación o, en algunas circunstancias (por ejemplo, impuesto a la renta por pagar) por los montos de efectivo o equivalentes de efectivo que se espera pagar para extinguir la correspondiente obligación.
Los activos se llevan contablemente por el monto de efectivo, o equivalentes de efectivo, que debería pagarse si se adquiriese en la actualidad el mismo activo u otro de similares características. Los pasivos se registran al monto de efectivo o equivalente de efectivo que se requiere para liquidar la obligación al momento presente.
Los activos se registran contablemente por el monto de efectivo o equivalentes de efectivo que podrían ser obtenidos, en el momento presente, en la venta no forzada de los mismos. Los pasivos se registran a sus valores de liquidación, esto es, los montos sin descontar de efectivo o equivalentes de efectivo, que se espera pagar por las obligaciones en el curso normal de las operaciones.
Los activos se registran contablemente al valor actual, descontando las futuras entradas netas de efectivo que se espera genere la
partida en el curso normal de las operaciones. Los pasivos se registran por el valor actual, descontando las salidas netas de efectivo que se necesitarán para pagar las obligaciones, en el curso normal de las operaciones.

Aunque la contabilidad se puede realizar de forma manual, actualmente está extendido el uso de aplicaciones informáticas que facilitan la labor contable. Se denomina software contable a las aplicaciones informáticas que están destinados a sistematizar y simplificar estas tareas en la empresa. Las aplicaciones pueden limitarse a la elaboración de la contabilidad o estar integrados con el resto del sistema informático de la empresa, como puede ser la facturación, nóminas, inventario etc.

En México, el marco jurídico que determina la obligación de llevar razón y cuenta de los negocios se encuentra en el Código de Comercio en su artículo 33°, así como en la Ley General de Sociedades Mercantiles, Código Fiscal de la Federación, Ley del Impuesto Sobre la Renta, Ley del Impuesto al Valor Agregado y finalmente en el reglamento de esta última. 




</doc>
<doc id="6529" url="https://es.wikipedia.org/wiki?curid=6529" title="Esporangio">
Esporangio

El esporangio es la estructura de las plantas, hongos o algas que produce y contiene las esporas. Se encuentran esporangios en las angiospermas, gimnospermas, helechos y sus parientes, en las briófitas, algas y hongos.

En relación al ciclo de vida de las plantas, en aquellas plantas cuya generación diplonte es multicelular (poseen "esporófito"), se llama esporangio a la estructura nacida en el esporófito que se especializa en llevar a cabo la meiosis que dará las esporas haploides (n).

Cuando alcanzan la madurez, los esporófitos producen esporangios en el tallo o más comúnmente en las hojas fértiles, que son los esporofilos. En ellos, los esporangios pueden situarse en la cara adaxial (haz), en el margen, o en la cara abaxial (envés). En los licófitos se forma un solo esporangio en cada esporofilo mientras que en los helechos se forman muchos, casi siempre reunidos en grupos definidos (soros). Las psilotópsida tienen los esporangios soldados en sinangios y las equisetópsida los presentan en la cara inferior de unos apéndices peltados, los esporangióforos, que se disponen formando estróbilos en el extremo de los tallos.

Atendiendo a su origen y modo de desarrollo se distinguen dos tipos básicos de esporangios: los eusporangios, que en la madurez tienen una pared formada al menos por dos capas de células y contienen un número elevado de esporas (entre 500 y un millón), y los leptosporangios, que en la madurez tienen una pared con una sola capa de células y contienen generalmente 64 esporas.

Cuando los esporangios están maduros, lo más general es que se abran para dispersar las esporas gracias a la existencia de algún mecanismo de dehiscencia en la pared esporangial. En las isoetales y en los helechos acuáticos no existe ningún sistema de apertura, y las esporas se liberan cuando se descompone la pared de los esporangios.


</doc>
<doc id="6532" url="https://es.wikipedia.org/wiki?curid=6532" title="Joseph John Thomson">
Joseph John Thomson

Joseph John "J.J." Thomson, (; Mánchester, Inglaterra, 18 de diciembre de 1856-Cambridge, Inglaterra, 30 de agosto de 1940) fue un científico británico, descubridor del electrón, de los isótopos e inventor del espectrómetro de masa. En 1906 fue galardonado con el .

Joseph John Thomson 
nació el 18 de diciembre de 1856 en Cheetham Hill, un distrito de Mánchester en Inglaterra, y tenía ascendencia escocesa. En 1870 estudió ingeniería en el Owens College, hoy parte de la Universidad de Mánchester, y se trasladó al Trinity College de Cambridge en 1876. En 1880, obtuvo su licenciatura en Matemáticas (Segunda Wrangler y segundo premio Smith) y Maestría en Artes (obteniendo el Premio Adams) en 1883. En 1884 se convirtió en profesor de Física en Cavendish. Uno de sus alumnos fue Ernest Rutherford, quien más tarde sería su sucesor en el puesto.
Thomson fue elegido miembro de la Royal Society el 12 de junio de 1884, y posteriormente fue su presidente de 1915 a 1920.
En 1897 descubrió el electrón y propuso un modelo en el cual los electrones poseían cargas negativas y se encontraban en el interior del átomo, el cual poseía carga positiva.

En 1890 se casó con Rose Elizabeth Paget, hija de "sir" Edward George Paget, médico, entonces Regius Profesor de Medicina (Regius Professor of Physic) en Cambridge. Con ella, fue padre de un hijo, George Paget Thomson, y una hija, Joan Paget Thomson. Su hijo se convirtió en un destacado físico, quien a su vez fue galardonado con el Premio Nobel de Física en 1937 por demostrar las propiedades de tipo ondulatorio de los electrones.

J. J. Thomson fue galardonado con el Premio Nobel de Física en 1906, «en reconocimiento de los grandes méritos de sus investigaciones teóricas y experimentales en la conducción de la electricidad generada por los gases». Fue nombrado caballero en 1908 y nombrado en la Orden del Mérito en 1912. En 1914 dio el Romanes Lecture en Oxford sobre la teoría atómica. En 1918 fue nombrado rector del Trinity College de Cambridge, donde conoció a Niels Bohr, donde permaneció hasta su muerte. Murió el 30 de agosto de 1940 y fue sepultado en la Abadía de Westminster, cerca de "sir" Isaac Newton.

Thomson realizó una serie de experimentos en tubos de rayos catódicos, que le condujeron al descubrimiento de los electrones. Thomson utilizó el tubo de Crookes en tres experimentos diferentes.

En su tercer experimento (1897), Thomson determinó la relación entre la carga y la masa de los rayos catódicos, al medir cuánto se desvían por un campo magnético y la cantidad de energía que llevan. Encontró que la relación carga/masa era más de un millar de veces superior a la del ion Hidrógeno, lo que sugiere que las partículas son muy livianas o muy cargadas.

Las conclusiones de Thomson fueron audaces: los rayos catódicos estaban hechos de partículas que llamó "corpúsculos", y estos corpúsculos procedían de dentro de los átomos de los electrodos, lo que significa que los átomos son, de hecho, divisibles. Thomson imaginó que el átomo se compone de estos corpúsculos en un mar lleno de carga positiva; a este modelo del átomo, atribuido a Thomson, se le llamó el modelo de pudín de pasas.

En 1906 fue galardonado con el por su "trabajo sobre la conducción de la electricidad a través de los gases".

La imposibilidad de explicar que el átomo está formado por un núcleo compacto y una parte exterior denominada "corteza" implica que otros científicos como Ernest Rutherford o Niels Bohr continuasen con su investigación y establecieron otras teorías en las que los átomos tenían partes diferenciadas...

También Thomson invento los rayos positivos y en 1911, descubrió la manera de utilizarlos para separar átomos de diferente masa. El objetivo se consiguió desviando los rayos positivos mediante campos eléctricos y magnéticos (espectrometría de masas). Así descubrió que el neón tiene dos isótopos (el neón-20 y el neón-22).

En la esquina inferior derecha de esta placa fotográfica hay marcas para los dos isótopos del neón, neón-20 y neón-22. En 1913, como parte de su exploración en la composición de los rayos canales, Thomson canalizó una corriente de neón ionizado mediante un campo magnético y un campo eléctrico y midió su desviación colocando una placa fotográfica en el camino del rayo. Thomson observó dos parches de luz sobre la placa fotográfica (ver imagen a la derecha), lo que supone dos parábolas de desviación. Thomson llegó a la conclusión de que el gas neón se compone de dos tipos de átomos de diferentes masas atómicas (neón-20 y neón-22).

Thomson en 1906 demostró que el hidrógeno tiene un único electrón. Permite confirmar o rechazar diversas teorías anteriores sobre número de los electrones, al igual que el carbono.

Thomson propuso el segundo modelo atómico (El primero fue propuesto por Dalton en 1794), que podía caracterizarse como una esfera de carga positiva en la cual se incrustan los electrones.

También analizó la propagación de ondas guiadas.

Aparte del Premio Nobel de Física (1906), le fueron concedidos los siguientes premios:

En 1991, el thomson (Th) fue propuesto por los químicos como unidad de medida masa-carga en espectroscopia de masas. Esta unidad se define como:
Sin embargo, ha pasado a ser una unidad obsoleta, y no se ha incorporado al Sistema Internacional.





</doc>
<doc id="6536" url="https://es.wikipedia.org/wiki?curid=6536" title="Sistema de Información Contable de la Seguridad Social de España">
Sistema de Información Contable de la Seguridad Social de España

La carencia de una adecuada informatización de la contabilidad se hacía notar en todos los estamentos de la seguridad social española desde mediados de la década de 1980, en la medida en la que la importancia relativa de la Seguridad Social respecto del conjunto de la economía nacional había ido creciendo sistemáticamente desde finales de la década de 1970.

Por ello, resultaba imprescindible establecer un sistema contable que, como el SICOSS (Sistema de Información Contable de la Seguridad Social), respetara las peculiaridades que presentan todas y cada una de estas entidades gestoras, permitiese un tratamiento homogéneo de sus operaciones y la consolidación de sus datos para poder ofrecer una imagen correcta de su actuación como grupo.

La informatización de la contabilidad de la Seguridad Social siguió varias etapas: 


</doc>
<doc id="6538" url="https://es.wikipedia.org/wiki?curid=6538" title="Continente">
Continente

Se considera como continente a una gran extensión de Tierra que se diferencia de otras menores o sumergidas por conceptos geográficos, como son los océanos; y culturales, como la etnografía.

La división de la Tierra en continentes es convencional, y suelen reconocerse entre cuatro y siete continentes; por ejemplo, una división en seis continentes suele ser: Asia, Antártida, Europa, África, Oceanía y América. 

Recientemente, se ha descrito también al nuevo continente hundido de Zelandia situado debajo de Nueva Zelanda, lo que reformaría el número total de continentes. 

La palabra continente viene del latín "continente", que significa «mantener juntos» y deriva del "continentes tierra", «las tierras continuas». Literalmente, el término se refiere a una gran extensión de tierra firme en la superficie del globo terrestre. Así, hay algunos modelos de continentes que consideran Europa y Asia como dos continentes, mientras que Eurasia se considera una región geopolítica, y otros lo hacen a la inversa.

En realidad, la noción de continente es una convención creada en Europa a partir del siglo XVI. Anteriormente el concepto continente era muy vago e impreciso; la diferencia entre una isla, una península y un continente era solo el tamaño, y muchas veces no concordaba la misma definición continental entre diversos autores. En general, se sostenía la idea de la unidad de toda las tierras emergidas rodeada por un mar primordial.A pesar de que en la actualidad, la partición en continentes se toma como un partición natural, y de ahí también la identificación con una unidad etnográfica en cada continente, en realidad se trata de una invención de los geógrafos, exploradores, científicos etc europeos, quienes fueron dividiendo las tierras emergidas. Está clasificación pasó a ser la "división natural" para el resto del mundo.

En realidad no existe una única forma de fijar el número de continentes y depende de cada área cultural determinar si dos grandes masas de tierra unidas forman uno o dos continentes, y en concreto, decidir los límites entre Europa y Asia (Eurasia) por una parte y América del Norte y América del Sur (América o "las Américas") por otra. Los principales modelos son los siguientes:






Los continentes del modelo usado por los países hispanohablantes son los siguientes:


La primera distinción semejante al concepto de continente fue hecha por los antiguos navegantes griegos, que dieron el nombre de Europa y Asia a las tierras localizadas a ambos lados de la extensión de agua que formaban el mar Egeo, el estrecho de los Dardanelos, el mar de Mármara, el Bósforo y el mar Negro. Esos nombres se emplearon en principio para designar a las tierras cercanas a la costa, y sólo después se extendió su uso al interior del país. Los pensadores de la Grecia antigua debatieron a continuación si África (entonces llamada Libia) debería ser considerada como una parte de Asia o como una tercera parte independiente del mundo y finalmente se impuso la división de la Tierra en tres partes. Desde la perspectiva griega, el mar Egeo era el centro del mundo, con Asia al este, Europa al oeste y al norte, y África al sur.

Los límites entre los continentes no quedaron fijados. Desde el principio, la frontera de Europa con Asia fue tomada partiendo del mar Negro a lo largo del río Rioni (conocido entonces como Phasis) en Georgia. Más tarde, se consideró que la frontera iba desde el mar Negro por el estrecho de Kerch, el mar de Azov y el río Don (llamado entonces Tanais), en Rusia.

El río Nilo se ha considerado generalmente desde antiguo la frontera entre Asia y África. Heródoto, en el siglo V antes de Cristo, sin embargo, ya se oponía a esta situación, que emplazaba Egipto a caballo entre dos continentes, y hacía coincidir el límite entre Asia y África con la frontera occidental de Egipto, dejando Egipto en Asia. También cuestionó la división en tres de lo que era en realidad una sola tierra continua, un debate que continúa casi dos milenios y medio más tarde.

Eratóstenes, en el siglo III antes de Cristo, señaló que algunos geógrafos dividían los continentes siguiendo los ríos (el Nilo y el Don), considerándolos por tanto como «islas», mientras que otros los dividían por istmos, siendo así que entonces serían «penínsulas». Estos geógrafos fijaban la frontera entre Europa y Asia en el istmo entre el mar Negro y el mar Caspio y la frontera entre Asia y África, en el istmo entre el mar Rojo y la desembocadura del lago Bardawil en el mar Mediterráneo.

En la época romana y en la Edad Media, algunos escritores tomaron el istmo de Suez como el límite entre Asia y África, pero la mayoría de los autores siguieron considerando que el río Nilo, o la frontera occidental de Egipto, eran el límite entre continentes. En la Edad Media, el mundo se representaba en un mapa de T en O, con la T aludiendo a las aguas de la división de los tres continentes.

Cristóbal Colón atravesó el océano Atlántico hasta llegar a las Antillas el 12 de octubre del 1492, allanando el camino para la exploración y colonización europea de América. A pesar de sus cuatro viajes hacia el oeste, Colón nunca llegó a saber que había arribado a un continente nuevo y pensaba que había alcanzado las costas de Asia. En 1501 Américo Vespucio viajó a América como piloto de una expedición que navegó a lo largo de la costa de Brasil. Los miembros de la expedición recorrieron un largo camino en dirección sur a lo largo de la costa de América del Sur, lo que confirmó que la tierra que bordeaban tenía proporciones continentales. De regreso a Europa, Vespucio publicó un relato de su viaje titulado "Mundus Novus" en 1502 o 1503, pero parece que ha habido adiciones o modificaciones por otro escritor. Cualquiera que sea el autor de esas palabras que se pueden leer en "Mundus Novus", «He descubierto un continente en aquellas regiones meridionales que está habitado por más numerosos pueblos y animales que nuestra Europa, o Asia o África» es la primera identificación explícita de América, como un continente distinto de los otros tres.

Desde que Vespucio anunciara el hallazgo del nuevo continente, este había recibido varios nombres, cuya aplicación y aceptación era generalmente regional. Así, los castellanos lo llamaban «Indias» o «La gran Tierra del Sur»; los portugueses, «Vera Cruz» o «Tierra de Santa Cruz». Algunos cartógrafos empleaban «Tierra del Brasil» (que sin embargo aludía a una isla imaginaria), «Tierra de Loros», «Nueva India», o simplemente «Nuevo Mundo». Después de unos años, el nombre de "Nuevo Mundo" comienza a aparecer como un nombre para América del Sur en los mapas y cartas, como en la Oliveriana (Pesaro) que data de poco después de 1503. Los mapas de la época, como los planisferios de Cantino y Caverio, muestran claramente América del Norte conectada con Asia y América del Sur como una tierra inexplorada, de la que sólo se dibuja la costa norte y este de Brasil, o bien, como en el "Planisferio de Sylvanus" en forma de una gran isla independiente.

En 1507, Martin Waldseemüller publicó un mapa del mundo, "Cosmographia Universalis", que fue el primero en mostrar que América del Norte y América del Sur estaban separadas de Asia y rodeadas de agua. Una pequeña nota sobre la carta principal, dice, por primera vez, que América se encuentra al este de Asia y están separadas por un océano, nota colocada para evitar confusión ya que América se encuentra en el extremo izquierdo del mapa y Asia en el extremo derecho. En el libro de acompañamiento, "Cosmographiae Introductio", Waldseemüller anotó que la tierra se dividía en cuatro partes, Europa, Asia, África y la cuarta parte que formó a partir del nombre de Américo Vespucio. En el mapa, la palabra «América» se colocó en una parte de América del Sur.

La voz tenía tal eufonía y guardaba tanta consonancia con las palabras «Asia» y «África» que inmediatamente se afincó en las lenguas noreuropeas. Sin embargo tardó en ser adoptado en la península ibérica y sus colonias, donde el nombre mayoritariamente usado siguió siendo por bastante tiempo el de «Indias occidentales».

Desde finales del siglo XVIII algunos geógrafos comenzaron a considerar que América del Norte y América del Sur eran dos partes del mundo, lo que hacía un total de cinco partes. Sin embargo, la división en cuatro fue en general la prevalente en el siglo XIX. Los europeos descubrieron Australia en 1606, pero durante algún tiempo, fue visto como una parte de Asia. A finales del siglo XVIII algunos geógrafos lo consideraron un continente en sí mismo, por lo que paso a ser el quinto (o sexto para aquellos quienes creen que América son dos continentes). En 1813, Samuel Butler (1774-1839) escribió en relación a Australia que «New Holland, an immense island, which some geographers dignify with the appellation of another continent» [Nueva Holanda, una isla inmensa, que algunos geógrafos dignifican con la apelación de otro continente] y el Oxford English Dictionary es igualmente equívoco algunas décadas más tarde.

La Antártida fue oficialmente descubierta hacia 1820 (aunque ahora se sabe que ya fue conocida desde 1599 o 1603) y descrita ya en 1838 como un continente por Charles Wilkes (1798-1877) de la United States Exploring Expedition (expedición Wilkes) (1838-42). Fue el último continente en ser identificado, aunque la existencia de un gran territorio Antártico había sido considerada desde hacia milenios. En la Antigüedad los pensadores griegos dedujeron que si la Tierra era esférica, por simetría, debía tener una contrapartida continental a la masa continental del hemisferio norte en el hemisferio sur en las latitudes polares, de este modo el cosmógrafo Claudio Ptolomeo confeccionó un célebre planisferio en el cual aparecía un inmenso territorio que en latín fue llamado "Terra Australis Incognita" [Tierra Austral Desconocida], aunque la extensión del supuesto continente incluía zonas que corresponden no solo a la Antártida propiamente dicha sino también a Australia, Nueva Zelanda y grandes extensiones oceánicas. En 1520 Magallanes al descubrir el estrecho que lleva hoy su nombre, estrecho de Magallanes, creyó que la isla de Tierra del Fuego era un sector de esa "Terra Australis Incognita". La exploración de Francisco de Hoces descubrió el gran pasaje marítimo que separa América de la Antártica y posteriormente el nombre de "Terra Australis Incognita" quedó reservado para Australia. En 1849, un atlas informó ya que la Antártida era un continente, pero pocos más lo hicieron hasta después de la Segunda Guerra Mundial.

En el siglo XIX, el mayanista Augustus Le Plongeon (1825-1908) propuso la hipótesis de un nuevo continente, llamado Mu, que habría existido y desaparecido en el océano Pacífico. Se basaba en la traducción —más tarde contestada— del "Códice Tro-Cortesiano" realizada por Brasseur de Bourbourg. En un momento en que la geología estaba menos avanzada que ahora, se propuso la existencia de muchos continentes hipotéticos. Este fue el caso de Lemuria, un continente considerado para explicar la desaparición de algunas especies de mamíferos, y también la Atlántida, mencionada por primera vez en el siglo IV antes de Cristo, aunque este hipotético continente hundido sigue siendo aún hoy fruto de especulaciones.

Desde mediados del siglo XIX los atlas de los Estados Unidos tratan a menudo América del Norte y América del Sur como dos continentes, lo que es coherente con el entendimiento de la geología y la tectónica de placas. Pero no es raro que los atlas americanos los traten como un único continente, por lo menos hasta la Segunda Guerra Mundial. Es esta última visión la que todavía prevalece hoy en día en algunos países de Europa.
La bandera olímpica, diseñada en 1913, tiene cinco anillos que representan los cinco continentes habitados, considerando América como uno solo y sin considerar a la Antártica.
Sin embargo, en los últimos años ha habido un impulso para que Europa y Asia, que tradicionalmente se consideraron como dos continentes, sean ahora un único continente, llamado Eurasia, para hacerlo compatible con los conocimientos derivados de la geología y la tectónica de placas. En este modelo, el mundo estaría dividido en seis continentes.

No existe una definición única de continente y por ello en distintos ámbitos culturales y científicos hay listas diferentes de continentes. En general, un continente debe ser una gran área conformada por tierras con importantes límites geológicos. El criterio de origen para la designación de un continente, el criterio geográfico, es ignorado muchas veces en favor de otros criterios más arbitrarios, de índole histórica y cultural. Aunque algunos consideran que solo hay cuatro o cinco continentes, modernamente se suele considerar que hay seis o siete.

Convencionalmente, se entiende que los «continentes son masas de tierra grandes, continuas y discretas, a ser posible separadas por extensiones de agua». Muchos de los siete continentes comúnmente reconocidos por convención no son masas de tierra discretas separadas por agua (realmente sólo la Antártica y Oceanía están verdaderamente separados de los otros continentes por agua). Del mismo modo, el criterio ideal que cada uno sea una masa de tierra continua con frecuencia es ignorado por la inclusión de la plataforma continental y las islas oceánicas. Las principales masas de la Tierra están bañadas por un único y continuo océano mundial, que está dividido en los principales componentes oceánicos por los continentes y otros criterios geográficos diferentes.

El sentido más estricto de continente como un área de tierra continua, con su costa y cualquier frontera terrestre que forme el borde del continente. En este sentido, la referencia a «Europa continental» se utiliza para referirse a la parte continental de Europa con exclusión de las islas como Gran Bretaña, Irlanda e Islandia, y el «continente australiano» puede referirse a la parte continental de Australia, con excepción de Tasmania. Del mismo modo, «Estados Unidos continental» se refiere a los 48 estados contiguos de Estados Unidos del centro de América del Norte y puede incluir a veces Alaska, en el noroeste del continente (separadas ambos por Canadá), pero excluyendo Hawái localizada en medio del océano Pacífico.

Desde el punto de vista de la geología o la geografía física, un continente puede extenderse más allá de los confines de la tierra firme continua a fin de incluir las aguas poco profundas, las áreas adyacentes sumergidas (la plataforma continental) y las islas de la plataforma (islas continentales), ya que son parte estructural del continente. Desde esta perspectiva el borde de la plataforma continental es el verdadero límite del continente, ya que las costas varían con los cambios del nivel del mar. En este sentido, Madagascar es parte de África, las islas de Gran Bretaña e Irlanda son parte de Europa, y Australia y la isla de Nueva Guinea, juntos forman un continente (Australia-Nueva Guinea).

Como construcción cultural, el concepto de continente puede ir más allá de la plataforma continental a fin de incluir las islas oceánicas y otros fragmentos continentales. De esta manera, Islandia se considera parte de Europa. Extrapolando el concepto al extremo, algunos geógrafos consideran Australia, Nueva Zelanda y todas las islas de Oceanía (algunas veces Australasia) como equivalente a un continente, consiguiendo así que toda la superficie de la Tierra se divida en continentes o cuasi-continentes.

Dado que la definición de continente es a menudo arbitraria, las separaciones entre ellos no siempre están claramente definidas.

El estrecho de Gibraltar de forma convencional marca la frontera entre África y Europa.

El límite entre Asia y África es en general fijado en el istmo de Suez, lo que excluye la península del Sinaí de África. Egipto se encuentra a caballo entre dos continentes, aunque algunos geógrafos proponen desplazar el límite entre ambos continentes a la frontera israelo-egipcia.

Por comodidad, la frontera entre Asia y América se ha fijado en la frontera ruso-estadounidense, a lo largo del estrecho de Bering. Las islas del Comandante son asiáticas, mientras que el resto de las islas Aleutianas son americanas.
La separación entre Oceanía y Asia es todavía hoy objeto de controversias. En 1831, el geógrafo y explorador Jules Dumont d'Urville a través de criterios geográficos dividió Oceanía en cuatro regiones: Polinesia, Micronesia, Melanesia e Insulindia (entonces llamada Malaya), de esta forma el límite entre Asia y Oceanía transcurre a través de los estrechos de Malaca y Luzón. La debilidad de los argumentos que sustentan este límite llevó a algunos geógrafos a repensar esa frontera. En 1860 el geógrafo Alfred Wallace propuso un nuevo límite basado en criterios biogeográficos que transcurre entre los estrechos de Macasar y Lombok conocido como la línea de Wallace. Algunos piensan que sería más apropiado utilizar la línea de Wallace como el verdadero límite entre los dos continentes. Posteriormente otros investigadores han propuestos nuevos límites como es el caso de Richard Lydekker en 1895 y Max Weber en 1907.

La frontera más controvertida es, sin duda, entre Asia y Europa, ya que las fronteras de Europa han ido desplazándose durante siglos y no están claramente definidas. En el siglo XVIII, el zar Pedro el Grande quiso convertir a Rusia en una potencia europea. El geógrafo Vassili Tatichtchev propuso en 1703 que los montes Urales, el río Ural y el Cáucaso constituyesen la frontera entre Europa y Asia en lugar del río Don, que situaba en esa época a Rusia en Asia.

Con la reciente ampliación de la Unión Europea hasta territorios situados en las puertas de Asia, tanto en los Balcanes como en la antigua Europa del Este, se plantea de nuevo el problema de la ubicación exacta de la frontera entre Europa y Asia. Algunos geógrafos, por conveniencia, extenderían el límite más allá del Cáucaso a fin de incluir en Europa a países como Armenia y Georgia. Otros, en cambio, fijarían el límite en la depresión de Kuma-Manych situada al norte del Cáucaso para incluir a los pueblos turcos del Cáucaso en Asia.

Las distintas partes del mundo se consideran como continentes en un sentido amplio. Por un lado, toda isla debe pertenecer a alguna parte del mundo ya que el conjunto de todas las partes del mundo debe contener todas las tierras emergidas, pero las islas no son parte de los continentes (en el sentido común o sentido científico) ya que su territorio no es continuo con el resto del continente. Sin embargo, en general se consideran como pertenecientes al continente del que estén más próximas. Por ejemplo, las islas Canarias, aunque españolas, están vinculadas a África; las islas Baleares forman parte de Europa y casi todas las islas del Pacífico pertenecen a Oceanía. Igual pasa con la isla de Reunión e isla Mauricio, que, a pesar de la distancia que las separa de África, son consideradas como islas africanas.

Algunas partes de los continentes son reconocidas como subcontinentes, en especial aquellas situadas en las diferentes placas tectónicas que dividen los continentes. Los más notables son el subcontinente indio, la península arábiga y Europa tomando el modelo que considera como continente a Eurasia. Groenlandia, sobre la placa norteamericana, también se considera como un subcontinente. Cuando América se considera un solo continente, a su vez se entiende dividida en dos subcontinentes (América del Norte y América del Sur), o en las diversas regiones.

Algunas áreas de la corteza continental están en gran parte cubiertas por el mar, pero se pueden considerar continentes sumergidos. Este es el caso de Zealandia, emergiendo del mar en Nueva Zelanda y Nueva Caledonia, o incluso la casi completamente sumergida meseta Kerguelen, en el sur del océano Índico.

Algunas islas se encuentran en las secciones de la corteza continental que se han fracturado y se alejan a la deriva de uno de los grandes continentes. Aunque no son considerados como continentes debido a su tamaño relativamente pequeño, pueden ser considerados microcontinentes. Madagascar, el ejemplo más extendido, es generalmente considerado parte de África, pero ya ha sido descrito como "el octavo continente".

Los geólogos utilizan el término «continente» de manera distinta a los geógrafos. Más que simplemente identificar grandes masas de tierra sólida, los geólogos usan un criterio distinto para identificarlos. Para los geólogos, en la superficie de la Tierra existen, de forma simplificada dos elementos estructurales distintos: la corteza continental, compuesta principalmente por granito y rocas asociadas, y la corteza oceánica, formada por basalto y gabro. Además, el límite entre el dominio continental y el dominio oceánico se encuentra por debajo de la superficie del mar: es entonces cuestión de la «plataforma continental» que a veces se extiende varios kilómetros más allá de la línea de costa. Durante la última edad de hielo (en la época de la glaciación de Würm, hace unos años), en Europa Occidental, la plataforma continental se extendía a varias decenas de kilómetros al oeste de la costa actual.

A comienzos del siglo XX, Alfred Wegener se dio cuenta de que por la disposición de los continentes, la costa este de América del Sur parecía encajar exactamente en la costa occidental de África. Otros antes que él ya lo habían advertido, pero fue el primero en sugerir a continuación, a partir de esta observación, la teoría de la deriva continental: un supercontinente, llamado Pangea, se habría fragmentado a principios de la era secundaria y, desde entonces, las masas continentales de esta fragmentación derivarían sobre la superficie de la Tierra.

Durante el siglo XX los geólogos aceptaron que los continentes se movían sobre la superficie de la Tierra, en una escala del tiempo geológica. Este proceso se conoce como «deriva de los continentes» y se explica por la tectónica de placas. La superficie de la Tierra está formado por siete placas tectónicas mayores (y muchos más de menor importancia). Estas son las que derivan, separadas y reunidas para formar con el tiempo los continentes que conocemos hoy en día.

En consecuencia, existieron otros continentes en el pasado geológico, los paleocontinentes. Se ha podido determinar que ha habido períodos en la historia de la Tierra en que únicamente había un gran continente en su superficie. El más reciente fue Pangea, hace 180 millones años. El próximo «continente único» deberá de aparecer en unos 250 millones de años, resultado de la combinación de África, Eurasia y América, que sería la Pangea Última.

Los geólogos consideran que un "continente" se define por la corteza continental: una plataforma de rocas metamórficas y rocas ígneas, en gran medida de composición granítica. Algunos geólogos restringen el término continente a las porciones de la corteza construidas en torno al estable Escudo Precámbrico, por lo general desde 1500 hasta 3800 millones años de edad, llamado cratón. El cratón en sí mismo es un complejo de acreción de los antiguos cinturones móviles (cinturones de montaña) de los ciclos anteriores de subducción, colisión continental y ruptura de las placas tectónicas. Un engrosamiento saliente hacia el exterior de rocas sedimentarias más jóvenes y mínimamente deformadas cubre gran parte del cratón. Los márgenes de los continentes geológicos se caracterizan por la actividad actual o relativamente reciente de los cinturones móviles y por profundos sedimentos marinos o deltaicos. Más allá del margen, puede haber o bien una plataforma continental que cae sobre la cuenca oceánica basáltica o el margen de otro continente, dependiendo de la actual placa tectónica del continente. Una frontera continental no tiene porque ser un cuerpo de agua. En el tiempo geológico, los continentes quedan periódicamente sumergidos bajo grandes mares epicontinentales, y las colisiones continentales dan como resultado en un continente nuevo conectado a otro continente. La era geológica actual es relativamente anómala ya que gran parte de las zonas continentales son "altas y secas" (high and dry) en comparación con gran parte de la historia geológica.

Algunos sostienen que los continentes son "balsas" de corteza acrecionales, que, a diferencia de la corteza basáltica más densa de las cuencas oceánicas, no están sujetas a la destrucción a través del proceso de subducción de la placa tectónica. Esto explicaría la gran antigüedad de las rocas comprendidas en los cratones continentales. Según esta definición, Europa Oriental, India y otras regiones podrían ser considerados como masas continentales distintas del resto de Eurasia, porque tienen áreas de escudo antiguas separadas (es decir, cratón de Europa del Este y cratón de la India). Cinturones móviles más jóvenes (como los montes Urales y los Himalayas) marcan los límites entre estas regiones y el resto de Eurasia.

Hay muchos microcontinentes que se han construido de la corteza continental, pero que no contienen un cratón. Algunos de ellos son fragmentos de Gondwana y otros antiguos continentes cratónicos: Zealandia, que incluye a Nueva Zelanda y Nueva Caledonia; Madagascar; la norte meseta de Mascareñas, que incluye las islas Seychelles; etc. Otras islas, como varias del mar Caribe, se componen principalmente de roca granítica, pero todos los continentes tienen la corteza tanto de granito como basalto, y no hay un límite claro de que islas podrían ser consideradas microcontinentes según esa definición. La meseta Kerguelen, por ejemplo, es en gran medida volcánica, pero se asocia con la ruptura de Gondwana y se considera un microcontinente, mientras que las volcánicas Islandia y Hawái no lo son. Las islas Británicas, Sri Lanka, Borneo y Terranova son los márgenes del continente laurasiano que sólo están separadas por mares interiores que han inundado sus márgenes.

La tectónica de placas ofrece otra forma de definir los continentes. Hoy en día, Europa y la mayor parte de Asia comprenden la unificada placa Euroasiática, que es aproximadamente coincidente con el continente euroasiático geográfico, con exclusión de la India, Arabia, Rusia y Extremo Oriente. India contiene un escudo central, y el geológicamente reciente cinturón móvil de los Himalayas forma su margen norte. América del Norte y América del Sur son continentes distintos y el istmo de conexión es en gran medida el resultado de la actividad volcánica de la relativamente reciente subducción tectónica. Las rocas continentales de América del Norte se extienden hasta Groenlandia (una parte del Escudo Canadiense), y en términos de límites de placas, la placa Norteamericana incluye la parte más oriental de la masa continental de Asia. Los geólogos no utilizan estos datos para sugerir que el Asia oriental sea parte del continente de América del Norte (aunque el límite de la placa se extiende hasta allí) y utilizan también la palabra continente generalmente en su sentido geográfico y en las nuevas definiciones («rocas continentales», «límites de placas») se utilizan según el caso.

Lista de países por continente según los países hispanohablantes:






</doc>
<doc id="6540" url="https://es.wikipedia.org/wiki?curid=6540" title="Contabilidad presupuestaria">
Contabilidad presupuestaria

La contabilidad pública en España es el sistema contable que utiliza el sector público español que se haya sujeto la Ley General Presupuestaria (LGP) española.

El presupuesto público español presenta los gastos e ingresos de los poderes públicos durante el año. Un gasto es toda transacción que implica una aplicación financiera (uso de fondos) y un recurso (también denominado ingreso) es toda operación que implica la utilización de un medio de financiamiento (fuente de fondos).


Pueden expresarse de distintas maneras. Para cumplir con los objetivos de Claridad y Programación que tiene el presupuesto, se elaboran las clasificaciones presupuestarias. Se trata de instrumentos normativos los recursos y gastos de acuerdo a ciertos criterios, cuya estructuración se basa en el establecimiento de aspectos comunes y diferenciados de las operaciones administrativas. Las clasificaciones presupuestarias facilitan la toma de decisiones por parte de las autoridades en todas las etapas del proceso presupuestario.

El grado de agregación de las cuentas públicas se corresponde con los niveles de gestión: el funcionario que administra determinado programa gubernamental precisará trabajar con una mayor segregación que el Ministro. Siguiendo este criterio, existen dos tipos de clasificaciones:


Los estados de gastos de los presupuestos públicos en España aplican las clasificaciones orgánica, funcional agregada en programas y económica.

La estructura de programas diferencia los programas de carácter finalista (con las letras A a L) y los programas instrumentales y de gestión (con las letras M a Z).

La previsión de ingresos de los presupuestos públicos en España emplea la clasificación económica, distinguiendo su origen, es decir si se trata de impuestos directos o indirectos, tasas, transferencias corrientes o de capital, enajenación de inversiones o de activos, incremento de pasivos, etc.

El modelo de clasificación económica que se emplea en los Presupuestos Generales del Estado en España, es el siguiente:




</doc>
<doc id="6541" url="https://es.wikipedia.org/wiki?curid=6541" title="Sistema Europeo de Cuentas">
Sistema Europeo de Cuentas

El Sistema Europeo de Cuentas (también denominado "SEC 2010" o, simplemente, "SEC") constituye el marco contable comparable a escala internacional, cuyo fin es realizar una descripción sistemática y detallada del total de una economía (una región, un país o un grupo de países), sus componentes y sus relaciones con otras economías. 

La elaboración de las políticas en la Unión Europea y la supervisión de las economías de los Estados miembros y de la unión económica y monetaria (UEM) exigen que se cuente con datos comparables, actualizados y fiables sobre la estructura de la economía y la evolución de la situación económica de cada Estado.

Un Sistema de Cuentas Nacionales constituye un marco contable que define las reglas precisas para la elaboración de la contabilidad nacional, establece las definiciones y conceptos de las operaciones económicas, la estructura ordenada de cuentas, etcétera. En definitiva, esa «normativa» no es más que una técnica de representación que permite obtener una descripción cuantitativa y simplificada de la actividad económica de un país, mostrando cómo se alcanza el equilibrio entre las principales magnitudes agregadas de esa economía.

El sistema de cuentas adoptados en la Unión Europea, denominado SEC 2010, fue aprobado por el Reglamento (CE) 549/2013, relativo al Sistema Europeo de Cuentas Nacionales y Regionales, que sustituyó al Reglamento (CE) 2223/96.

La Contabilidad nacional agrupa a las unidades que constituyen la economía nacional por categorías normalizadas. La clasificación de los sectores institucionales según el SEC es la siguiente:

El sistema de contabilidad nacional adoptado en la Unión Europea (SEC-95) sirve para realizar un análisis en profundidad de las operaciones económicas que se realizan entre los sujetos que intervienen en la economía. Se describe el ciclo económico en una serie ordenada de Cuentas relacionadas entre sí mediante las operaciones económicas. Así, una operación como puede ser la exportación de mercancías va a tener incidencia en la "cuenta de bienes y servicios" y en la "cuenta del resto del mundo". El sistema distingue tres tipos de cuentas:

La Contabilidad Nacional de España elabora un conjunto del cuentas, cuadros y datos, fijados en dicho Reglamento de aprobación del Sistema europeo de cuentas. En los años ochenta se empezó a elaborar el denominado subsistema de Cuentas Regionales (CRE) y al inicio de los años noventa, la Contabilidad Nacional Trimestral (CNTR).

Las principales aplicaciones del sistema de contabilidad europeo de contabilidad son:




</doc>
<doc id="6544" url="https://es.wikipedia.org/wiki?curid=6544" title="Gasto">
Gasto

Un gasto es un egreso o salida de dinero que una persona o empresa debe pagar para acreditar su derecho sobre un artículo o a recibir un servicio.
Sin embargo, hay bultos y diferencias entre el dinero que destina una persona (porque ella no lo recupera) del dinero que destina una empresa. Porque la empresa sí lo recupera al generar ingresos, por lo tanto «no lo gasta» sino que lo utiliza como parte de su inversión.

En contabilidad, se denomina gasto o egreso a la anotación o partida contable que disminuye el beneficio o aumenta la pérdida de una sociedad o persona física. Se diferencia del término costo porque precisa que hubo o habrá un desembolso financiero (movimiento de caja o bancos). 

El gasto es una salida de dinero que «no es recuperable», a diferencia del costo, que sí lo es, por cuanto la salida es con la intención de obtener una ganancia y esto lo hace una inversión que es recuperable: es una salida de dinero y además se obtiene una utilidad.

Podemos decir también que el gasto es la corriente de recursos o potenciales de servicios que se consumen en la obtención del producto neto de la entidad: sus ingresos.

El gasto se define como expiración de elementos del activo en la que se han incurrido voluntariamente para producir ingresos.

También podemos definir el gasto como la inversión necesaria para administrar la empresa o negocio, ya que sin eso sería imposible que funcione cualquier ente económico; el gasto se recupera en la misma medida que el cálculo del precio de la venta del bien o servicio se tenga en cuenta. 

Las pérdidas son expiraciones involuntarias de elementos del activo que no guardan relación con la producción de ingresos.


</doc>
<doc id="6545" url="https://es.wikipedia.org/wiki?curid=6545" title="Gasto público">
Gasto público

El gasto público es el total de gastos realizados por el sector público, en la adquisición de bienes y servicios. En una economía de mercado, el destino primordial del gasto público es la satisfacción de las necesidades colectivas, mientras que los gastos públicos destinados a satisfacer el consumo público solo se producen para remediar las deficiencias del mercado. También tienen una importancia reseñable los gastos públicos de transferencia tendientes a lograr una redistribución de la renta y la riqueza.

La autorización de gasto público es el instrumento legal que le permite al funcionario aprobar previamente el gasto que deberá cancelar la administración para luego ser reflejado contablemente, en virtud del cual, la autoridad competente acuerda realizarlo y gestiona un gasto con cargo a un crédito, determinando su cuantía en forma cierta o de la forma más aproximada posible, cuando no puede hacerse de forma cierta, reservando, a tal fin la totalidad o una parte del crédito presupuestado. Este acto no implica aún relación sin interesado ajenos a la entidad, pero supone la puesta en marcha del proceso administrativo.

Gasto público: recoge aquellos bienes y servicios adquiridos por la Administración Pública, bien para su consumo (material de oficina, servicios de seguridad y limpieza...), bien como elemento de inversión (ordenadores, construcción de carreteras, hospitales...). También incluye el pago de salarios a los funcionarios.

No incluye, sin embargo, el gasto de pensiones: cuando paga el salario a un funcionario compra un servicio, su trabajo (hay una transacción económica), mientras que cuando paga una pensión se trata simplemente de una transferencia de rentas (no recibe nada a cambio), por lo que no se contabiliza en el PIB.

Los gastos realizados por el gobierno son de naturaleza diversa. Van desde cumplir con sus obligaciones inmediatas como la compra de un bien o servicio hasta cubrir con las obligaciones incurridas en años fiscales anteriores. Sin embargo, muchos de ellos están dirigidos a cierta parte de la población para reducir el margen de desigualdad en la distribución del ingreso.

Por lo tanto, saber en qué se gasta el dinero del presupuesto público resulta indispensable y sano, pues a través de este gasto se conoce a quiénes se ayuda en forma directa e indirecta. En esta sección encontrará diversos documentos que dan luz sobre cómo se gasta el dinero público. Además según el modelo keynesiano existe un mecanismo conocido como "multiplicador del gasto" por el cual los rendimientos económicos de una cierta cantidad de gasto superan a la cantidad gastada, vía reactiviación de la actividad económica. Este mecanismo ha sido refutado por la Escuela Austriaca y se ha demostrado que el gasto por encima de los recursos solo incrementa el déficit fiscal sin generar beneficio a las arcas públicas...

Desde un punto de vista económico se distinguen cuatro tipos de gasto público:

El gasto corriente se refiere a la adquisición de bienes y servicios que realiza el sector público durante el ejercicio fiscal sin incrementar el patrimonio federal. Este tipo de gasto incluye las erogaciones necesarias para que las instituciones del gobierno proporcionen servicios públicos de salud, educación, energía eléctrica, agua potable y alcantarillado, entre otros, así como para cubrir el pago de las pensiones y los subsidios destinados a elevar el bienestar de la población de menores ingresos. Se incluyen aquí también los subsidios para los programas de desarrollo rural, la compra de medicamentos y las remuneraciones a maestros, médicos, enfermeras, policías y personal militar.
El gasto corriente puede ser dividido en dos categorías distintas. Los primeros reflejan los gastos de consumo colectivo (justicia, defensa, policía) los cuales benefician a la sociedad en conjunto, o a una buena parte de la sociedad, y son generalmente conocidas como bienes públicos y servicios.
Los segundos se refieren a los gastos de consumo individual (educación, servicios públicos de salud, servicios de agua potable, entre otros.), que reflejan gastos incurridos por el gobierno para elevar el bienestar de la población individualmente.
Debido a que los bienes y servicios producidos por el gobierno no tienen un precio de Mercado, el valor de los productos está determinado por el total de los costos necesarios para producir estos bienes y servicios. Estos costos consisten principalmente en el pago de salarios de empleados, consumo intermedio y depreciación.


Depende de: 


La mezcla exacta de participación del sector público y privado en la economía está influida por la filosofía política de cada gobierno en particular.

Son los siguientes: 




</doc>
<doc id="6546" url="https://es.wikipedia.org/wiki?curid=6546" title="Inmovilizado">
Inmovilizado

En contabilidad se entiende por inmovilizado, en sentido genérico, el conjunto de elementos patrimoniales reflejados en el activo, con carácter permanente y que no están destinados a la venta. Se distinguen dos tipos de inmovilizados:




</doc>
<doc id="6547" url="https://es.wikipedia.org/wiki?curid=6547" title="Pago">
Pago

El pago es uno de los modos de extinguir las obligaciones, y consiste en el cumplimiento efectivo de la prestación debida .

El pago es el cumplimiento de la obligación, a través del cual se extingue ésta, satisfaciendo el interés del acreedor y liberando al deudor. El pago de la deuda debe ser completo (excepto en casos en donde se acuerde un cumplimiento parcial). 

Algunos autores como Díez-Picazo lo entiende como el acto debido y otros como Ferrara como un acto jurídico. 

El pago debe hacerlo, en primer lugar, el deudor. Este debe tener capacidad para enajenar y libre disposición de dar si la obligación es de dar y legitimación.
Teniendo en cuenta en las obligaciones de hacer se da el pago solo cuando el objeto o acción mandada a realizar se realiza o se entrega el objeto al acreedor desprendiéndose de la obligación y liberando así al deudor de la obligación; y en las de no hacer, el cumplimiento del pago es que el sujeto deudor no haga la obligación hasta el término pactado por la obligación.
En las obligaciones de dar es el entregar el objeto convenido y en las de dinero es cumplir con la deuda.
Muchos hablan de la satisfacción por parte del acreedor al cumplir el deudor con el pago, lo cual es irrelevante porque si el acreedor no se encuentra satisfecho con el pago no habría pago ya que no extinguirá la obligación.

También puede hacer el pago un tercero en nombre del deudor, con consentimiento cabría supererogación, sin consentimiento (ignorando el pago) no cabría subrogación pues el deudor debe tener conocimiento o en contra del deudor, en cuyo caso no podrá repetir en aquello que le hubiese sido útil.
El pago por un tercero siempre extingue la obligación pagada pero hace nacer otras obligaciones.

El tercero paga en nombre y representación del deudor o paga con conocimiento y autorización del deudor, se crea entonces una nueva obligación. El tercero tiene toda la protección del sistema jurídico para exigir el pago.
El deudor no tiene idea de que se realiza el pago.
La tercera persona paga en contra de la voluntad del deudor. 
El derecho romano señala que el tercero carece de acción para exigir al deudor que le regrese su pago (obligación natural)

Dentro del CCDF se llega a la conclusión de que el pago puede ser hecho por cualquier persona, tanto por el deudor como por un tercero, ya que el acreedor se encuentra obligado a aceptar el pago.

El pago debe hacerse al acreedor o a quien lo represente legalmente. Este último caso puede ser la persona a quien le ha otorgado un poder o aquella que tiene la representación legal de un incapaz (padre o tutor de un menor de edad, curador de un demente declarado en juicio. etc.).

Debe coincidir con el contenido de la obligación. Si la obligación consistía en la entrega de una cosa determinada y ésta se hubiese deteriorado sin culpa del deudor, el acreedor debe aceptarla en el estado que se encuentre.

Sin embargo, el deudor puede cumplir con una protección distinta siempre que el acreedor dé su consentimiento. A esta modalidad de pago se le llamó dación en pago ("datio in solutio"). Ante esta pregunta, el pago debe hacerse tal cual se estipuló en el contrato.

Hay casos en que el deudor realiza el pago sin coincidir con lo estipulado. Hoy día en materia de derecho privado no es común. Pero en materia pública se prestan cláusulas de dación en pago.


También es necesario que se constaten las características de la mercancía antes de realizar el pago.

Será el estipulado en el contrato, en caso contrario se siguen las siguientes reglas.
Si se trataba de cosas inciertas (genéricas) o de cosas fungibles (cosas que pueden ser reemplazadas unas por otras), el cumplimiento debe hacerse en el domicilio del deudor, donde el acreedor podía reclamarlo judicialmente, si por el contrario se trataba de la entrega de un bien inmueble o de otra cosa cierta (específica), el lugar era aquel en donde estuvieran los bienes. Hoy día en materia procesal si nada se ha dicho, para cosas genéricas o fungibles en el domicilio del deudor si se trataba de la entrega de un bien inmueble en donde está ese bien.
En lo que respecta al tiempo del pago este debe cumplirse en el tiempo estipulado en la obligación, pero si no lo hubieran establecido las partes, se aplica la regla de que la prestación se debe cumplir desde el día en que nace la obligación. 
No obstante, lo anterior, el cumplimiento de la obligación estará sujeta a la naturaleza y al alcance de la propia prestación. De manera tal que el deudor debe cumplirla cuando razonablemente pudiera hacerlo. Ejemplo: Al comprometerse hacer un puente, no se estipula la fecha, no se puede cumplir en un solo día.
Aunque para pintar un cuadro no se establezca un plazo de tiempo, no significa que se pueda demorar un mes cuando se puede realizar el trabajo en unos cuantos días.

En caso de insolvencia declarada judicialmente (concurso de acreedores o quiebra) las obligaciones pendientes se tornan exigibles en forma inmediata.

Si la obligación es pura (no depende de ninguna condición), es desde el momento en que nazca la obligación; si es condicional, cuando se cumpla la condición; y si es a plazos (con día cierto), será exigible cuando el día llegue a no ser que el deudor haya perdido su derecho.

Acto por el que el deudor tiene varias deudas de la misma especie a favor de un solo acreedor; determina a cuál de ellas atribuye el pago.

El único requisito es la aceptación por el acreedor, el cual una vez acepte el pago no podrá impugnarlo posteriormente, salvo que se pruebe causa invalidante del contrato. Pero tiene límites; si la deuda devenga intereses, el pago no podrá entenderse imputado al principal mientras no estén cubiertos los intereses. 

Si ni acreedor ni deudor efectúan la imputación usamos las reglas generales:

Todo pago supone la existencia de una deuda antecedente. De ahí que al pagar una deuda que nunca existió, se le permite al que ha pagado recuperar su dinero. 

Hay subrogación cuando un acreedor sustituye a otro en el derecho de una deuda. La deuda en sí no sufre modificación. Existen dos tipos de subrogación: la "convencional" se da cuando el acreedor recibe de un tercero el pago de la deuda, y es así sustituido en sus derechos. La "legal" se da de pleno derecho en distintas disposiciones de la ley. Por ejemplo, el coobligado solidario al que se le exige toda la obligación, tendrá el derecho de exigirle su parte a los demás obligados como si fuera un acreedor.

La responsabilidad del deudor no disminuye ni aumenta por la subrogación; sigue siendo exactamente la misma.

Casos de subrogación convencional:

La subrogación convencional es aquella que se produce cuando el acreedor recibe el pago de un tercero y el acreedor decide voluntariamente subrogarlo en todos los derechos que tiene como tal, lógicamente opera cuando no opera la subrogaciones legal.
para que se dé la subrogación convencional debe estar expresa mediante una carta de pago a efectos de la subrogación convencional y legal al nuevo a creedor se le traspasan todos los derechos, privilegios, prendas e hipotecas para hacerlas efectivas tanto al deudor principal como al subsidiario.

Se produce cuando el deudor queda liberado, cuando pese a sus esfuerzos,el pago no ha podido efectuarse, debido a que el acreedor se negare a recibirlo sin razón, estuviere ausente o incapacitado para recibirlo. Se produce depositando los bienes muebles a la autoridad judicial. 

El objetivo es detener el curso de los intereses, transferir al acreedor el riesgo de la cosa y hacer recaer sobre éste los gastos de conservación. El acreedor deberá pagar al deudor los gastos de conservación de la cosa así como los gastos del juicio de consignación.

Para operar la consignación, se necesita que el deudor haya hecho repetidos intentos de pago al acreedor, de toda la obligación, y que un notario de fe de los repetidos intentos de pago por parte del deudor.

La Ley establece esta modalidad de pago, como medio de defensa que tiene el deudor contra su acreedor que no quiere recibir el pago o que se encuentra en un estado de repugnancia del mismo y que no manifiesta las razones por las cuales no le recibe dicho pago al deudor.



</doc>
<doc id="6548" url="https://es.wikipedia.org/wiki?curid=6548" title="Pasivo">
Pasivo

En contabilidad financiera, mientras el activo comprende los bienes y derechos financieros que tiene la persona o empresa, el pasivo recoge sus obligaciones, es decir, es el financiamiento provisto por un acreedor y representa lo que la persona o empresa debe a terceros. como el pago a bancos, proveedores, impuestos, salarios a empleados, etcétera.

Según las Normas Internacionales de Contabilidad, un pasivo financiero es todo aquel que incluye:


El pasivo está agrupado según su exigibilidad, es decir, a su mayor y menor urgencia. Así, existen pasivos a corto plazo y pasivos a largo plazo. Los pasivos cuyo pago es más urgente producen más tensión sobre el efectivo, por lo que las empresas suelen hacer una lista de sus pasivos en el orden en que se vence la fecha de pago. El poder saber qué cantidad de los pasivos de la empresa son a corto plazo y que cantidad son a largo plazo, permite a los acreedores evaluar la factibilidad de su empresa de obtener efectivo.




</doc>
<doc id="6549" url="https://es.wikipedia.org/wiki?curid=6549" title="Transferencia">
Transferencia

El término transferencia puede referirse a:


</doc>
<doc id="6550" url="https://es.wikipedia.org/wiki?curid=6550" title="Balance general">
Balance general

El balance general, balance de situación o estado de situación patrimonial es un informe financiero contable que refleja la situación económica y financiera de una empresa en un momento determinado.

El estado de situación financiera se estructura a través de tres conceptos patrimoniales, el activo, el pasivo y el patrimonio neto, desarrollados cada uno de ellos en grupos de cuentas que representan los diferentes elementos penales.

El activo incluye todas aquellas cuentas que reflejan los valores de los que dispone la entidad. Todos los elementos del activo son susceptibles de traer dinero a la empresa en el futuro, bien sea mediante su uso, su venta o su cambio. Por el contrario, el pasivo: muestra todas las obligaciones ciertas del ente y las contingencias que deben registrarse. Estas obligaciones son naturalmente económicas: préstamos, compras con pago diferido, entre otros

El patrimonio neto puede calcularse como el activo menos el pasivo y representa los aportes de los propietarios o accionistas más los resultados no distribuidos. Del mismo modo, cuando se producen resultados negativos (pérdidas), harán disminuir el Patrimonio Neto. El patrimonio neto o capital contable muestra también la capacidad que tiene la empresa de autofinanciarse.

El balance de situación forma parte de las cuentas anuales (estados financieros) que deben elaborar todas las sociedades cada ejercicio contable (habitualmente tiene una duración anual). Otros componentes de las cuentas anuales son:


Las partidas de balance son agrupadas y ordenadas de acuerdo a criterios fijados que faciliten su interpretación y homologación.
En el activo normalmente se ordenan los elementos en función de su liquidez, es decir en función de la facilidad que tiene un bien para convertirse en dinero, el dinero depositado en la caja es el más líquido que hay. En España según establece el Plan General de Contabilidad se colocan en primer lugar los activos menos líquidos y en último los más líquidos, así en primer lugar se sitúa el Activo no corriente y después el Activo corriente. En muchos países de Hispanoamérica y Estados Unidos el orden es el inverso al expuesto, los activos se ordenan de mayor a menor liquidez, en primer lugar se colocan los activos más líquidos para dejar al final los menos líquidos. 

El patrimonio neto y pasivo se suelen ordenar en función de su exigibilidad; un elemento será más exigible cuanto menor sea el plazo en que vence. El capital es el elemento menos exigible, mientras que las deudas con proveedores suelen ser exigible a muy corto plazo. De acuerdo con este criterio, en España, se ordenan de menor a mayor exigibilidad, se colocan en primer lugar el patrimonio neto, después el pasivo no corriente y por último el pasivo corriente. En países de Hispanoamérica es al contrario y se ordenan de mayor exigibilidad a menor exigibilidad.



</doc>
<doc id="6552" url="https://es.wikipedia.org/wiki?curid=6552" title="Subvención">
Subvención

Una subvención es la entrega de dinero o bienes y servicios realizada por una administración pública a un particular, persona física o jurídica, sin que exista la obligación de reembolsarlo. Suelen utilizarse en actividades consideradas de interés público, o en circunstancias de interés social. Como técnica de intervención administrativa, pertenece al conjunto de instrumentos propios de la actividad de fomento.

La subvención crea una relación jurídica que vincula a la Administración y al beneficiario. El beneficiario tiene, cumplidas las condiciones legales, un derecho a recibir la subvención, obligándose en consecuencia a realizar la actividad beneficiada.

La Administración, que está obligada a entregar las sumas pertinentes, se reserva para sí un haz de potestades.

Numerosas actividades económicas son subvencionadas hoy en día. Así, por ejemplo, la educación concertada, el transporte, la agricultura, las producciones cinematográficas, las actividades de I+D+I de las empresas, los programas de cooperación al desarrollo de la ONG, la producción de energía "renovable", etc.

También numerosas circunstancias sociales, personales y familiares son subvencionadas. Hay que diferenciar siempre entre subvención, ayuda y prestaciones por derechos adquiridos. 

La subvención es una parte importante de la actividad financiera del sector público, con la que se pretende dar respuesta a demandas sociales y económicas de personas y entidades públicas o privadas. 

Desde el punto de vista de la teoría económica, el fenómeno subvencional encuentra su justificación en la función de reasignación que debe cumplir la actividad financiera de la hacienda pública, y en la teoría de los fallos del mercado. Según esta teoría, las subvenciones se justifican por la necesidad de internalizar los beneficios de determinadas conductas, producciones y actividades, que generan externalidades positivas para la sociedad, beneficios que el mercado, por sus fallos, no atribuye directamente a sus ejecutores, promotores o participantes.

Desde la perspectiva administrativa, las subvenciones son una de las herramientas más importantes empleadas por las Administraciones para el fomento de sus políticas públicas de interés general e incluso un procedimiento de colaboración entre la Administración Pública y los particulares para la gestión de actividades de interés público. Desde la perspectiva financiera, constituyen una modalidad de gasto público que debe estar sujeto a las reglas para la ejecución de los créditos del presupuesto de gastos destinados a las subvenciones. Desde el punto de vista contable, las subvenciones recibidas no reintegrables se califican como ingresos contabilizados, con carácter general, como patrimonio neto que se imputarán posteriormente a la cuenta de resultado económico patrimonial sobre una base sistemática y racional de forma correlacionada con los gastos derivados de la subvención, mientras que las subvenciones recibidas reintegrables se registran como un pasivo hasta que adquieran la condición de no reintegrables.

El Diccionario de la lengua española, en su 23.ª edición, define “subvención” como “la acción y efecto de subvenir o subvencionar” o “la ayuda económica que se da a una persona o institución para que realice una actividad considerada de interés general”, y “subvenir” como “venir en auxilio de alguien o acudir a las necesidades de algo”.

Según el Diccionario Económico y Financiero (Bernard-Colli) subvención es “"el gasto otorgado a título definitivo a una persona pública o privada a fin de aligerar o compensar una carga o fomentar una actividad determinada"” y, de modo más especializado, son “"las transferencias efectuadas por una colectividad pública en provecho de otras colectividades públicas, instituciones sociales o empresas"”.

En el ámbito supranacional, son múltiples las definiciones del concepto subvención.

El Acuerdo de la Organización Mundial del Comercio sobre Subvenciones y Medidas Compensatorias somete a disciplina la utilización de subvenciones, y reglamenta las medidas que los países pueden adoptar para contrarrestar los efectos de las subvenciones. El citado Acuerdo, en su artículo 1, recoge una definición del término “subvención”, que comprende:

El Reglamento (UE, Euratom 966/2012, de 25 de octubre de 2012, sobre las normas financieras aplicables al presupuesto general de la Unión y por el que se deroga el Reglamento (CE, Euratom) 1605/2002 del Consejo define las subvenciones como contribuciones financieras directas a cargo del presupuesto que se conceden a título de liberalidad con objeto de financiar cualquiera de las actividades siguientes:

Completa esta definición una relación de supuestos excluidos de la naturaleza de subvenciones recogidos en su artículo 121.

Por su parte, el Sistema Europeo de Cuentas Nacionales y Regionales de la Unión Europea, regulado en el Reglamento (UE) 549/2013 del Parlamento Europeo y del Consejo, de 21 de mayo de 2013, define las subvenciones como pagos corrientes sin contrapartida que las administraciones públicas o las instituciones de la Unión Europea efectúan a los productores residentes, citando como objetivos de estas subvenciones:

Asimismo, el citado Reglamento excluye del concepto de subvenciones una serie de pago sin contrapartida (4.38) como las transferencias corrientes de las administraciones públicas a los hogares en su calidad de consumidores o las transferencias corrientes entre las diferentes administraciones públicas en su condición de productores de bienes y servicios no de mercado.

La doctrina coincide en señalar tres notas características de las subvenciones:



</doc>
<doc id="6553" url="https://es.wikipedia.org/wiki?curid=6553" title="Desplazamiento patrimonial">
Desplazamiento patrimonial

Desplazamiento patrimonial es un término jurídico. Es más concreto que el recogido en la definición de "atribución patrimonial", ya que requiere que la citada ventaja o beneficio se materialice en un bien dinerario o no dinerario, que cambia de titularidad, dejando así al margen toda atribución patrimonial que suponga para el beneficiario la evitación de un gasto.



</doc>
<doc id="6554" url="https://es.wikipedia.org/wiki?curid=6554" title="Atribución patrimonial">
Atribución patrimonial

Una atribución patrimonial es, en derecho, un acto jurídico por medio del cual una persona proporciona a otra una ventaja o un beneficio de carácter patrimonial.



</doc>
<doc id="6555" url="https://es.wikipedia.org/wiki?curid=6555" title="Activo (contabilidad)">
Activo (contabilidad)

Activo es construido con bienes y servicios, con capacidades funcionales y operativas que se mantienen durante el desarrollo completo de cada actividad socio-económica específica. Los activos de las empresas varían de acuerdo con la naturaleza de la actividad desarrollada.

Es el conjunto de bienes económicos, derechos a cobrar que posee una empresa y aquellos gastos en que serán aprovechadas en ejercicios futuros.

El Marco Conceptual para la Información Financiera del IASB (Junta de Normas Internacionales de Contabilidad), emitido el 1 de enero de 2012, establece la siguiente definición:

« activo es un recurso controlado por la entidad como resultado de sucesos pasados, del que la entidad espera obtener, en el futuro, beneficios económicos». En este se refleja también, la inversión realizada por la empresa para realizar su actividad económica.

En las registraciones o registros contables cuando se produce una variación de un elemento de activo, ésta puede ser de dos tipos:


Su saldo al finalizar el ejercicio contable es siempre deudor o cero.

Ejemplos de activos: Caja, Valores a depositar, Rodados, Marcas Registradas, Mercaderías, Deudores por venta...


Según define el Marco conceptual del Plan General de Contabilidad español, los activos son los bienes, derechos y otros recursos controlados económicamente por la empresa, resultantes de sucesos pasados de los que se espera obtener beneficios o rendimientos económicos en el futuro.

Según el apartado 5º del Marco conceptual del PGC, los activos deben reconocerse en el balance cuando sea probable la obtención a partir de los mismos de beneficios o rendimientos económicos para la empresa en el futuro, y siempre que se puedan valorar con fiabilidad. El reconocimiento contable de un activo implica también el reconocimiento simultáneo de un pasivo, la disminución de otro activo o el reconocimiento de un ingreso u otros incrementos en el patrimonio neto. dependiendo el contexto de la contabilidad

Criterios de valoración de activos utilizados en el PGC:

De acuerdo con el Plan General Contable, el activo se desglosa como suma del activo corriente y no corriente.
Activo total es la suma del activo corriente y del activo no corriente. con pasivo




</doc>
<doc id="6556" url="https://es.wikipedia.org/wiki?curid=6556" title="Resultado contable">
Resultado contable

En contabilidad pública, el resultado contable es el resultado económico-patrimonial; es la variación de los fondos propios de una entidad, producida en determinado período como consecuencia de sus operaciones de naturaleza presupuestaria y no presupuestaria. Este resultado se determina considerando la diferencia entre los ingresos y los gastos producidos en el período de referencia.

El resultado presupuestario es la diferencia entre la totalidad de ingresos presupuestarios realizados durante el ejercicio, excluidos los derivados de la emisión y creación de pasivos financieros, y la totalidad de gastos presupuestarios del mismo ejercicio, excluidos los derivados de la amortización y reembolso de pasivos financieros.


</doc>
<doc id="6559" url="https://es.wikipedia.org/wiki?curid=6559" title="Circuito (desambiguación)">
Circuito (desambiguación)

Circuito hace referencia a varios artículos:





</doc>
<doc id="6560" url="https://es.wikipedia.org/wiki?curid=6560" title="Río de la Plata">
Río de la Plata

El Río de la Plata es el estuario formado por la unión de los ríos Paraná y Uruguay, en el Cono Sur de América. Puede ser dividido en dos sectores: el sector "interior", compuesto por los tramos superior y medio, el cual es de poca profundidad y se encuentra desprovisto de intrusión salina, y el sector "exterior", comprendido entre Punta del Este, Uruguay y la bahía de Samborombón, Argentina tramo de mayor profundidad, y con una importante influencia marina al ser ya un estuario del océano Atlántico. Tiene una forma que tiende a la triangular de 325 km de largo, sirviendo de frontera en todo su recorrido entre Argentina y Uruguay. Posee un rumbo general noroeste-sureste, volcando en el océano Atlántico la escorrentía de su cuenca hidrográfica más la de sus afluentes, sumando alrededor de 3 250 000 km². Entre los geógrafos no hay consenso absoluto sobre si la definición amplia del Río de la Plata deba incluir los términos de río, golfo o mar marginal del océano Atlántico. Es llamado comúnmente "río" tanto del lado uruguayo como del lado argentino. En caso de ser considerado también un río, sería el río más ancho del mundo, debido a su anchura máxima de 221 km.

Tiene una extensa cuenca hidrográfica que recoge las aguas de los ríos Paraná, Paraguay, el río Uruguay, sus afluentes y diversos humedales, como el Pantanal, los Esteros del Iberá y el Bañado la Estrella. Es la segunda cuenca más extensa de Sudamérica, solo superada por la del río Amazonas. Su primer nombre fue río Jordán, por Américo Vespucio.

Fluye de noroeste a sureste desde el paralelo de punta Gorda —que marca el punto extremo del Río de la Plata (km 320)— hacia el sur y del río Uruguay hacia el norte. Al sur de este paralelo se hallan las secciones insulares 1° y 4° pertenecientes a la provincia argentina de Entre Ríos.

Según la Organización Hidrográfica Internacional, su desembocadura está determinada por la línea que une Punta del Este con el cabo San Antonio, —siendo más precisos es la punta norte de dicho cabo, llamada punta Rasa—.

De acuerdo con el Tratado del Río de la Plata, suscripto por la Argentina y Uruguay, su límite exterior está determinado por la línea imaginaria de 219 km que une Punta del Este (Uruguay) con punta Rasa del cabo San Antonio (Argentina).

La salinidad —halinidad— del agua se mide en gramos de «sal» por litro de solución, donde 10 g/L es igual a 10 psu, a 10 ppt, a 10 ‰ o a 1 %. El promedio de salinidad en los mares del mundo es de 35 ‰ y en la región del Atlántico donde desemboca este río es de algo más de 32 ‰, considerándose aguas marinas a partir de los 30 ‰ y hasta los 50 ‰.

La influencia de las aguas de este río se hace sentir aún muy adentro de las aguas oceánicas, llegando a los 32 ‰ de salinidad recién a más de 100 km del límite exterior fluvial convenido en el Tratado del Río de la Plata. En cuanto a ambas riberas, Punta del Este supera levemente los 32 ‰, mientras que en la costa argentina solo se alcanza a la altura de Nueva Atlantis, 60 km al sur de punta Rasa, contando dicha punta con 29 ‰, el mismo tenor que posee en la ribera opuesta la zona de El Pinar, en la boca del arroyo Pando, aunque entre estos dos últimos puntos una lengua extiende ese grado de halinidad hasta 45 km mar adentro hacia el sudeste de la línea del Tratado. Contorneando dicha lengua, el límite marino de 30 ‰ se logra en la banda uruguaya en Costa Azul, y en la argentina en Las Toninas, aunque esta frontera marina se logra recién 75 km mar adentro del límite del Tratado.

La intrusión salina hacia el interior del río alcanza hasta la zona entre punta Piedras y la boca del río Santa Lucía, pues allí la salinidad promedio anual apenas ronda los 0,5 ‰.

Sobre la margen norte (República Oriental del Uruguay):

Sobre la margen sur (República Argentina):

El Río de la Plata es navegable en casi toda su extensión, aunque debido a los múltiples bancos de arena producidos por la sedimentación proveniente de sus afluentes, es necesario el constante dragado de canales, tanto para el acceso a los puertos de ambas márgenes, como para navegar en demanda de los puertos ubicados en los ríos Paraná, Uruguay y Paraguay.

La profundidad del río varía entre una mínima de 0.60m. en la zona noroeste, en proximidades del Delta del Paraná y la desembocadura del río Uruguay, y una máxima de 25m. sobre la línea imaginaria que determina su límite exterior.

La navegación de buques de porte por los canales y acceso a puertos debe estar asistida por prácticos, cuando se trate de buques que superen los 120m. de eslora, para buques argentinos o uruguayos, y sin límite de eslora para buques de porte de otras nacionalidades.

En el balizamiento y señalización de los canales de navegación y acceso a puertos rige el sistema de boyado marítimo IALA para la Región B.

Tanto en el estuario como en la costa atlántica se pueden encontrar delfines de la especie franciscana, una especie de río que prefiere las aguas salobres. En ocasiones este delfín de especie franciscana remonta las aguas de los ríos Paraná y Uruguay. También se encuentran tres especies de tortugas marinas: la tortuga verde ("Chelonia mydas"), la tortuga cabezona ("Caretta caretta") y la tortuga siete quillas ("Dermochelys coriacea").

Físicamente el Río de la Plata se divide en tres zonas geográficas:

Desde el paralelo de Punta Gorda Latitud: 33º55'00" 10 S, hasta la línea imaginaria que une al faro de Colonia, con Punta Lara, la que se caracteriza por un sustrato de arena fina, limo y arcilla. Es la prolongación del Delta del Paraná bajo el Plata, donde se depositan los sedimentos más gruesos.

Es el tramo entre la línea que une Colonia, con Punta Lara hasta otra que une Punta Espinillo, en Montevideo con Punta Piedras. Aquí se produce el transporte de los sedimentos finos en suspensión.

Es el tramo entre esa segunda línea hasta el límite exterior, la línea imaginaria que une al Hito Faro de Punta del Este con el Hito Faro Punta Rasa. Es el único de los tramos donde se presenta influencia marina; las aguas son salobres con tenores de halinidad variados y cambiantes, según las mareas, vientos, etc. Los sedimentos son pelíticos con un importante cuerpo arenoso.

Cada año son transportados hasta el Río de la Plata unos 160 millones de toneladas de sedimento. Esta carga está compuesta por: limo —el 56 %— (90 millones ton/año), arcilla —el 28 %— (45 millones ton/año), y arena —el 16 %— (25 millones ton/año). De toda la carga, el 90 % viaja en suspensión (145 millones ton/año). El material suspendido está integrado por la totalidad de las arcillas y limos que llegan al Plata, más 10 millones ton/año de la arena más fina. Mediante carga de fondo también son transportados 15 millones ton/año de arena más gruesa. En el sector del Río de la Plata que enfrenta al delta se deposita toda la arena transportada (los 25 millones ton/año) y una porción de los limos.

La fuente principal de los materiales finos es el sur de Bolivia y las provincias del noroeste de la Argentina, regiones que son drenadas por la alta cuenca del río Bermejo, por la cual primero los sedimentos son tributados al río Paraguay, luego por este pasan al río Paraná, y finalmente este último los transporta hasta el Plata, donde allí se depositan, luego de un recorrido de casi 2000 km desde su origen.

El cauce del río está dominado por la presencia de extensos bancos de baja profundidad que dificultan la navegación con embarcaciones de calado, que debe hacerse siguiendo diversos canales naturales y artificiales, muchos de los cuales, en especial la ruta que comunica Buenos Aires con el océano Atlántico, son objeto de constante dragado para evitar la acumulación de sedimentos y así mantenerlos abiertos a la navegación. Los principales bancos de arena son Ortiz, Arquímedes, Inglés y Rouen.

La morfología del lecho del área específica de Martín García se caracteriza por un bajío (la formación Playa Honda) atravesado por canales y compuesto por sedimentos grises-marrones, arenosos hacia el norte, arenas limosas y limos arenosos hacia el sur y limos arcillosos (lodos) hacia la margen argentina. El delta sufre un continuo avance de 15 m por año y se produce una disminución de las profundidades del lecho con la formación de bancos que luego se transforman en islas.

La región responde a la «falla de Punta del Este», y a la «subfalla del Río de la Plata»; con sismicidad baja; y su última expresión se produjo el de silencio sísmico), a las 3.20 UTC-3, con una magnitud de 5,5 en la escala de Richter. (Terremoto del Río de la Plata de 1888).

En cuanto a las costas del río, éstas presentan características muy diversas.

La costa uruguaya pertenece a la formación geológica del macizo de Brasilia, con costas altas y playas de arena bordeadas de dunas separadas por cabos rocosos.

Los principales afluentes por la ribera uruguaya son los ríos San Juan, Rosario, Santa Lucía, y el arroyo Solís Grande.

La costa argentina corresponde a la cuenca sedimentaria de la Pampa, formada por mesetas de limo que alternan con planicies barrosas. Es, por lo general, baja, formada por limos, siendo abundantes los juncales. En ella se destaca la bahía de Samborombón, cuya costa tiene unos 180 km de longitud. En esta bahía desembocan varios cursos de agua, muchas veces canalizados, siendo los principales los ríos Samborombón y Salado.

Esta isla, a diferencia de las cercanas islas deltaicas, posee un núcleo rocoso, al ser un afloramiento de rocas arcaicas del macizo de Brasilia, con una antigüedad de 1800 millones de años. Tiene una superficie de 184 ha y una altura máxima de 27 msnm. La sedimentación en el lecho del área específica de la isla Martín García es muy activa, formándose bancos que luego se transforman en islas bajas, como ocurrió con el crecimiento de la isla Timoteo Domínguez y de bancos como el de Santa Ana y del Medio.

La isla Martín García —en su superficie histórica— es de soberanía argentina, pero desde la entrada en vigencia del Tratado del Río de la Plata de 1973 quedó situada como un exclave en aguas de uso común para los dos países, pero rodeada del sector del Río de la Plata cuyo lecho y subsuelo fue adjudicado a Uruguay por el tratado. Por esta razón los depósitos aluvionales que la rodean —incluso la isla Timoteo Domínguez— pertenecen a Uruguay, con excepción de los aluviones que se depositen en las costas que miran al "canal de Martín García" (o "Buenos Aires"), y al "canal del Infierno", los cuales son de uso común para ambos países, en virtud del artículo 46 del tratado.

Desde el acuerdo del 18 de junio de 1988, se ha establecido la única "frontera seca" del límite argentino-uruguayo, entre las islas Martín García y Timoteo Domínguez.

Según algunas fuentes, el estuario fue descubierto por Américo Vespucio en 1501, al cual llamó "río Jordán". La conquista del Río de la Plata, nombre que entonces se dio a la región de la que hoy son parte: Argentina, Paraguay y Uruguay, la inició el español Juan Díaz de Solís, quien buscando un paso para llegar a Oriente, se internó en el actual Río de la Plata en 1516 al cual llamó "Mar Dulce", en referencia a la baja salinidad del agua.<br>
En 1527 durante su expedición, Sebastián Caboto desembarcó sobre la banda oriental del río de Solís, en un puerto al que nombró San Lázaro. Fundó allí el primer asentamiento español documentado de la región, donde depositó durante meses la hacienda de la corona, al cuidado de sus marinos.  <br>
El 2 de febrero de 1536, el español Pedro de Mendoza fundó la ciudad de Santa María de Buenos Aires, situada en la costa occidental del Río de la Plata cerca de la desembocadura de los Ríos Paraná y Uruguay.<br>
El 15 de agosto de 1537, Juan de Salazar fundó la ciudad de Asunción, cerca del río Paraguay.

El primer no americano documentado, en navegar sus aguas fue Juan Díaz de Solís, en 1516, mientras intentaba hallar un pasaje desde el océano Atlántico al océano Pacífico. 
Desembarcó Solís en la costa oriental del río, hoy Colonia, con un grupo de sus hombres y el grumete de 14 años Francisco del Puerto, fueron atacados y muertos por los indígenas. En 1527 durante la expedición de Sebastián Caboto, al desembarcar en San Lázaro, se encontraron con el sobreviviente Francisco del Puerto, educado como guerrero indígena. Vivió entre indígenas que vestían adornos de plata, que supuso llegaban desde el Alto Perú, y al tomar contacto con los portugueses se inició así la leyenda de la plata.

El nombre “de la Plata” se refiere a la mitológica “Sierra de Plata” en el país del “rey Blanco” que buscaron Alejo García, Sebastián Caboto y otros, remontando los ríos de la Plata, Paraná, Paraguay y Uruguay y realizando expediciones terrestres hacia el Chaco y Chiquitos. Es posible que la Sierra de Plata haya sido un remoto influjo del Cerro Rico de Potosí que los indígenas pasaban de boca en boca, aunque es más probable que el mito del "rey Blanco" haya llegado por influencia de la civilización incaica. Según Dick Edgar Ibarra Grasso y otros investigadores y aficionados, basándose en gran medida en las Crónicas del Inca Garcilaso de la Vega, los monarcas del Tawuantinsuyu eran «blancos», llegando Grasso a suponer que descendientes de algunas remotas expediciones europeas precolombinas; aunque lo más probable es que el adjetivo «blanco» fuera una metonimia por la blanca plata. En 1525 Sebastián Caboto dio con algunos de los acompañantes indígenas de Alejo García, quienes llevaban plata que recogieron en la expedición, pensó que en la zona abundaba la plata y desde entonces muchos quisieron expedicionar hacia el Río de la Plata.

En 1536 los españoles al mando de Pedro de Mendoza fundaron a la ciudad de Santa María de los Buenos Aires en la costa sur y oeste del Río de la Plata; tal ciudad fue momentáneamente abandonada hasta que fue definitivamente refundada en 1580 por el español Juan de Garay. En 1680, transgrediendo el Tratado de Tordesillas los portugueses fundaron, sobre la ribera oriental del río, la Colonia del Sacramento, frente a Buenos Aires. Colonia fue objeto de disputas entre españoles y portugueses por cerca de un siglo, cambiando varias veces de manos y obligando a los españoles a colonizar la Banda Oriental, hasta entonces mayormente ignorada. Colonia fue tomada por las fuerzas virreinales españolas el mismo año de su fundación, para ser devuelta a Portugal en 1681. Para detener el avance lusitano, el rey español Felipe V ordenó la construcción de Montevideo y sus fortificaciones sobre la banda oriental del río. Esta tarea se realizó entre 1724 y 1730.

En los años 1806-1807 el Río de la Plata fue testigo de las Invasiones Inglesas al Virreinato que llevaba su nombre.

El 5 de junio de 1888 se produjo el terremoto del Río de la Plata, a las 3,20, con una magnitud en la escala de Richter de 5,5, su epicentro estuvo en , a 30 m de profundidad. Jamás se tomaron medidas mínimamente antisísmicas. Afectó a todas las poblaciones de la costa del Río de la Plata, especialmente a las ciudades de Buenos Aires y de Montevideo. Produjo leves daños y su epicentro se localizó en el centro de dicho río.

El 13 de diciembre de 1939 se desarrolló la Batalla del Río de la Plata, frente a Punta del Este, Uruguay, primer enfrentamiento importante de navíos alemanes y británicos durante la Segunda Guerra Mundial.

En aguas del Río de la Plata se han producido numerosos naufragios. Se lo considera de navegación difícil por la intensidad y la rapidez con la cual se forman tormentas.

La sudestada es un fenómeno climático que se caracteriza por vientos persistentes, regulares a fuertes del sudeste, temperaturas relativamente bajas y generalmente acompañado por lluvias de variada intensidad.

Esto ocurre cuando después del pasaje de un sistema frontal frío, un sistema de alta presión, cuyo centro se ubica al sudoeste de la provincia de Buenos Aires produce por su circulación vientos persistentes del mencionado sector sobre la costa del Río de la Plata.

Se genera por el efecto combinado de dos sistemas, uno de alta presión ubicado sobre el océano Atlántico, frente a las costas de la Patagonia central, que transporta aire frío y de origen marítimo hacia el Este de la provincia de Buenos Aires, extremo sur del Litoral y Sur del Uruguay y un sistema de baja presión, localizado sobre el Centro y sur de la Mesopotamia y la región occidental del Uruguay y que por su circulación produce un aporte de aire cálido y húmedo sobre la región.

Cuando se profundiza la depresión, se intensifica la circulación del viento del sector sudeste produciéndose este fenómeno climático. Durante una sudestada, el río puede alcanzar una altura de 3,96 metros (sobre el plano de marea altura cero) y oleaje intenso, lo que provoca anegamientos en toda la zona ribereña.

El Pampero es un viento que sopla del sudoeste, precisamente del Anticiclón del Pacífico Sur hasta el Ciclón del Río de la Plata, que se caracteriza por tener una masa de aire frío y seca gracias a las lluvias de convección que provocó en la cordillera de Los Andes al atravesarla. En verano causa tormentas intensas seguidas por un marcado descenso de la temperatura y cielo despejado.

Dentro de las actividades deportivas más antiguamente practicadas en el Río de la Plata se encuentran el nado en aguas abiertas y el "yachting". En ambas márgenes del río, los clubes náuticos desarrollan regatas de distintas clases de veleros, siendo una de las más importantes la regata “Buenos Aires-Punta del Este”.

El Río de la Plata permite la práctica del windsurf y, en especial, embravecido, también del surf, presentando olas de unos dos metros. También se puede practicar "kite surfing".

En Montevideo, las playas donde se "surfea" son Playa Honda, Playa Verde y Playa Malvín.

En la margen argentina, la práctica del deporte está muy extendida en la zona de Buenos Aires y alrededores. En prácticamente toda la costa, destacando la localidad de San Isidro, se practica "windsurf", "kite surfing", kayak y "outrigger" (canoas hawaianas).

La navegación en kayak tomó relevancia en los últimos años en el Río de la Plata y delta del Paraná por su bajo costo (un kayak es, en comparación con otras, una embarcación muy barata) y por la casi inexistencia de requisitos legales para su adquisición, tenencia y manejo, requisitos abundantes en casi toda otra embarcación. El Encuentro Anual de Kayakistas tiene lugar durante la Semana Santa de cada año, tratándose de un encuentro libre; quienes concurren se reúnen espontáneamente en la Isla Martín García a la cual arriban piloteando sus kayaks.
En los ríos Paraná, San Antonio y Canal del Este se puede practicar esquí acuático, "wake board" y "wake surf".

Las aguas del Río de la Plata están muy contaminadas principalmente en las cercanías de la ciudad de Buenos Aires y el conurbano bonaerense. El río recibe efluentes del Riachuelo, que es uno de los ríos más contaminados del planeta, y del río Reconquista, otro río muy contaminado, entre otros. Por esta razón, el baño está prohibido en sus aguas en las costas de la ciudad de Buenos Aires, Vicente López, San Isidro y Avellaneda, entre otros.

La contaminación que recibe consiste principalmente en desechos cloacales y residuos industriales sin tratamiento.

Del lado uruguayo, los gobiernos de los departamentos de Montevideo y Canelones monitorean semanalmente la calidad del agua de sus playas, permitiendo el baño en aquellas no alcanzadas por esta contaminación.




</doc>
<doc id="6561" url="https://es.wikipedia.org/wiki?curid=6561" title="Verso">
Verso

El verso es una de las unidades en que puede dividirse un poema, superior generalmente al pie e inferior a la estrofa. En la literatura en lenguas romances, los testimonios en verso preceden a los testimonios en prosa. Aunque ambas formas de expresión manifiestan históricamente una tendencia innegable a la especialización (el verso para la lírica y la prosa para la narrativa, los textos argumentativos y expositivos), no faltan ejemplos tanto de verso no lírico (épico, narrativo en general, dramático o expositivo, como en la poesía didáctica grecolatina) como de prosa lírica. Es una forma especial de expresarse, es un conjunto de palabras sujetas a medida, ritmo y cadencia.


En función de su medida y cadencia cabe distinguir múltiples tipos de verso. Una clasificación elemental es la que distingue versos de arte menor (de dos a ocho sílabas) bisílabos, trisílabos, tetrasílabos, pentasílabos, hexasílabos, heptasílabos, octosílabos y de arte mayor (de nueve o más). Los versos pares de arte mayor, como el decasílabo, el dodecasílabo y el alejandrino, suelen dividirse en dos mitades, generalmente iguales, llamadas hemistiquios.

La manera de colocar los acentos marca el ritmo del verso. Dentro de un mismo tipo de verso caben varias disposiciones acentuales, que reciben nombres específicos. Así, un verso puede ser trocaico (_U), si los acentos de palabra caen en las sílabas impares; yámbico (U_), si caen en las pares; o mixto, si se mezclan las dos cadencias.




</doc>
<doc id="6562" url="https://es.wikipedia.org/wiki?curid=6562" title="Prosa">
Prosa

Prosa es la forma que toma naturalmente el lenguaje (tanto el oral como el escrito) para expresar los conceptos, y no está sujeta, como el verso, a medida y cadencia. Se identifica con lo contrapuesto al ideal y la perfección. Coloquialmente, "prosa" es equivalente a "palabrería".

La prosa es una forma de la lengua escrita, definida por oposición al verso, con figuras que se agrupan en el llamado paralelismo. Se ha definido la prosa por oposición al verso, porque aquella no tiene ni ritmo métrico, ni repetición (formas fijas) ni periodicidad (rima) como aquel (Ducrot y Todorov, 1974). Ritmo, repetición y periodicidad son justamente los elementos caracterizadores de la oralidad.

El término se originó en la expresión latina "prosa oratio" "discurso directo" (sin los ornamentos del verso), donde "prosa" es el femenino de "prosus", antes prorsus "directo", del antiguo latín "provorsus" "(moverse) adelante", de "pro-" "adelante" + "vorsus" "vuelto", participio pasado de "vertere" "volverse".

En la liturgia de la misa, tras la aleluya o el tracto se "cantaba" una secuencia denominada "prosa".


El origen de la prosa proviene de la Jonia en el , primero por una prosa narradora para describir lugares, costumbres o relatos, en una lengua más racionalista, diferente de la lírica. Sin embargo, su mayor desarrollo se efectuaría en Atenas durante los siglos y 

Por primera vez se dispuso de un instrumento lingüístico capaz de servir al pensamiento abstracto: el "estilo imperiódico" de los logógrafos (literalmente, "los que escriben en prosa").

Los principales autores de la historiografía griega en prosa son:

En la cultura romana la prosa no se ligó a los géneros narrativos sino a la oratoria. Marco Tulio Cicerón en "Oratore" distingue tres niveles de estilo: bajo, medio y elevado, y profundiza los caracteres musicales de la prosa estableciendo reglas relativas a la disposición de las partes de la frase, el ritmo y sobre todo las cláusulas del periodo, disponiendo la parte final según métricas análogas a las de la poesía. A través de Quintiliano este modelo llega a la Edad Media influyendo en el "ars dictandi" de escuelas (escuelas monásticas, escuelas episcopales, escuelas palatinas, "Studia Generalia") y cancillerías. En el Juan de Garlandia describe y clasifica algunos tipos de estilo prosaico, y en este periodo se elabora una prosa latina científica y filosófica, por encima del gusto del "ornatus", hace prevalecer el rigor de los esquemas lógico-demostrativos (escolástica).

El Renacimiento propone una gama más amplia de géneros en prosa: la poética del clasicismo presenta modelos a imitar en los diversos géneros literarios. La inversión de tendencia en el Barroco trae artificios espectaculares. En la Ilustración la prosa se convierte en un instrumento importante para la divulgación y la polémica narrativa, filosófica, satírica, etc. En el la distinción entre prosa y poesía se profundiza, creando la distinción entre prosa de función teórico-narrativa y poesía de función lírica; a esta distinción se refiere la comprensión del dominio de la prosa en el naturalismo.

Conforme el Imperio romano de Occidente desaparecía, el latín tradicional se mantuvo vivo gracias a escritores como Casiodoro, Boecio y Símaco. Las artes liberales florecieron en Rávena bajo Teodorico el Grande, donde los reyes godos se rodearían con maestros de retórica y gramáticos. Algunas escuelas se asentaron en Italia, así como notables autores como Enodio de Pavía (un poeta pagano), Arator, Venancio Fortunato, Felix el Gramático, Pedro de Pisa o Paulino de Aquilea.

Mientras los italianos que estaban interesados en teología gravitaban hacia Francia, aquellos que permanecieron estaban atraídos normalmente por el estudio del Derecho Romano. Esto promovió la posterior creación de universidades medievales como las de Bolonia, Padua, Vicenza, Nápoles, Salerno, Módena y Parma, las cuales a su vez colaboraron en la expansión de la cultura y prepararon el camino por el cual se iba a desarrollar la nueva literatura vernácula. La tradición clásica no llegó a desaparecer, y el cariño del recuerdo de Roma, la preocupación por la política y la preferencia de la práctica sobre la teoría se combinaron para influir en el desarrollo de la literatura italiana.

La literatura española se engloba dentro de la literatura en español, en la que se incluyen las literaturas en castellano y español de todos los países hispanohablantes. Por otro lado, también está englobada en la literatura de España, junto con las demás literaturas de las lenguas habladas en el país.

"Los primeros textos plenamente escritos en lengua vulgar autóctona datan de finales del , en Cataluña, de finales del , en Castilla y Navarra, y de la primera mitad del , en León, Galicia y Portugal, y, por lo general, se trata de documentos no emanados de la cancillería regia. ... Si el primer testimonio del empleo del vernáculo en la cancillería navarra es de 1169 o 1171 -excluyendo los Fueros de Estella y de Laguardia, el de la cancillería castellana es de 1194." La cancillería de Fernando III el Santo estableció la práctica de redactar los documentos en lengua vulgar (el castellano del ).

Solo a partir del y en un sentido exclusivamente geográfico es posible hablar de literatura española escrita. Hasta este período, se supone la coexistencia de una poesía de transmisión oral en lengua romance, tanto lírica como épica, junto a unos usos escriturales cultos cuya lengua de expresión y transmisión era el latín.

El primer autor castellano de nombre conocido utiliza el término "prosa" para denominar sus propios versos:

"Antes del el término "prosa" se usaba ya para distinguir la poesía rítmica de la poesía clásica cuantitativa."

"Las prosas no se dirigen a los ricos de los palacios, sino a los pobres de las casuchas, más bien."

La lengua francesa es el resultado de la fusión entre diversas lenguas de oïl, cuya forma predominante fue progresivamente impuesta desde la sede del poder institucional, la Isla de Francia, que le dio su nombre. Tiene una amalgama de orígenes entre los que se destacan el romano, el germánico, el celta y varias lenguas regionales. El idioma francés por sí mismo, se puede considerar como una forma moderna del latín vulgar.

La literatura francesa nace en el , con los primeros escritos en lengua romance. Su importante producción a lo largo de los siglos ha dado lugar a la creación de nuevos movimientos literarios y artísticos, cuya poderosa influencia sobre otras literaturas le hace ocupar una preeminente posición en la literatura universal.

Molière pone en boca a sus personajes una definición simplificada de "prosa" y "verso", con un cómico resultado:

La literatura inglesa es toda aquella escrita en lengua inglesa, independientemente de la procedencia de sus autores. Bajo esta denominación se reúnen obras escritas en inglés antiguo, inglés medieval, inglés moderno e inglés contemporáneo, así como aquellas escritas en las variedades dialectales que el idioma actual tiene alrededor del mundo. 

La literatura alemana o (en alemán) comprende la literatura de textos originarios de los pueblos germanohablantes de Europa central. Su desarrollo habiendo ya trascendido las tan volubles fronteras políticas, incluye no solamente los escritos de la actual Alemania reunificada sino también los de Austria y Suiza.

En la literatura alemana se incluyen además trabajos no poéticos o sin exigencia literaria particular: esto es, trabajos de historiografía, de historia de la literatura, de ciencias sociales, de filosofía, etc. También diarios o epístolas. 

Con el término literatura rusa se alude no solo a la literatura de Rusia, sino también a la literatura escrita en ruso por miembros de otras naciones que se independizaron de la extinta Unión de Repúblicas Socialistas Soviéticas (URSS) o por emigrados que fueron acogidos en ella. Con la disolución de la URSS varias culturas y países han reclamado a varios escritores exsoviéticos que, sin embargo, escribían en ruso. La literatura rusa se caracteriza por su marcada profundidad con figuras claves para la literatura universal como Dostoievski o Tolstói, y empezó, como todas, en forma de tradición oral sin cultivo escrito hasta la cristianización de la Rus de Kiev en 989 y, con ésta, de un alfabeto adecuado para acogerla.

Los creadores de dicho alfabeto fueron los misioneros bizantinos Cirilo y Metodio; ellos tomaron distintas grafías de los alfabetos latino, griego y hebreo, e ingeniaron otras. Al principio el lenguaje escrito ruso usó dos sistemas gráficos —los alfabetos cirílico y glagolítico—; el glagolítico, supuestamente inventado también por Cirilo y Metodio, fue abandonado, y la literatura rusa tal como la conocemos actualmente se escribe y lee en alfabeto cirílico, en su modalidad denominada alfabeto ruso.<br>
Si la primera mitad del fue la edad de oro de la poesía rusa, la segunda mitad del siglo fue la edad de oro de la prosa rusa.

La literatura china tiene una historia que se remonta desde los más antiguos archivos oficiales dinásticos conservados hasta las obras de ficción surgidas durante la dinastía Ming para el entretenimiento de las masas letradas de China. Se calcula que hasta el se habían producido en China más textos escritos que en el resto del mundo.

La literatura china ha influido de forma extraordinaria en la literatura de países cercanos, especialmente Japón y Corea. Algunas obras de la literatura china son muy populares y se reeditan constantemente en todo el mundo, como por ejemplo el Dào Dé Jing.

Durante siglos la literatura china ha sido no solo una reflexión sobre la sociedad y la vida, sino que también ha tenido un fuerte contenido político. Muchos literatos eran altos funcionarios o filósofos que estudiaban y proponían nuevas formas de gobierno para China.


<|----NO TIENEN SENTIDO EN ESTA SECCIÓN



</doc>
<doc id="6563" url="https://es.wikipedia.org/wiki?curid=6563" title="Tangente">
Tangente

En matemáticas, tangente puede referirse a:


</doc>
<doc id="6565" url="https://es.wikipedia.org/wiki?curid=6565" title="Continuidad">
Continuidad

Continuidad puede hacer referencia a:


</doc>
<doc id="6569" url="https://es.wikipedia.org/wiki?curid=6569" title="Dimensión de un espacio vectorial">
Dimensión de un espacio vectorial

La dimensión de un espacio vectorial (también llamada dimensión de Hamel de un espacio vectorial, para distinguirla de la dimensión de Hilbert en el caso de los espacios de Hilbert) es el número de vectores que forman una base [de Hamel] del espacio vectorial.

Dado un espacio vectorial pueden considerarse conjuntos de vectores "S" de un espacio vectorial "V" y se puede examinar si poseen algunas de estas propiedades:


Un conjunto que sea linealmente independiente (1) y generador del espacio vectorial (2) se dice que es una base vectorial. Puede demostrarse que todas las bases de un espacio vectorial son conjuntos con el mismo número de elementos (es decir, conjuntos que tienen el mismo cardinal). Y el número común de elementos de una base cualquiera es precisamente la dimensión del espacio vectorial.

Nótese un hecho importante, si se cambia el cuerpo de los escalares, de formula_2 a formula_3, entonces el mismo punto M será determinado por el complejo "z" ="x" + "y"i, es decir por un solo parámetro.

La dimensión de P es 1 sobre formula_3 y dos sobre formula_2:

Un plano real es por lo tanto una recta compleja. La apelación "plano complejo" para designar un plano real con escritura compleja de las coordenadas ( "x" + "y"i en vez de (x; y) ) es errónea, pero muy común.

El espacio ambiente es tridimensional y se requiere por lo tanto tres reales ("x", "y", "z") para definir un punto. No se le puede considerar como un espacio sobre formula_3.

En la teoría de la relatividad, se añade una cuarta variable: el tiempo, y un punto ("x", "y", "z", "t") de este espacio cuadridimensional corresponde a un evento o acontecimiento (las coordenadas nos dicen donde y cuando ocurrió).

En algunas teorías actuales, los físicos trabajan en un modelo del espacio con once dimensiones, pero sobre el conjunto de los enteros, y no los reales. Como el conjunto formula_7 de los enteros no es un cuerpo sino un anillo, el espacio no es vectorial (se dice que es un módulo). Sin embargo, la definición de la dimensión es válida en tales espacios. En este ejemplo, la mayoría de las dimensiones son enrolladas sobre sí mismas, como una serpiente que se muerde la cola. Su curvatura es enorme, pues su radio es microscópico, menor que el de un núcleo. Los espacios vectoriales no tienen curvatura.

Más formalmente la dimensión de un espacio vectorial se define como el cardinal de una base vectorial para dicho espacio. Por el axioma de elección todo espacio tiene una base (incluso el espacio {0}, ya que el vacío es una base), y puesto que puede demostrarse que todas las bases vectoriales tienen el mismo cardinal, el concepto de dimensión está bien definido. Conviene notar que existen espacios vectoriales de tanto de dimensión finita como de dimensión infinita (el espacio vectorial de los polinomios de una variable, por ejemplo tiene dimensión formula_8.

La dimensión de un espacio coincide además con los dos cardinales siguientes:

La definición sigue siendo la misma en el caso de un subespacio, pero existe un método particular de calcularla cuando el subespacio es definido como espacio generado por sistema de vectores.
Veámoslo en un ejemplo. En el espacio formula_9, sean los vectores:

Cuatro vectores no pueden ser independientes en formula_9, por lo tanto tienen que existir relaciones de dependencia:

lo que se puede escribir en forma matricial :

Llamemos "A" a la matriz anterior, y "X" el vector columna. Esta relación significa que el vector "X" pertenece al núcleo de "A", que se nota Ker A (del alemán "Kern", núcleo). El espacio generado es el conjunto de los 

es decir de los A·X: es la imagen de A.

Resulta intuitivo que cuanto mayor es el núcleo, menor es la imagen, en términos de dimensiones.
Concretamente, si llamamos rango de A a la dimensión de su imagen: rg A = dim (Im A), tenemos la relación:

Busquemos dim (Ker A): 

formula_15
formula_16
Quedan dos ecuaciones no proporcionales, por lo tanto independientes, y cada una resta 1 a la dimensión, que vale inicialmente 4. Resulta que dim (Ker A ) = 2. 
Se puede constatarlo de otra manera: Las dos ecuaciones permiten expresar "y",luego "x" en función de "z" y "t", por consiguiente solo quedan dos variables libres, y la dimensión es 2. 

Aplicando la fórmula : rg A = 4 - 2 = 2. El subespacio es un plano.

Si U y U son subespacios de un espacio vectorial de dimensión finita, se cumple: 
satisface formula_28.




</doc>
<doc id="6570" url="https://es.wikipedia.org/wiki?curid=6570" title="Producto cartesiano">
Producto cartesiano

En matemáticas, el producto cartesiano de dos conjuntos es una operación, que resulta en otro conjunto, cuyos elementos son todos los pares ordenados que pueden formarse de forma que el primer elemento del par ordenado pertenezca al primer conjunto y el segundo elemento pertenezca al segundo conjunto.

El producto cartesiano recibe su nombre de René Descartes, cuya formulación de la geometría analítica dio origen a este concepto.

Por ejemplo, dados los conjuntos:

y

su producto cartesiano de A por B es:

que se representa:

y el producto cartesiano de B por A es:

que se representa:

Ver que:

Dado que son pares ordenados.

Un par ordenado es una colección de dos objetos distinguidos como "primero" y "segundo", y se denota como , donde es el «primer elemento» y el «segundo elemento». Dados dos conjuntos y , su producto cartesiano es el conjunto de todos los pares ordenados que pueden formarse con estos dos conjuntos:

Puede definirse entonces el cuadrado cartesiano de un conjunto como .

Sea también el conjunto de todos los números enteros . El producto cartesiano de consigo mismo es , es decir, el conjunto de los pares ordenados cuyos componentes son enteros. Para representar los números enteros se utiliza la recta numérica, y para representar el conjunto se utiliza un plano cartesiano (en la imagen).

Sean los conjuntos de tubos de pintura, y de pinceles:

El producto cartesiano de estos dos conjuntos, , contiene todos los posibles emparejamientos de pinceles y tubos de pintura. De manera similar al caso de un plano cartesiano en el ejemplo anterior, este conjunto puede representarse mediante una tabla:
El conjunto vacío actúa como el cero del producto cartesiano, pues no posee elementos para construir pares ordenados:
El producto cartesiano de dos conjuntos no es conmutativo en general, salvo en casos muy especiales. Lo mismo ocurre con la propiedad asociativa.
Puesto que el producto cartesiano puede representarse como una tabla o un plano cartesiano, es fácil ver que el conjunto producto es el producto de los cardinales de cada factor:

En teoría de conjuntos, la fórmula anterior de cardinal del producto cartesiano como producto de los cardinales de cada factor, sigue siendo cierta utilizando cardinales infinitos.

Dado un número finito de conjuntos , , ..., , su producto cartesiano se define como el conjunto n-tuplas cuyo primer elemento está en , cuyo segundo elemento está en , etc.

Puede definirse entonces potencias cartesianas de orden superior a 2, como , etc. Dependiendo de la definición de n-tupla que se adopte, esta generalización puede construirse a partir de la definición básica como:

o construcciones similares.

En el caso de una familia de conjuntos arbitraria (posiblemente infinita), la manera de definir el producto cartesiano consiste en cambiar el concepto de tupla por otro más cómodo. Si la familia está indexada, una aplicación que recorra el conjunto índice es el objeto que distingue quién es la «entrada -ésima»:

donde denota la unión de todos los . Dado un , la proyección sobre la coordenada es la aplicación:

En el caso de una familia finita de conjuntos indexada por el conjunto , según la definición de n-tupla que se adopte, o bien las aplicaciones de la definición anterior son precisamente n-tuplas, o existe una identificación natural entre ambos objetos; por lo que la definición anterior puede considerarse como la más general.

Sin embargo, a diferencia del caso finito, la existencia de dichas aplicaciones no está justificada por las hipótesis más básicas de la teoría de conjuntos. Estas aplicaciones son de hecho funciones de elección cuya existencia sólo puede demostrarse en general si se asume el axioma de elección. De hecho, la existencia de funciones de elección (cuando todos los miembros de son no vacíos) es equivalente a dicho axioma.



</doc>
<doc id="6571" url="https://es.wikipedia.org/wiki?curid=6571" title="Asociatividad (álgebra)">
Asociatividad (álgebra)

La asociatividad es una propiedad en el álgebra y la lógica proposicional que se cumple si, dados tres o más "elementos" cualquiera de un conjunto determinado, se verifica que existe una "operación": formula_1, que cumpla la igualdad:

Es decir, en una expresión asociativa con dos o más ocurrencias seguidas de un mismo "operador" asociativo, el orden en que se ejecuten las "operaciones" no altera el resultado, siempre y cuando se mantenga intacta la secuencia de los "operandos". En otras palabras, reorganizar los paréntesis en una expresión asociativa no cambia su valor final. 

La suma y el producto de números reales cumplen la propiedad asociativa, siendo válidas las igualdades:

para la suma y para la multiplicación:
En ambas, la ubicación de los "paréntesis" no altera el resultado. Nótese que los "operandos" se han mantenido en su posición original dentro de la expresión. Muchas operaciones importantes son "no asociativas," por ejemplo la resta y la exponenciación. Las expresiones que contienen tanto operaciones "asociativas" como operaciones "no asociativas" dan como resultado "expresiones" "no asociativas." 

No se debe confundir la asociatividad con la conmutatividad, la cual establece que sí se puede cambiar el orden de los "operandos" sin afectar el resultado final.

Sea A un conjunto en el cual se ha definido una operación binaria interna formula_1 tal que

Se dice que la operación formula_1 es asociativa si:
La ley asociativa también puede ser expresada en notación funcional así:

Partiendo del conjunto de los números naturales 

para la operación suma, definida como:

formula_12 tiene la propiedad asociativa, dado que:

Por ejemplo:

"Sin embargo", para la operación resta, definida como:

formula_16 "no" tiene la propiedad asociativa, dado que:

Por ejemplo:



En la lógica proposicional estándar, la "asociación", o "asociatividad" son dos reglas de reemplazo válidas. Estas reglas permiten mover los paréntesis en expresiones lógicas usadas en pruebas lógicas. Las reglas son:
y
donde "formula_31" es un símbolo metalógico que representa "puede ser reemplazado en una prueba por."

"Asociatividad" es una propiedad de algunas conectivas lógicas en las funciones de verdad de la lógica proposicional. Las siguientes equivalencias lógicas demuestran que la asociatividad es una propiedad de conectivas lógicas particulares. Son asimismo tautologías de funciones de verdad.

Asociatividad de la disyunción:
Asociatividad de la conjunción:
Asociatividad de la equivalencia:



</doc>
<doc id="6573" url="https://es.wikipedia.org/wiki?curid=6573" title="División (matemática)">
División (matemática)

En matemática, la división es una operación parcialmente definida en el conjunto de los números naturales y los números enteros; en cambio, en el caso de los números racionales, reales y complejos es siempre posible efectuar la división, exigiendo que el divisor sea distinto de cero, sea cual fuera la naturaleza de los números por dividir. En el caso de que sea posible efectuar la división, esta consiste en indagar cuántas veces un número (divisor) está "contenido" en otro número (dividendo). El resultado de una división recibe el nombre de cociente. De manera general puede decirse que la división es la "operación inversa" de la multiplicación, siempre y cuando se realice en un campo.

Debe distinguirse la división «exacta» (sujeto principal de este artículo) de la «división con resto» o "residuo" (la división euclídea). A diferencia de la suma, la resta o la multiplicación, la división entre números enteros no está siempre definida; en efecto: 4 dividido 2 es igual a 2 (un número entero), pero 2 entre 4 es igual a ½ (un medio), que ya no es un número entero. La definición formal de «división» , «divisibilidad» y «conmensurabilidad», dependerá luego del conjunto de definición.

Como cualquier operación, en "el resultado de una división tiene que ser único," por eso existe una definición para cociente y resto.

Conceptualmente, la división describe dos nociones relacionadas, aunque diferentes, la de «separar» y la de «repartir». De manera formal, la división es una operación binaria que a dos números asocia el producto del primero por el inverso del segundo. Para un número no nulo, la función «división por ese número» es el recíproco de «multiplicación por ese número». De este modo, el cociente formula_1 dividido formula_2 se interpreta como el producto formula_3 por formula_4.

Si la división no es exacta, es decir, el divisor no está contenido un número exacto de veces en el dividendo, la operación tendrá un resto o "residuo", donde:

Etimología: la palabra deriva del latín dividiere: partir, separar.divisiones decimales asta En matemática, la división es una operación parcialmente definida en el conjunto de los números naturales y los números enteros; en cambio, en el caso de los números racionales, reales y complejos es siempre posible efectuar la división, exigiendo que el divisor sea distinto de cero, sea cual fuera la naturaleza de los números por dividir. En el caso de que sea posible efectuar la división, esta consiste en indagar cuántas veces un número (divisor) está "contenido" en otro número (dividendo). El resultado de una división recibe el nombre de cociente. De manera general puede decirse que la división es la operación inversa de la multiplicación, siempre y cuando se realice en un campo.1​

Debe distinguirse la división «exacta» (sujeto principal de este artículo) de la «división con resto» o residuo (la división euclídea). A diferencia de la suma, la resta o la multiplicación, la división entre números enteros no está siempre definida; en efecto: 4 dividido 2 es igual a 2 (un número entero), pero 2 entre 4 es igual a ½ (un medio), que ya no es un número entero. La definición formal de «división» , «divisibilidad» y «conmensurabilidad», dependerá luego del conjunto de definición.

En álgebra y ciencias, la división se denota generalmente a modo de fracción, con el dividendo escrito sobre el divisor. Por ejemplo formula_6 se lee: "tres dividido cuatro". También puede emplearse una barra oblicua: formula_7; este es el modo más corriente en los lenguajes de programación por computadora, puesto que puede ser fácilmente inscrito como secuencia simple del código ASCII. 

Otro modo de indicar una división es por medio del símbolo óbelo (formula_8) (también llamado "signo de la división"). Este símbolo también se usa para representar la operación de división en sí, como es de uso frecuente en las calculadoras. Otras variantes son los dos puntos (:) o el punto y coma (;).

La división no es propiamente dicho una "operación" (es decir, una ley de composición interna definida por todas partes), sus «propiedades» no tienen implicaciones estructurales sobre el conjunto de números, y deben ser comprendidas dentro del contexto de los "números fraccionarios".


Hasta el siglo XVI fue muy común el algoritmo de la división por galera, muy similar a la división larga y a la postre (sustituido por ésta como método predilecto de división). El proceso usual de división (división larga) suele representarse bajo el diagrama:

También se usa un diagrama equivalente con la línea debajo del dividendo

Y también se usa otro diagrama equivalente 

Otro método consiste en la utilización de una «tabla elemental», similar a las tablas de multiplicar, con los resultados preestablecidos.

Consideremos el conjunto ℕ = {0, 1, 2, ..."n", ...} de los números naturales y sean "a","b" no nulo, "c" números naturales, diremos que 
si 

Si es así se dirá que "a" es el dividendo; "b", el divisor; y "c", el cociente si existe.

Sin embargo, dados dos números naturales "a" y "b" ≠ 0, existen dos únicos números naturales "q" y "r" tal que se cumplen las relaciones formula_15. 

El algoritmo que permite encontrar "q" y "r", conociendo "a" y "b", se denomina "división entera", entre otros nombres.

La división no es una operación cerrada, lo cual quiere decir que, en general, el resultado de dividir dos números enteros no será otro número entero, a menos que el dividendo sea un múltiplo entero del divisor. 

Existen "criterios de divisibilidad" para números enteros (por ejemplo, todo número terminado en 0,2,4,6 u 8 será divisible entre 2), utilizados particularmente para descomponer los enteros en factores primos, lo que se usa en cálculos como el mínimo común múltiplo o el máximo común divisor.

La división en ℚ siempre es posible, toda vez que el divisor no sea nulo. Pues el cociente formula_16, no es sino el producto formula_17

En los racionales, el resultado de dividir dos números racionales (a condición de que el divisor no sea 0) puede calcularse con cualesquiera de las fracciones representativas. Se puede definir de la manera siguiente: dados "p"/"q" y "r"/"s",

Esta definición demuestra que la división funciona como la "operación inversa" de la multiplicación.

El resultado de dividir dos números reales es otro número real (siempre y cuando el divisor no sea 0). Se define como "a"/"b" = "c" si y solo si "a" = "cb" y "b" ≠ 0.

La división de cualquier número entre cero es una «indefinición». Esto resulta del hecho que cero multiplicado por cualquier cantidad finita es otra vez cero, es decir que el cero no posee un inverso multiplicativo.

El resultado de dividir dos números complejos es otro número complejo (siempre y cuando el divisor no sea 0). Se define como

en donde "r" y "s" no son ambos iguales a 0.

En la forma trigonométrica formula_21

En forma exponencial:





</doc>
<doc id="6574" url="https://es.wikipedia.org/wiki?curid=6574" title="Número trascendente">
Número trascendente

Un número trascendente, también número trascendental, es un número complejo que no es raíz de ninguna ecuación algebraica con coeficientes enteros no todos nulos.
Un número real trascendente no es un número algebraico, pues no es solución de ninguna ecuación algebraica con coeficientes racionales. Tampoco es número racional, ya que estos resuelven ecuaciones algebraicas de primer grado, al ser real y no ser racional, necesariamente, es un número irracional.
En este sentido, "número trascendente" es antónimo de "número algebraico". La definición no proviene de una simple relación algebraica, sino que se define como una propiedad fundamental de las matemáticas. Los números trascendentes más conocidos son π y e.

En general, si tenemos dos cuerpos formula_1 y formula_2 de forma que el segundo es extensión del primero, diremos que formula_3 es trascendente sobre formula_4 si no existe ningún polinomio formula_5 del que formula_6 es raíz (formula_7).

El conjunto de números algebraicos es numerable, mientras el conjunto de números reales es no numerable; por lo tanto, el conjunto de números trascendentes es también no numerable. O tiene la potencia del continuo.

Sin embargo, existen muy pocos números trascendentes conocidos, y demostrar que un número es trascendente puede ser extremadamente difícil. Por ejemplo, todavía no se sabe si la constante de Euler (formula_8) lo es, siendo

formula_9 formula_10 cuando formula_11.

De hecho, ni siquiera se sabe si formula_12 es racional o irracional.

La propiedad de normalidad de un número puede contribuir a demostrar si es trascendente o no.

La denominación «"trascendental"» la acuñó Leibniz cuando en un artículo de 1682 demostró que la función formula_14 no es una función algebraica de formula_15,posteriormente Euler definió los números trascendentes en el sentido moderno. La existencia de los números trascendentes fue finalmente probada en 1844 por Joseph Liouville, en 1851 mostró algunos ejemplos entre los que estaba la «constante de Liouville»:

formula_16

donde el enésimo dígito después de la coma decimal es 1 si "n" es un factorial (es decir, 1, 2, 6, 24, 120, 720, etc.) y 0 en cualquier otro caso. El primer número del que se demostró que era trascendente sin haber sido específicamente construido para ello fue "e", por Charles Hermite en 1873. En 1882, Carl Louis Ferdinand von Lindemann publicó una demostración de que π es trascendente. En 1874, Georg Cantor encontró el argumento descrito anteriormente estableciendo la ubicuidad de los números trascendentes.

El descubrimiento de estos números ha permitido la demostración de la imposibilidad de resolver varios antiguos problemas de geometría que sólo permiten utilizar regla y compás. El más conocido de ellos es el de la cuadratura del círculo, y su imposibilidad radica en que π es trascendente. No ocurre lo mismo con los otros dos "problemas griegos" más famosos, la duplicación del cubo y la trisección del ángulo, que se deben a la imposibilidad de construir con regla y compás números derivados de polinomios de grado superior a dos (véase Número construible) es significativo que estos otros dos problemas puedan resolverse con modificaciones relativamente simples del método (permitiendo marcar la regla, acción que la geometría euclídea no toleraba) o con métodos similares a la regla y compás, como el origami, en tanto que la cuadratura del círculo, al depender de la trascendencia de π, tampoco es resoluble con esos métodos.

Una lista de los números trascendentes más comunes:



</doc>
<doc id="6575" url="https://es.wikipedia.org/wiki?curid=6575" title="Conjunto difuso">
Conjunto difuso

Un conjunto difuso es un conjunto que puede contener elementos de forma parcial, es decir, que la propiedad de que un elemento formula_1 pertenezca al conjunto formula_2 (formula_3) puede ser cierta con un grado parcial de verdad. Este grado de pertenencia es una proposición en el contexto de la lógica difusa, y no de la lógica usual binaria, que sólo admite dos valores: cierto o falso.

El grado de pertenencia de formula_1 a formula_2, o el grado de verdad de pertenecer al conjunto, se mide con un número real formula_6 comprendido entre 0 y 1, ambos inclusive. De forma rigurosa, el valor correspondiente a cada elemento define una función indicatriz formula_7, donde formula_8 representa el conjunto universal del que el conjunto formula_2 toma sus elementos. Por ello se suele hablar de subconjuntos difusos y no de conjuntos difusos.

Si el valor de esta función es 0, formula_1 no pertenece a formula_2. Si es 1, entonces formula_12 totalmente, y si formula_13 entonces formula_1 pertenece a formula_15 de una manera parcial.

La teoría de los subconjuntos difusos o borrosos (palabras intercambiables en este contexto) fue desarrollada por Lofti A. Zadeh en 1965 con el fin de representar matemáticamente la imprecisión intrínseca de ciertas categorías de objetos.

Los subconjuntos difusos (o partes borrosas de un conjunto) fueron inventados para modelar la representación humana de los conocimientos (por ejemplo para medir nuestra ignorancia o una imprecisión objetiva) y mejorar así los sistemas de decisión, de ayuda a la decisión, y de inteligencia artificial.

Con los conjuntos difusos se pueden realizar las mismas acciones que con los conjunto clásico. Siendo dos conjuntos difusos formula_2 y formula_17 se definen las operaciones usuales:


Por lo tanto, un conjunto difuso equivale, en concepto de información, a una familia infinita no numerable de conjuntos clásicos. La teoría de los subconjuntos difusos es por lo tanto muy distinta y mucho más compleja que la teoría de los conjuntos usuales. Por ejemplo, un conjunto finito clásico tiene un número finito de subconjuntos clásicos, pero un número infinito de subconjuntos difusos.



</doc>
<doc id="6576" url="https://es.wikipedia.org/wiki?curid=6576" title="Identidad de Euler">
Identidad de Euler

Se llama identidad de Euler a un caso especial de la fórmula desarrollada por Leonhard Euler, notable por relacionar cinco números muy utilizados en la historia de las matemáticas y que pertenecen a distintas ramas de la misma:

donde:

<br>
<br>

La identidad es un caso especial de la Fórmula de Euler, la cual especifica que

para cualquier número real "x". (Nótese que los argumentos para las funciones trigonométricas "sen" y "cos" se toman en radianes.) En particular si

entonces

y ya que 

y que

se sigue que

Lo cual implica la identidad

Para una forma alternativa de notar que la identidad de Euler es tanto verdadera como profunda, supongamos que:

en el desarrollo polinómico de e a la potencia x:
para obtener:
simplificando (usando i = -1):
Al separar el segundo miembro de la ecuación en subseries real e imaginarias:

Se puede comprobar la convergencia de estas dos subseries infinitas, lo cual implica

El logaritmo natural de un número complejo z = a+bi (donde a y b son números reales) se define como: 

Donde formula_18 es:

Notar que con esta definición, arg(z) está en el intervalo formula_20 (el argumento en este intervalo es conocido como el "valor principal del argumento" o simplemente "argumento principal"). Esta definición no es la única posible, ya que se pudo haber definido en [0, 2π), etc.

Para logaritmos de otras bases, se tiene la siguiente relación mediante "cambio de base" :

Por ejemplo :

Y también se cumple:

Lo anterior se puede deducir de la definición. También se puede obtener formula_24 a partir de la identidad de Euler, pero no es la razón de la deducción de ln(-1). Este detalle se explicará a continuación.
Se sabe que formula_25, pero también es cierto que formula_26 o formula_27. De hecho en general:

El error que se puede cometer aquí, es que si formula_29, entonces a = b. Lo anterior es válido si a y b son números reales, pero en complejos esto no se siempre se cumple. Por ende si bien formula_30, no es cierto que formula_31. De esta forma, se puede ver que:

Antes se mencionó que si se puede obtener formula_24 con la identidad de Euler, pero no es recomendable hacerlo, porque se puede cometer errores como lo descrito más arriba, ya que no siempre se cumple el hecho de que si formula_34 entonces a = ln(b).
Otro error es lo siguiente:

El error aquí ocurre en formula_36. Esto último no es correcto y el motivo es que 

Porque formula_38 solo se cumple de manera general si a es positivo. Por un lado formula_39, pero formula_40 no es real, puesto que ln(-e) no es un número real.

El número áureo (también llamado número de oro​) es un número irracional,​ representado por la letra griega φ (phi) o Φ (Phi) = 1,61803398874988... 

Una de sus propiedades es:

formula_41

Por tanto: formula_42

Reemplazando '1' en la identidad de Euler, formula_1, se tiene:

formula_44

Por tanto:

formula_45

formula_46

formula_47

Ordenando los términos de la ecuación queda:

formula_48

De esta manera se relacionan seis números muy utilizados, cinco operaciones de las matemáticas y la ecuación cuadrática.



</doc>
<doc id="6577" url="https://es.wikipedia.org/wiki?curid=6577" title="Lógica difusa">
Lógica difusa

La lógica difusa (también llamada lógica borrosa) se basa en lo relativo de lo observado como posición diferencial. Este tipo de lógica toma dos valores aleatorios, pero contextualizados y referidos entre sí. Así, por ejemplo, una persona que mida dos metros es claramente una persona alta, si previamente se ha tomado el valor de persona baja y se ha establecido en un metro. Ambos valores están contextualizados a personas y referidos a una medida métrica lineal.

Fue formulada en 1965 por el matemático e ingeniero Lotfi A. Zadeh.

La lógica difusa ("fuzzy logic," en inglés) se adapta mejor al mundo real en el que vivimos, e incluso puede comprender y funcionar con nuestras expresiones, del tipo «hace mucho calor», «no es muy alto», «el ritmo del corazón está un poco acelerado», etc.

La clave de esta adaptación al lenguaje se basa en comprender los cuantificadores de cualidad para nuestras inferencias (en los ejemplos de arriba, «mucho», «muy» y «un poco»).

En la teoría de conjuntos difusos se definen también las operaciones de unión, intersección, diferencia, negación o complemento, y otras operaciones sobre conjuntos (véase también subconjunto difuso), en los que se basa esta lógica.

Para cada conjunto difuso, existe asociada una función de pertenencia para sus elementos, que indica en qué medida el elemento forma parte de ese conjunto difuso. Las formas de las funciones de pertenencia más típicas son trapezoidal, lineal y curva.

Se basa en reglas heurísticas de la forma SI (antecedente) ENTONCES (consecuente), donde el antecedente y el consecuente son también conjuntos difusos, ya sea puros o resultado de operar con ellos. Sirvan como ejemplos de regla heurística para esta lógica (nótese la importancia de las palabras «muchísimo», «drásticamente», «un poco» y «levemente» para la lógica difusa):


Los métodos de inferencia para esta base de reglas deben ser sencillos, versátiles y eficientes. Los resultados de dichos métodos son un área final, fruto de un conjunto de áreas solapadas entre sí (cada área es resultado de una regla de inferencia). Para escoger una salida concreta a partir de tanta premisa difusa, el método más usado es el del centroide, en el que la salida final será el centro de gravedad del área total resultante.

Las reglas de las que dispone el motor de inferencia de un sistema difuso pueden ser formuladas por expertos o bien aprendidas por el propio sistema, haciendo uso en este caso de redes neuronales para fortalecer las futuras tomas de decisiones.

Los datos de entrada suelen ser recogidos por sensores que miden las variables de entrada de un sistema. El motor de inferencias se basa en chips difusos, que están aumentando exponencialmente su capacidad de procesamiento de reglas año a año.

Un esquema de funcionamiento típico para un sistema difuso podría ser de la siguiente manera:

En la figura, el sistema de control hace los cálculos con base en sus reglas heurísticas, comentadas anteriormente. La salida final actuaría sobre el entorno físico, y los valores sobre el entorno físico de las nuevas entradas (modificado por la salida del sistema de control) serían tomadas por sensores del sistema.

Por ejemplo, imaginando que nuestro sistema difuso fuese el climatizador de un coche que se autorregula según las necesidades: Los chips difusos del climatizador recogen los datos de entrada, que en este caso bien podrían ser la temperatura y humedad simplemente. Estos datos se someten a las reglas del motor de inferencia (como se ha comentado antes, de la forma SI... ENTONCES... ), resultando un área de resultados. De esa área se escogerá el centro de gravedad, proporcionándola como salida. Dependiendo del resultado, el climatizador podría aumentar la temperatura o disminuirla dependiendo del grado de la salida.

La LDC es un modelo lógico multivalente que permite la modelación simultánea de los procesos deductivos y de toma de decisiones. El uso de la LDC en los modelos matemáticos permite utilizar conceptos relativos a la realidad siguiendo patrones de comportamiento similares al pensamiento humano. Las características más importantes de estos modelos son: La flexibilidad, la tolerancia con la imprecisión, la capacidad para moldear problemas no lineales y su fundamento en el lenguaje de sentido común. Bajo este fundamento se estudia específicamente cómo acondicionar el modelo sin condicionar la realidad.

La LDC utiliza la escala de la LD, la cual puede variar de 0 a 1 para medir el grado de verdad o falsedad de sus proposiciones, donde las proposiciones pueden expresarse mediante predicados. Un predicado es una función del universo X en el intervalo [0, 1], y las operaciones de conjunción, disyunción, negación e implicación, se definen de modo que restringidas al dominio [0, 1] se obtenga la Lógica Booleana. 

Las distintas formas de definir las operaciones y sus propiedades determinan diferentes lógicas multivalentes que son parte del paradigma de la LD. Las lógicas multivalentes se definen en general como aquellas que permiten valores intermedios entre la verdad absoluta y la falsedad total de una expresión. Entonces el 0 y el 1 están asociados ambos a la certidumbre y la exactitud de lo que se afirma o se niega y el 0,5 a la vaguedad y la incertidumbre máximas. 
En los procesos que requieren toma de decisiones, el intercambio con los expertos lleva a obtener formulaciones complejas y sutiles que requieren de predicados compuestos. Los valores de verdad obtenidos sobre estos predicados compuestos deben poseer sensibilidad a los cambios de los valores de verdad de los predicados básicos. 

Esta necesidad se satisface con el uso de la LDC, que renuncia al cumplimiento de las propiedades clásicas de la conjunción y la disyunción, contraponiendo a éstas la idea de que el aumento o disminución del valor de verdad de la conjunción o la disyunción provocadas por el cambio del valor de verdad de una de sus componentes, puede ser “compensado” con la correspondiente disminución o aumento de la otra. Estas propiedades hacen posible de manera natural el trabajo de traducción del lenguaje natural al de la Lógica, incluidos los predicados extensos si éstos surgen del proceso de modelación. 

En la LDC, el operador conjunción, expresado como c (and) es la media geométrica.

En la LDC la modelización de la vaguedad se logra a través de variables lingüísticas, lo que permite aprovechar el conocimiento de los expertos, al contrario de lo que ocurre en otros métodos más cercanos a las cajas negras y exclusivamente basados en datos, como por ejemplo las redes neuronales.

Existen autores como Jesús Cejas Montero en su Artículo La Lógica Difusa Compensatoria publicado en el 2011 por la Revista Ingeniería Industrial del Instituto Superior Politécnico José Antonio Echeverría, que marcó un hito en la difusión de la LDC, que recomiendan el uso de funciones de pertenencia sigmoidales para funciones crecientes o decrecientes. Los parámetros de estas funciones quedan determinados fijando dos valores. El primero de ellos es el valor a partir del cual se considera que la afirmación contenida en el predicado es más cierta que falsa, por ejemplo pudiera establecerse a partir de 0.5. El segundo es el valor para el cual el dato hace casi inaceptable la afirmación correspondiente, por ejemplo pudiera establecerse a partir de 0.1.

En la actualidad existe un Sistema de Soporte a Decisiones Basado en Árboles con Operadores de Lógica Difusa cuyo nombre es Fuzzy Tree Studio 1.0, desarrollado en forma conjunta entre Universidad CAECE y la Universidad Nacional de Mar del Plata (Argentina), que posee un módulo que trabaja con la LDC. Ello permite al agente decisor despreocuparse por el trasfondo matemático y centrarse en la formulación verbal del modelo que le permita tomar una decisión. 

En general los modelos basados en LDC combinan la experiencia y el conocimiento con datos numéricos, por lo que puede ser visto como una “caja gris”. Los modelos basados en LD pueden verse como “cajas blancas”, dado que permiten ver su estructura explícitamente. En contraposición a los modelos basados en datos exclusivamente, como las Redes Neuronales, que corresponderían a “cajas negras”.
Estos modelos pueden ser optimizados cuando se dispone de datos reales numéricos. El método de optimización puede provenir de la Inteligencia Computacional. En este contexto, los Algoritmos Genéticos presentan una alternativa interesante. Este enfoque constituye el fundamento de los sistemas híbridos. 

La tendencia de las investigaciones sobre gestión empresarial, mediante las técnicas de la LDC, está orientada a la creación de sistemas híbridos que integren esta con las habilidades de las Redes Neuronales y las posibilidades de los Algoritmos Genéticos y la Lógica de Conjuntos. La creación e implementación de estos sistemas mixtos permite resolver problemas complejos y de difícil solución; en las que se usan estimaciones subjetivas sustentadas en la experiencia y en la información disponible, como son: modelos de decisión utilizados con criterios de optimización, ubicación de centros comerciales, estrategia de entrada a mercados, selección de carteras de productos y servicios, desarrollo de aplicaciones informáticas, métodos para problemas de descubrimiento de conocimiento, métodos para evaluar la eficiencia de diferentes tipos de instituciones, entre otras. 

La Lógica Difusa Compensatoria es un modelo lógico multivalente que renuncia a varios axiomas clásicos para lograr un sistema idempotente y “sensible”, al permitir la “compensación” de los predicados. En la LD el valor de verdad de la conjunción es menor o igual a todas las componentes, mientras que el valor de verdad de la disyunción es mayor o igual a todas las componentes. La renuncia de estas restricciones constituye la idea básica de la LDC.

En conclusión la LDC es un nuevo enfoque para los sistemas multivalentes basado en la Media Geométrica que, además de aportar un sistema formal con propiedades lógicas de notable interés, constituye un puente entre la Lógica y la Toma de Decisiones. La LDC entra a formar parte del arsenal de métodos para la evaluación multicriterio, adecuándose especialmente a aquellas situaciones en que el agente decisor puede describir verbalmente, frecuentemente en forma ambigua, la heurística que utiliza cuando ejecuta acciones de evaluación/clasificación multicriterio. Sin embargo, la consistencia de la plataforma lógica dota a esta propuesta de una capacidad de formalización del razonamiento que rebasa los enfoques descriptivos de los procesos de decisión. Es una oportunidad para usar el lenguaje como elemento clave de comunicación en la construcción de modelos semánticos que faciliten la evaluación, la toma de decisiones y el descubrimiento de conocimiento.

La lógica difusa se utiliza cuando la complejidad del proceso en cuestión es muy alta y no existen modelos matemáticos precisos, para procesos altamente no lineales y cuando se envuelven definiciones y conocimiento no estrictamente definido (impreciso o subjetivo). 

En cambio, no es una buena idea usarla cuando algún modelo matemático ya soluciona eficientemente el problema, cuando los problemas son lineales o cuando no tienen solución.

Esta técnica se ha empleado con bastante éxito en la industria, principalmente en Japón, extendiéndose sus aplicaciones a multitud de campos. La primera vez que se usó de forma importante fue en el metro japonés, con excelentes resultados. Posteriormente se generalizó según la teoría de la incertidumbre desarrollada por el matemático y economista español Jaume Gil Aluja.

A continuación se citan algunos ejemplos de su aplicación:

La lógica difusa es una rama de la inteligencia artificial que le permite a una computadora analizar información del mundo real en una escala entre lo falso y lo verdadero, manipula conceptos vagos, como "caliente" o "húmedo", y permite a los ingenieros construir dispositivos que juzgan la información difícil de definir. 

En Inteligencia artificial, la "lógica difusa", o "lógica borrosa" se utiliza para la resolución de una variedad de problemas, principalmente los relacionados con control de procesos industriales complejos y sistemas de decisión en general, la resolución y la compresión de datos. Los sistemas de lógica difusa están también muy extendidos en la tecnología cotidiana, por ejemplo en cámaras digitales, sistemas de aire acondicionado, lavadoras, etc. Los sistemas basados en lógica difusa imitan la forma en que toman decisiones los humanos, con la ventaja de ser mucho más rápidos. Estos sistemas son generalmente robustos y tolerantes a imprecisiones y ruidos en los datos de entrada. Algunos lenguajes de programación lógica que han incorporado la lógica difusa serían por ejemplo las diversas implementaciones de "Fuzzy PROLOG" o el lenguaje Fril.

Consiste en la aplicación de la lógica difusa con la intención de imitar el razonamiento humano en la programación de computadoras. Con la lógica convencional, las computadoras pueden manipular valores estrictamente duales, como verdadero/falso, sí/no o ligado/desligado. En la lógica difusa, se usan modelos matemáticos para representar nociones subjetivas, como "caliente"/"tibio"/"frío", para valores concretos que puedan ser manipuladas por los ordenadores.

En este paradigma, también tiene un especial valor la variable del tiempo, ya que los sistemas de control pueden necesitar retroalimentarse en un espacio concreto de tiempo, pueden necesitarse datos anteriores para hacer una evaluación media de la situación en un período anterior.

Como principal ventaja, cabe destacar los excelentes resultados que brinda un sistema de control basado en lógica difusa: ofrece salidas de una forma veloz y precisa, disminuyendo así las transiciones de estados fundamentales en el entorno físico que controle. Por ejemplo, si el aire acondicionado se encendiese al llegar a la temperatura de 30º, y la temperatura actual oscilase entre los 29º-30º, nuestro sistema de aire acondicionado estaría encendiéndose y apagándose continuamente, con el gasto energético que ello conllevaría. Si estuviese regulado por lógica difusa, esos 30º no serían ningún umbral, y el sistema de control aprendería a mantener una temperatura estable sin continuos apagados y encendidos.

También está la indecisión de decantarse bien por los expertos o bien por la tecnología (principalmente mediante redes neuronales) para reforzar las reglas heurísticas iniciales de cualquier sistema de control basado en este tipo de lógica.



</doc>
<doc id="6584" url="https://es.wikipedia.org/wiki?curid=6584" title="Tuna">
Tuna

Una tuna es una agrupación o hermandad de estudiantes universitarios o miembros de una sociedad que, portando la vestimenta antigua de la universidad o vestimenta que represente su cultura de origen, se caracterizan por cantar, tocar y viajar por el mundo gracias a estas habilidades —a pesar de que no todos o muy pocos sean músicos de profesión— o por interpretar temas musicales haciendo uso generalmente de instrumentos de cuerda y percusión.

La tuna universitaria es una antigua tradición que surgió en España, principalmente en Salamanca, y posteriormente gracias a su carácter viajero, se extendió a diversas partes de Europa, como Portugal y Holanda, y en América en países como México, Guatemala, Colombia, Perú, Chile o Argentina, etcétera. Llegó a finales del siglo XIX a través de tunos españoles, concretamente madrileños.

Para algunos su origen se ubica entre el siglo XIII o XIV con los continuadores de la tradición goliarda: en la Edad Media era un tipo de clérigo itinerante (giróvagos o sarabaítas) que aprovechaba la tradición de hospedaje de los monasterios para vivir sin trabajar destacándose por su predilección por la música, la bebida, la comida, el juego y los amoríos. Otro origen, no excluyente, sería el de los estudiantes pobres o sopistas, que vivían de la sopa boba que daban gratis en los conventos a los necesitados. Estos "sopistas" se valdrían de sus habilidades musicales, súplicas o picaresca "tunante" para cubrir al menos en parte sus estudios y medios de subsistencia. Sea cual fuera el origen, dejarían con el tiempo una huella que se refleja ya en el Siglo de Oro como estereotipo del estudiante de carácter alegre y pícaro que podemos encontrar, por ejemplo, en el entremés cervantino de "La cueva de Salamanca".

Existen muchas versiones del origen de la palabra «tuna», para algunos esta deriva de la palabra “"tunar o ‘correr la tuna’," "que significa: llevar una vida viajera, vagabunda, tocando y cantando." […] "también se [cree que] deriva de la expresión francesa Roi de Thunes (Rey de Túnez), un apelativo utilizado para designar a líderes de vagabundos,"” para otros deriva de la palabra atún y hacen esta similitud de los tunantes por “la naturaleza migratoria de estos peces y el carácter ambulatorio de los Tunos”. Según esta teoría, "Tunos" serían los trabajadores estacionales que se desplazaban hacia el sur de España buscando trabajo, siendo éste proporcionado por la temporada del atún del Mediterráneo. Estos trabajadores de temporada pudieron haber inspirado a los estudiantes a llevar una vida errante.

Así mismo también se cree que la palabra tuna proviene de "tunante", que era una palabra despectiva referida a esos estudiantes músicos y nocherniegos que hacían ruido cuando se dormía, a los que alude el Arcipreste de Hita en su "Libro de Buen Amor", que por uso derivó en "tuna". Esta teoría se apoya en el carácter mendicante de los "sopistas", estudiantes, no necesariamente pobres, que tras dilapidar sus mesadas, sobrevivían a expensas de la "sopa boba", distribuida gratuitamente en ese tipo de albergues conventuales, de modo que, (he aquí la conexión) debieron de llamarles "tunos".

Otras teorías más ampliamente difundidas procuran situar el origen en el latín "tonare" (sonido), aunque esta evolución es contraria a la legislación de los cambios fonéticos del latín a las lenguas ibéricas. Sin embargo, existen otras teorías.

La tuna, en sus albores, la constituían estudiantes que, debido a sus escasos recursos, tenían que cantar o tocar de lugar en lugar para poder ganarse la vida, o simplemente, para sustentarse durante el viaje de vuelta a sus casas cuando llegaban las vacaciones (en algunos casos). De ahí que esa actividad se designe con un verbo específico: "tunar", o "correr la tuna".Siendo una tradición íntimamente ligada a las universidades, las tunas mantienen vivas las costumbres heredadas de los estudiantes universitarios del siglo XIII. Alfonso X el Sabio se refirió a los tunos como juglares, en su "Código de las Siete Partidas" al escribir: "Esos escolares que trovan y tañen instrumentos para haber mantenencia". Coetánea suya, también lo hizo la obra "Razón de amor con los denuestos del agua y el vino", describiendo las cintas que aún penden sobre la capa del tuno: una por cada conquista amorosa, una por cada mujer.

El Arcipreste de Hita, en "El Libro de buen amor", subraya su carácter mendicante.
La tuna en sus orígenes aglutinaba a aquellos estudiantes que por su condición económica no podían costearse su estancia en la universidad, y trovaban por las fondas y mesones para conseguir algo de dinero y un plato de sopa con los que mantenerse. Por esta razón se les conocía como "sopistas", y se decía que vivían de la sopa boba.

Para tales menesteres portaban guitarras y bandurrias, y cantaban coplas populares. También se servían de sus habilidades musicales para enamorar a las doncellas que pretendían. Constancia de ello queda en la primera referencia escrita que hay sobre las tunas, que se encuentra en el archivo de la Universidad de Lérida, y en la que se prohíbe a los estudiantes hacer rondas nocturnas bajo pena de confiscarles los instrumentos.

No puede hablarse de tunos, hasta 1538, año en que los sopistas se acogieron a las viviendas benéficas que les ofrecía la Instrucción para bachilleres de pupilos. A partir de ese momento, comenzaron a cantar sin que en ello les fuera la supervivencia. Porque, entonces, los ya ex sopistas, en calidad de estudiantes veteranos, se hicieron servir como escuderos por los "bobos", "pardillos" o estudiantes nuevos, a los que supuestamente debían apoyar, según la norma, a cambio de legarles su gaya ciencia musical.

Lo cuenta el "Guzmán de Alfarache", haciendo hincapié en el estatus de estudiante rico que así alcanzaron los otrora sopistas. Luego, en "El Buscón" de Francisco de Quevedo, se habla de las bromas que les aguantaban los estudiantes novatos, hasta cumplir el meritoriaje que les terminara equiparando a ellos.

El tuno mendicante casi desaparece de la escena española merced a la abolición de la obligatoriedad en el uso del traje talar (traje de estudiante), viéndose constreñidos los estudiantes a colgar manteos y tricornios y a utilizar ropas de gentes, imposibilitándose la identificación de quienes corren la tuna como pertenecientes a la corporación escolar en el año de 1835, y posteriormente a mediados de siglo durante la regencia de María Cristina, que permite la libre asociación, se crean asociaciones de músicos y artistas entre las que sobresalen las "estudiantinas", grupos musicales a la batuta de un director, con un formato de número musical que fue todo un éxito en la época, haciendo que estudiantinas como la "Figaro" trascendiera fronteras y continentes.

A imagen de estas estudiantinas, se recrean en las universidades españolas las primeras tunas como las vemos hoy, que evocan las otrora comparsas de estudiantes que con sotana y manteo raído recorrían ciudades y campos, pero ahora con el traje y formato musical de la estudiantina, multiplicándose sus tradicionales galanteos y rondas nocturnas.Actualmente el asociacionismo en la universidad se ha divesificado en extremo, lo que ha hecho que las asociaciones de más tradición como el coro, la tuna y el teatro universitario pierdan estudiantes, foco y respaldo en favor de otras más recientes como las de cooperación internacional, cinefórum y juegos rol. Sin embargo perdura en muchas ciudades el hecho de realizar encuentros de estudiantinas como un elemento propio y algunas estudiantinas realizan recitales anuales en sus respectivas universidades con respaldo institucional.

Pese a la antigüedad de la institución o, quizá, debido a ella, la tuna despierta en la actualidad opiniones encontradas. Partidarios y detractores esgrimen distintos argumentos a favor y en contra de su existencia. Entre los argumentos de sus partidarios están que la tuna es una institución "simpática", un grupo cohesionado que defiende valores como la hermandad, la lealtad, la defensa de las tradiciones estudiantiles, que celebra la alegría de la juventud, la despreocupación estudiantil y el amor por la música y la sana diversión.

El novataje es el periodo en el que los nuevos y futuros miembros de la Tuna se forman tanto en los aspectos musicales como personales, siendo este proceso un método de reforzar los puntos fuertes y débiles del joven ,aunque con variaciones entre las distintas tunas, en general, los nuevos miembros de la tuna son considerados indignos de portar el traje y la beca hasta haber demostrado distintos grados de capacidad o pericia tanto musical como en los usos y forma de comportarse de los tunos veteranos. El periodo de tiempo que los novatos y pardillos (un pardillo posee traje pero no beca) deben pasar siéndolo varía, dependiendo de su velocidad de aprendizaje.
Durante ese tiempo son considerados aprendices, es por ello por lo que ayudan a los veteranos en el desempeño del tunar y están excluidos de " ciertos derechos", asimismo pueden ser objeto de novatadas aleatorias a capricho de cualquiera de los miembros más veteranos (siempre con el objetivo de que aprendan las habilidades propias de la tuna de esas novatadas. También es tradición que novatos y pardillos traten de librarse de ellas, hacer que otro las cumpla y/o cualquier otro menester que les aligere la carga, ayudándose mutuamente aunque acaben de conocerse, pues esto es lo que hace hermandad). Transcurrido ese periodo, los novatos que ameriten acceder al grado de veterano y portar la beca suelen pasar por un rito de paso en forma de festejo privado en el que se les hace pasar una noche llena de diversión , hermandad, música y risas. A la finalización de éste, al nuevo miembro de pleno derecho de la tuna se le impone la beca distintiva que le acredita como tal. En algunas tunas el rito de paso se realiza como condición inicial para entrar en la tuna y acceder a la condición de novato. En otras, existen grados de "impericia": pardillo, novato, etc. hasta acceder al de veterano.

Las tunas en la actualidad se suelen clasificar según la facultad universitaria a la que pertenecen sus miembros: así tendremos "Tuna de Derecho", "Tuna de Medicina", "Tuna de Peritos", Tuna de Filosofía, Tuna de Magisterio, Tuna de Económicas, etcétera. En casos de universidades con menor antigüedad o tradición, suele haber una sola tuna Universitaria, que englobe a estudiantes de varios estudios, e incluso en ciudades con varios "distritos universitarios", crean una Tuna de Distrito (también con estudiantes de varios estudios).

La indumentaria del tuno está compuesta de capa o manteo, jubón, camisa, calzas, abullonadas o cervantinas sobre éstas, zapatos o botas y finalmente la beca que es lo que identifica a cada tuna y varía su color de acuerdo a la facultad a la que pertenezca según la tradición española: rojo para Derecho, azul turquí para Ciencias, amarillo para Medicina, azul celeste para Filosofía y Letras, morado para Farmacia, verde para Empresariales, naranja para Economía, etcétera.

La primera representación iconográfica de un tuno se encuentra en la parte inferior del pasamanos de la escalera del Rectorado de la Universidad de Salamanca, antiguo hospital universitario, una pequeña talla que lo muestra con el bicornio decorado con cuchara y tenedor, un instrumento de cuerda que parece una bandurria y capa.

En las representaciones correspondientes del siglo XVIII, aparecen vestidos con calzón corto, jubón y capa, y bicornio. En la actualidad las tunas se dividen entre las de gregüesco o trusas y las de calzón corto."






Las becas tienen un largo suficiente para alcanzar (aproximadamente) desde la mitad superior de la espalda hasta que la punta de la uve se sitúe en el pecho. Las becas de las tunas de Valladolid son particularmente diferentes al resto, dado que son más largas (de forma que por la espalda se cruzan y llegan sus extremos hasta más abajo del cinturón del jubón) y tienen en uno de sus extremos un aro (quizá como recuerdo de un sombrero que se llevaba cosido a la beca).

Hay algunas curiosidades: la Tuna Compostelana (no lleva beca, porta cosida en el jubón una Cruz de Santiago bordada) y la Tuna de Distrito de Granada (lleva al pecho el escudo de la Universidad de Granada), la conocida como Tuna de Peritos de Sevilla (o Tuna de Peritos e Ingenieros Técnicos Industriales) lleva un fajín blanco impuesto en honor a su anual acto de ofrenda a la Virgen de la Inmaculada Concepción (acto que forma parte de las fiestas tradicionales de Sevilla, en la noche del 7 al 8 de diciembre).Hay otra indumentaria de tuna propia de las tunas portuguesas. Llevan el traje de estudiante negro y la capa negra. En vez de cintas dejan que les rasguen la capa personas que son importantes para ellos.

"Los instrumentos actuales de la tuna son principalmente los llamados de pulso y púa, los populares españoles: laúd, guitarra y bandurria y la pandereta, y antes la vihuela, de péñola o de arco, antecesora del violín. Estos instrumentos aparecen en el "Libro del Buen Amor.""La guitarra que se utiliza como acompañamiento armónico de la melodía. La melodía la crean las voces y los cantos, que se apoyan en la bandurria (primera voz) y el laúd español (segunda voz). El contrabajo se ha convertido en la actualidad en un instrumento habitual en muchas tunas, complementado armónicamente el conjunto de cuerdas. No nos podemos olvidar, sin embargo, de otro instrumento característico de la música estudiantil: la pandereta, así como también podemos encontrar al pandero y las castañuelas.

Además de los instrumentos básicos, sin los cuales no se podría crear música de tuna, utilizan muchos otros que le confieren una riqueza muy especial. Estos han llegado gracias a la fusión con la cultura de muchos pueblos, y también muchas veces por los propios instrumentos regionales de las localidades de origen de la tuna. Entre los más destacados encontramos el timple canario, la bandola y el charango. También suelen usarse en las tunas de todo el mundo el cuatro venezolano, tres cubano, cuatro puertorriqueño y el acordeón para acrecentar la variedad sonora. En la actualidad también se han añadido instrumentos de viento tales como la quena y la flauta traversa. En conclusión la tuna recoge la cultura musical de cada país y/o región que visita y la adopta en su repertorio.

Las Estudiantinas portuguesas suelen usar mandolina en vez de bandurria y laúd; también incluyen tradicionalmente el bandoneón y la guitarra portuguesa.

La mayoría de las tunas y estudiantinas de México, adaptan dos instrumentos básicos del mariachi como son: la vihuela mexicana y el guitarrón, esto con el fin de enriquecer más aún las melodías. Cabe mencionar que algunas estudiantinas parroquiales, también utilizan el acordeón para armonizar la pieza musical. Incluso algunas tunas y estudiantinas de México hacen uso de violines cuando interpretan piezas de folclore mexicano como las canciones de mariachi o algunos sones y huapangos, sobre todo, las tunas y Estudiantinas de los estados de Hidalgo y Veracruz.""El repertorio de la Tuna es sobre todo canción estudiantil, empezando por el género propiamente que es la canción de ronda, que aparece desde las primeras canciones líricas europeas, que tiene su versión en la música popular, donde siempre se encuentran canciones de serenata y los pasacalles etc. con sus elementos que vemos en otro lugar. Buena parte del repertorio tunantesco, las más populares, no tienen autor conocido.""Después, su cancionero se ha enriquecido con temas populares españoles, así como pasodobles y vals, y folclore regional, como jotas, isas, malagueñas, etc. También, por su carácter viajero el Tuno ha engrosado su repertorio con canciones de todo el mundo en miles de idiomas, siempre para poder sorprender y alegrar cada fiesta en la que se encuentren.

Algunas tunas son cantautoras de temas propios. Por ejemplo, la Tuna de Derecho de Sevilla es cantautora de temas como "Rumor en los Balcones", "Las Plazas de Mi Sevilla", "Tus Ojos" o "Maicena". Cabe destacar el legado de la Tuna de Medicina de Salamanca, con "Tuno Galeno" (en referencia a su profesión), "El Trovador" (que habla de la faceta galante del tuno) o "La Solera de mi Menda" (hablando de su ropaje). También podemos encontrarnos con la Tuna de Magisterio de Málaga como autora de "Tradición", "Mujer malagueña", "Luna negra", "Noche sin final", "Aires de Libertad" o "De tu piel", entre otras. Muchas de estas canciones y letras se pueden encontrar recogidas en el Museo del Estudiante. Desde su fundación en 1982, la Tuna de Derecho de Alicante (TDA) ha realizado una importante aportación al cancionero actual, gracias especialmente a la prolífica y longeva labor de Juan Carlos Berrueco, "Charly", con composiciones como "Llorarán las estrellas" o "Cumbres del Teide", así como de Isidoro Vila, "Lucky" y su laureado "Plazas y Calles de Chueca", y el compromiso musical de varias generaciones de integrantes que, además de sus tareas profesionales, participan en otros proyectos musicales, incluyendo la estrecha colaboración con otras tunas de su ciudad.






</doc>
<doc id="6589" url="https://es.wikipedia.org/wiki?curid=6589" title="Quevedo (desambiguación)">
Quevedo (desambiguación)

Quevedo puede referirse a Francisco de Quevedo, célebre escritor del Siglo de Oro español, o también a las siguientes personalidades:


Además, puede hacer referencia a:

</doc>
<doc id="6593" url="https://es.wikipedia.org/wiki?curid=6593" title="Vim">
Vim

Vim (del inglés "Vi IMproved") es una versión mejorada del editor de texto Vi, presente en todos los sistemas UNIX. 

Su autor, Bram Moolenaar, presentó la primera versión en 1991, fecha desde la que ha experimentado muchas mejoras. La principal característica tanto de Vim como de Vi consiste en que disponen de diferentes modos entre los que se alterna para realizar ciertas operaciones, lo que los diferencia de la mayoría de editores comunes, que tienen un solo modo en el que se introducen las órdenes mediante combinaciones de teclas o interfaces gráficas.

Vim, como su antecesor Vi, se utiliza desde un Terminal en modo texto. Se controla por completo mediante el teclado. Esto es en parte a causa de que Vi fue desarrollado a mediados de la década de 1970, cuando los terminales se comunicaban con un ordenador principal (host) mediante una conexión serie, que no era muy rápida (20 Kbps). Las limitaciones de los terminales de la época dieron lugar al concepto de diferentes modos, idea que ha resultado muy provechosa desde entonces. Vim es casi 100 % compatible con Vi, aunque tiene muchas mejoras e incluso cuenta con versiones dotadas de interfaz gráfica y menús que pueden operarse mediante el ratón (gvim o kvim), así como una versión simplificada, evim que se comporta como un editor sin diferentes modos.

Hay versiones de Vim disponibles para muchos sistemas operativos y se puede encontrar en casi cualquier sistema GNU/Linux y en todos los sistemas *BSD, donde en muchas ocasiones se puede ejecutar a través de la orden "vi", que invoca a Vim a través de un enlace simbólico o un alias. Cuando se inicia vim, lo hace en modo comando, y muestra la versión por pantalla.

Cuando Bram Moolenar compró una computadora Amiga a finales de la década de 1980, quería seguir usando el editor de Unix al que se había acostumbrado, pero los clones de Vi disponibles para Amiga no le convencían. Así que en 1988 partió del clon de Vi Stevie 1.0 como base para empezar a desarrollar Vim. En un principio le llamó "Vi IMitation" (imitación de Vi), pues al principio su objetivo principal era emular las funcionalidades de que Vi disponía en su nuevo sistema. En 1991 apareció la versión 1.14 en "Fred Fish disk #591", una colección de software libre para Amiga. La versión 1.22 fue la primera que apareció con versiones para Unix y MS-DOS. Por aquellos tiempos el acrónimo que le daba nombre ya había cambiado a "Vi IMproved". 

En los años siguientes Vim experimentó grandes mejoras. Se dio un paso importante al incorporar ventanas en la versión 3.0 (1994) (Figura 1). Con Vi se pueden tener varios ficheros abiertos en la misma sesión, pero solo se puede ver uno a la vez; las ventanas de Vim permiten verlos simultáneamente. Desde la versión 4.0 (1996) está disponible una interfaz gráfica de usuario (GUI en inglés), que empezó a desarrollar Robert Webb. Desde la versión 5.0 (1998) Vim dispone de resaltado de sintaxis (Figura 2).


Vim dispone de una excelente documentación, que se rige por la máxima "Una característica no documentada es una característica inútil". La documentación, en formato texto, es muy amplia y fácil de entender. El usuario accede mediante una búsqueda a la descripción de varias funcionalidades que pueden solucionar su problema. A través del resaltado de la sintaxis propia de la ayuda de Vim se resaltan las palabras clave (Figura 3). 

Mediante combinaciones de teclas ejecutadas cuando el cursor está sobre una palabra clave se puede navegar por la ayuda, volviendo atrás en caso necesario. En la versión gráfica también se puede utilizar el ratón para este propósito.
La orden codice_1 es importante, pues permite al usuario buscar una palabra en los textos de la ayuda, sin que sea necesario que se trate de una palabra clave (Figura 4). Completan la ayuda una versión en HTML disponible para su consulta en internet y una extensa lista de preguntas frecuentes (FAQ).

Vim es muy personalizable y extensible, lo que lo hace muy atractivo para usuarios que demandan gran cantidad de control y flexibilidad en la edición de texto.
La entrada de texto se facilita con una serie de funciones diseñadas para incrementar la eficiencia del teclado. Los usuarios pueden ejecutar comandos complejos como el «mapeado» de teclas, que puede personalizarse y extenderse. La característica de grabación permite la creación de macros para automatizar secuencias de pulsaciones y llamar funciones internas o definidas por el usuario. Las «abreviaturas», similares a las macros y los mapeos, facilitan la expansión de cadenas de texto cortas hacia largas y pueden usarse también para corregir errores. Vim también tiene un modo fácil para los usuarios que buscan un editor sencillo.
Hay muchas extensiones (plugins) disponibles que añaden o extienden funcionalidades como comprobación de sintaxis y errores semánticos, integración con Git, mostrar colores en CSS. Estos scripts complejos normalmente están escritos en el lenguaje interno «vimscript» (conocido también como VimL), pero también pueden escribirse en otros lenguajes.

Vim es un editor hecho por programadores para programadores. Para facilitar la programación, Vim dispone de un modo "editar, compilar, corregir". De la misma forma que los entornos de desarrollo integrados, puede editar el código fuente además llamar a un compilador externo, e interpretar sus resultados. Si hay errores de compilación, éstos se muestran en una ventana. Los mensajes de error dirigen al usuario a la zona en la que se han encontrado para poder así corregirlos. Entonces vuelve a empezar el ciclo "editar, compilar, corregir" y, si es necesario, corregir nuevos errores. El trabajo del programador también se ve facilitado por el resaltado de sintaxis y la funcionalidad de plegado de código (véase ':help quickfix').

Comparar dos (o más) versiones de un fichero es una tarea frecuente para algunos usuarios. Vim ofrece una solución simple, las dos versiones diferentes del fichero se muestran en dos ventanas contiguas en las que se resaltan las diferencias (Figura 5). De esta forma se pueden ver coloreadas las diferencias intercaladas en las versiones. Por ejemplo, las eliminaciones aparecen en rojo y las nuevas inserciones en violeta. 

Hay órdenes que permiten situar el cursor en las diferencias siguientes y anteriores ([c y ]c), además de volcar estas diferencias hacia el otro archivo (do y dp). 

Vim dispone de un lenguaje interpretado, o de scripting, para programar nuevas funcionalidades, mediante el que se pueden automatizar operaciones particulares demasiado complejas para realizarlas con una macro. Los scripts pueden ser leídos e interpretados mediante la orden codice_2.

El lenguaje se llama "vimscript" o "VimL")

Las macros de Vim pueden contener una secuencia de comandos del modo normal, pero también pueden invocar comandos exteriores o funciones escritas en vimscript para tareas más complejas. Casi todas las extensiones (plugins) de las funciones troncales de Vim están escritas en vimscript, pero también pueden usar otros lenguajes como Perl, Python, Lua, Ruby, Tcl, o Racket.

Estas extensiones pueden instalarse manualmente o mediante un gestor de extensiones como Vundle, Pathogen, o Vim-Plug.

Los ficheros de vimscript se guardan como texto plano, similar a cualquier otro código, y la extensión suele ser codice_3. Una excepción a eso es la del fichero de configuración de Vim que tiene la extensión codice_4.

" This is the Hello World program in Vim script.
echo "Hello, world!"
" This is a simple while loop in Vim script.
let i=1
while i < 5
endwhile
unlet i

Vim es un "editor modal", lo que significa que se puede trabajar en diferentes modos para realizar una tarea determinada. Para ver en qué modo se encuentra Vim se debe de tener activada la opción codice_5. A continuación se describen los seis modos de Vim.
Los tres primeros son los modos del vi original. Los cinco modos adicionales no deben entenderse por separado, sino en combinación con el modo base. Véase también la ayuda de Vim a este respecto: ':help vim-modes'.
Vim empieza en modo comando, también conocido como modo normal. En este modo se pueden emplear combinaciones de teclas para, por ejemplo, copiar líneas y trabajar en el formato del texto. Éste es el "modo central", desde el que se cambia a los otros modos. Si no se sabe qué se está haciendo, pulsando dos veces la tecla Escape siempre se puede volver al modo normal. Si ya se estaba en modo normal y tanto la configuración de Vim como la del terminal lo permiten, Vim emite un pitido.

En modo inserción cuando se pulsan las teclas se edita el texto como en otros editores. Se puede cambiar del modo comandos al modo inserción pulsando la tecla i. Hay un gran abanico de comandos para pasar al modo inserción, que difieren sustancialmente, pues permiten por ejemplo editar al final de la línea, en un punto concreto del texto, editar borrando una palabra, entre muchas otras. Un usuario experto puede sacar un gran provecho de la existencia de esta variedad de órdenes.

En el modo inserción todas las teclas tienen alguna función además de la mera inserción, que se activan pulsando simultáneamente las teclas Tecla control o Mayúsculas. La tecla Esc es muy importante en modo inserción, pues permite cambiar de modo inserción a modo comandos.

Cambiando al modo comandos para realizar ciertas tareas se incrementa en gran medida la eficiencia en la edición, y se puede aprovechar la potencia completa de Vim.

A este modo se accede pulsando la tecla dos puntos :. Tras los dos puntos se pueden introducir órdenes complejas, como por ejemplo buscar y reemplazar con expresiones regulares. Pulsando la tecla Esc se puede volver al modo órdenes. Las búsquedas se pueden realizar con la orden / (hacia adelante) y ? (hacia atrás). También se pueden filtrar líneas mediante !.

Este modo es una mejora respecto a vi. Mediante unas ciertas combinaciones de teclas en combinación con las teclas de movimiento del cursor, se puede marcar un área de texto, ya sea un grupo de líneas o un bloque. Una vez se tiene el texto marcado se pueden usar órdenes del modo comandos para manipularlo. Las operaciones que se pueden realizar en este modo son más simples que las del modo comandos.

Este modo empieza como el modo visual, pues hay que seleccionar un bloque de texto. Tras la selección, se puede cambiar al modo selección mediante Control-G. Una vez en el modo, si se pulsa una tecla imprimible, el texto seleccionado se borra, se termina el modo selección y aparece el símbolo correspondiente a la tecla pulsada. La selección se puede extender pulsando mayúsculas y las teclas de flechas, el comportamiento habitual en los programas de Microsoft Windows. Este modo se puede finalizar pulsando la tecla Escape.

Este modo se asemeja al modo línea de órdenes, con la diferencia de que tras la ejecución de una orden no se vuelve al modo comandos. Se entra en este modo pulsando Q y se termina con vi. En este modo Vim imita al editor de UNIX "ex", que manipulaba el texto línea a línea debido a las limitaciones de la época, en lugar de editar toda la página.

gVim es una versión gráfica del editor de textos Vim. gVim funciona con las librerías gtk. Mantiene las funcionalidades del Vim, y añade menús y un entorno gráfico (funciona "fuera" de la consola/terminal). 

Su principal ventaja, para los nuevos usuarios, los menús desplegables implican una curva de aprendizaje algo menos dura. Su principal desventaja es que no está instalado por defecto en todos los sistemas (por ejemplo, no está en los servidores sin entorno gráfico).

Una virtud de Vim es que se puede ejecutar en muchos sistemas operativos. Esto es importante para usuarios, como por ejemplo administradores de sistemas que deben trabajar en muchas plataformas distintas. Se puede ejecutar en los siguientes sistemas operativos: AmigaOS, Atari MiNT, BeOS, DOS, GNU/Linux, Mac OS, NextStep, OS/2, OSF, RISC OS, IRIX, Unix (muchas variedades, como por ejemplo BSD, AIX y HP-UX), VMS, y Windows 3.x/95/98/ME/2000/NT/XP.

Las críticas se aplican a Vi y Vim por igual, pues están basados en el mismo concepto y sus modos de operación son completamente diferentes de la mayoría los editores convencionales actuales. Sobre todo conciernen a la separación de las operaciones en distintos modos y la necesidad de aprender numerosas combinaciones de teclas, sin las que no es posible trabajar de forma eficiente.
Actualmente la falta de empleo del ratón como complemento extra a las combinaciones de teclas se considera como un anacronismo, pues puede llevar al usuario ocasional a la frustración. Sólo tras un aprendizaje prolongado se consigue aumentar la productividad.

La mayoría de los usuarios que usan Vim aseguran que este editor incrementa su productividad comparándolo con editores más simples una vez se ha superado la curva de aprendizaje. Las combinaciones de teclas se pueden memorizar empleando métodos mnemotécnicos, pues guardan relación con palabras inglesas. La complejidad intrínseca de aprender las instrucciones se ve recompensada por la mejora en la eficiencia. Los usuarios expertos pueden, usando unas pocas combinaciones de teclas, copiar texto, formatearlo u ordenarlo de muchas formas diferentes, que sólo se pueden realizar en la mayoría de editores mediante operaciones considerablemente más complejas. Basta con un poco de experiencia para notar que las combinaciones de instrucciones que permiten ediciones de texto complejas se facilitan con Vim. Por otra parte las nuevas versiones permiten emplear el ratón e incorporan menús gráficos, que facilitan trabajar con Vim de una forma similar a la de otros editores. A menudo se instala como editor base por su pequeño tamaño y su rapidez en plataformas con recursos limitados basadas en UNIX.

Vim fue el ganador de los "Readers' Choice Awards" de "Linux Journal" en la categoría "Favorite Text Editor" desde 2001 a 2005 y obtuvo el "Slashdot Bernie Award" como el "Mejor editor de texto Open Source" así como en 1999 el "Linuxworld Editors' Choice Award".

Vim se distribuye bajo una licencia Charityware (del inglés "charity": caridad) compatible con Licencia pública general de GNU. y por sus siglas en inglés GPL (General Public License). Esto significa que Vim se rige por las mismas condiciones, pero anima a los usuarios a realizar donaciones para los niños huérfanos de Uganda a través de la organización ICCF Holland.

Vim es desarrollado por Bram Moolenaar y muchos voluntarios. La página de ayuda de Vim de la versión actual menciona a más de 50 colaboradores. Además hay un gran número de personas que han ayudado, no sólo en el desarrollo del software, sino también portándolo a otros sistemas operativos, probando nuevas versiones, reportando bugs, redactando documentación y traduciendo el sistema de ayuda. También contestan a las preguntas de los usuarios, adaptando Vim a otros proyectos y muchas otras cosas.

Todos los usuarios pueden aportar realizando scripts o dando consejos. Hay una lista de correo muy activa, útil tanto para nuevos usuarios como para los experimentados en la que se dan respuestas rápidas y competentes.


Vim agrega funcionalidades muy importantes respecto al antiguo editor Vi, algunas de estas mejoras son:

El editor de texto Vim contiene al menos un huevo de Pascua como homenaje a la Guía del autoestopista galáctico del escritor Douglas Adams. La orden produce el texto:


"<vim@vim.org>" – véase también: Instrucciones de la Lista de Correo




</doc>
<doc id="6594" url="https://es.wikipedia.org/wiki?curid=6594" title="Centauro">
Centauro

En la mitología griega, el centauro (en griego Κένταυρος "Kentauros" matador de toros, cien fuertes, plural Κένταυρι "Kentauri"; en latín "Centaurus/Centauri") es una criatura con la cabeza, los brazos y el torso de un humano y el cuerpo y las patas de un caballo. Las hembras son llamadas centáurides.

Vivían en las montañas de Tesalia y se les consideraba hijos de Centauro —un hijo de Ixión y Néfele—, y algunas yeguas magnesias. Otras tradiciones decían que Centauro, en cambio, era hijo de Apolo y Estilbe, o que los centauros habían nacido directamente de Ixión y Néfele.

Los centauros son muy conocidos por la lucha que mantuvieron con los lápitas, provocada por su intento de raptar a Hipodamía el día de su boda con Pirítoo, rey de los lápitas y también hijo de Ixión. La riña entre estos primos es una metáfora del conflicto entre los bajos instintos y el comportamiento civilizado en la humanidad y por el famoso centauro (Quirón). Teseo, un héroe y fundador de ciudades que estaba presente, inclinó la balanza del lado del orden correcto de las cosas, y ayudó a Pirítoo. Los centauros huyeron. Escenas de la batalla entre los lápitas y los centauros fueron esculpidas en bajorrelieves en el friso del Partenón, que estaba dedicado a la sabia Atenea.

Como la titanomaquia, la derrota de los titanes por los dioses olímpicos, las contiendas con los centauros representan la lucha entre la civilización y el barbarismo y es conocida como centauromaquia.

El personaje general de los centauros es el de seres salvajes, sin leyes ni hospitalidad, esclavos de las pasiones animales. Dos excepciones a esta regla son Folo y Quirón, que expresaban su «buena» naturaleza, siendo centauros sabios y amables.

Entre los centauros, el tercero con una identidad individual es Neso. El episodio mitológico del centauro Neso raptando a Deyanira, la prometida de Heracles, también proporcionó a Giambologna (1529-1608), un escultor flamenco que trabajó en Italia, espléndidas oportunidades de concebir composiciones con dos formas en violenta interacción. Giambologna realizó varias versiones de Neso raptando a Deyanira, representados por los ejemplos conservados en diversos museos. Sus seguidores, como Adriaen de Vries y Pietro Tacca, continuaron esculpiendo incontables repeticiones del tema. Cuando Albert-Ernest Carrier-Belleuse abordó la misma composición de formas en el siglo XIX, la tituló "Rapto de Hipodamía".

En antiguas vasijas pintadas áticas los centauros eran representados como seres humanos de frente, con el cuerpo y las patas traseras de un caballo sujetos a la espalda. Posteriormente, fueron hombres sólo hasta la cintura. La batalla con los lápitas y la aventura de Heracles con Folo (Apolodoro, ii. 5; Diodoro Sículo, iv. li) son temas favoritos del arte griego.

En Grecia, la constelación de Centaurus fue observada por Eudoxo de Cnido en el siglo IV a. C. y por Arato en el siglo III a. C.

El escritor Robert Graves especuló con que los centauros de la mitología griega fueran una reminiscencia de una tribu prehelénica que considerase al caballo un tótem. 

Por otra parte, Paléfato consideraba que la forma híbrida de los centauros era fruto de un error de percepción por parte de gentes que nunca habían conocido la monta de caballos. Al observar por primera vez a jinetes, habrían tenido la impresión de que eran mitad hombres mitad caballos. Paléfato menciona además una posible etimología del nombre, que vendría a significar «matador de toros». 

Algunos dicen que los griegos tomaron la constelación Centaurus, y también su nombre «toro penetrante», de Mesopotamia, donde se simboliza al dios Baal, que representa la lluvia y la fertilidad, y luchando y perforando con sus cuernos el demonio Mot que representa la sequía de verano.

Los centauros han aparecido muchas veces y en muchos lugares en obras de ficción modernas.

Aunque se dice que la palabra griega "kentauros" está compuesta por un único morfema —quizá no griego en su origen—, el sufijo "-tauro" ha sido inventado por escritores y diseñadores de juegos a finales del siglo XX para otros híbridos animal-humanos fantásticos.

Aunque las mujeres centauros, llamadas centáurides (κενταύριδες), no son nombradas en la literatura arcaica ni en el arte arcaico, aparecen ocasionalmente a partir del periodo helenístico. Un mosaico macedonio de principios del siglo III a. C. que se halla actualmente en el museo arqueológico de la ciudad de Pela es uno de los primeros ejemplos de la presencia de centáurides en el arte. 

El autor romano Ovidio en sus "Metamorfosis" menciona a una centáuride llamada Hilónome, que se suicidó cuando su amante Cílaro murió durante la guerra contra los lápitas.

En la descripción de una pintura que vio en Neápolis, el retórico griego Filóstrato el Viejo presenta a las centáurides como hermanas y esposas de los centauros masculinos que vivían en el monte Pelión con sus hijos.

Dentro del campo de la medicina se conoce con el nombre de Genu recurvatum a la condición física que poseen algunas personas de doblar sus rodillas más allá de los 180° habiéndose conocido algunos casos a lo largo de la historia, como los de Robert Huddleston, apodado el "hombre pony" o el de Ella Harper también llamada "la niña camello".



Otras criaturas híbridas aparecen también en la mitología griega, siempre con alguna conexión liminal que enlaza la cultura helénicas con otras arcaicas o extranjeras:

Otras criaturas similares:








</doc>
<doc id="6598" url="https://es.wikipedia.org/wiki?curid=6598" title="Sofista">
Sofista

El término sofista (del griego ["sophía"], «sabiduría», y ["sophós"], «sabio») es el nombre dado en la Grecia clásica al que hacía profesión de enseñar la sabiduría. "Sophós" y "Sophía" en sus orígenes denotaban una especial capacidad para realizar determinadas tareas como se refleja en la "Ilíada" (XV, 412). Más tarde se atribuiría a quien dispusiera de «inteligencia práctica» y era un experto y sabio en un sentido genérico. Sería Eurípides quien le añadiría un significado más preciso como «el arte práctico del buen gobierno» (Eur. I.Á.749) y que fue usado para señalar las cualidades de los Siete Sabios de Grecia. Sin embargo, al transcurrir el tiempo hubo diferencias en cuanto al significado de "sophós": por una parte, Esquilo denomina así a los que dan utilidad a lo sabido, mientras que para otros es al contrario, siéndolo quien conoce por naturaleza. A partir de este momento se creará una corriente, que se aprecia ya en Píndaro, que da un cariz despectivo al término "sophós" asimilándolo a «charlatán». Ya que se identificaban (los Sofistas) con el relativismo, la verdad era lo que opinase la mayoría, por lo que pasaron de enseñar "sabiduria" a oratoria. La opinión como criterio de verdad, ya que afirmaban que la verdad objetiva no existía, postura criticada por contemporáneos como Sócrates.

Ya en la "Odisea", Ulises es calificado de "sophón" como «ingenioso». Por el contrario, Eurípides llama a la "sophía" «listeza» y al "sophón" «sabiduría», tratando con ello de diferenciar la intensidad y grado de conocimiento de las cosas que tienen respectivamente los hombres y los dioses.

Los sofistas eran pensadores que desarrollaron su actividad en la Atenas democrática del siglo V a. C. Los filósofos de la naturaleza, los presocráticos, habían elaborado diferentes teorías para explicar el cosmos. Los sofistas y Sócrates van a cambiar el objeto de la filosofía. Ahora, el tema de reflexión es el hombre y la sociedad. Como los sofistas eran viajeros, conocían diferentes culturas, totalmente distintas a la griega. Por eso se plantearon problemas referidos a las costumbres y las leyes. ¿Son las costumbres y leyes un simple acuerdo, una convención, o son naturales? Así surgió la idea de relativismo.

Los sofistas eran maestros que iban de ciudad en ciudad enseñando a ser buenos ciudadanos y a triunfar en la política. El arte de hablar en público, la retórica, era esencial en la democracia griega, donde los ciudadanos participaban constantemente. Las enseñanzas de los sofistas tenían un fin práctico, saber desenvolverse en los asuntos públicos. Fueron los primeros pensadores que cobraron dinero por sus enseñanzas. Unos de los principales sofistas fue Protágoras (480-410 a. C.).

El verbo "sophídsesthai", «practicar la "sophía"», sufrió una evolución similar al terminar por entenderse como «embaucar». La derivación "sophistés" se dio a los Siete Sabios en el sentido de «filósofos» y así llama Heródoto a Pitágoras, a Solón, y a quienes fundaron el culto dionisiaco. También se llamaba así a los "mousike" y a los poetas y, en general, a todos los que ejercían una función educadora. El uso peyorativo empezó a tomar forma en el siglo V a. C., coincidiendo con la extensión del uso del término a los prosistas. El momento coincide con un incremento de las suspicacias de los atenienses hacia los que mostraban una mayor inteligencia. Isócrates denostaba que el término «hubiera caído en deshonor» y Sófocles lo atribuye al hecho de que los educadores y maestros recibieran una remuneración por su trabajo. Esta es la tesis más extendida en la actualidad.

No obstante, era aceptado en la Grecia Antigua que los poetas cobrasen por sus servicios. El desprecio con el que los sofistas eran tratados en ocasiones no nacía del hecho mismo de recibir remuneración, sino de hacerlo, sobre todo, por la formación en la llamada "areté", el arte de la política y la ciudadanía, que incluía todas las técnicas persuasivas para hacerse un lugar en la administración de la "polis".

Platón criticaba a los sofistas por su formalismo y sus trampas dialécticas, pretendiendo enseñar la virtud y a ser hombre, cuando nadie desde un saber puramente sectorial, como el del discurso retórico, puede arrogarse tal derecho.

La primera exigencia de esa "areté" era el dominio de las palabras para ser capaz de persuadir a otros. «Poder convertir en sólidos y fuertes los argumentos más débiles», dice Protágoras. Gorgias dice que con las palabras se puede envenenar y embelesar. Se trata, pues, de adquirir el dominio de razonamientos engañosos. El arte de la persuasión no está al servicio de la verdad sino de los intereses del que habla. Llamaban a ese arte «conducción de almas». Platón dirá más tarde que era «captura» de almas.

Según algunos autores, no eran, pues, propiamente filósofos. Para quienes son de esa opinión, tenían sin embargo en común con los filósofos una actitud que sí puede llamarse filosófica: el escepticismo y relativismo. No creían que el ser humano fuese capaz de conocer una verdad válida para todos. Cada quien tiene «su» verdad.

Por el contrario, hay quien sostiene que sí lo eran, y que las ácidas críticas de Platón corresponden a una disputa por un mismo grupo de potenciales discípulos y a sus diferencias políticas y filosóficas.

De Aristóteles provendrá también el sentido peyorativo: sofista es quien utiliza del sofisma para razonar. Los más destacados miembros de la sofística fueron: Protágoras, Gorgias, Hipias, Pródico, Trasímaco, Critias y Calicles.

Frente a la tradición filosófica, algunos autores a partir del siglo XX han tratado de reivindicar la importancia filosófica de los sofistas. Por ejemplo, Giorgio Colli ha destacado que no es menor el rigor lógico de Gorgias de Leontinos que el de Platón. Además, plantea la hipótesis de que tal vez el sofista fuera el creador de la refutación por reducción al absurdo. También Michel Onfray ha tratado de destacar el papel de los sofistas en la filosofía griega.











</doc>
<doc id="6599" url="https://es.wikipedia.org/wiki?curid=6599" title="27 de abril">
27 de abril

El 27 de abril es el 117.º (centésimo decimoséptimo) día del año en el calendario gregoriano y el 118.º en los años bisiestos. Quedan 248 días para finalizar el año.









</doc>
<doc id="6604" url="https://es.wikipedia.org/wiki?curid=6604" title="Universo">
Universo

El universo es la totalidad del espacio y del tiempo, de todas las formas de la materia, la energía, el impulso, las leyes y constantes físicas que las gobiernan. Sin embargo, el término también se utiliza en sentidos contextuales ligeramente diferentes y alude a conceptos como cosmos, mundo o naturaleza. Su estudio, en las mayores escalas, es el objeto de la cosmología, disciplina basada en la astronomía y la física, en la cual se describen todos los aspectos de este universo con sus fenómenos.

La ciencia modeliza el universo como un sistema cerrado que contiene energía y materia adscritas al espacio-tiempo y que se rige fundamentalmente por principios causales. Basándose en observaciones del universo observable, los físicos intentan describir el continuo espacio-tiempo en el que nos encontramos, junto con toda la materia y energía existentes en él.

Los experimentos sugieren que el universo se ha regido por las mismas leyes físicas, constantes a lo largo de su extensión e historia. Es homogéneo e isotrópico. La fuerza dominante en distancias cósmicas es la gravedad, y la relatividad general es actualmente la teoría más exacta para describirla. Las otras tres fuerzas fundamentales, y las partículas en las que actúan, son descritas por el modelo estándar.

El universo tiene por lo menos tres dimensiones de espacio y una de tiempo, aunque experimentalmente no se pueden descartar dimensiones adicionales. El espacio-tiempo parece estar conectado de forma sencilla, y el espacio tiene una curvatura media muy pequeña o incluso nula, de manera que la geometría euclidiana es, como norma general, exacta en todo el universo.

La teoría actualmente más aceptada sobre la formación del universo, fue teorizada por el canónigo belga Lemaître, a partir de las ecuaciones de Albert Einstein. Lemaitre concluyó (en oposición a lo que pensaba Einstein), que el universo no era estacionario, que el universo tenía un origen. Es el modelo del Big Bang, que describe la expansión del espacio-tiempo a partir de una singularidad espaciotemporal. El universo experimentó un rápido periodo de inflación cósmica que arrasó todas las irregularidades iniciales. A partir de entonces el universo se expandió y se convirtió en estable, más frío y menos denso. Las variaciones menores en la distribución de la masa dieron como resultado la segregación fractal en porciones, que se encuentran en el universo actual como cúmulos de galaxias.

Las observaciones astronómicas indican que el universo tiene una edad de 13 799±21 millones de años (entre 13 778 y 13 820 millones de años con un intervalo de confianza del 68%) y por lo menos 93000 millones de años luz de extensión. 

Debido a que, según la teoría de la relatividad especial, la materia no puede moverse a una velocidad superior a la velocidad de la luz, puede parecer paradójico que dos objetos del universo puedan haberse separado 93 000 millones de años luz en un tiempo de únicamente 13 000 millones de años; sin embargo, esta separación no entra en conflicto con la teoría de la relatividad general, ya que esta solo afecta al movimiento en el espacio, pero no al espacio mismo, que puede extenderse a un ritmo superior, no limitado por la velocidad de la luz. Por lo tanto, dos galaxias pueden separarse una de la otra más rápidamente que la velocidad de la luz si es el espacio entre ellas el que se dilata.

Observaciones recientes han demostrado que esta expansión se está acelerando, y que la mayor parte de la materia y la energía en el universo son las denominadas materia oscura y energía oscura, la materia ordinaria (bariónica), solo representaría algo más del 5 % del total.

Las mediciones sobre la distribución espacial y el desplazamiento hacia el rojo ("redshift") de galaxias distantes, la radiación cósmica de fondo de microondas, y los porcentajes relativos de los elementos químicos más ligeros, apoyan la teoría de la expansión del espacio, y más en general, la teoría del Big Bang, que propone que el universo en sí se originó en un momento específico en el pasado.

En cuanto a su destino final, las pruebas actuales parecen apoyar las teorías de la expansión permanente del universo ("Big Freeze" o "Big Rip", Gran Desgarro), que nos indica que la expansión misma del espacio, provocará que llegará un punto en que los átomos mismos se separarán en partículas subatómicas. Otros futuros posibles que se barajaron, especulaban que la materia oscura podría ejercer la fuerza de gravedad suficiente para detener la expansión y hacer que toda la materia se comprima nuevamente; algo a lo que los científicos denominan el "Big Crunch" o la Gran Implosión, pero las últimas observaciones van en la dirección del gran desgarro.

Los cosmólogos teóricos y astrofísicos utilizan de manera diferente el término "universo", designando bien el sistema completo o únicamente una parte de él. Frecuentemente se utiliza el término "el universo" para designar la parte observable del espacio-tiempo o el espacio-tiempo entero.

Según el convenio de los cosmólogos, el término "universo" se refiere frecuentemente a la parte finita del espacio-tiempo que es directamente observable utilizando telescopios, otros detectores y métodos físicos, teóricos y empíricos para estudiar los componentes básicos del universo y sus interacciones. Los físicos cosmólogos asumen que la parte observable del espacio comóvil (también llamado nuestro universo) corresponde a una parte del espacio entero y normalmente no es el espacio entero. 

En el caso del universo observable, este puede ser solo una mínima porción del universo existente y, por consiguiente, puede ser imposible saber realmente si el universo está siendo completamente observado. La mayoría de cosmólogos creen que el universo observable es una parte extremadamente pequeña del universo «entero» realmente existente y que es imposible ver todo el espacio comóvil. En la actualidad se desconoce si esto es correcto, ya que de acuerdo a los estudios de la forma del universo, es posible que el universo observable esté cerca de tener el mismo tamaño que todo el espacio. La pregunta sigue debatiéndose.

El hecho de que el universo esté en expansión se deriva de las observaciones del corrimiento al rojo realizadas en la década de 1920 y que se cuantifican por la ley de Hubble. Dichas observaciones son la predicción experimental del modelo de Friedmann-Robertson-Walker, que es una solución de las ecuaciones de campo de Einstein de la relatividad general, que predicen el inicio del universo mediante un big bang.

El "corrimiento al rojo" es un fenómeno observado por los astrónomos, que muestra una relación directa entre la distancia de un objeto remoto (como una galaxia) y la velocidad con la que este se aleja. Si esta expansión ha sido continua a lo largo de la vida del universo, entonces en el pasado estos objetos distantes que siguen alejándose tuvieron que estar una vez juntos. Esta idea da pie a la teoría del "Big Bang"; el modelo dominante en la cosmología actual.

Durante la era más temprana del "Big Bang", se cree que el universo era un caliente y denso plasma. Según avanzó la expansión, la temperatura decreció hasta el punto en que se pudieron formar los átomos. En aquella época, la energía de fondo se desacopló de la materia y fue libre de viajar a través del espacio. La energía remanente continuó enfriándose al expandirse el universo y hoy forma el fondo cósmico de microondas. Esta radiación de fondo es remarcablemente uniforme en todas direcciones, circunstancia que los cosmólogos han intentado explicar como reflejo de un periodo temprano de inflación cósmica después del "Big Bang".

El examen de las pequeñas variaciones en el fondo de radiación de microondas proporciona información sobre la naturaleza del universo, incluyendo la edad y composición. La edad del universo desde el "Big Bang", de acuerdo a la información actual proporcionada por el WMAP de la NASA, se estima en unos 13.700 millones de años, con un margen de error de un 1 % (137 millones de años). Otros métodos de estimación ofrecen diferentes rangos de edad, desde 11 000 millones a 20 000 millones.

Hasta hace poco, la primera centésima de segundo era más bien un misterio, impidiendo a los científicos describir exactamente cómo era el universo. Los nuevos experimentos en el RHIC, en el Brookhaven National Laboratory, han proporcionado a los físicos una luz en esta cortina de alta energía, de tal manera que pueden observar directamente los tipos de comportamiento que pueden haber tomado lugar en ese instante.

En estas energías, los quarks que componen los protones y los neutrones no estaban juntos, y una mezcla densa supercaliente de quarks y gluones, con algunos electrones, era todo lo que podía existir en los microsegundos anteriores a que se enfriaran lo suficiente para formar el tipo de partículas de materia que observamos hoy en día.

Los rápidos avances acerca de lo que pasó después de la existencia de la materia aportan mucha información sobre la formación de las galaxias. Se cree que las primeras galaxias eran débiles "galaxias enanas" que emitían tanta radiación que separarían los átomos gaseosos de sus electrones. Este gas, a su vez, se estaba calentando y expandiendo, y tenía la posibilidad de obtener la masa necesaria para formar las grandes galaxias que conocemos hoy.

El destino final del universo tiene diversos modelos que explican lo que sucederá en función de diversos parámetros y observaciones. De acuerdo con la teoría general de la relatividad el destino final más probable dependerá del valor auténtico de la densidad de materia, en función de ese parámetro se barajan dos tipos de finales:

A partir de los años 1990 se comprobó que el universo parece tener una expansión acelerada, hecho que dentro de la relatividad general solo es explicable acudiendo a un mecanismo de tipo constante cosmológica. No se conoce si ese hecho puede dar lugar a un tercer tipo de final.

Si el universo es suficientemente denso, es posible que la fuerza gravitatoria de toda esa materia pueda finalmente detener la expansión inicial, de tal manera que el universo volvería a contraerse, las galaxias empezarían a retroceder, y con el tiempo colisionarían entre sí. La temperatura se elevaría, y el universo se precipitaría hacia un destino catastrófico en el que quedaría reducido nuevamente a un punto.

Algunos físicos han especulado que después se formaría otro universo, en cuyo caso se repetiría el proceso. A esta teoría se la conoce como la teoría del universo oscilante.

Hoy en día esta hipótesis parece incorrecta, pues a la luz de los últimos datos experimentales, el Universo se está expandiendo cada vez más rápido.

El Gran Desgarramiento o Teoría de la Eterna Expansión, en inglés Big Rip, es una hipótesis cosmológica sobre el destino último del universo. Este posible destino final del universo depende de la cantidad de energía oscura existente en el Universo. Si el universo contiene suficiente energía oscura, podría acabar en un desgarramiento de toda la materia.

El valor clave es "w", la razón entre la presión de la energía oscura y su densidad energética. A "w" < -1, el universo acabaría por ser desgarrado. Primero, las galaxias se separarían entre sí, luego la gravedad sería demasiado débil para mantener integrada cada galaxia. Los sistemas planetarios perderían su cohesión gravitatoria. En los últimos minutos, se desbaratarán estrellas y planetas, y los átomos serán destruidos.

Los autores de esta hipótesis calculan que el fin del tiempo ocurriría aproximadamente 3,5×10 años después del Big Bang, es decir, dentro de 2,0×10 años.

Una modificación de esta teoría denominada "Big Freeze", aunque poco aceptada, afirma que el universo continuaría su expansión sin provocar un "Big Rip".

Muy poco se conoce con certeza sobre el tamaño del universo. Puede tener una longitud de billones de años luz o incluso tener un tamaño infinito. Un artículo de 2003 dice establecer una cota inferior de 24 gigaparsecs (78000 millones de años luz) para el tamaño del universo, pero no hay ninguna razón para creer que esta cota está de alguna manera muy ajustada "(Véase forma del Universo)".

El universo "observable" (o "visible"), que consiste en toda la materia y energía que podría habernos afectado desde el "Big Bang" dada la limitación de la velocidad de la luz, es ciertamente finito. La distancia comóvil al extremo del universo visible ronda los 46.500 millones de años luz en todas las direcciones desde la Tierra. Así, el universo visible se puede considerar como una esfera perfecta con la Tierra en el centro, y un diámetro de unos 93000 millones de años luz. Hay que notar que muchas fuentes han publicado una amplia variedad de cifras incorrectas para el tamaño del universo visible: desde 13700 hasta 180000 millones de años luz. "(Véase universo observable)".

En el Universo las distancias que separan los astros son tan grandes que, si las quisiéramos expresar en metros, tendríamos que utilizar cifras muy grandes. Debido a ello, se utiliza como unidad de longitud el año luz, que corresponde a la distancia que recorre la luz en un año.

Anteriormente, el modelo de universo más comúnmente aceptado era el propuesto por Albert Einstein en su Relatividad General, en la que propone un universo "finito pero ilimitado", es decir, que a pesar de tener un volumen medible no tiene límites, de forma análoga a la superficie de una esfera, que es medible pero ilimitada.
Esto era propio de un universo esférico. Hoy, gracias a las últimas observaciones realizadas por el WMAP de la NASA, se sabe que tiene forma plana. Aunque no se descarta un posible universo plano cerrado sobre sí mismo. Estas observaciones sugieren que el universo es infinito.

Una pregunta importante abierta en cosmología es la forma del universo. Matemáticamente, ¿qué 3-variedad representa mejor la parte espacial del universo?

Si el universo es espacialmente "plano", se desconoce si las reglas de la geometría Euclidiana serán válidas a mayor escala. Actualmente muchos cosmólogos creen que el Universo observable está muy cerca de ser espacialmente plano, con arrugas locales donde los objetos masivos distorsionan el espacio-tiempo, de la misma forma que la superficie de un lago es casi plana. Esta opinión fue reforzada por los últimos datos del WMAP, mirando hacia las "oscilaciones acústicas" de las variaciones de temperatura en la radiación de fondo de microondas.

Por otra parte, se desconoce si el universo es conexo. El universo no tiene cotas espaciales de acuerdo al modelo estándar del Big Bang, pero sin embargo debe ser espacialmente finito (compacto). Esto se puede comprender utilizando una analogía en dos dimensiones: la superficie de una esfera no tiene límite, pero no tiene un área infinita. Es una superficie de dos dimensiones con curvatura constante en una tercera dimensión. La 3-esfera es un equivalente en tres dimensiones en el que las tres dimensiones están constantemente curvadas en una cuarta.

Si el universo fuese compacto y sin cotas, sería posible, después de viajar una distancia suficiente, volver al punto de partida. Así, la luz de las estrellas y galaxias podría pasar a través del universo observable más de una vez. Si el universo fuese múltiplemente conexo y suficientemente pequeño (y de un tamaño apropiado, tal vez complejo) entonces posiblemente se podría ver una o varias veces alrededor de él en alguna (o todas) direcciones. Aunque esta posibilidad no ha sido descartada, los resultados de las últimas investigaciones de la radiación de fondo de microondas hacen que esto parezca improbable.

Históricamente se ha creído que el Universo es de color negro, pues es lo que observamos al momento de mirar al cielo en las noches despejadas. En 2002, sin embargo, los astrónomos Karl Glazebrook e Ivan Baldry afirmaron en un artículo científico que el universo en realidad es de un color que decidieron llamar café con leche cósmico. Este estudio se basó en la medición del rango espectral de la luz proveniente de un gran volumen del Universo, sintetizando la información aportada por un total de más de 200.000 galaxias.

Mientras que la estructura está considerablemente fractalizada a nivel local (ordenada en una jerarquía de racimo), en los órdenes más altos de distancia el universo es muy homogéneo. A estas escalas la densidad del universo es muy uniforme, y no hay una dirección preferida o significativamente asimétrica en el universo. Esta homogeneidad e isotropía es un requisito de la Métrica de Friedman-Lemaître-Robertson-Walker empleada en los modelos cosmológicos modernos.

La cuestión de la anisotropía en el universo primigenio fue significativamente contestada por el WMAP, que buscó fluctuaciones en la intensidad del fondo de microondas. Las medidas de esta anisotropía han proporcionado información útil y restricciones sobre la evolución del Universo.

Hasta el límite de la potencia de observación de los instrumentos astronómicos, los objetos irradian y absorben la energía de acuerdo a las mismas leyes físicas a como lo hacen en nuestra propia galaxia. Basándose en esto, se cree que las mismas leyes y constantes físicas son universalmente aplicables a través de todo el universo observable. No se ha encontrado ninguna prueba confirmada que muestre que las constantes físicas hayan variado desde el "Big Bang".

El universo observable actual parece tener un espacio-tiempo geométricamente plano, conteniendo una densidad masa-energía equivalente a 9,9 × 10 gramos por centímetro cúbico. Los constituyentes primarios parecen consistir en un 73 % de energía oscura, 23 % de materia oscura fría y un 4 % de átomos. Así, la densidad de los átomos equivaldría a un núcleo de hidrógeno sencillo por cada cuatro metros cúbicos de volumen. La naturaleza exacta de la energía oscura y la materia oscura fría sigue siendo un misterio. Actualmente se especula con que el neutrino, (una partícula muy abundante en el universo), tenga, aunque mínima, una masa. De comprobarse este hecho, podría significar que la energía y la materia oscura no existen.
Durante las primeras fases del "Big Bang", se cree que se formaron las mismas cantidades de materia y antimateria. Materia y antimateria deberían eliminarse mutuamente al entrar en contacto, por lo que la actual existencia de materia (y la ausencia de antimateria) supone una violación de la simetría CP "(Véase Violación CP)", por lo que puede ser que las partículas y las antipartículas no tengan propiedades exactamente iguales o simétricas, o puede que simplemente las leyes físicas que rigen el universo favorezcan la supervivencia de la materia frente a la antimateria.
En este mismo sentido, también se ha sugerido que quizás la materia oscura sea la causante de la bariogénesis al interactuar de distinta forma con la materia que con la antimateria.
Antes de la formación de las primeras estrellas, la composición química del universo consistía primariamente en hidrógeno (75 % de la masa total), con una suma menor de helio-4 (He) (24 % de la masa total) y el resto de otros elementos. Una pequeña porción de estos elementos estaba en la forma del isótopo deuterio (²H), helio-3 (³He) y litio (Li). La materia interestelar de las galaxias ha sido enriquecida sin cesar por elementos más pesados, generados por procesos de fusión en las estrellas, y diseminados como resultado de las explosiones de supernovas, los vientos estelares y la expulsión de la cubierta exterior de estrellas maduras.

El "Big Bang" dejó detrás un flujo de fondo de fotones y neutrinos. La temperatura de la radiación de fondo ha decrecido sin cesar con la expansión del universo y ahora fundamentalmente consiste en la energía de microondas equivalente a una temperatura de 2725 K. La densidad del fondo de neutrinos actual es de 150 por centímetro cúbico.

Según la física moderna, el Universo es un sistema cuántico aislado, un campo unificado de ondas que entra en decoherencia al tutor de la observación o medición. En tal virtud, en última instancia, el entorno del Universo sería no local y no determinista.

Los cosmólogos teóricos estudian modelos del conjunto espacio-tiempo que estén conectados, y buscan modelos que sean consistentes con los modelos físicos cosmológicos del espacio-tiempo en la escala del universo observable. Sin embargo, recientemente han tomado fuerza teorías que contemplan la posibilidad de "multiversos" o varios universos coexistiendo simultáneamente. Según la recientemente enunciada Teoría de Multiexplosiones se pretende dar explicación a este aspecto, poniendo en relieve una posible convivencia de universos en un mismo espacio.

Científicos del King's College de Londres lograron recrear las condiciones inmediatamente seguidas al Big Bang a través del conocimiento adquirido durante dos años de la partícula de Higgs y llegaron a la conclusión de que, posiblemente, el universo colapsó, hasta dejar de existir casi tan pronto cuando empezó, lo que plantea la idea de que todo lo que vemos no existe y solo es el pasado de los astros.

A gran escala, el universo está formado por galaxias y agrupaciones de galaxias. Las galaxias son agrupaciones masivas de estrellas, y son las estructuras más grandes en las que se organiza la materia en el universo. A través del telescopio se manifiestan como manchas luminosas de diferentes formas. A la hora de clasificarlas, los científicos distinguen entre las galaxias del Grupo Local, compuesto por las treinta galaxias más cercanas y a las que está unida gravitacionalmente nuestra galaxia (la Vía Láctea), y todas las demás galaxias, a las que llaman "galaxias exteriores".

Las galaxias están distribuidas por todo el universo y presentan características muy diversas, tanto en lo que respecta a su configuración como a su antigüedad. Las más pequeñas abarcan alrededor de 3000 millones de estrellas, y las galaxias de mayor tamaño pueden llegar a abarcar más de un billón de astros. Estas últimas pueden tener un diámetro de 170 000 años luz, mientras que las primeras no suelen exceder de los 6000 años luz.

Además de estrellas y sus astros asociados (planetas, asteroides, etc...), las galaxias contienen también materia interestelar, constituida por polvo y gas en una proporción que varía entre el 1 y el 10 % de su masa.

Se estima que el universo puede estar constituido por unos 100 000 millones de galaxias, aunque estas cifras varían en función de los diferentes estudios.

La creciente potencia de los telescopios, que permite observaciones cada vez más detalladas de los distintos elementos del universo, ha hecho posible una clasificación de las galaxias por su forma. Se han establecido así cuatro tipos distintos: galaxias elípticas, espirales, espirales barradas e irregulares.

En forma de elipse o de esferoide, se caracterizan por carecer de una estructura interna definida y por presentar muy poca materia interestelar. Se consideran las más antiguas del universo, ya que sus estrellas son viejas y se encuentran en una fase muy avanzada de su evolución.

Las galaxias de este tipo fueron en su momento galaxias espirales, pero consumieron o perdieron gran parte de materia interestelar, por lo que hoy carecen de brazos espirales y solo presenta su núcleo. Aunque a veces existe cierta cantidad de materia interestelar, sobre todo polvo, que se agrupa en forma de disco alrededor de esta. Estas galaxias constituyen alrededor del 3 % de las galaxias del universo.

Están constituidas por un núcleo central y dos o más brazos en espiral, que parten del núcleo. Este se halla formado por multitud de estrellas y apenas tiene materia interestelar, mientras que en los brazos abunda la materia interestelar y hay gran cantidad de estrellas jóvenes, que son muy brillantes. Alrededor del 75 % de las galaxias del universo son de este tipo.

Es un subtipo de galaxia espiral, caracterizados por la presencia de una barra central de la que típicamente parten dos brazos espirales. Este tipo de galaxias constituyen una fracción importante del total de galaxias espirales. La Vía Láctea es una galaxia espiral barrada.

Incluyen una gran diversidad de galaxias, cuyas configuraciones no responden a las tres formas anteriores, aunque tienen en común algunas características, como la de ser casi todas pequeñas y contener un gran porcentaje de materia interestelar. Se calcula que son irregulares alrededor del 5 % de las galaxias del universo.

La Vía Láctea es nuestra galaxia. Según las observaciones, posee una masa de 10 masas solares y es de tipo espiral barrada. Con un diámetro medio de unos 100 000 años luz se calcula que contiene unos 200 000 millones de estrellas, entre las cuales se encuentra el Sol. La distancia desde el Sol al centro de la galaxia es de alrededor de 27 700 años luz (8,5 kpc)
A simple vista, se observa como una estela blanquecina de forma elíptica, que se puede distinguir en las noches despejadas. Lo que no se aprecian son sus brazos espirales, en uno de los cuales, el llamado brazo de Orión, está situado nuestro sistema solar, y por tanto la Tierra.

El núcleo central de la galaxia presenta un espesor uniforme en todos sus puntos, salvo en el centro, donde existe un gran abultamiento con un grosor máximo de 16 000 años luz, siendo el grosor medio de unos 6000 años luz.
Todas las estrellas y la materia interestelar que contiene la Vía Láctea, tanto en el núcleo central como en los brazos, están situadas dentro de un disco de 100 000 años luz de diámetro, que gira sobre su eje a una velocidad lineal superior a los 216 km/s.

Tan solo tres galaxias distintas a la nuestra son visibles a simple vista. Tenemos la Galaxia de Andrómeda, visible desde el Hemisferio Norte; la Gran Nube de Magallanes, y la Pequeña Nube de Magallanes, en el Hemisferio Sur celeste. El resto de las galaxias no son visibles al ojo desnudo sin ayuda de instrumentos. Sí que lo son, en cambio, las estrellas que forman parte de la Vía Láctea. Estas estrellas dibujan a menudo en el cielo figuras reconocibles, que han recibido diversos nombres en relación con su aspecto. Estos grupos de estrellas de perfil identificable se conocen con el nombre de constelaciones. La Unión Astronómica Internacional agrupó oficialmente las estrellas visibles en 88 constelaciones, algunas de ellas muy extensas, como Hidra o la Osa Mayor, y otras muy pequeñas como Flecha y Triángulo.

Son los elementos constitutivos más destacados de las galaxias. Las estrellas son enormes esferas de gas que brillan debido a sus gigantescas reacciones nucleares. Cuando debido a la fuerza gravitatoria, la presión y a la temperatura del interior de una estrella que sea suficientemente intensa, se inicia la fusión nuclear de sus átomos, y comienzan a emitir una luz roja oscura, que después se mueve hacia el estado superior, que es en el que está nuestro Sol, para posteriormente, al modificarse las reacciones nucleares interiores, dilatarse y finalmente enfriarse.
Al acabarse el hidrógeno, se originan reacciones nucleares de elementos más pesados, más energéticas, que convierten la estrella en una gigante roja. Con el tiempo, esta se vuelve inestable, a la vez que lanza hacia el espacio exterior la mayor parte del material estelar. Este proceso puede durar 100 millones de años, hasta que se agota toda la energía nuclear, y la estrella se contrae por efecto de la gravedad hasta hacerse pequeña y densa, en la forma de enana blanca, azul o marrón. Si la estrella inicial es varias veces más masiva que el Sol, su ciclo puede ser diferente, y en lugar de una gigante, puede convertirse en una supergigante y acabar su vida con una explosión denominada supernova. Estas estrellas pueden acabar como estrellas de neutrones. Tamaños aún mayores de estrellas pueden consumir todo su combustible muy rápidamente, transformándose en una entidad supermasiva llamada agujero negro.
Los púlsares son fuentes de ondas de radio que emiten con periodos regulares. La palabra «púlsar» significa "pulsating radio source" (fuente de radio pulsante). Se detectan mediante radiotelescopios y se requieren relojes de extraordinaria precisión para detectar sus cambios de ritmo. Los estudios indican que un púlsar es una estrella de neutrones pequeña que gira a gran velocidad. El más conocido está en la Nebulosa del Cangrejo. Su densidad es tan grande que una muestra de cuásar del tamaño de una bola de bolígrafo tendría una masa de cerca de 100 000 toneladas. Su campo magnético, muy intenso, se concentra en un espacio reducido. Esto lo acelera y lo hace emitir gran cantidad de energía en haces de radiación que aquí recibimos como ondas de radio.

La palabra «cuásar» es un acrónimo de "quasi stellar radio source" (fuentes de radio casi estelares). Se identificaron en la década de 1950. Más tarde se vio que mostraban un desplazamiento al rojo más grande que cualquier otro objeto conocido. La causa era el Efecto Doppler, que mueve el espectro hacia el rojo cuando los objetos se alejan. El primer cuásar estudiado, denominado 3C 273, está a 1500 millones de años luz de la Tierra. A partir de 1980 se han identificado miles de cuásares, algunos alejándose de nosotros a velocidades del 90 % de la luz.

Se han descubierto cuásares a 12 000 millones de años luz de la Tierra; prácticamente la edad del universo. A pesar de las enormes distancias, la energía que llega en algunos casos es muy grande, equivalente a la recibida desde miles de galaxias: como ejemplo, el es unas 60 000 veces más brillante que toda la Vía Láctea.

Los planetas son cuerpos que giran en torno a una estrella y que, según la definición de la Unión Astronómica Internacional, deben cumplir además la condición de haber limpiado su órbita de otros cuerpos rocosos importantes, y de tener suficiente masa como para que su fuerza de gravedad genere un cuerpo esférico. En el caso de cuerpos que orbitan alrededor de una estrella que no cumplan estas características, se habla de planetas enanos, planetesimales, o asteroides. En nuestro Sistema Solar hay 8 planetas: Mercurio, Venus, Tierra, Marte, Júpiter, Saturno, Urano y Neptuno, considerándose desde 2006 a Plutón como un planeta enano. A finales de 2009, fuera de nuestro sistema solar se habían detectado más de 400 planetas extrasolares, pero los avances tecnológicos están permitiendo que este número crezca a buen ritmo.

Los satélites naturales son astros que giran alrededor de los planetas. El único satélite natural de la Tierra es la Luna, que es también el satélite más cercano al sol. A continuación se enumeran los principales satélites de los planetas del sistema solar (se incluye en el listado a Plutón, considerado por la UAI como un planeta enano).

En aquellas zonas de la órbita de una estrella en las que, por diversos motivos, no se ha producido la agrupación de la materia inicial en un único cuerpo dominante o planeta, aparecen los discos de asteroides: objetos rocosos de muy diversos tamaños que orbitan en grandes cantidades en torno a la estrella, chocando eventualmente entre sí. Cuando las rocas tienen diámetros inferiores a 50 m se denominan meteoroides. A consecuencia de las colisiones, algunos asteroides pueden variar sus órbitas, adoptando trayectorias muy excéntricas que periódicamente les acercan la estrella. Cuando la composición de estas rocas es rica en agua u otros elementos volátiles, el acercamiento a la estrella y su consecuente aumento de temperatura origina que parte de su masa se evapore y sea arrastrada por el viento solar, creando una larga cola de material brillante a medida que la roca se acerca a la estrella. Estos objetos se denominan cometas. En nuestro sistema solar hay dos grandes discos de asteroides: uno situado entre las órbitas de Marte y Júpiter, denominado el Cinturón de asteroides, y otro mucho más tenue y disperso en los límites del sistema solar, a aproximadamente un año luz de distancia, denominado Nube de Oort.

La teoría general de la relatividad, que fue publicada por Albert Einstein en 1916, implicaba que el cosmos se hallaba en expansión o en contracción. Pero este concepto era totalmente opuesto a la noción de un universo estático, aceptada entonces hasta por el propio Einstein. De ahí que este incluyera en sus cálculos lo que denominó “constante cosmológica”, ajuste mediante el cual intentaba conciliar su teoría con la idea aceptada de un universo estático e inmutable.
Sin embargo, ciertos descubrimientos que se sucedieron en los años veinte llevaron a Einstein a decir que el ajuste que había efectuado a su teoría de la relatividad era el ‘mayor error de su vida’. Dichos descubrimientos se realizaron gracias a la instalación de un enorme telescopio de 254 centímetros en el monte Wilson (California). Las observaciones formuladas en los años veinte con la ayuda de este instrumento demostraron que el universo se halla en expansión.

Hasta entonces, los mayores telescopios solo permitían identificar las estrellas de nuestra galaxia, la Vía Láctea, y aunque se veían borrones luminosos, llamados nebulosas, por lo general se tomaban por remolinos de gas existentes en nuestra galaxia. Gracias a la mayor potencia del telescopio del monte Wilson, Edwin Hubble logró distinguir estrellas en aquellas nebulosas. Finalmente se descubrió que los borrones eran lo mismo que la Vía Láctea: galaxias. Hoy se cree que hay entre 50 000 y 125 000 millones de galaxias, cada una con cientos de miles de millones de estrellas.

A finales de los años veinte, Hubble también descubrió que las galaxias se alejan de nosotros, y que lo hacen más velozmente cuanto más lejos se hallan. Los astrónomos calculan la tasa de recesión de las galaxias mediante el espectrógrafo, instrumento que mide el espectro de la luz procedente de los astros. Para ello, dirigen la luz que proviene de estrellas lejanas hacia un prisma, que la descompone en los colores que la integran.

La luz de un objeto es rojiza (fenómeno llamado corrimiento al rojo) si este se aleja del observador, y azulada (corrimiento al azul) si se le aproxima. Cabe destacar que, salvo en el caso de algunas galaxias cercanas, todas las galaxias conocidas tienen líneas espectrales desplazadas hacia el rojo. De ahí infieren los científicos que el universo se expande de forma ordenada. La tasa de dicha expansión se determina midiendo el grado de desplazamiento al rojo.
¿Qué conclusión se ha extraído de la expansión del cosmos? Pues bien, un científico invitó al público a analizar el proceso a la inversa —como una película de la expansión proyectada en retroceso— a fin de observar la historia primitiva del universo. Visto así, el cosmos parecería estar en recesión o contracción, en vez de en expansión y retornaría finalmente a un único punto de origen.

El físico Stephen Hawking concluyó lo siguiente en su libro Agujeros negros y pequeños universos (y otros ensayos), editado en 1993: «La ciencia podría afirmar que el universo tenía que haber conocido un comienzo».
Pero hace años, muchos expertos rechazaban que el universo hubiese tenido principio. El científico Fred Hoyle no aceptaba que el cosmos hubiera surgido mediante lo que llamó burlonamente "a big bang" («una gran explosión»). Uno de los argumentos que esgrimía era que, de haber existido un comienzo tan dinámico, deberían conservarse residuos de aquel acontecimiento en algún lugar del universo: tendría que haber radiación fósil, por así decirlo; una leve luminiscencia residual.

El diario "The New York Times" (8 de marzo de 1998) indicó que hacia 1965 «los astrónomos Arno Penzias y Robert Wilson descubrieron la omnipresente radiación de fondo: el destello residual de la explosión primigenia». El artículo añadió: «Todo indicaba que la teoría [de la gran explosión] había triunfado».

Pero en los años posteriores al hallazgo se formuló esta objeción: Si el modelo de la gran explosión era correcto, ¿Por qué no se habían detectado leves irregularidades en la radiación? (La formación de las galaxias habría requerido un universo que contase con zonas más frías y densas que permitieran la fusión de la materia.) En efecto, los experimentos realizados por Penzias y Wilson desde la superficie terrestre no revelaban tales irregularidades.

Por esta razón, la NASA lanzó en noviembre de 1989 el satélite COBE (siglas de Explorador del Fondo Cósmico, en inglés), cuyos descubrimientos se calificaron de cruciales. “Las ondas que detectó su radiómetro diferencial de microondas correspondían a las fluctuaciones que dejaron su impronta en el cosmos y que hace miles de millones de años llevaron a la formación de las galaxias.”

Diferentes palabras se han utilizado a través de la historia para denotar "todo el espacio", incluyendo los equivalentes y las variantes en varios lenguajes de "cielos", "cosmos" y "mundo". El macrocosmos también se ha utilizado para este efecto, aunque está más específicamente definido como un sistema que refleja a gran escala uno, algunos, o todos estos componentes del sistema o partes. Similarmente, un microcosmos es un sistema que refleja a pequeña escala un sistema mucho mayor del que es parte.

Aunque palabras como mundo y sus equivalentes en otros lenguajes casi siempre se refieren al planeta Tierra, antiguamente se referían a cada cosa que existía (se podía ver). En ese sentido la utilizaba, por ejemplo, Copérnico. Algunos lenguajes utilizan la palabra "mundo" como parte de la palabra "espacio exterior". Un ejemplo en alemán lo constituye la palabra "Weltraum".


En inglés:


</doc>
<doc id="6608" url="https://es.wikipedia.org/wiki?curid=6608" title="28 de abril">
28 de abril

El 28 de abril es el 118.º (centésimo decimoctavo) día del año en el calendario gregoriano y el 119.º en los años bisiestos. Quedan 247 días para finalizar el año.








</doc>
<doc id="6609" url="https://es.wikipedia.org/wiki?curid=6609" title="29 de abril">
29 de abril

El 29 de abril es el 119.º (centésimo decimonoveno) día del año en el calendario gregoriano y el 120.º en los años bisiestos. Quedan 246 días para finalizar el año.








</doc>
<doc id="6613" url="https://es.wikipedia.org/wiki?curid=6613" title="Reutilización">
Reutilización

Reutilizar es la acción que permite volver a utilizar los bienes o productos desechados, denominados residuo, y darles un uso igual o diferente a aquel para el que fueron concebidos.

Este proceso hace que cuantos más objetos volvamos a reutilizar menos basura produciremos y menos recursos tendremos que gastar. La reutilización ocupa el segundo puesto en la jerarquía de residuos, después de la prevención y por encima del reciclaje.

Objetivos :

•Concientizar: Lograr que los jóvenes Desarrollen o fortalezcan la conciencia sobre la necesidad de proteger el medio ambiente, que tomen consecuencia de las propias decisiones y acciones cotidianas y profundizando en el conocimiento de la realidad. 

•Reducir: Sería volver a su estado anterior, así como disminuir, resumir o aminorar la cantidad de residuos producidos por una persona o la sociedad. La minimización de residuos implica esfuerzo personal y social.

•Colaborar: Lograr que las familias minimicen residuos cambiando sus hábitos de consumo rechazando aquellos envases que les resultan inútiles, recuperando materiales y ayudando al reciclaje. La comunidad debe separar una o varias de distintas fracciones de residuos que generan en el mismo lugar de producción (domicilios, escuelas, industrias,etc) evitando que se conviertan en basura. El municipio por su parte debe garantizar la existencia de los sitios adecuados.

•Reciclar: Es más fácil de lo que parece generar ideas para reciclar *envases de plástico: contenedor amarillo.

<nowiki>*</nowiki> papel y cartón: contenedor azul.

<nowiki>*</nowiki>cristal y vidrio: contenedor verde. 

<nowiki>*</nowiki>orgánica: contenedor marrón.

<nowiki>*</nowiki>resto de desechos: contenedor gris o verde oscuro.

•Utilizar: Esta acción conocida también como reutilizar permite que sepan cómo volver a utilizar los bienes o productos desechados y darles un uso concebidos. Esto hace que cuanto más objetos volvamos a reutilizar menos basura producimos y menos recursos tenemos que gastar, este ocupa el segundo puesto en la jerarquía de residuos después de la prevención y por encima del reciclaje.

En España está regulada la producción y gestión de los residuos procedentes de todo tipo de obras: edificación, urbanización, demolición, reforma, etc.
Tiene por objeto fomentar, por este orden, su prevención, reutilización, reciclado y otras formas de valorización, asegurando que los destinados a operaciones de eliminación reciban un tratamiento adecuado, y contribuir a un desarrollo sostenible de esta actividad. A tales efectos es preceptiva la redacción de un Plan de Gestión de Residuos Construcción-Demolición (RCD).



</doc>
<doc id="6618" url="https://es.wikipedia.org/wiki?curid=6618" title="Telefonía móvil">
Telefonía móvil

La telefonía móvil o telefonía celular es un medio de comunicación inalámbrico a través de ondas electromagnéticas.
Como cliente de este tipo de redes, se utiliza un dispositivo denominado «teléfono móvil», «teléfono celular» o «móvil». En la mayor parte de Hispanoamérica se prefiere la denominación «teléfono celular» o simplemente «celular», aunque en Cuba se dice de ambas formas, y mientras que en España es más común el término «teléfono móvil» o simplemente «móvil». Hoy día los teléfonos táctiles o de última generación, son denominados "smartphones" o teléfonos inteligentes/autómatas, en inglés.

Los primeros antecedentes de la telefonía móvil o celular, se remontan ya a mediados del siglo XX. Sin embargo fueron comercialmente disponibles de forma generalizada a mediados de la década de 1980, y popularizándose globalmente a finales de la década de 1990 y principios de los 2000.

El primer antecedente técnico de la telefonía móvil fueron los servicios de comunicación públicos de radiotelefonía establecidos en algunas ciudades estadounidenses durante los años 1940. Así, AT&T estableció un servicio de ese tipo en la ciudad de San Luis (Misuri) en 1946, que usaba un único transmisor y ofrecía seis canales de transmisión. La popularidad del servicio hizo que rápidamente quedara bloqueado, pero en 1947 AT&T dio con la solución: en lugar de utilizar un único transmisor, creó una red de transmisores de baja potencia, cada uno para un área concreta o "célula" (de ahí derivó el término "teléfono celular" que en muchos países es la forma de referirse a la telefonía móvil). Sin embargo, la noción de telefonía móvil había sido ya anticipada mucho tiempo antes, así William Edward Ayrton (1847-1908), catedrático de física aplicada e ingeniería eléctrica en una conferencia en el Brittish Imperial Institute en 1897 dijo:

A finales de la década de 1950, el científico soviético Leonid Ivanovich Kupriyanovich desarrolló un sistema de comunicación móvil que culminó en el modelo KL-1, que utiliza ondas de radio y es capaz de alcanzar una distancia de 30 km y puede dar servicio a varios clientes. Este teléfono móvil se patentó el 11 de enero de 1957 con el Certificado de Patente n.º 115494. Fue la base para la investigación que Kupriyanovich comenzó el año siguiente en el Instituto de Investigación Científica de Voronezh. De esta investigación surgió el Altai, que se distribuyó comercialmente en 1963, llegó a estar presente en más de 114 ciudades de la Unión Soviética y dio servicio a hospitales y médicos. Con un Altai los usuarios se podían comunicar a otro Altai, a teléfonos fijos y a cabinas de teléfono convencionales. El sistema se extendió por otros países de Europa del Este, como Bulgaria, que lo mostraría en la Exposición Internacional Inforga.

Martin Cooper fue el pionero en esta tecnología. A él se le considera «el padre de la telefonía celular», al introducir el primer radio-teléfono en 1973, en Estados Unidos, mientras trabajaba para Motorola. La primera red comercial automática fue la de NTT de Japón en 1979, seguida por la NMT, que funcionaba en simultáneo en Suecia, Dinamarca, Noruega y Finlandia en 1981 usando teléfonos de Ericsson y Mobira (el ancestro de Nokia). 

En Estados Unidos las primeras redes de prueba de teléfonos celulares aparecieron en Chicago en 1978 (aunque comercialmente recién en 1983), donde 10 "células" comunicaban a 2000 usuarios (red analógica AMPS o 1G). El primer antecedente respecto al teléfono móvil en Estados Unidos es de la compañía Motorola, con su modelo DynaTAC 8000X, lanzado por la compañía Ameritech en 1983. El modelo pesaba poco menos de un kilo y tenía un valor de casi 4000 dólares estadounidenses. Krolopp se incorporaría posteriormente al equipo de investigación y desarrollo de Motorola liderado por Martin Cooper. Tanto Cooper como Krolopp aparecen como propietarios de la patente original. 

Con ese punto de partida, en varios países se diseminó la telefonía celular como una alternativa a la telefonía convencional inalámbrica y el innovador de un nuevo medio de comunicación. La tecnología tuvo gran aceptación, por lo que a los pocos años de implantarse se empezó a saturar el servicio. En ese sentido, hubo la necesidad de desarrollar e implantar otras formas de acceso múltiple al canal y transformar los sistemas analógicos a digitales, con el objeto de darle cabida a más usuarios. Para separar una etapa de la otra, la telefonía celular se ha caracterizado por contar con diferentes generaciones. En la actualidad tienen gran importancia los teléfonos móviles táctiles.

A partir del DynaTAC 8000X, Motorola desarrollaría nuevos modelos como el Motorola MicroTAC, lanzado en 1989, y el Motorola StarTAC, lanzado en 1996 al mercado, este último siendo uno de los primeros celulares más populares del mundo. 

Básicamente podemos distinguir dos tipos de redes de telefonía móvil: La primera es la Red de Telefonía móvil analógica (TMA/1G) -vigente durante la década de 1980 y parte de 1990-, la misma establecía la comunicación mediante señales vocales analógicas, tanto en el tramo radioeléctrico como en el tramo terrestre; la primera versión permitía solo llamadas de voz, y de la misma funcionó en la banda radioeléctrica de los 450 MHz, luego trabajaría en la banda de los 900 MHz; En países como España, esta red fue retirada en 2003. Luego tenemos la red de telefonía móvil digital (2G), -como GSM y D-AMPS, vigentes desde la década de 1990-: aquí ya la comunicación se lleva a cabo mediante señales digitales, lo que nos permite optimizar tanto el aprovechamiento de las bandas de radiofrecuencia como la calidad de la transmisión de las señales, permitiendo realizar otras funciones aparte de llamar. El exponente más significativo que esta red posee actualmente el GSM y su tercera generación UMTS (ambos funcionan en las bandas de 850/900 MHz) que en el 2004, llegó a alcanzar los 100 millones de usuarios.
A principios de los años 2000, los teléfonos móviles/celulares fueron adquiriendo distintas funcionalidades que iban mucho más allá de limitarse a solo llamar o enviar mensajes de texto (SMS); se puede decir que han incorporado las funciones de otros dispositivos tales como cámara de fotos, cámara de video, videojuegos, agenda electrónica, reloj despertador, calculadora, radio portátil, GPS, aplicaciones y reproductores multimedia. Estas funciones incursaron tanto en la población al punto de causar la obsolescencia de los aparatos destinados exclusivamente a ellos, siendo que con un teléfono celular tenías "todo en uno". Con la inclusión de la tecnología 3G en el transcurso de esta década, se popularizó la navegación por internet en los teléfonos celulares (anteriormente relegada solo a computadoras).

Los teléfonos con pantalla táctil empezaron a popularizarse durante la década del 2010. A este tipo de evolución del teléfono móvil se le conoce como teléfono inteligente (o smartphones, en inglés). Actualmente estos teléfonos funcionan en su mayoría en redes LTE (4G) con una tarjeta SIM especial para ello, y permiten una experiencia de navegación por internet como nunca antes se ha tenido en el mundo celular. Las tiendas de aplicaciones, permiten descargar lo que uno desee para personalizar el teléfono; y las aplicaciones de mensajes instantáneos como: Facebook o WhatsApp, se popularizaron tanto que dejaron prácticamente obsoleto el uso del SMS.

En julio de 2020, Samsung dio a conocer su estrategia para el 6G a través de un libro blanco. Estiman que se comercialice a partir de 2028 y se generalice en 2030. Contando con una velocidad máxima de datos de 1.000 gigabits por segundo, una latencia de al menos 100 microsegundos y 50 veces la velocidad de datos máxima del 5G.

La comunicación telefónica es posible gracias a la interconexión entre centrales móviles y públicas. Según las bandas o frecuencias en las que opera el móvil, podrá funcionar en una parte u otra del mundo. La telefonía móvil consiste en la combinación de una red de estaciones transmisoras o receptoras de radio (repetidores, estaciones base o BTS) y una serie de centrales telefónicas de conmutación de y 5.º nivel (MSC y BSC respectivamente), que posibilita la comunicación entre terminales telefónicos portátiles (teléfonos móviles) o entre terminales portátiles y teléfonos de la red fija tradicional.

La red de telefonía móvil, debemos entenderla en varios «segmentos».

En su operación, el teléfono móvil establece comunicación con una estación base y, a medida que se traslada, los sistemas computacionales que administran la red van transmitiendo la llamada a la siguiente estación base de forma transparente para el usuario. Por eso se dice que las estaciones base forman una red de celdas, sirviendo cada estación base a los equipos móviles que se encuentran en su celda.

La evolución del teléfono móvil ha permitido disminuir su tamaño y peso, desde el Motorola DynaTAC, el primer teléfono móvil en 1983 que pesaba 800 gramos, a los actuales más compactos y con mayores prestaciones de servicio. El desarrollo de baterías más pequeñas y de mayor duración, pantallas más nítidas y de colores, la incorporación de software más amigable, hacen del teléfono móvil un elemento muy apreciado en la vida moderna.

El avance de la tecnología ha hecho que estos aparatos incorporen funciones que no hace mucho parecían futuristas, como juegos, reproducción de música MP3 y otros formatos, correo electrónico, SMS, agenda electrónica PDA, fotografía digital y video digital, videollamada, navegación por Internet, GPS, y hasta Televisión digital. Las compañías de telefonía móvil ya están pensando nuevas aplicaciones para este pequeño aparato que nos acompaña a todas partes. Algunas de esas ideas son: medio de pago, localizador e identificador de personas.

Con la aparición de la telefonía móvil digital, fue posible acceder a páginas de Internet especialmente diseñadas para móviles, conocido como tecnología WAP. Desde ese momento hasta la actualidad, se creó el protocolo para el envío de configuración automática del móvil para poder acceder a Internet denominado OMA Client Provisioning.

Las primeras conexiones se efectuaban mediante una llamada telefónica a un número del operador a través de la cual se transmitían los datos, de manera similar a como lo haría un módem de línea fija para PC.

Posteriormente, nació el GPRS (o 2G), que permitió acceder a Internet a través del Protocolo de Internet. La velocidad del GPRS es de 54 kbit/s en condiciones óptimas, tarificándose en función de la cantidad de información transmitida y recibida.

Otras tecnologías más recientes permiten el acceso a Internet con banda ancha, como son EDGE, EV-DO, HSPA y 4G.

Por otro lado, cada vez es mayor la oferta de tabletas (tipo iPad, Samsung Galaxy Tab, libro electrónico o similar) por los operadores para conectarse a internet y realizar llamadas GSM (tabletas 3G).

Aprovechando la tecnología UMTS, han aparecido módems que conectan a Internet utilizando la red de telefonía móvil, consiguiendo velocidades similares a las de la ADSL o WiMAX. Dichos módems pueden conectarse a bases Wi-Fi 3G (también denominadas gateways 3G) para proporcionar acceso a internet a una red inalámbrica doméstica. En cuanto a la tarificación, aún es cara ya que no es una verdadera tarifa plana, debido a que algunas operadoras establecen limitaciones en cuanto a la cantidad de datos. Por otro lado, han comenzado a aparecer tarjetas prepago con bonos de conexión a Internet.

En 2011, el 20 % de los usuarios de banda ancha tiene intención de cambiar su conexión fija por una conexión de Internet móvil.

Según datos del tercer cuatrimestre de 2013, los resultados fueron los siguientes:

Por sistema operativo:

La denominada contaminación electromagnética, también conocida como electropolución, es la supuesta contaminación producida por las radiaciones del espectro electromagnético generadas por equipos electrónicos u otros elementos producto de la actividad humana.

Numerosos organismos como la Organización Mundial de la Salud, la Comisión Europea, la Universidad Complutense de Madrid, la Asociación Española Contra el Cáncer, el Ministerio de Sanidad y Consumo de España, o el Consejo Superior de Investigaciones Científicas de España han emitido informes que descartan daños a la salud debido a las emisiones de radiación electromagnética, incluyendo las de los teléfonos móviles.

No obstante existen estudios que indican lo contrario como el publicado en 2003 por el TNO (Instituto Holandés de Investigación Tecnológica), que afirmaba que las radiaciones de la tecnología UMTS podrían ser peligrosas, (aunque otra investigación de la Universidad de Zúrich, que utilizó hasta 10 veces la intensidad utilizada por el estudio del TNO, arrojó resultados contrarios). También hay numerosos estudios que investigan la posible asociación entre la presencia de antenas de telefonía celular y diversas enfermedades.

Las normativas en vigor en los diversos países consideran seguro vivir en un edificio con una antena de telefonía y en los que lo rodean, dependiendo del nivel de emisiones de la misma. 
No se ha podido demostrar con certeza que la exposición por debajo de los niveles de radiación considerados seguros suponga un riesgo para la salud, pero tampoco se dispone de datos que permitan asegurar que no existen efectos a largo plazo. El "Informe Steward" encargado por el Gobierno del Reino Unido aconseja que los niños no usen el teléfono móvil más que en casos de emergencia. Existen organizaciones que, aludiendo a estos posibles riesgos, reclaman que se observe el principio de precaución y se mantengan las emisiones al mínimo.

La mayoría de los mensajes que se intercambian por este medio, no se basan en la voz, sino en la escritura. En lugar de hablar al micrófono, cada vez más usuarios —sobre todo jóvenes— recurren al teclado para enviarse mensajes de texto. Sin embargo, dado que hay que introducir los caracteres en el terminal, ha surgido un lenguaje en el que se abrevian las palabras valiéndose de letras, símbolos y números. A pesar de que redactar y teclear es considerablemente más incómodo que conversar, dado su reducido coste, se ha convertido en una seria alternativa a los mensajes de voz.

El lenguaje SMS, consiste en acortar palabras, sustituir algunas de ellas por simple simbología o evitar ciertas preposiciones, utilizar los fonemas y demás. La principal causa es que el SMS individual se limita a 160 caracteres, si se sobrepasa ese límite, el mensaje individual pasa a ser múltiple, lógicamente multiplicándose el coste del envío. Por esa razón se procura reducir el número de caracteres, para que de un modo sencillo de entender, entre más texto o bien cueste menos.

Según un estudio británico, entre los usuarios de 18 a 24 años un 42 % los utilizan para coquetear; un 20 %, para concertar citas románticas, y un 13 %, para romper una relación.

A algunos analistas sociales les preocupa que estos mensajes, con su jerga ortográfica y sintáctica, lleven a que la juventud no sepa escribir bien. Sin embargo, otros opinan que “favorecen el renacer de la comunicación escrita en una nueva generación”. La portavoz de una editorial que publica un diccionario australiano hizo este comentario al rotativo "The Sun-Herald": «No surge a menudo la oportunidad de forjar un nuevo estilo [de escritura] […]; los mensajes de texto, unidos a Internet, logran que los jóvenes escriban mucho más. Necesitan tener un dominio de la expresión que les permita captar el estilo y defenderse bien con el vocabulario y el registro […] correspondientes a este género».

Algunas personas prefieren enviar mensajes de texto (SMS) en vez de hablar directamente, sobre todo por cuestiones económicas, dado que el coste de SMS es muy accesible frente al establecimiento de llamada y la duración de la llamada.

Hay restricciones en los sectores ortodoxos de la religión judía que, debido a algunas interpretaciones, los teléfonos móviles estándar no cumplen. Para resolver este problema, algunas organizaciones rabínicas han recomendado que los niños judíos no utilicen las funciones de mensajes de texto de los celulares. Estos se conocen con el nombre de teléfonos kosher, y los rabinos que practican el judaísmo ortodoxo autorizaron que los practicantes del judaísmo los utilizaran en Israel y en otros lugares. Aunque se pretende que estos teléfonos sirvan para fomentar la , algunos vendedores de los aparatos dicen haber tenido buenas ventas con adultos que prefieren la simplicidad de los dispositivos. Incluso se ha autorizado el uso de algunos teléfonos durante el sabbat, sobre todo entre trabajadores de la salud, de la seguridad y de otros servicios públicos, a pesar de que en esa fecha suele prohibirse el uso de cualquier dispositivo eléctrico.



</doc>
<doc id="6621" url="https://es.wikipedia.org/wiki?curid=6621" title="29 de julio">
29 de julio

El 29 de julio es el 210.º (ducentésimo décimo) día del año en el calendario gregoriano y el 211.º en los años bisiestos. Quedan 155 días para finalizar el año.











</doc>
<doc id="6626" url="https://es.wikipedia.org/wiki?curid=6626" title="Piel">
Piel

La piel (del latín "pellis") o cutis (del latín "cutis") o sistema tegumentario, es la cubierta externa de los animales vertebrados y uno de sus órganos más importantes.Las cubiertas de otros animales, como el exoesqueleto de los insectos, tiene otra estructura, composición química y desarrollo embrionario. Mientras otros animales poseen una epidermis similar, la dermis, la capa de tejido conjuntivo debajo, es característica de los cordados. 

Actúa como barrera protectora que aísla al organismo del medio que lo rodea, protegiéndolo y contribuyendo a mantener íntegras sus estructuras, funciona también como sistema de comunicación con el entorno y es uno de los principales órganos sensoriales, contiene terminaciones nerviosas que actúan como receptores de tacto, presión, dolor y temperatura. Está formado por la piel propiamente dicha y las faneras o anexos cutáneos: pelos, uñas, glándulas sebáceas y sudoríparas. Las enfermedades de la piel son estudiadas por la dermatología. 

En el ser humano adulto ocupa una extensión de 2 m² y pesa 4.1 kg. Tiene un espesor que oscila entre 0,5 mm en los párpados y 4 mm en el talón. Se divide en dos capas principales que, de superficie a profundidad, se llaman epidermis y dermis. Por debajo de la dermis se encuentra la hipodermis, también llamada tejido subcutáneo, la mayoría de los textos consideran que la hipodermis no forma parte de la piel.

De la piel dependen varias estructuras llamadas anexos cutáneos: pelos, uñas, glándulas sebáceas y sudoríparas..

La piel consta de dos capas, la epidermis y la dermis. En la epidermis hay 4 estratos: córneo, lúcido, granuloso y germinal. En la dermis se encuentran los folículos pilosos, las glándulas sudoríparas, las fibras nerviosas y el tejido conectivo. 

La estructura histológica básica de la piel es igual en todos los vertebrados, incluyendo la piel humana. 
De manera general, la piel está formada por tres elementos; de afuera hacia adentro:
Cada una de las capas tiene funciones y componentes diferentes, la epidermis deriva embriologicamente del ectodermo y la dermis y la hipodermis del mesodermo. Dentro de la dermis suelen encontrarse los anexos tegumentarios o faneras, incluso aquellos de origen epidérmico, como el pelo.

La dermis se compone de dos estratos, superficial y compacto, que presentan las mismas características y son homólogos en todos los grupos, pero reciben distintos nombres de acuerdo al grupo de vertebrados:
La piel tiene distintas funciones, más o menos marcadas según la especie de que se trate

Las faneras son estructuras anexas a la piel, cada una con una función determinada. Escamas, plumas, pelos tienen una función básica de recubrimiento para servir de protección o mantener la temperatura, aunque estas funciones se pueden ampliar y modificar (ejemplo: las plumas se utilizan en el vuelo de las aves). Otras faneras como cuernos, garras, etc. están al servicio de la depredación, o a la defensa. Finalmente, hay toda una serie de glándulas exocrinas que secretan sustancias para mantener la impermeabilización, la temperatura, grado de humedad, etc. Pero también venenosas para defenderse de los depredadores, o sustancias nutritivas como las glándulas mamarias exclusivas de mamíferos.

En el ser humano el grosor de la piel es variable y oscila entre los 0.5 mm en los párpados y 4 mm en los talones. En la mayor parte del cuerpo el espesor de la piel está comprendido entre 1 y 2 mm. Puede distinguirse una piel fina que cuenta con pelo y glándulas sebáceas, se encuentra distribuida en la mayor parte de la superficie del cuerpo, y una piel gruesa sin pelo ni glándulas sebáceas que está presente sobre todo en las palmas de las manos y plantas de los pies. La piel del varón tiene más vello, es más gruesa y produce mayor secreción sebácea que la de la mujer, debido a los andrógenos (hormona sexual masculina).

Las células principales que forman la epidermis se llaman queratinocitos. Contiene también melanocitos que dan la pigmentación a la piel y células de Langerhans y linfocitos, que se encargan de dar protección inmunológica. La epidermis crece constantemente pero mantiene siempre el mismo espesor debido a un proceso de descamación. Las células situadas en el estrato germinativo se dividen frecuentemente y forman células hijas que emigran progresivamente desde el estrato germinitivo situado en profundidad hasta la superficie, cuando alcanzan el estrato disyuntivo acaban por desprenderse. El proceso completo dura alrededor de cuatro semanas. Si el ciclo de descamación tiene una duración menor de 2 semanas o mayor de 4 se considera patológico. La tinción empleada habitualmente para la observación de la piel mediante microscopia óptica es la de hematoxilina y eosina.







La dermis se encuentra debajo de la epidermis, tienen la peculiaridad de presentar gran abundancia de fibras de colágeno y elásticas que se disponen de forma paralela y que le dan a la piel la consistencia y elasticidad característica del órgano. Histológicamente se divide en 2 capas:


La dermis es más gruesa que la epidermis. En ella se encuentran los anexos cutáneos, que son de dos tipos: córneos (pelos y uñas) y glandulares (glándulas sebáceas y sudoríparas). Cuenta también con vasos sanguíneos y terminaciones nerviosas. Las estructuras de la dermis son las siguientes:


En ocasiones se denomina también fascia superficial. Se encuentra situada debajo de la dermis. Está formada por tejido conjuntivo laxo que dispone de fibras para unirse tanto a la dermis como a los tejidos subyacentes. Contiene adipocitos que sirven como reserva de grasa y dispone de numerosos vasos sanguíneos que aportan sangre a las capas más superficiales de la piel. Algunas de las estructuras que se encuentran en la hipodermis son las siguientes:
La superficie de la piel no es lisa, presenta surcos, hendiduras y líneas que forman dibujos variables según el sector y el individuo. Por ejemplo las impresiones de los extremos de los dedos que son características de cada persona. 

La piel realiza diferentes funciones básicas que pueden agruparse en cinco:

La dermatología es la disciplina médica que estudia y trata el sistema integumentario. Debido a que la piel es el órgano más visible, su apariencia o síntomas proporciona importantes indicios, no solo acerca de sus enfermedades, sino también de las de otros órganos, como el hígado. Así mismo, la piel es el órgano más vulnerable, expuesto a radiaciones, traumatismos, infecciones y productos químicos nocivos.

A lo largo del proceso de envejecimiento la estructura de la piel se afecta de forma significativa. Entre los 30 y los 80 años la epidermis se adelgaza progresivamente y se reduce la superficie de contacto entre la epidermis y la dermis, lo que ocasiona un aumento de la fragilidad y la aparición de ampollas ante traumatismos leves. Los melanocitos que son las células que pigmentan la piel disminuyen a partir de los 30 años, entre 8 y 20% por década, y tienden a formar pequeñas acumulaciones que provocan la aparición de manchas pigmentadas que se conocen con el nombre de léntigo. Por otra parte las células de la piel senil tienen mayor tendencia a sufrir mutaciones que provocan cáncer de piel. Todos estos procesos se aceleran en aquellas personas que han sufrido intensa exposición solar a lo largo de su vida, por efecto de la radiación ultravioleta. Otro factor importante que contribuye al deterioro prematuro de la piel es el consumo de tabaco que provoca aumento de las arrugas faciales y apariencia de envejecimiento facial, estos efectos se deben a que causa un engrosamiento y fragmentación de las fibras elásticas y reducción de la hidratación del estrato córneo, aumentando la atrofia y desecación de la piel.

La piel puede sufrir diferentes enfermedades. Algunas de las más usuales son las siguientes:

Presenta la misma estructura que los vertebrados pero muy simplificada. La epidermis es una capa simple de células. La dermis es delgada y carece de pigmentos.

La epidermis es un poco más compleja que la de los cefalocordados, pero no posee estrato córneo. Dentro de la dermis poseen pigmento y unos tabiques a intervalos regulares llamados miocommata.

 La epidermis es muy sencilla, con una capa superficial de queratina. La epidermis de los peces dispone de glándulas que secretan una sustancia llamada mucus que le proporciona protección, lubrica la superficie y disminuye la resistencia al roce con el agua.La dermis es más compleja y está dividida en los dos estratos de tejido conjuntivo fibroso y laxo. En la dermis se originan las escamas y se encuentran los cromatóforos, por ejemplo con melanina, que dan el color a la piel.
La piel se compone de dos capas: epidermis revestida por una cutícula y dermis en la que se originan las escamas que en realidad son placas flexibles calcificadas e imbricadas. 

La piel de los anfibios es muy fina lo que hace posible la respiración cutánea. Carece de pelo pero posee glándulas mucosas productoras de mucus que la mantiene húmeda continuamente. Algunas especies disponen de glándulas que secretan sustancias venenosas que las protegen de depredadores. 
La piel de los reptiles no posee glándulas para humedecerla, lo que le da un aspecto seco y duro, presenta una capa córnea que contiene escamas córneas que la hace impermeable al agua y resistente a la desecación. En muchas especies se produce el fenómeno de la muda que es el proceso de cambio de la capa más externa de la piel, necesaria para permitir el crecimiento del animal, se produce con una periodicidad variable entre 1 y 12 meses. Cocodrilos y quelonios presentan placas osificadas en la dermis que reciben el nombre de osteodermos, y tienen una función protectora. Presenta dos capas dermis y epidermis, pero está última está cubierta por una tercera capa casi traslúcida y ornamentada que recibe el nombre de epidermícula. 

Tienen la piel cubierta por plumas de diferentes tipos. Las plumas son desde el punto de vista estructural protuberancias corneas que surgen de la epidermis. Poseen glándula uropígea que está situada en la base de la cola y produce una secreción grasa que el mismo animal distribuye con el pico por el plumaje para impermeabilizarlo. Esta glándula se desarrolla especialmente en las aves acuáticas. Algunas aves marinas también poseen glándulas especializadas de la sal.

Lo más característico de la piel de los mamíferos es el pelo y las glándulas mamarias. También presentan faneras especializadas como los cuernos y las astas.




</doc>
<doc id="6634" url="https://es.wikipedia.org/wiki?curid=6634" title="Franz Schubert">
Franz Schubert

Franz Peter Schubert (Viena, 31 de enero de 1797-"ibidem", 19 de noviembre de 1828) fue un compositor austriaco de los principios del Romanticismo musical pero, a la vez, también continuador de la sonata clásica siguiendo el modelo de Ludwig van Beethoven. A pesar de su corta vida, Schubert dejó un gran legado, que incluye más de 600 obras vocales seculares (principalmente lieder), siete sinfonías completas, música sacra, óperas, música incidental y gran cantidad de obras para piano y música de cámara. Sus obras principales incluyen el "Quinteto La trucha", la "Sinfonía inacabada", la "Sinfonía Grande", las tres últimas sonatas para piano (D. 958, 959 y 960), la ópera "Fierrabras" (D. 796), la música incidental de la obra de teatro "Rosamunda" (D. 797) y los ciclos de canciones "La bella molinera" (D. 795) y "Viaje de invierno" (D. 911).

Nació en el suburbio Himmelpfortgrund de Viena y sus dotes poco comunes para la música fueron evidentes desde una edad temprana. Su padre le dio sus primeras lecciones de violín y su hermano mayor le dio lecciones de piano, pero Schubert pronto superó sus habilidades. En 1808, a la edad de once años, se convirtió en alumno en la escuela "Stadtkonvikt", donde conoció la música orquestal de Joseph Haydn, Wolfgang Amadeus Mozart y Ludwig van Beethoven. Abandonó el "Stadtkonvikt" a finales de 1813 y regresó a su hogar para vivir con su padre, donde comenzó a estudiar para convertirse en maestro de escuela. A pesar de esto, continuó sus estudios de composición con Antonio Salieri. En 1821, la "Gesellschaft der Musikfreunde" lo admitió como miembro intérprete, lo que ayudó a que su nombre fuera conocido entre la ciudadanía vienesa. Dio un concierto de sus propias obras con gran éxito de crítica en marzo de 1828, la única vez que lo hizo en su carrera. Murió ocho meses después a la edad de 31 años y la causa oficialmente atribuida fue la fiebre tifoidea, aunque algunos historiadores creen que fue por sífilis.

La valoración de la música de Schubert mientras estaba vivo se limitó a un círculo relativamente pequeño de admiradores en Viena, pero el interés en su obra aumentó significativamente en las décadas posteriores a su muerte. Felix Mendelssohn, Robert Schumann, Franz Liszt, Johannes Brahms y otros compositores del descubrieron y defendieron sus obras. Hoy, Schubert está clasificado entre los mejores compositores de música clásica occidental y su música sigue siendo popular.

Franz Peter Schubert nació en Himmelpfortgrund (ahora parte de Alsergrund, Viena) el 31 de enero de 1797 y lo bautizaron al Catolicismo al día siguiente. Fue el duodécimo hijo de Franz Theodor Florian Schubert (1763-1830) y Maria Elisabeth Katharina Vietz (1756-1812). Los antepasados ​​inmediatos de Schubert provenían de la provincia de Zlaté Hory en la Silesia austríaca. Su padre, hijo de un campesino de Moravia, era un conocido maestro de escuela parroquial y a su escuela en Lichtental (en el noveno distrito de Viena) asistieron numerosos estudiantes. Llegó a Viena desde Zlaté Hory en 1784 y fue nombrado maestro de escuela dos años después. La madre de Schubert era hija de un maestro cerrajero silesio y había sido criada de una familia vienesa antes del matrimonio. De los catorce hijos de Franz Theodor y Elisabeth (uno de ellos ilegítimo, nacido en 1783), nueve murieron en la infancia.

A la edad de cinco años, Schubert comenzó a recibir clases regulares de su padre y un año después se matriculó en la escuela de su padre. Aunque no se sabe exactamente cuándo recibió su primera instrucción musical, su hermano Ignaz le dio lecciones de piano, pero duraron muy poco tiempo, ya que este lo sobrepasó en unos pocos meses. Ignaz después recordó:

Su padre le dio sus primeras lecciones de violín cuando tenía ocho años, entrenándolo hasta el punto en que podía tocar duetos fáciles de manera competente. Poco después, recibió sus primeras lecciones fuera de la familia a cargo de Michael Holzer, organista y director de coro de la iglesia parroquial local en Lichtental. Holzer a menudo le aseguraba al padre de Schubert, con lágrimas en los ojos, que nunca había tenido un alumno como él y las lecciones pudieron haber consistido en gran medida en conversaciones y expresiones de admiración. Holzer dio al joven clases de piano y órgano, así como en bajo cifrado. Según Holzer, sin embargo, no le dio ninguna instrucción real ya que Schubert ya sabía todo lo que intentó enseñarle. Más bien, lo miraba con «asombro y silencio». Parecía que el chico aprendía más conocimientos con un aprendiz de carpintero que lo llevaba a un almacén de pianofortes cercano donde practicaba con mejores instrumentos. También tocaba la viola en el cuarteto de cuerda familiar, con sus hermanos Ferdinand e Ignaz en el primer y segundo violín y su padre en el violonchelo. Escribió sus primeros cuartetos de cuerda para este conjunto.

En 1804, el joven Schubert llamó la atención de Antonio Salieri, entonces la principal autoridad musical de Viena, cuando se reconoció su talento vocal. En noviembre de 1808, se convirtió en alumno en el "Stadtkonvikt" (Seminario Imperial) a través de una beca para el coro. En el "Stadtkonvikt", conoció a las oberturas y , las y su hermano menor Michael Haydn y las oberturas y , un compositor por el que desarrolló una admiración significativa. Su contacto con estas y otras obras, combinado con visitas ocasionales a la ópera, sentaron las bases para una educación musical más amplia. Una importante influencia musical provino de las canciones de Johann Rudolf Zumsteeg, un importante compositor de lieder. El joven estudiante precoz «quería modernizar» las canciones de Zumsteeg, según lo informado por Joseph von Spaun, amigo de Schubert. Su amistad con Spaun comenzó en el "Stadtkonvikt" y duró toda su corta vida. En aquellos primeros días, el económicamente acomodado Spaun proporcionó al empobrecido Schubert gran parte de su papel para manuscritos.

Mientras tanto, su genio comenzó a mostrarse en sus composiciones. Salieri decidió comenzar a darle clases en privado de teoría musical e incluso en composición. Según Ferdinand, la primera composición para piano del niño fue una "Fantasía para cuatro manos". Su primera canción, "Klagegesang der Hagar", la escribió un año después. Se le permitió ocasionalmente dirigir la orquesta del "Stadtkonvikt" y fue la primera orquesta para la que escribió. Dedicó gran parte del resto de su tiempo en el "Stadtkonvikt" a componer música de cámara, varias canciones, piezas para piano y, más ambiciosamente, obras corales litúrgicas en forma de «Salve Regina» (D. 27), un «Kyrie» (D. 31), además del inacabado «Octeto para viento» (D. 72, que se dice que conmemora la muerte de su madre en 1812), la cantata "Wer ist groß?" para voces masculinas y orquesta (D. 110, para el cumpleaños de su padre en 1813) y su "Primera Sinfonía" (D. 82).

A finales de 1813, Schubert abandonó el "Stadtkonvikt" y regresó a su casa para recibir capacitación docente en la St. Anna Normal- hauptschule. En 1814, ingresó en la escuela de su padre como maestro de los alumnos más jóvenes. Durante más de dos años, el joven sufrió una gran monotonía. Sin embargo, había intereses que lo compensaban. Continuó recibiendo lecciones privadas de composición de Salieri, quien le dio más capacitación técnica real que cualquiera de sus otros maestros, antes de separarse en 1817.

En 1814, conoció a una joven soprano llamada Therese Grob, hija de un fabricante local de seda, y escribió varias de sus obras litúrgicas (incluyendo una «Salve Regina» y un «Tantum ergo») para ella. También ella fue soprano solista en el estreno de su "Misa n.º 1" (D. 105) en septiembre de 1814. Schubert quería casarse con ella, pero se vio obstaculizado por la severa ley de consentimiento matrimonial de 1815, que exigía que un aspirante a novio demostrara que tenía los medios para mantener a una familia. En noviembre de 1816, después de no poder obtener un puesto musical en Laibach (ahora Liubliana, Eslovenia), Schubert envió al hermano de Therese, Heinrich, una colección de canciones, retenidas por la familia hasta el .

Uno de sus años más prolíficos fue 1815. Compuso más de 20000 compases de música, más de la mitad de los cuales fueron para orquesta, incluyendo nueve obras sacras (a pesar de ser agnóstico), una sinfonía y alrededor de 140 lieder. En ese año, también le presentaron a Anselm Hüttenbrenner y Franz von Schober, quienes se convirtieron en sus amigos durante el resto de su vida. Spaun también le presentó a otro amigo, Johann Mayrhofer. En estos años compuso el lied "Erlkönig", inspirado en un poema de Johann Wolfgang von Goethe.

A lo largo de 1815, vivió con su padre en casa. Su madre murió en 1812. Continuó enseñando en la escuela y dando lecciones musicales privadas, ganando suficiente dinero para sus necesidades básicas, incluyendo ropa, papel para manuscritos, plumas y tinta, pero con poco o ningún dinero sobrante para lujos. Spaun sabía muy bien que Schubert estaba descontento con su vida en la escuela y estaba preocupado por su desarrollo intelectual y musical. En mayo de 1816, Spaun se mudó de su departamento en Landskrongasse (en el centro de la ciudad) a un nuevo hogar en el suburbio de Landstraße. Una de las primeras cosas que hizo después de instalarse en el nuevo hogar fue invitarlo a pasar unos días con él. Esta fue probablemente la primera visita de Schubert fuera de casa o la escuela. Su infelicidad durante sus años como maestro de escuela posiblemente mostró signos tempranos de depresión y es una certeza virtual de que sufrió de ciclotimia a lo largo de su vida.

El musicólogo Maynard Solomon sugirió que Schubert se sentía eróticamente atraído por los hombres, una tesis que, a veces, ha sido debatida acaloradamente. En cambio, la musicóloga y experta en Schubert Rita Steblin afirmó que «perseguía mujeres». La teoría de la homosexualidad de Schubert ha comenzado a influir en la interpretación de su obra en artículos académicos.

En 1816, ocurrieron cambios significativos. Franz von Schober, un estudiante y de buena familia y algunos medios, invitó a Schubert a una habitación con él en la casa de su madre. La propuesta fue particularmente oportuna, ya que Schubert acababa de presentar la solicitud sin éxito para el puesto de "kapellmeister" en Laibach y también había decidido no reanudar las tareas de enseñanza en la escuela de su padre. A finales de año, se convirtió en huésped de Schober. Durante un tiempo, intentó aumentar los recursos del hogar dando clases de música, pero pronto las abandonó y se dedicó a la composición. «Compongo todas las mañanas y cuando una pieza está terminada, comienzo otra». Durante ese año, se centró en obras orquestales y corales, aunque también continuó escribiendo lieder. Gran parte de este trabajo fue inédito, pero circularon manuscritos y copias entre amigos y admiradores.

A principios de 1817, Schober le presentó a Johann Michael Vogl, un barítono prominente veinte años mayor que Schubert. Vogl, para quien el compositor escribió muchas canciones, se convirtió en uno de sus principales defensores en los círculos musicales vieneses. También conoció a Joseph Hüttenbrenner (hermano de Anselm), quien también jugó un papel en la promoción de su música. Estos, y un creciente círculo de amigos y músicos, se hicieron responsables de promover, recopilar y, después de su muerte, preservar su obra.

A finales de 1817, su padre obtuvo un nuevo puesto en una escuela en Rossau, no lejos de Lichtental. Schubert se reincorporó con su padre y a regañadientes asumió las tareas de enseñanza allí. A principios de 1818, solicitó la afiliación en la prestigiosa "Gesellschaft der Musikfreunde", con la intención de obtener la admisión como acompañante, pero también para que su música, especialmente sus , se pudiera interpretar en los conciertos nocturnos. Lo rechazaron sobre la base de que «no era un aficionado», aunque en ese tiempo había estado empleado como maestro de escuela y ya había músicos profesionales entre los miembros de la sociedad. Sin embargo, comenzó a ganar más atención en la prensa y la primera presentación pública de una obra secular, una obertura realizada en febrero de 1818, recibió elogios de la prensa en Viena y en el extranjero.

Schubert pasó el verano de 1818 como profesor de música para la familia del conde Johann Karl Esterházy en su castillo en Zseliz (ahora Želiezovce, Eslovaquia). La paga era relativamente buena y su función de enseñar piano y canto a las dos hijas eran relativamente leves, lo que le permitía componer felizmente. Pudo haber escrito su "Marcha militar en re mayor" (D. 733 n.º 1) para Marie y Karoline, además de otros duetos de piano. A su regreso de Zseliz, se instaló con su amigo Mayrhofer.

A principios de la década de 1820, Schubert era parte de un círculo muy unido de artistas y estudiantes que tenían reuniones sociales que se conocieron como schubertiadas. Muchas de ellas tuvieron lugar en el gran apartamento de Ignaz von Sonnleithner en Gundelhof (Brandstätte 5, Viena). El cerrado círculo de amigos con el que se rodeó recibió un duro golpe a principios de 1820. Schubert y cuatro de sus amigos fueron arrestados por la policía austríaca, quienes (después de la Revolución francesa y las Guerras napoleónicas) estaban en guardia contra las actividades revolucionarias y de cualquier reunión sospechosa de jóvenes o estudiantes. Uno de sus amigos, Johann Senn, fue juzgado, encarcelado por más de un año y luego se le prohibió permanentemente entrar en Viena. Los otros cuatro, incluido el compositor, fueron «severamente reprendidos», en parte por «burlarse de [los funcionarios] con un lenguaje ofensivo y deshonroso». Si bien Schubert nunca volvió a ver a Senn, puso música a algunos de sus poemas, "Selige Welt" (D. 743) y "Schwanengesang" (D. 744). El incidente pudo haber sido motivo de una pelea con Mayrhofer, con quien vivía en ese momento.

Sus amigos lo apodaron «Schwammerl», lo que Gibbs describe como una traducción a «Panzón» o «Pequeño champiñón», ya que medía poco más de metro y medio de alto. Gibbs también afirma que ocasionalmente podía beber en exceso y señala que las referencias a su consumo excesivo de alcohol «... no solo aparecen en informes posteriores, sino también en documentos durante su vida».

Las composiciones de 1819 y 1820 muestran un marcado avance en el desarrollo y madurez del estilo. En febrero inició el oratorio inacabado "Lazarus" (D. 689). Más tarde, compuso, en medio de una serie de obras más pequeñas, el himno «Der 23. Psalm» (D. 706), el octeto «Gesang der Geister über den Wassern» (D. 714), el "Quartettsatz" en do menor (D. 703 ) y la "Wanderer-Fantasie" en "do" mayor para piano (D. 760). En 1820, se representaron dos de sus óperas: "Los hermanos gemelos" (D. 647) en el Theater am Kärntnertor el 14 de junio y "Die Zauberharfe" (D. 644) en el Theater an der Wien el 21 de agosto. Hasta ese momento, sus composiciones más grandes (aparte de sus misas) se habían restringido a la orquesta amateur en Gundelhof (Brandstätte 5, Viena), una sociedad que surgió de las fiestas cuarteto en su casa. Comenzó a asumir una posición más prominente, dirigiéndose a un público más amplio. Los editores, sin embargo, permanecieron distantes y Anton Diabelli aceptó vacilante imprimir algunas de sus obras por encargo. Los primeros siete números opus (todas las canciones) aparecieron en estos términos. Entonces cesó el encargo y comenzó a recibir escasos ingresos por derechos de autor. La situación mejoró un poco en marzo de 1821 cuando Vogl interpretó la canción "Erlkönig" (D. 328) en un concierto que fue un rotundo éxito. Ese mes, Schubert compuso una variación sobre un vals de Diabelli (D. 718) y fue uno de los cincuenta compositores que contribuyeron a la publicación de "Vaterländischer Künstlerverein".

Schubert centró su atención más firmemente que nunca en la producción de dos óperas, donde, por diversas razones, apenas tuvo éxito. Con todo, se embarcó en veinte proyectos escénicos, cada uno de los cuales rápidamente se olvidaron. En 1822, "Alfonso y Estrella" fue rechazada, en parte debido a su libreto (escrito por su amigo, Franz von Schober). En 1823, "Fierrabras" (D. 796) también fue rechazada: Domenico Barbaia, "impresario" de los teatros de la corte, perdió gran interés en la nueva ópera alemana debido a la popularidad de Gioachino Rossini y el estilo operístico italiano y el fracaso de "Euryanthe" de Carl Maria von Weber. El censor prohibió "Die Verschworenen" (D. 787), aparentemente por su título, y "Rosamunda" (D. 797) se retiró después de dos noches, debido a la pobre calidad de la obra para la que Schubert había escrito música incidental.

A pesar de sus fracasos operísticos, su reputación estaba creciendo constantemente en otros frentes. En 1821, la "Gesellschaft der Musikfreunde" finalmente lo aceptó como miembro y el número de interpretaciones de su música creció notablemente. Estas actuaciones ayudaron a que su reputación creciera rápidamente entre los miembros de la "Gesellschaft" y a que su nombre fuera conocido para el público en general. Algunos de los miembros de la "Gesellschaft", en particular Ignaz von Sonnleithner y su hijo Leopold von Sonnleithner, tenían una influencia considerable en los asuntos de la sociedad y, como resultado de eso se produjo la creciente reputación de Schubert, sus obras se incluyeron en tres conciertos importantes de la "Gesellschaft" en 1821. En abril, se interpretó uno de sus cuartetos de voz masculina y en noviembre tuvo lugar la primera actuación pública de su "Obertura en mi menor" (D. 648). En un concierto diferente del mismo día del estreno de la "Obertura", se interpretó su canción «Der Wanderer» (D. 489).

En 1822, Schubert conoció tanto a Weber como a Beethoven, pero poco resultó en ambos casos: sin embargo, se dice que Beethoven reconoció su don en algunas ocasiones. En su lecho de muerte, se dice que Beethoven examinó algunas de las obras del joven y exclamó: «¡En verdad, la chispa del genio divino reside en este Schubert!». Según los informes, Beethoven también predijo que «causaría una gran sensación en el mundo» y lamentó no haber estado más familiarizado con él antes. Deseaba ver sus óperas y obras para piano, pero su grave enfermedad le impidió hacerlo.

A pesar de su preocupación por las representaciones y más tarde con sus deberes oficiales, Schubert encontró tiempo durante estos años para una cantidad significativa de composición. Completó la "Misa en mi bemol mayor" (D. 678) en 1822 y más tarde ese año se embarcó repentinamente en una obra que de manera más decisiva que casi cualquier otra en esos años mostró su visión personal madura, la "Sinfonía en si menor", conocida como "Sinfonía inacabada" (D. 759). La razón por la que la dejó sin terminar, después de escribir dos movimientos y bocetos en un tercero, continúa siendo discutida y analizada y también es notable que no lo mencionara a ninguno de sus amigos, aunque como Brian Newbould señala, debió sentirse emocionado por lo que estaba logrando. El manuscrito con ambos movimientos completos pasó a manos de su amigo, Anselm Hüttenbrenner, quien los conservó en un cajón durante más de 40 años. En 1865 se los entregó al director de orquesta Johann von Herbeck, quien en diciembre de ese mismo año dirigió en Viena el estreno de la obra incompleta. No hay una conclusión a la cuestión sobre los motivos que condujeron a Schubert a dejarla inconclusa. Una posibilidad sugiere que parte del manuscrito se perdiera. También se ha sugerido que el "Entreacto en si menor de la música de escena para Rosamunda", de 1823, fuera en realidad el último movimiento sinfónico. A favor de esta tesis: las coincidencias en orquestación con ambos movimientos existentes, incluido el añadido de los tres trombones incorporados a la orquesta clásica convencional, así como la tonalidad. A pesar de todo, la explicación más verosímil para la crítica es la que cuestiona la madurez autorial para completar dos movimientos más con la misma altura y calidad expresiva de los previos. Así, la obra queda: un díptico asimétrico, pero equilibrado: primero un "Allegro moderato", en el que se contraponen la tensión dramática inicial y la naturalidad lírica, seguido de un "Andante con moto en mi mayor", pleno de un agitado y tumultuoso vagabundeo, que alcanza al final el descanso en una coda.

En 1823, Schubert escribió su primer ciclo de canciones a gran escala, "La bella molinera" (D. 795), basadas en poemas de Wilhelm Müller. Esta serie, junto con el ciclo posterior "Viaje de invierno" (D. 911, que también emplea textos de Müller en 1827) es considerada ampliamente como una de las cumbres del lied. Ese mismo año también compuso la canción «Du bist die Ruh» (D. 776) y aparecieron por primera vez los síntomas de sífilis. Esto comenzó a trastornar su vida y fue cuando apareció el lado más amargo y melancólico de Schubert. Necesitado de comprensión, escribió en 1824 estas líneas angustiadas a su amigo Leopold Kupelwieser:

En 1824, escribió las variaciones en "mi" menor para flauta y piano "Trockne Blumen", una canción del ciclo "La bella molinera" y varios cuartetos de cuerda. También escribió la "Sonata arpeggione" (D. 821) para arpeggione y piano en el momento en que el instrumento tuvo una cierta popularidad. En la primavera de ese año, escribió el "Octeto en fa mayor" (D. 803), un boceto de una "Gran Sinfonía" y en el verano volvió a Zseliz. Allí se sintió atraído por el lenguaje musical húngaro y escribió el "Divertissement à la hongroise" en "sol" menor para dúo de piano (D. 818) y el "Cuarteto de cuerda en la menor" ("Rosamunde", D. 804). Se ha dicho que tenía una pasión desesperada por su alumna, la condesa Caroline Esterházy, pero la única obra que le dedicó fue su "Fantasía en fa menor" para dúo de piano (D. 940). Su amigo Eduard von Bauernfeld escribió el siguiente verso, que parece hacer referencia a los sentimientos no correspondidos de Schubert:

Los reveses de años anteriores fueron compensados ​​por la prosperidad y la felicidad de 1825. Había publicado más rápido, se alivió el estrés por la pobreza por un tiempo y en el verano tuvo unas agradables vacaciones en la Alta Austria, donde fue recibido con entusiasmo. Fue durante esta gira cuando produjo el ciclo de siete canciones "Fräulein am See", basada en "La dama del lago" de Walter Scott, y que incluía "Ellens dritter Gesang" ("Ave María", D. 839). La letra de la traducción al alemán del poema de Scott realizada por Adam Storck se reemplaza con frecuencia por el texto completo de la oración tradicional católica «Ave María», pero no era la composición original de la melodía de Schubert. El original solo se abre con el saludo «Ave María», que también se repite solo en el estribillo. En 1825, también escribió la "Sonata para piano en la menor" (D. 845, publicada por primera vez como op. 42) y comenzó la "Sinfonía Grande" (D. 944), que completó al año siguiente.

De 1826 a 1828, residió continuamente en Viena, excepto por una breve visita a Graz en 1827. En 1826, dedicó una sinfonía (D. 944, que más tarde se conocería como la «Grande» en "do" mayor) a la "Gesellschaft der Musikfreunde" y recibió a cambio una remuneración. El "Cuarteto de cuerda n.º 14" en re menor (D. 810), con las variaciones de «La muerte y la doncella», los escribió durante el invierno de 1825-1826 y se representaron por primera vez el 25 de enero de 1826. Más tarde en ese mismo año llegó el "Cuarteto de cuerda n.º 15" en "sol" mayor (D. 887, publicado por primera vez como op. 161), el "Rondó en si menor para violín y piano" (D. 895), "Rondeau brillant" y la "Sonata para piano en sol mayor" (D 894, publicada por primera vez como "Fantasía en sol," op. 78). También produjo en 1826 tres canciones basadas en obras de William Shakespeare, de las cuales «Ständchen» (D. 889) y «An Sylvia» (D. 891) fueron supuestamente escritas el mismo día, la primera en una taberna donde puso fin al paseo de la tarde y la segunda a su regreso a su alojamiento en la noche.

Las obras de sus últimos dos años revelan a un compositor entrando en una nueva etapa profesional y compositiva. Aunque partes de su personalidad estaban influidas por sus amigos, él alimentó una dimensión intensamente personal en la soledad y fue fuera de esta dimensión por lo que escribió su mejor música. La muerte de Beethoven lo afectó profundamente y pudo haberlo motivado a alcanzar nuevas cimas artísticas. En 1827, Schubert escribió el ciclo de canciones "Viaje de invierno" (D. 911), la "Fantasía en do mayor para violín y piano" (D. 934, publicada por primera vez como op. Post. 159), el "Impromptu" para piano y los dos tríos de piano (el primero en "si" bemol mayor,D. 898 y el segundo en "mi" bemol mayor, D. 929). En 1828, la cantata "Mirjams Siegesgesang" (D. 942) sobre un texto de Franz Grillparzer, la "Misa en mi bemol mayor" (D. 950), el "Tantum ergo" (D. 962) en la misma tonalidad, el "Quinteto para cuerdas" en do mayor (D. 956), el segundo «Benedictus» de la "Misa en do mayor" (D. 961), las tres "Últimas sonatas" para piano (D. 958, 959 y 960) y la colección de "13 Lieder nach Gedichten von Rellstab und Heine" para voz y piano, también conocida como "Schwanengesang" (D. 957). La "Sinfonía Grande" está fechada en 1828, pero los expertos en Schubert creen que esta sinfonía la escribió en gran parte en 1825-1826 y fue revisada para su posible representación en 1828. La orquesta de la "Gesellschaft", según los informes, leyó la sinfonía en un ensayo, pero nunca programó una actuación pública. Las razones siguen siendo desconocidas, aunque la dificultad de la sinfonía es la posible explicación. En las últimas semanas de su vida, comenzó a esbozar tres movimientos para una nueva "Sinfonía en re mayor" (D. 936A). En esta obra, anticipaba el uso de armónicos de canciones populares y paisajes sonoros desnudos de Gustav Mahler. Schubert expresó el deseo de sobrevivir a su enfermedad final, desarrollar aún más su conocimiento de la armonía y el contrapunto, y de hecho se había citado para recibir lecciones del maestro del contrapunto Simon Sechter.

El 26 de marzo de 1828, aniversario de la muerte de Beethoven, Schubert ofreció, por única vez en su carrera, un concierto público de sus propias obras. El concierto fue un éxito popular y económico, a pesar de que sería eclipsado por las primeras apariciones de Niccolò Paganini en Viena poco después.

En medio de esta actividad creativa, su salud se deterioró. Se decía de Schubert que hacía tiempo ya «andaba por el mal camino», se hablaba de su afición al alcohol y la «sensualidad» —que lo llevó a tener relaciones esporádicas. A finales de la década de 1820, su salud empeoraba y le confió a algunos amigos que temía estar cerca de la muerte. A finales del verano de 1828, vio al médico Ernst Rinna, quien pudo haber confirmado sus sospechas de que estaba enfermo sin remedio y que probablemente moriría pronto. Algunos de sus síntomas coincidían con los del envenenamiento por mercurio. A principios de noviembre, volvió a enfermar, experimentando dolores de cabeza, fiebre, inflamación de las articulaciones y vómitos. Generalmente no podía retener alimentos sólidos y su condición empeoró. Cinco días antes de su muerte, su amigo, el violinista Karl Holz, y su cuarteto de cuerda lo visitaron para tocar para él. La última obra musical que deseó escuchar fue el "Cuarteto de cuerdas n.º 14" de Beethoven. Holz comentó: «El rey de la armonía ha enviado al rey de la canción una invitación amistosa para la travesía».

Franz Schubert falleció en Viena, a los 31 años, el 19 de noviembre de 1828, en el apartamento de su hermano Ferdinand. La causa de su muerte fue diagnosticada oficialmente como fiebre tifoidea, aunque se han propuesto otras teorías, incluida la etapa terciaria de la sífilis. Fue enterrado a petición propia cerca de Beethoven, a quien había admirado toda su vida, en el cementerio de Währing (Viena). Schubert había portado la antorcha en el funeral de Beethoven un año antes de su propia muerte.

En 1872, se erigió un monumento a Franz Schubert en el Stadtpark de Viena. En 1888, las tumbas de Schubert y Beethoven fueron trasladadas al Zentralfriedhof, donde ahora se pueden encontrar junto a las de Johann Strauss (hijo) y Johannes Brahms. Anton Bruckner estuvo presente en ambas exhumaciones y buscó en ambos ataúdes y sostuvo los venerados cráneos en sus manos. El cementerio de Währing se convirtió en un parque en 1925, llamado Schubert Park y su antigua tumba está marcada por un busto. Su epitafio, escrito por su amigo, el poeta Franz Grillparzer, dice: «"Die Tonkunst begrub hier einen reichen Besitz, aber noch viel schönere Hoffnungen"» («El arte de la música ha enterrado aquí un preciado tesoro, sino esperanzas aún más hermosas»).

En julio de 1947, el compositor austríaco Ernst Krenek discutió el estilo de Schubert, admitiendo avergonzado que al principio había «compartido la opinión generalizada de que Schubert era un inventor de melodías agradables con suerte ... carente del poder dramático y la inteligencia de búsqueda que distinguía a los maestros "reales" como Johann Sebastian Bach o Ludwig van Beethoven». Krenek escribió que llegó a una evaluación completamente diferente después de un estudio minucioso de las piezas de Schubert a instancias de su amigo y compañero compositor Eduard Erdmann. Krenek señaló que las sonatas para piano daban «una amplia evidencia de que [Schubert] era mucho más que un simple constructor melódico que no conocía, y no le importaba, el oficio de la composición». Cada sonata impresa, según Krenek, exhibía «una gran riqueza de delicadeza técnica» y revelaba a Schubert como «lejos de estar satisfecho con verter sus encantadoras ideas en moldes convencionales, por el contrario, era un artista pensante con un gran apetito por la experimentación».

Schubert fue notablemente prolífico, ya que compuso más de 1500 obras en su corta carrera. Su estilo compositivo progresó rápidamente a lo largo de su corta vida. La mayor parte de sus composiciones son canciones para voz solista y piano (aproximadamente 630). También compuso un número considerable de obras seculares para dos o más voces, especialmente part songs, coros y cantatas. Completó ocho oberturas orquestales y siete sinfonías completas, además de fragmentos de otras seis. Si bien no compuso conciertos, escribió tres obras concertantes para violín y orquesta. Escribió una gran cantidad de música para piano solo, incluyendo once sonatas incontrovertiblemente completadas y al menos nueve más en diferentes estados de finalización, numerosas obras misceláneas y muchos danzas cortas, además de producir un gran conjunto de obras para piano a cuatro manos. También escribió más de cincuenta obras de cámara, incluidas algunas obras fragmentarias. La producción sacra de Schubert incluye siete misas, un oratorio y un réquiem, entre otros movimientos de misas y numerosas composiciones más pequeñas. Completó solo once de sus veinte obras escénicas.

Ese «apetito por la experimentación» que mencionaba Krenek se manifiesta repetidamente en la producción de Schubert en una amplia variedad de formas y géneros, incluyendo ópera, música sacra, música de cámara y para piano solo y obras sinfónicas. Quizás lo más familiar es que su atrevimiento se refleja en su sentido de modulación notablemente original. Por ejemplo, el segundo movimiento del "Quinteto para cuerdas" (D. 956), que está en "mi" mayor, presenta una sección central en la tonalidad vecina de "fa" menor. También aparece en opciones inusuales de instrumentación, como en la "Sonata en la menor" para arpeggione y piano (D. 821) o la partitura no convencional del "Quinteto La trucha" (D. 667), que es para piano, violín, viola, violonchelo y contrabajo, mientras que los quintetos de piano convencionales tienen partitura para piano y cuarteto de cuerda.

Aunque Schubert estuvo claramente influido por las formas sonata clásica de Beethoven y Mozart, sus estructuras formales y sus desarrollos tienden a dar la impresión más del desarrollo melódico que del drama armónico. Esta combinación de forma clásica y melodía romántica de larga duración a veces les da un estilo discursivo: Robert Schumann describió a su "Sinfonía Grande" en "do" mayor como alcanzando «longitudes celestiales».

Schubert dejó su marca más inborrable en el género del lied. Leon Plantinga comenta que «en sus más de seiscientos lieder exploró y amplió las potencialidades del género, como ningún compositor antes que él». Antes de su influencia, los lieder tendían a un tratamiento estrófico y silábico del texto, evocando las cualidades de la canción popular engendradas por las agitaciones del nacionalismo romántico.

Entre los enfoques de Schubert de la poesía de Johann Wolfgang von Goethe, sus arreglos para "Gretchen am Spinnrade" (D. 118) y "Erlkönig" (D. 328) son particularmente llamativos por su contenido dramático, usos progresivos de la armonía y su uso de figuraciones pictóricas elocuentes del teclado, como la representación de la rueca y el pedal en el piano en "Gretchen" y el galope incesante y furioso en "Erlkönig". Compuso música usando los poemas de una infinidad de poetas, con Goethe, Johann Mayrhofer y Friedrich Schiller como los tres más frecuentes, y otros como Heinrich Heine, Friedrich Rückert y Joseph Freiherr von Eichendorff. De particular interés adicional son sus dos ciclos de canciones sobre los poemas de Wilhelm Müller, "La bella molinera" y "Viaje de invierno", que ayudaron a establecer el género y su potencial para la narrativa dramática musical, poética y casi operística. Su última colección de canciones publicada en 1828 después de su muerte, "Schwanengesang", también es una contribución innovadora a la literatura de lieder alemana, ya que presenta poemas de diferentes poetas, en especial Ludwig Rellstab, Heine y Johann Gabriel Seidl. El periódico austríaco "The Wiener Theaterzeitung" escribió sobre "Viaje de invierno" en ese momento que era una obra que «nadie puede cantar ni escuchar sin sentirse profundamente conmovido».

Antonín Dvořák escribió en 1894 que Schubert, a quien consideraba uno de los compositores verdaderamente grandes, fue claramente influyente en obras más cortas, especialmente lieder y obras de piano más cortas: «La tendencia de la escuela romántica ha sido hacia formas cortas y aunque Weber ayudó a mostrar el camino, a Schubert pertenece el principal crédito de originar los modelos cortos de piezas de pianoforte que la escuela romántica ha cultivado preferiblemente. [...] Schubert creó una nueva época con el lied. [...] Todos los otros compositores han seguido sus pasos».

Cuando Schubert murió, había publicado alrededor de 100 opus, principalmente canciones, música de cámara y pequeñas composiciones para piano. Continuaron las publicaciones de piezas más pequeñas, entre las que se incluían números de opus hasta 173 en la década de 1860, 50 entregas con canciones publicadas por Diabelli y docenas de primeras publicaciones Peters, pero los manuscritos de muchas de las obras más largas, cuya existencia era poco conocida, permanecieron ocultos en los gabinetes y archivadores de la familia, amigos y editores de Schubert. Incluso algunos de sus amigos desconocían el alcance total de lo que escribió y durante muchos años fue reconocido principalmente como el «príncipe de la canción», aunque hubo un reconocimiento de algunos de sus creaciones a gran escala. En 1838, Robert Schumann, en una visita a Viena, encontró el polvoriento manuscrito de la "Sinfonía en do mayor" (D. 944) y lo llevó de regreso a Leipzig, donde fue interpretado por Felix Mendelssohn y bien recibido por la "Neue Zeitschrift für Musik". Un paso importante hacia la recuperación de las obras abandonadas fue el viaje a Viena que el historiador de la música George Grove y el compositor Arthur Sullivan realizaron en octubre de 1867. Desenterraron los manuscritos de seis de las sinfonías, partes de la música incidental de "Rosamunda", la "Misa n.º 1 en fa mayor" (D. 105) y las óperas "Des Teufels Lustschloss" (D. 84), "Fernardo" (D. 220), "Der vierjährige Posten" (D. 190) y "Los amigos de Salamanca" (D. 326), y varias otras obras sin nombre. Con estos descubrimientos, Grove y Sullivan pudieron informar al público de la existencia de estas obras. Además, pudieron copiar la "Cuarta" y "Sexta" sinfonías, la música incidental de "Rosamunda" y la obertura de "Los amigos de Salamanca". Esto condujo a un interés público más generalizado en la obra de Schubert.

La obra completa de Schubert, "Franz Schubert's Werke", se publicó entre 1884 y 1897 en la editorial Breitkopf & Härtel. Fue especialmente relevante, dentro de esta, la edición de las canciones, encomendada al musicólogo y compositor Eusebius Mandyczewski, quien realizó un trabajo tan meticuloso que todavía hoy es de referencia. También incluía una contribución realizada por Johannes Brahms, editor de la primera serie que contiene ocho sinfonías. En la segunda mitad del comenzó la publicación del "Neue Schubert-Ausgabe" por Bärenreiter.

Dado que relativamente pocas de sus obras se publicaron en su vida, solo un pequeño número de ellas tiene números opus asignados e incluso en esos casos, la secuencia de los números no da una buena indicación del orden de composición. El musicólogo austríaco Otto Erich Deutsch compiló el primer catálogo completo de las obras de Schubert. El catálogo de Deutsch ("Deutsch-Verzeichnis") se publicó por primera vez en inglés en 1951 y posteriormente fue revisado para una nueva edición en alemán en 1978. La numeración tradicional de las obras de Schubert fue sustituida poco a poco por la catalogación que hizo Deutsch. La notación se compone de la letra "D" o "D." seguida por un número y, en algún caso, una letra minúscula para insertos o hallazgos posteriores. Por ejemplo, la "Sinfonía n.º 8 «inacabada» o «incompleta»" lleva como número de catálogo D. 759.

La confusión surgió bastante temprano sobre la numeración de las últimas sinfonías de Schubert. Su última sinfonía completa, la "Sinfonía Grande" en do mayor D. 944, recibió los números 7, 8, 9 y 10, dependiendo de la publicación. Del mismo modo, la "Inacabada" D. 759 fue numerada con los números 7, 8 y 9.

El orden seguido generalmente para estas sinfonías tardías es:


Surgió una confusión aún mayor sobre la numeración de las sonatas para piano, con sistemas de numeración que van de 15 a 23 sonatas.

Grillparzer expresó en el epitafio de Schubert el sentimiento de pérdida que generaba su prematura muerte a los 31 años: «El arte de la música ha enterrado aquí un preciado tesoro, sino esperanzas aún más hermosas». Algunos músicos prominentes comparten una opinión similar, incluido el pianista Radu Lupu, quien dijo: «[Schubert] es el compositor por el que realmente lamento que haya muerto tan joven ... Justo antes de morir, cuando escribió su hermoso "Quinteto para cuerda de dos violonchelos en do", dijo muy modestamente que estaba tratando de aprender un poco más sobre el contrapunto y tenía toda la razón. Nunca sabremos en qué dirección iba o habría ido». Sin embargo, otros han expresado su desacuerdo con esta visión inicial. Por ejemplo, Robert Schumann dijo: «No tiene sentido adivinar qué más pudo haber logrado [Schubert]. Hizo lo suficiente y que se estén satisfechos los que se han esforzado y hayan conseguido lo que él logró» y el pianista András Schiff dijo que «Schubert vivió una vida muy corta, pero fue una vida muy concentrada. En 31 años, vivió más de lo que otras personas vivirían en 100 y no es necesario especular qué podría haber escrito si hubiera vivido otros 50 años. Es irrelevante, al igual que con Mozart. Estos son los dos genios naturales de la música».

La "Wiener Schubertbund", una de las principales sociedades corales de Viena, se fundó en 1863, mientras se desarrollaba el Gründerzeit. La "Schubertbund" se convirtió rápidamente en un punto de encuentro para los maestros de escuela y otros miembros de la clase media vienesa que se sintieron cada vez más enfrentados durante el Gründerzeit y las secuelas del pánico de 1873. En 1872, se erigió el "Schubert Denkmal" («Monumento a Schubert»), un regalo a la ciudad del principal coro masculino de Viena, el Wiener Männergesang-Verein, que actuaron en el evento. El "Denkmal" lo diseñó el escultor austríaco Carl Kundmann y está ubicado en el Stadtpark de Viena.

La música de cámara de Schubert sigue siendo popular. En una encuesta realizada por la emisora de radio australiana ABC Classic FM en 2008, sus obras de cámara dominaron el campo, con el "Quinteto La Trucha" en primer lugar, el "Quinteto para cuerda en do mayor" en segundo y el "Nocturno" en tercera posición. Además, ocho de sus obras de cámara se encontraban entre las 100 piezas clasificadas: ambos tríos para piano, el "Cuarteto de cuerda n.º 14" («La muerte y la doncella»), el "Cuarteto de cuerda n.º 15", la "Sonata arpeggione", el "Octeto", la "Fantasía en fa menor" para piano a cuatro manos y el "Adagio y rondó concertante" para cuarteto de piano.

El principal crítico de música de "The New York Times" Anthony Tommasini, que ubica a Schubert como el cuarto compositor más grande, escribió de él:

Desde la década de 1830 hasta la década de 1870, Franz Liszt transcribió y arregló varias obras de Schubert, particularmente las canciones. Liszt, quien fue una fuerza significativa en la difusión de la obra de Schubert después de su muerte, dijo que era «el músico más poético que jamás haya existido». Las sinfonías de Schubert fueron de particular interés para Antonín Dvořák. Héctor Berlioz y Anton Bruckner reconocieron la influencia de la "Sinfonía Grande". Fue Robert Schumann quien, después de haber visto el manuscrito de la "Sinfonía Grande" en Viena en 1838, llamó la atención de Mendelssohn, quien dirigió la primera interpretación de la sinfonía, en una versión muy resumida, en Leipzig en 1839. En el , compositores como Richard Strauss, Anton Webern, Benjamin Britten, George Crumb y Hans Zender impulsaron u homenajearon al compositor en algunas de sus obras. Britten, un pianista consumado, acompañó a muchos de los lieder de Schubert e interpretó muchas obras de piano solo y dúo.

El grupo alemán de música electrónica Kraftwerk tiene una pieza instrumental titulada «Franz Schubert» en su álbum "Trans Europa Express" de 1977.

En 1897, el centenario del nacimiento de Schubert estuvo marcado en el mundo musical por festivales y actuaciones dedicadas a su música. En Viena, hubo diez días de conciertos y el emperador Francisco José dio un discurso reconociendo a Schubert como el creador del lieder y uno de los hijos favoritos de Austria. Karlsruhe vio la primera producción de su ópera "Fierrabras".

En 1928, se celebró en Europa y Estados Unidos la Semana Schubert para conmemorar el centenario de la muerte del compositor. Sus obras se interpretaron en iglesias, salas de conciertos y emisoras de radio. Se llevó a cabo una competición, con un premio mayor de 10000 dólares y el patrocinio de la Columbia Phonograph Company para «obras sinfónicas originales presentadas como una apoteosis del genio lírico de Schubert y dedicadas a su memoria». La ganadora fue la "Sexta sinfonía" de Kurt Atterberg.

Schubert ha aparecido como personaje en varias películas, incluyendo "Schubert's Dream of Spring" (1931), "Gently My Songs Entreat" (1933), "Serenade" (1940), "The Great Awakening" (1941), "It's Only Love" (1947), "Franz Schubert" (1953), "Das Dreimäderlhaus" (1958), and "Mit meinen heißen Tränen" (1986). Su música también ha aparecido en numerosas películas posteriores a la era muda, incluida "Fantasía" de Walt Disney (1940), que incluye el «Ave María» (D. 839); y la película biográfica "Carrington" (1995), que presenta el segundo movimiento del "Quinteto para cuerda en do mayor" (D. 956), así como la versión en inglés de "Chatrán" (1989), que presenta «Serenade» y «Auf dem Wasser zu singen» (D. 774). Además, se ha usado su música en más de 840 películas y programas de televisión.

La vida de Schubert fue cubierta en el documental "Franz Peter Schubert: The Greatest Love and the Greatest Sorrow" de Christopher Nupen (1994) y en el documental "Schubert – The Wanderer" de András Schiff y Mischa Scorer (1997), ambos producidos para BBC.



Obras de Otto Erich Deutsch

Otto Erich Deutsch, trabajó en la primera mitad del y fue probablemente el más destacado experto en la vida y obra de Schubert. Además del catálogo de obras de Schubert, recopiló y organizó gran cantidad del material sobre Schubert.

Expertos del siglo XIX y comienzos del XX

Expertos modernos

Numeración de las sinfonías

Las siguientes fuentes ilustran la confusión sobre la numeración de las últimas sinfonías de Schubert. La "Inacabada" se publicó tanto como n.º7 como n.º 8, tanto en inglés como en alemán.



</doc>
<doc id="6635" url="https://es.wikipedia.org/wiki?curid=6635" title="Computación distribuida">
Computación distribuida

Desde el inicio de la era de la computadora moderna (1945), hasta cerca de 1985, sólo se conocía la computación centralizada.
A partir de la mitad de la década de los ochenta aparecen dos avances tecnológicos fundamentales:
Aparecen los sistemas distribuidos, en contraste con los sistemas centralizados.

Un sistema distribuido es un sistema en el que los componentes hardware o software:

Los S. O. para sistemas distribuidos han tenido importantes desarrollos, pero todavía existe un largo camino por recorrer.
Los usuarios pueden acceder a una gran variedad de recursos computacionales:
Características de un sistema distribuido: 

El concepto de transmisión de mensajes se originó a finales de la década de los 60. A pesar de que el multiprocesador de propósito general y las redes de computadoras no existían en ese momento, surgió la idea de organizar un sistema operativo como una colección de procesos de comunicación donde cada proceso tiene una función específica, en la cual, no pueden interferir otros (variables no compartidas). De esta forma, en los años 70 nacieron los primeros sistemas distribuidos generalizados, las redes de área local (LAN) como Ethernet . 

Este suceso generó inmediatamente una gran cantidad de lenguajes, algoritmos y aplicaciones, pero no fue hasta que los precios de las LANs bajaron, cuando se desarrolló la computación cliente/servidor.
A finales de la década de 1960 se creó ‘Advanced Research Projects Agency Network’ (ARPANET). Esta agencia fue la espina dorsal de Internet hasta 1990, tras finalizar la transición al protocolo TCP/IP, iniciada en 1983. En los primeros años de 1970, nace el correo electrónico ARPANET, el que es considerado como la primera aplicación distribuida a gran escala. .

Durante las dos últimas décadas se han realizado investigaciones en materia de algoritmos distribuidos y se ha avanzado considerablemente en la madurez del campo, especialmente durante los años ochenta. Originalmente la investigación estaba muy orientada hacia aplicaciones de los algoritmos en redes de área amplia (WAN), pero hoy en día se han desarrollado modelos matemáticos que permiten la aplicación de los resultados y métodos a clases más amplias de entornos distribuidos.

Existen varias revistas y conferencias anuales que se especializan en los resultados relativos a los algoritmos distribuidos y la computación distribuida. La primera conferencia sobre la materia fue el simposio ‘Principles of Distributed Computing’ (PoDC) en 1982, cuyos procedimientos son publicados por ‘Association for Computing Machinery, Inc’. ‘International Workshops on Distributed Algorithms’ (WDAG) se celebró por primera vez en Ottawa en 1985 y después en Ámsterdam (1987) y Niza (1989). Desde entonces, sus actas son publicadas por Springer-Verlag en la serie ‘Lecture Notes on Computer Science’. En 1998, el nombre de esta conferencia cambió a Distributed Computing (DISC). Los simposios anuales sobre teoría de computación (‘SToC’) y fundamentos de informática (FoCS) cubren toda las áreas fundamentales de la informática, llevando a menudo documentos sobre computación distribuida. Las actas de ‘SToC’ son publicadas por ‘Association for Computing Machinery, Inc.’ y los de FoCS por el IEEE. ‘The Journal of Parallel and Distributed Computing (JPDC), ‘Distributed Computing’ e ‘Information Processing Letters’ (IPL) publican algoritmos distribuidos regularmente. .

Así fue como nacieron los sistemas distribuidos. .

Al igual que ocurre con los sistemas distribuidos, en los sistemas paralelos no existe una definición clara. Lo único evidente es que cualquier sistema en el que los eventos puedan ordenarse de manera parcial se consideraría un sistema paralelo y por lo tanto, esto incluiría a todos los sistemas distribuidos y sistemas de memoria compartida con múltiples hilos de control. De esta forma, se podría decir que los sistemas distribuidos forman una subclase de sistemas paralelos, donde los espacios de estado de los procesos no se superponen.

Algunos distinguen los sistemas paralelos de los sistemas distribuidos en función de sus objetivos: los sistemas paralelos se centran en el aumento del rendimiento, mientras que los sistemas distribuidos se centran en la tolerancia de fallos parciales. 

Desde otro punto de vista, en la computación paralela, todos los procesadores pueden tener acceso a una memoria compartida para intercambiar información entre ellos y en la computación distribuida, cada procesador tiene su propia memoria privada, donde la información se intercambia pasando mensajes entre los procesadores.

Luego se podría decir que la computación en paralelo es una forma particular de computación distribuida fuertemente acoplada, y la computación distribuida una forma de computación paralela débilmente acoplada .

La figura que se encuentra a la derecha ilustra la diferencia entre los sistemas distribuidos y paralelos. La figura a. es un esquema de un sistema distribuido, el cual se representa como una topología de red en la que cada nodo es una computadora y cada línea que conecta los nodos es un enlace de comunicación. En la figura b) se muestra el mismo sistema distribuido con más detalle: cada computadora tiene su propia memoria local, y la información sólo puede intercambiarse pasando mensajes de un nodo a otro utilizando los enlaces de comunicación disponibles. En la figura c) se muestra un sistema paralelo en el que cada procesador tiene acceso directo a una memoria compartida.

 Hay numerosos ejemplos de sistemas distribuidos que se utilizan en la vida cotidiana en una variedad de aplicaciones. La mayoría de los sistemas están estructurados como sistemas cliente-servidor, en los que la máquina servidora es la que almacena los datos o recursos, y proporcionan servicio a varios clientes distribuidos geográficamente. Sin embargo, algunas aplicaciones no dependen de un servidor central, es decir, son sistemas peer-to-peer, cuya popularidad va en aumento. Presentamos aquí algunos ejemplos de sistemas distribuidos:

 Los sistemas destinados a ser utilizados en entornos del mundo real deben estar diseñados para funcionar correctamente en la gama más amplia posible de circunstancias y ante posibles dificultades y amenazas. Las propiedades y los problemas de diseño de sistemas distribuidos pueden ser capturados y discutidos mediante el uso de modelos descriptivos. Cada modelo tiene la intención de proporcionar una descripción abstracta y simplificada pero consistente de un aspecto relevante del diseño del sistema distribuido.

Algunos aspectos relevantes pueden ser: el tipo de nodo y de red, el número de nodos y la responsabilidad de estos y posibles fallos tanto en la comunicación como entre los nodos. Se pueden definir tantos modelos como características queramos considerar en el sistema, pero se suele atender a esta clasificación:

Modelos físicos: Representan la forma más explícita para describir un sistema, identifican la composición física del sistema en términos computacionales, principalmente atendiendo a heterogeneidad y escala. Podemos identificar tres generaciones de sistemas distribuidos:

Modelos arquitectónicos: El objetivo general de este tipo de modelo es garantizar el reparto de responsabilidades entre componentes del sistema distribuido y la ubicación de dichos componentes. Las principales preocupaciones son determinar la relación entre procesos y hacer al sistema confiable, adaptable y rentable.

Modelos fundamentales: Todos los modelos anteriores comparten un diseño y un conjunto de requisitos necesarios para proporcionar confiabilidad y seguridad a los recursos del sistema. 
Un modelo fundamental toma una perspectiva abstracta, de acuerdo al análisis de aspectos individuales del sistema distribuido; debe contener sólo lo esencial a tener en cuenta para comprender y razonar sobre algunos aspectos de un comportamiento del sistema. 

Por lo tanto, para que pueda afirmarse que existe una comunicación fiable entre dos procesos debe asegurarse su integridad y su validez.
Cuando hablamos de modelos en un sistema distribuido, nos referimos principalmente al hecho de automatizar tareas, usando un computador, del tipo pregunta-respuesta, es decir, que cuando realicemos una pregunta al computador, este nos debe contestar con una respuesta apropiada. En la informática teórica, este proceso se conoce como problemas computacionales.

Formalmente, un problema computacional consiste en instancias junto con una solución a cada una de ellas. Las instancias se pueden traducir como preguntas que nosotros hacemos al computador y las soluciones como las respuestas del mismo a nuestras preguntas.

Teóricamente, la informática teórica busca encontrar la relación entre problemas que puedan resolverse mediante un computador (teoría de la computabilidad) y la eficiencia al realizarlo(teoría de la complejidad computacional).

Comúnmente, hay tres puntos de vista:
Un algoritmo paralelo define múltiples operaciones para ser ejecutadas en cada paso. Esto incluye comunicación/coordinación entre las unidades de procesamiento.

Un ejemplo claro para este tipo de modelos sería el modelo de máquinas de acceso aleatorio paralelo (PRAM) .

Detalles del modelo PRAM

Hay mucha más información acerca de este tipo de algoritmo de una forma más resumida en los siguientes libros. 

Ambiente geográficamente distribuido: En primer lugar, en muchas situaciones, el entorno informático en sí mismo está geográficamente distribuido. Como ejemplo, consideremos una red bancaria. Se supone que cada banco debe mantener las cuentas de sus clientes. Además, los bancos se comunican entre sí para monitorear las transacciones interbancarias, o registrar las transferencias de fondos de los cajeros automáticos geográficamente dispersos. Otro ejemplo común de un entorno informático geográficamente distribuido es la Internet, que ha influido profundamente en nuestra forma de vida. La movilidad de los usuarios ha añadido una nueva dimensión a la distribución geográfica.

Speed up: En segundo lugar, existe la necesidad de acelerar los cálculos. La velocidad de cómputo en los uniprocesadores tradicionales se está acercando rápidamente al límite físico. Mientras que los procesadores superescalares y VLIW estiran el límite introduciendo un paralelismo a nivel arquitectónico (cuestión de la instrucción), las técnicas no se escalan mucho más allá de un cierto nivel. Una técnica alternativa para obtener más potencia de cálculo es utilizar procesadores múltiples. Dividir un problema entero en subproblemas más pequeños y asignar estos subproblemas a procesadores físicos separados que puedan funcionar simultáneamente es un método potencialmente atractivo para aumentar la velocidad de cálculo. Además, este enfoque promueve una mejor escalabilidad, en la que los usuarios pueden aumentar progresivamente la potencia de cálculo adquiriendo elementos o recursos de procesamiento adicionales. A menudo, esto es más sencillo y económico que invertir en un único uniprocesador superrápido.

Compartir recursos: En tercer lugar, existe la necesidad de compartir los recursos. El usuario de la computadora A puede querer usar una impresora láser conectada con la computadora B, o el usuario de la computadora B puede necesitar un poco de espacio extra en el disco disponible en la computadora C para almacenar un archivo grande. En una red de estaciones de trabajo, es posible que la estación de trabajo A quiera utilizar la potencia de cálculo en reposo de las estaciones de trabajo B y C para aumentar la velocidad de un determinado cálculo. Las bases de datos distribuidas son buenos ejemplos del intercambio de recursos de software, en los que una gran base de datos puede almacenarse en varias máquinas anfitrionas y actualizarse o recuperarse sistemáticamente mediante una serie de procesos de agentes.

Tolerancia a fallos: Por último, los poderosos uniprocesadores, o sistemas de computación construidos alrededor de un solo nodo central son propensos a un completo colapso cuando el procesador falla. Muchos usuarios consideran que esto es arriesgado. Sin embargo, están dispuestos a transigir con una degradación parcial del rendimiento del sistema, cuando un fallo paraliza una fracción de los muchos elementos de procesamiento o enlaces de un sistema distribuido. Esta es la esencia de la degradación gradual. La otra cara de este enfoque es que, al incorporar elementos de procesamiento redundantes en un sistema distribuido, se puede aumentar potencialmente la fiabilidad o la disponibilidad del sistema. Por ejemplo, en un sistema que tiene triple redundancia modular (TMR), se utilizan tres unidades funcionales idénticas para realizar el mismo cómputo, y el resultado correcto se determina por mayoría de votos. En otros sistemas distribuidos tolerantes a fallos, los procesadores se comprueban mutuamente en puntos de control predefinidos, lo que permite la detección automática de fallos, el diagnóstico y la eventual recuperación. Así pues, un sistema distribuido ofrece una excelente oportunidad para incorporar la tolerancia a fallos y la degradación grácil.

Escalabilidad: El sistema debe diseñarse de tal manera que la capacidad pueda ser aumentada con la creciente demanda del sistema.

Heterogeneidad: La infraestructura de comunicaciones consiste en canales de diferentes capacidades. 

Gestión de los recursos: En los sistemas distribuidos, los recursos se encuentran en diferentes lugares. El enrutamiento es un problema en la capa de red y en la capa de aplicación. 

Seguridad y privacidad: Dado que los sistemas distribuidos tratan con datos e información sensible, se deben tener fuertes medidas de seguridad y privacidad. La protección de los activos del sistema distribuido así como los compuestos de nivel superior de estos recursos son cuestiones importantes en el sistema distribuido.

Transparencia: La transparencia significa hasta qué punto el sistema distribuido debe aparecer para el usuario como un sistema único. El sistema distribuido debe ser diseñado para ocultar en mayor medida la complejidad del sistema.

Apertura: La apertura significa hasta qué punto un sistema es diseñado utilizando protocolos estándar para apoyar la interoperabilidad. Para lograr esto, el sistema distribuido debe tener interfaces bien definidas.

Sincronización: Uno de los principales problemas es la sincronización de los cálculos consistentes en miles de componentes. Métodos actuales de sincronización como los semáforos, los monitores, las barreras, la llamada a procedimientos remotos, la invocación de métodos de objetos y el paso de mensajes, no escalan bien.

Interbloqueo y condiciones de carrera: El interbloqueo y las condiciones de carrera son otras grandes cuestiones en el sistema distribuido, especialmente en el contexto de las pruebas. Se convierte en un tema más importante especialmente en el entorno de multiprocesadores de memoria compartida.





</doc>
<doc id="6637" url="https://es.wikipedia.org/wiki?curid=6637" title="Varón">
Varón

Varón u hombre es un ser humano de sexo masculino, independientemente de si es niño o adulto. La palabra «varón» en español deriva del latín "varo" («valiente», «esforzado»), muy probablemente relacionada con "vir" («varón», «héroe») bajo la influencia del germánico "baro" («hombre libre»).

El Día Internacional del Hombre se celebra el 19 de noviembre.

Después de la fecundación durante las primeras etapas celulares se define a nivel biológico si es ser futuro será masculino o femenino resutando en este caso que el cromosoma 23 sea tipo XY determiando el desarrollo futuro del infante y del adulto, generando pene y testículos externos y posteriormente desencadenando un proceso hormonal segregando principalemnte testosterona a partir de la adolescencia que creará como consecuencia las características sexuales secundarias másculinas.

La testosterona es una hormona androgénica propia del macho en muchas especies, que permite desarrollar los músculos del varón con poco esfuerzo y es determinante en parte de su desarrollo físico y de las características sexuales secundarias.

El aparato reproductor masculino garantiza que el varón tenga la capacidad de fecundar el óvulo femenino y en ello la transmisión de la información genética por medio de la célula espermatozoidal. Los órganos sexuales primarios del varón son exteriores, a diferencia de los de la mujer que son internos. La andrología es la ciencia que estudia el aparato reproductor masculino.

Entre las características secundarias más comunes que empiezan a desarrollarse a partir de la pubertad y la edad viril (y que no necesariamente son siempre así) sin que su ausencia vaya en contra de la identidad masculina, se cuentan las siguientes


Un ser humano del sexo masculino es varón desde el momento en el cual es concebido: el espermatozoide contiene los cromosomas sexuales diferenciados X o Y, mientras la hembra tiene el cromosoma homogamético X. La combinación cromosómica entre el espermatozoide y el óvulo determina el sexo del individuo concebido, lo que da como resultado que un feto pueda ser determinado como “hembra” si la combinación cromosómica es XX y como varón si es XY. La combinación genética XY es más frecuente que la combinación genética XX, mientras que la mortalidad infantil es menor en varones recién nacidos que en niñas.

El varón infante recibe el nombre de “niño” al menos hasta el inicio de su pubertad. También es popular llamarlo “mozo”, palabra que lo determina hasta su primera juventud (aproximadamente hasta los 20 años de edad). Durante este tiempo comienza todo el proceso de desarrollo físico, psicológico y social como “varón” que le permitiría desarrollar un rol determinado por la cultura a su condición humana masculina.

Tanto varones como mujeres son víctimas del mismo tipo de enfermedades que afectan al género humano, pero cada género tiene una tendencia mayor a un determinado tipo. Las enfermedades que más se manifiestan en el varón son el Autismo, el Daltonismo y el Mal de Alzheimer, que ataca principalmente en la edad mayor, pero puede presentarse en varones jóvenes.

Las expectativas de vida masculina, como las femeninas, varían considerablemente de acuerdo al desarrollo de cada sociedad.

En cuanto a la tasa de mortalidad infantil a nivel global, se considera que los varones recién nacidos tienen una mayor esperanza de vida que las niñas. 

La desfase entre la población neonata masculina y femenina se equipara durante la adolescencia, tiempo en el cual aumenta en todos los continentes la morbilidad masculina por encima de la femenina debido a la mayor participación de los varones en confrontaciones armadas, guerras o simplemente en el desafío del peligro. Otros riesgos como el consumo de estupefacientes, alcohol, enfermedades de transmisión sexual y violencia urbana, mayor entre los varones que entre las muchachas, reducen la población masculina adolescente en todo el mundo.

La más popular alteración física de la constitución sexual del varón es la circuncisión, una práctica muy antigua y que tiene desde razones religiosas hasta de salud. La circuncisión es una operación que se práctica por lo general al recién nacido con la remoción del prepucio de su pene. Aparte de las razones religiosas que se tienen, la circuncisión ha probado ser un método de prevención contra el cáncer de pene. Pero la circuncisión no es tan restringida a un grupo religioso como muchos piensan. Las estadísticas hablan de que en el mundo por lo menos un 20% de los varones son circuncisos, especialmente en las sociedades judías, América del Norte, las Filipinas, Corea del Sur y los países musulmanes.

La circuncisión también es vista como una forma de ablación genital masculina, entre otras. Entre las causas de que la circuncisión no provoque el mismo impacto social que otras formas de mutilación, atendería a razones de proximidad y tradición, al ser algo relativamente común y cercano, esta práctica sería vista como algo no trágico a pesar de que el debate sanitario sobre los beneficios de esta práctica no es concluyente. Aún habiendo hombres damnificados por esta práctica se ignora la repercusión negativa de la circuncisión. 

De acuerdo con los datos del Servicio de Endocrinología y Metabolismo de la Unidad Asistencial Dr. César Milstein de Buenos Aires, en Argentina, el Síndrome de Klinefelter (en adelante, SK) tiene una prevalencia de 0.2%. Sin embargo, esta condición raramente es diagnosticada en la infancia. Las primeras consultas en relación con el SK suelen aparecer durante la adolescencia, suscitadas fundamentalmente por los primeros atisbos de cambios en el cuerpo que conlleva la pubertad. Hasta no hace mucho tiempo, las y los niños que tenían esta particularidad crecían y se desarrollaban sin ser conscientes de ninguna anomalía y desarrollaban una vida sexual común. Otro motivo para consultar es la preocupación en torno a problemáticas vinculadas con la infertilidad, a pesar de que en algunos casos se pudo observar que en sujetos donde había bastantes células germinales funcionando normalmente en los testículos, podrían fecundar sin inconvenientes. En la actualidad muchos agentes de la salud e investigadores científicos de disciplinas conexas fueron dejando en desuso el término "síndrome de Klinefelter", usando en su lugar la descripción de "varones XXY".

En general no afecta al fenotipo, pero en ocasiones se tiene estatura y extremidades mayores por lo que a veces se le llama sindrome del supermacho.

En sociedades de caza y recolección, los varones suministraban la carne mediante la caza de animales.

La discusión acerca de las diferencias entre varones y mujeres, especialmente en Occidente no es unánime. Psicológicamente, la asociación tradicional de aptitudes y actitudes a un género normalmente se basa en suposiciones consolidadas por el hábito de la observación directa, de la actividad y personalidad de las personas de ambos géneros en el contexto social. Esta asociación se arraiga principalmente en la edad infantil.

Los estereotipos masculinos varían según el nivel cultural de la sociedad, la edad y el momento histórico. Por ejemplo, estudiantes y personas adultas definen de forma diferente lo que se considera masculino. Los estudiantes elaboran unos estereotipos de rol de género más claramente definidos que las personas adultas. Los estereotipos masculinos normalmente está más definido que los estereotipos femeninos. No obstante, esta asignación de características es cada vez más alejada de la realidad, por lo que los mismos estereotipos de género van cambiando paulatinamente, conforme al cambio de tareas tradicionalmente asignadas a uno de los dos sexos como, por ejemplo, la incorporación de la mujer al mundo laboral. Así mismo, el incremento de la actividad de las mujeres en los ámbitos deportivos propicia un cambio del estereotipo tradicional masculino.

Las sociedades y culturas orientales o más conservadoras, asumen muchos de esos estereotipos como lo que es o debe ser en el varón, pero la era de la globalización poco a poco los hace entrar en el debate. Entre los "estereotipos" más comunes se pueden enumerar:


Muchos de estos paradigmas tienen fundamento científico, mientras que otros no (aunque la sociedad ha hecho que muchos de estos estereotipos sean realidad como por ejemplo, el saludo de dos mujeres puede ser beso, entre hombre y mujer también, pero entre hombres es "raro" sin ninguna razón, etc). Por ejemplo, no es sencillo separar los elementos innatos de la biología masculina de aquellos que han sido influenciados por la cultura. En tal caso, la agresividad puede darse tanto en el varón como en la mujer de acuerdo al ambiente en que estos se desenvuelvan. La mayor masa corporal y muscular del varón y las culturas patriarcales contribuyen a acentuar el estereotipo de la agresividad masculina. Los grupos feministas en sus estudios señalan que en la violencia intrafamiliar, el abuso infantil, el maltrato infantil y la violencia contra la mujer, tienen como principal verdugo en la mayoría de los casos al varón tanto de países industrializados como en vías de desarrollo.

Algunos de estos estereotipos se asocian, en ocasiones erróneamente y en ocasiones acertadamente con los niveles de hormonas sexuales masculinas, como la testosterona, o la menor cantidad de hormonas sexuales femeninas, como los estrógenos. En el caso de la agresividad, tradicionalmente relacionada con el nivel de testosterona, algunos estudios indican que dicha relación no corresponde con sus resultados.

Desde su nacimiento se viste a los varones de celeste y se les enseña a creer que productividad, conquista, poder, hiperactividad y penetración son sinónimos de virilidad. De pequeños se les enseña a no llorar, a no ser vulnerables, a no quejarse, a no mostrar sus debilidades ni sus sentimientos y a ser autosuficientes y no pedir ayuda. Se les enseña a confundir acción y agresión con masculinidad, a rendir en los deportes aún a expensas de su propia salud, a exponerse a peligros y a deportes de riesgo. Las consecuencias de la adecuación a este marcado estereotipo social se las puede encontrar en los servicios de terapia intensiva de los hospitales con mayoría masculina, en la población carcelaria, donde la gran mayoría de los reclusos son varones, en las estadísticas de accidentes y en los hechos delictivos que leemos en los diarios.

La educación masculina depende en gran parte de la discusión de los estereotipos masculinos en el grado en que estos sean asumidos por una sociedad. La educación entonces que parte desde el hogar dada al niño, pasa por la formal y se expresa en las relaciones sociales y en la imagen que presentan los medios de comunicación, tiene diversos matices que dependen de la cultura del país, continente o región del mundo.

La primera educación de la sexualidad y socialización del niño parte del hogar. El padre y la madre son los encargados de transmitir la primera información sobre el rol sexual que desempeñará el niño en sociedad. En general, el padre transmitirá al hijo varón las características psicológicas de su sexualidad. En ello entran en juego los paradigmas asumidos y las maneras de ser del varón en la sociedad en la que nació. La manera de vestirse, de llevar el cabello, de hablar, de modular la voz, el tipo de juegos, los juguetes, las exigencias disciplinarias diferenciadas entre el varón y la mujer, la casi ausencia de cosméticos y otros muchos elementos, determinan poco a poco la conciencia propia del ser un varón en sociedad.
Llegada la pubertad, el papel del padre adquiere un rol más activo en la educación del hijo varón. En muchas culturas este paso entre el niño y el hombre es celebrado. Entre culturas del orden natural como tribus y clanes, el muchacho debe afrontar un número determinado de desafíos que le permitirán ser respetado en su grupo social como un varón adulto. En antiguas culturas célebres por su formación militar como los griegos (Esparta por ejemplo), China, Japón (los Samurái), los Azteca, los Quechua y los Chibcha, el paso a la edad adulta del muchacho era marcado por su capacidad de prepararse como un guerrero y su aceptación y aprecio social nacían de su coraje demostrado en las luchas, artes marciales y batallas. Pero también la religión tiene un papel del primer orden en la formación masculina del muchacho. La pubertad está marcada por un rito de iniciación que da al muchacho un estatus social y religioso. Por ejemplo, para el Judaísmo este viene representado en el bar mitzvah, celebración que le da al varón adolescente el derecho de leer los libros sagrados en la Asamblea. Para el Cristianismo ese momento viene marcado por la Confirmación.

Pasada la pubertad, el muchacho comienza un camino de desarrollo final hacia la adultez en la cual compite por demostrar la capacidad de su identidad como varón. Los deportes de competencia y fuerza física, por ejemplo, adquieren una enorme importancia, el afán por tener una pareja, el ingreso en un grupo social de adolescentes (la pandilla), la búsqueda de una vocación y otros son la preocupación del muchacho, situaciones no siempre pacíficas. Resta el peligro del consumo de drogas, alcohol, fumar, delincuencia y otros males sociales en el cual el joven ingresa en muchos casos llevado por el ánimo de una búsqueda de su propia identidad e independencia.

El rol sexual del varón adquiere su máxima plenitud en el matrimonio como marido y como padre. El rol masculino ha tenido una diversidad de influencias a lo largo de la historia. La Revolución industrial, la Revolución Femenina y otros momentos, han tenido sus consecuencias en la figura del padre y marido. Obviamente partimos de una lectura de Occidente, porque en otras culturas no occidentales, este papel puede estar marcado por una concepción más tradicionalista como la llamada Familia patriarcal en la cual la figura paterna es el centro de toda autoridad. En India y otros sitios de la tierra, se practica la dote en la cual el padre de la hija paga una cierta cantidad al padre del hijo varón. Dicha práctica trae como desventaja principal un cierto desdén en la concepción de las niñas, las cuales son vistas más como una carga y abre las puertas al infanticidio femenino. En otros países en cambio, como Camboya, la tradición es al contrario, es el padre del hijo varón quien da la dote al padre de la hija. Pero en ambos casos, la libertad de ambos jóvenes se ve restringida en la escogencia del cónyuge, la cual es decisión de sus padres. Casos similares se presentan entre las culturas musulmanas, muchas de las cuales todavía practican la poligamia, es decir, el varón puede casarse con varias mujeres como solución a la escasez de hombres que morían en la guerra.

No siempre la heterosexualidad en el varón fue vista como la única opción. De hecho, en sociedades antiguas la atracción hacia otros varones y la actividad sexual con ellos era considerada tan normal como la expresada hacia las mujeres, y esta característica predomina en la cultura grecorromana. La milicia utilizó este tipo de relaciones para unir a los guerreros con fines de autoprotección y compañerismo, mientras que ciertos autores griegos y latinos dan por hecho que todos los hombres sienten deseo homosexual en algún momento.





</doc>
<doc id="6638" url="https://es.wikipedia.org/wiki?curid=6638" title="Mujer">
Mujer

Mujer (del latín "mulĭer", "-ēris"), o fémina (lat. "femĭna"), es el ser humano femenino o hembra, independientemente de si es niña o adulta. Tiene diferencias biológicas con el varón, como la cintura más estrecha, cadera más ancha y pelvis más amplia, diferente distribución y cantidad de vello, tejido adiposo y musculatura. Sus genitales son diferentes y sus mamas, a diferencia del varón, están desarrolladas. 

Mujer también remite a diferencias de carácter cultural y social que se le atribuyen por género. 

El Día Internacional de la Mujer se conmemora el día 8 de marzo.

Después de la fecundación durante las primeras etapas celulares se define a nivel biológico si es ser futuro será masculino o femenino resultando en este caso que el cromosoma 23 sea tipo XX determinando el desarrollo futuro del infante y del adulto, generando ovarios y órganos sexuales externos como la vulva e internos como la vagiba. Posteriormente eso desencadenará un proceso hormonal segregando varias hormonas algunas responsables de las características sexuales secundarias de la mujer que se producirán a partir de la adolescencia.

Un gran número e interacción de hormonas forman parte de la biología de la mujer, la principal es el estrógeno que afecta las características sexuales secundarias.

El aparato reproductor femenino permite que la mujer pueda ser fecundada por los espermatozoides del varón y con ello la transmisión de la información genética por medio del óvulo. Los órganos sexuales primarios de la mujer son internos a diferencia de los del hombre que son exteriores.

El cuerpo de la mujer se caracteriza por la presencia de pechos con glándulas mamarias, cuyo fin es alimentar a futuros hijos. La cadera es diferente al hombre, es más ancha que en el varón para posibilitar la salida de un nuevo ser humano al término de la gestación o embarazo. Las siguientes características se pueden encontrar en el sexo femenino:


La mujer ha realizado y realiza el esfuerzo de trabajo reproductivo que permite la supervivencia de individuos y sociedades. A lo largo de la historia y hasta fechas recientes, con el objeto de garantizar la supervivencia social y en un contexto de altísima mortalidad (tanto en tasa bruta de mortalidad como en mortalidad infantil), ha sido necesario mantener una muy alta natalidad (tanto en la tasa bruta de natalidad como en la tasa de fecundidad) para garantizar un reemplazo suficiente de las poblaciones.

La mejora en la alimentación, la generalización de la higiene, la sanidad y la difusión de medicamentos han sido decisivos para el fuerte crecimiento de la población mundial que ha pasado de los casi 1.000 millones en el año 1800 a más de 6000 millones en el año 2000 y a 7000 millones a finales de 2011.

La necesidad de una alta reproducción ha dejado de ser uno de los tradicionales problemas de las sociedades -y por supuesto del mundo en su conjunto- para incluso convertirse, para algunos autores de corte neomalthusiano, en un nuevo problema, la superpoblación.

La reducción de la tasa bruta de mortalidad es característica de la denominada transición demográfica así como una fuerte reducción de las tasa de natalidad es característica de la segunda transición demográfica junto con cambios sociológicos que afectan básicamente al papel tradicional de la mujer.

Los avances y difusión de los métodos anticonceptivos junto con la reducción de la presión social sobre la mujer para mantener la población -al alcanzarse una alta supervivencia de las poblaciones- permiten que se produzca lo que algunos autores como John MacInnes y Julio Pérez Díaz denominan revolución reproductiva. El esfuerzo reproductivo se reduce, la supervivencia de los individuos -la baja mortalidad- permite entonces reducir sustancialmente el número de hijos. En las sociedades modernas se da una alta eficiencia reproductiva que libera a la mujer de buena parte del trabajo que desarrollaba tradicionalmente y la permite incorporarse al mercado de trabajo modificándose sustancialmente las relaciones sociales antes establecidas y advirtiéndose cambios sustanciales: declive del trabajo reproductivo (fundamentalmente en la mujer), derrumbamiento del patriarcado, privatización de la sexualidad y reducción del control social sobre la sexualidad; desaparición de la punibilidad de las relaciones sexuales no reproductivas; alto control sobre la procreación con el uso de métodos anticonceptivos y apoyo intergeneracional muy amplio a hijos y nietos, reforzamiento de los lazos familiares profundos; aumento de los años vividos o madurez de masas; centralidad de la familia y reforzamiento de los lazos e importancia de la misma.

Aunque existe gran diversidad, dependiendo del tipo de sociedad matrilineal, patrilineal, cazadores recolectores, agrícola filiación, puede decirse que desde la prehistoria, las mujeres han asumido un papel cultural particular normalmente diferenciado y todas las sociedades documentadas han conocido la división sexual del trabajo, por el cual las tareas necesarias para la subsistencia eran asignadas en función del sexo de la persona. En sociedades de caza y recolección, las mujeres casi siempre eran las que recogían los productos vegetales, mientras que los varones suministraban la carne mediante la caza de animales. A causa de su conocimiento profundo de la flora, la mayor parte de los antropólogos creen que fueron las mujeres quiénes condujeron las sociedades antiguas hacia el Neolítico y se convirtieron en las primeras agricultoras. De hecho, los datos de muchas sociedades recolectoras modernas han mostrado que la mayor parte de las calorías ingeridas provienen de la recolección realizada por las mujeres.
En la antigüedad clásica, tanto griegos como romanos documentaron ampliamente que muchos otros pueblos mediterráneos y europeos de hecho no eran patriarcados. En muchos pueblos existía una organización matrilineal, esto está documentado en varias sociedades protohistóricas de Europa. Existen abundantes elementos para pensar que originalmente entre los lidios, etruscos, astures, minoicos y algunos pueblos germanos las mujeres tenían un papel mucho más preponderante que en la sociedad griega y romana. Igualmente en la antigua Persia las mujeres tenían un papel social más preponderante que las mujeres romanas. La situación en otras partes del planeta está menos clara.

En la Edad Media europea, los autores masculinos, pertenecientes a una estirpe, religiosos, tratadistas laicos y sobre todo, predicadores, hablaron de las condiciones y conductas que les exigían a las niñas, a las jóvenes y a las mayores. La conducta femenina fue pautada para cada momento y situación de la vida. Casi siempre la edad corresponde a un estado civil determinado y a una función de acuerdo a ella. Tal es así que la mujer se representaba en la imagen de la novia, la prometida, la casada, la viuda, es decir, siempre ligada inexorablemente a un varón que debía responsabilizarse de ella y su conducta. El papel más importante atribuido a la mujer era el de esposa y madre.

En la historia reciente, las funciones de las mujeres han cambiado enormemente. La burguesía trajo consigo una nueva concepción de la familia donde la mujer desempeñaba un papel restringido al hogar. Hasta entonces la mujer había participado, aunque de modo distinto al varón, en tareas de aprovisionamiento y trabajo para la supervivencia familiar fuera del domicilio u hogar. Las funciones sociales tradicionales de las mujeres de la clase media consistían en las tareas domésticas, acentuando el cuidado de niños, y no solían acceder a un puesto de trabajo remunerado. Para las mujeres más pobres, sobre todo entre las clases obreras, esta situación era a veces un objetivo, ya que la necesidad económica las ha obligado durante mucho tiempo a buscar un empleo fuera de casa, aunque las ocupaciones en que se empleaban tradicionalmente las mujeres de clase obrera eran inferiores en prestigio y salario que aquellas que llevaban a cabo los varones. Eventualmente, el liberar a las mujeres de la necesidad de un trabajo remunerado se convirtió en una señal de riqueza y prestigio familiar, mientras que la presencia de mujeres trabajadoras en una casa denotaba a una familia de clase inferior.

La mujer española durante la conquista de América, viajaba con su esposo o sino llegaba lo más pronto posible a su localización. Para el varón, estar casado era un beneficio; se respetaba a los varones casados con hijos. Igualmente para la mujer era un beneficio, especialmente si estaba con un varón de alto título, así poseía riquezas y poder. Cuando los conquistadores iban a misiones, las que se encargaban de mantener las "cosas corriendo" en los territorios conquistados eran las mujeres españolas. Estas mujeres aportaron grandemente al proceso de la conquista de América.

En el siglo XIX ocurre una transformación en los ámbitos social, político y económico. En esta etapa se movilizaron mujeres feministas en pos de la igualdad de género. Las mujeres en países de primer mundo, recibieron libertad en el sentido de expresión hasta poder ser parte del mundo laboral. En 1979, se aprobó la Eliminación de todas las formas de Discriminación contra la Mujer. Este evento, aprobado por la Asamblea de Naciones Unidas, fue un logro para las mujeres quienes lucharon por sus derechos en la sociedad. La mujer a través de la historia ha tenido que combatir muchos problemas. Con los siglos los derechos, roles y estereotipos de las mujeres han evolucionado; desde la Edad Media hasta el Siglo XXI. Los derechos humanos de la mujer, define la discriminación contra la mujer como "toda distinción, exclusión o restricción basada en el sexo.
Las mujeres nativas y africanas, se consideraban mujeres guerreras y ayudantes en el periodo de conquista. Los conquistadores españoles se enfrentaban a estas mujeres poderosas durante sus invasiones. No retrocedían al momento de batallar a los europeos contra sus armas. Al contrario, la estrategia de los europeos fue utilizar a las mujeres españolas para controlar las sociedades nativas y a la misma vez empezar la transmisión cultural. El pensar era que los varones guerreros no iban a rebelarse estando mujeres y niños presentes. 
Un descubrimiento importante de la antropología moderna fue que fuera de Europa las sociedades no siempre eran patriarcales. Así los antropólogos descubrieron durante el siglo XIX y XX, centenares de sociedades matrilineales en las que las mujeres tenían un papel mucho más destacado, que el de las mujeres de Europa y sus colonias en América. De hecho un número signififcativo de las sociedades humanas podrían haber sido matrilineales y no patrilineales como la mayor parte de sociedades históricas europeas. Un análisis contenido en el "Ethnographic Atlas" (1967) de George P. Murdock sobre 752 sociedades históricamente documentadas dio los siguientes datos:

En los datos de Murdock, se observa que algo más de una quinta parte de las sociedades tienen un régimen de filiación matrilineal, en el que los individuos reciben el nombre familiar, la herencia y el prestigio de su rama materna. De acuerdo a centenares de descripciones antropológicas, queda claro que, en general, en las sociedades matrilineales las mujeres tienen un estatus social más alto que en sociedades patrilineales. Aunque no puede decirse que las sociedades matrilineales sean matriarcados, ya que aunque la mayor parte del poder económico y familiar está en manos de las mujeres, en estas sociedades matrilineales muchas de las más altas responsabilidades políticas y legislativas están en manos de ambos sexos.

El movimiento feminista ha perseguido el reconocimiento de la igualdad de oportunidades y la igualdad de derechos entre mujeres y hombres. Como movimiento político coherente y organizado colectivamente, el feminismo arranca en las sociedades europeas del siglo XIX. Durante el siglo XX, el feminismo ha crecido notoriamente en todo el mundo, primero en América y más tarde también Oceanía, Asia y África.

Factores asociados al modo de producción como la división sexual del trabajo, además de factores históricos, en combinación con las costumbres y las tradiciones sociales y religiosas condicionaron fuertemente que especialmente en las sociedades europeas de la antigüedad, la edad media y moderna, el patriarcado fuera la forma de organización social dominante, en la que los hombres tenían un papel muy preponderante, especialmente en los asuntos públicos. Las revoluciones burguesas y proletarias de los siglos XIX y XX, ayudaron a que las tesis de los movimientos feministas tuvieran reflejo y amparo en las legislaciones de la mayor parte de estados modernos.

Actualmente, en la mayor parte de estados, debido a los cambios económicos, el apoyo del poder económico y las reivindicaciones del movimiento feminista y otros movimientos de derechos humanos, las mujeres tienen acceso a carreras profesionales y trabajos similares a los de los hombres en la mayor parte de las sociedades. En muchas sociedades modernas las mujeres tienen plena igualdad jurídica tanto en el ámbito laboral como en el familiar, pudiendo ser cabezas de familia, detentar cargos altos tanto en política como en grandes empresas. Así que se podría decir que las condiciones de las mujeres han mejorado.

Algunas corrientes feministas cambian constantemente el significado de la palabra mujer, entendiéndose que la categoría mujer está estrechamente vinculada a la expresión de genitalidad, por lo que frecuentemente se presupone que mujer es aquella cuya expresión gonádica es igual a XX. Esta articulación discursiva se soporta sobre fundamentos biológicos y esencialistas. La naturalización del concepto impide su cuestionamiento, dogmatizándolo. Sin embargo, desde diferentes corrientes feministas, esto ha sido criticado. El rol sexual y el ejercicio de la sexualidad son en sí mismo, construcciones socioculturales motivadas por un mecanismo de control social, y de una reproducción de las estructuras de poder. Además, la categoría mujer se conceptualiza en tanto que opuesta a la categoría hombre, formando así un binomio, mutuamente excluyente, a partir del cual se articula la distinción de sexo (femenino - masculino, respectivamente). En esta situación existe opresión social cuando las personas no reproducen los esquemas preestablecidos de acuerdo a lo esperado, limitando la diversidad sexual, omitiendo y dejando al margen situaciones tales como la transexualidad y la intersexualidad.
El sufragio femenino ha sido garantizado y revocado, varias veces en varios países del mundo. En muchos países, el sufragio femenino se ha garantizado antes que el sufragio universal; así, una vez concedido este, a mujeres y varones de ciertas razas, aún se les seguía negando el derecho a votar.

El primer sufragio femenino, con las mismas características propias que el masculino, se garantizó en Nueva Jersey en 1776, aunque rescindió en 1807. Pitcairn garantizó el sufragio femenino en 1838. Varios países y estados garantizaron un sufragio femenino restringido en la segunda mitad del siglo XIX, empezando por Australia del Sur en 1861. El primer sufragio femenino sin restringir, en lo que a derecho a votar se refiere, ya que a las mujeres no se les permitía presentarse a elecciones, se garantizó en Nueva Zelanda en 1893.

La primera mujer en ejercer formalmente el derecho al voto político en América Latina fue Matilde Hidalgo de Procel en 1924, en la ciudad de Loja, convirtiendo al Ecuador en el primero de la región que permitió el voto femenino. Sin embargo no se descarta anteriores brotes de lucha por la participación de la mujer en la política. Seguramente Matilde Hidalgo de Prócel, quien además sería la primera mujer en recibirse de una carrera universitaria y doctorarse en medicina en el Ecuador, abrazaría la influencia de un importante movimiento femenino chileno por el derecho al sufragio que "apoyándose en la resolución del ministro Zenteno, se inscribió para votar por Benjamín Vicuña Mackenna en las elecciones presidenciales de 1876. Al calor de la campaña antioligárquica de este candidato, las mujeres reclamaron el derecho a sufragio y, a pesar de la negativa de las autoridades, alcanzaron a inscribirse en La Serena.".

A lo largo de la historia, en la mayoría de las culturas, las mujeres han sido sometidas a estructuras patriarcales que les han negado derechos reconocidos a los varones.
Las leyes antiguas y los sistemas tradicionales, como el cristianismo y el islamismo, antecedentes de los sistemas modernos, han provocado la dependencia de la mujer, de forma análoga a la esclavitud, a la explotación de las clases desfavorecidas y a la mano de obra.

La Declaración de los Derechos de la Mujer y de la Ciudadana fue un texto redactado en 1791 por Olympe de Gouges parafraseando la Declaración de Derechos del Hombre y del Ciudadano del 26 de agosto de 1789, el texto fundamental de la revolución francesa. Es uno de los primeros documentos históricos que propone la emancipación femenina en el sentido de la igualdad de derechos o la equiparación jurídica y legal de las mujeres en relación a los varones.

En algunos países la mujer ha tardado muchos siglos en conseguir igualdad, aunque solo sea teórica, ante la ley. Y aun cuando la ley hable de igualdad, suele haber un gran abismo entre la teoría y la práctica.

La publicación de las Naciones Unidas titulada The World’s Women—1970-1990 dice: “Esta brecha [en la política gubernamental] ha quedado recogida en gran parte en las leyes que niegan a la mujer la igualdad con el varón en lo que respecta a sus derechos de tenencia de tierras, solicitud de préstamos y firma de contratos”. Una mujer de Uganda declaró: “Seguimos siendo ciudadanas de segunda clase... o de tercera clase más bien, pues nuestros hijos varones van delante nuestro. Hasta los burros y los tractores reciben a veces mejor trato”.

El libro Men and Women, editado por Time-Life, dice: “En 1920, la Decimonovena Enmienda de la Constitución de Estados Unidos garantizó a las mujeres el derecho al voto, mucho después que en bastantes países europeos. Pero en el Reino Unido no se les concedió ese privilegio hasta el año 1928 ...”. Como protesta por la injusticia política a la que se sometía a las mujeres, Emily Wilding Davison, sufragista británica, se echó delante del caballo del rey en el derby de 1913, y perdió la vida. Se convirtió en una mártir en la causa de la igualdad de derechos para la mujer.

El propio hecho de que en fechas tan tardías como el año 1990 el senado de Estados Unidos promulgase el decreto Violence Against Women Act, indica que las legislaturas dominadas por el varón han sido lentas a la hora de responder a las necesidades de la mujer.




</doc>
<doc id="6641" url="https://es.wikipedia.org/wiki?curid=6641" title="Pistacia vera">
Pistacia vera

El alfóncigo, alfónsigo o pistachero (Pistacia vera L.), Anacardiaceae, o algunas veces Pistaciaceae) es un árbol pequeño del género "Pistacia", originario de las regiones montañosas de Grecia, Siria, Turquía, Kirguistán, Turkmenistán, Irán, Pakistán y Afganistán occidental, que produce un importante fruto para uso culinario llamado pistacho, o alfóncigo. A la "Pistacia vera" se la confunde a menudo con otras especies del género "Pistacia", pueden diferenciarse de la "P. vera" por su distribución geográfica originaria y por sus frutos, más pequeños, con un intenso sabor a trementina y un pericarpio (cáscara) duro. 

Tiene como principal productor a Irán, con 472097 toneladas de pistachos en 2011.
El alfóncigo moderno, Pistacia vera, fue plantado por primera vez en Asia occidental. Su cultivo se extendió al mundo mediterráneo pasando por Irán Central, donde ha sido una cosecha importante durante mucho tiempo. El manuscrito escrito por Anthimus, a principios del siglo VI d. C. “De observatione ciborum” (Acerca de la observación de los alimentos) indica que los pistachos ("pistacia" en latín vulgar) eran bien conocidos en Europa hacia el final de los tiempos de la dominación Romana.

Para su venta en el mundo de habla inglesa, el alfóncigo se ha cultivado más recientemente en Australia, Nuevo México y California. El Departamento de Agricultura de los Estados Unidos (USDA) introdujo este árbol a California alrededor de 1904, pero no fue promovido como un producto comercial en California hasta 1929.

El alfóncigo crece hasta los 10 metros de altura y tiene hojas pinnadas, con (1)3 a 5 folíolos de 10-20 centímetros (4-8 pulgadas) de largo, que se desprenden en la estación fría.

Es una planta desértica y por esto tiene una alta tolerancia al suelo salino. Se ha informado que crece bien cuando se la irriga con agua que contiene 3000-4000 ppm de sales solubles. Los alfóncigos son bastante resistentes bajo las condiciones correctas, y pueden sobrevivir en temperaturas que van desde –10 °C en invierno, hasta 40 °C en verano. Necesitan estar orientados hacia el sol y en suelo bien drenado.

Los alfóncigos no se desarrollan bien en condiciones de alta humedad, sino que son susceptibles a que sus raíces se pudran durante el invierno si reciben demasiada agua y el suelo no tiene suficiente drenaje libre. Se requieren largos veranos para la adecuada maduración del fruto. Las plantas son dioicas, tienen pies masculinos y femeninos separados. Las flores son apétalas y se reúnen en inflorescencias llamadas panículas (popularmente racimos). 

El fruto es una drupa que contiene una semilla alargada, que es la porción comestible. Está cubierto por una piel carnosa fina, de color verde. En su interior, bajo una cáscara dura y blanquecina, la semilla, de un color verde pálido, está cubierta a su vez por una piel fina de tono malva, y tiene un sabor característico. Comúnmente considerada como fruto, esta semilla es para uso culinario. Cuando el fruto madura, la piel cambia del verde a un amarillo rojizo otoñal y su cáscara se rompe y abre parcialmente, de manera abrupta.

A este rompimiento se le conoce como dehiscencia/eclosión y ocurre acompañado por un sonido audible. La tendencia a esta apertura es una característica que ha sido seleccionada por los humanos. Los cultivares comerciales varían en cuanto a cómo se abren. Cada alfóncigo da en promedio 50 kg de semillas cada dos años.

Los árboles se plantan en huertos y necesitan de siete a diez años para lograr una producción considerable. La producción es alterna, o bienal, lo cual significa que la cosecha es más abundante cada dos años. La producción pico se alcanza aproximadamente a los 20 años. Por lo general, a los árboles se les poda hasta un tamaño que permita realizar la cosecha con mayor facilidad. Un árbol macho produce suficiente polen para que den frutos de ocho a doce árboles hembra. A menudo, la cosecha en los Estados Unidos se lleva a cabo mediante el uso de equipo para sacudir al árbol y hacer caer los frutos.

Los alfóncigos son vulnerables a una amplia variedad de enfermedades, entre las que destaca la infección por el hongo "Verticillium dahliae" que puede llegar a matar a la planta y por el "". Este último provoca panoja y antracnosis (es decir, mata a las flores y a los brotes jóvenes), y puede dañar huertas enteras de alfóncigos.

En California, casi todos los árboles hembra pertenecen al cultivar “Kerman” el cual suele asociarse con el cultivar macho "Peter". Una mata de un árbol maduro de estas variedades se injerta en un pie de dos años de edad. Como patrón se ha venido usando mayoritariamente otra especie del género Pistacia llamada "Pistacia atlantica", la cual está siendo sustituida en los últimos años por un híbrido de ésta con denominada UCB-1 debido a su inmunidad a la Verticilosis.

Los envíos de pistachos empacados al por mayor tienden al auto-calentamiento y a la combustión espontánea a causa de su elevado contenido de grasas y su bajo contenido de agua.

En el estudio ‘Dieta basada en pistachos mejora los parámetros de función eréctil y los perfiles lipídicos en suero en pacientes con disfunción eréctil' se demuestra los beneficios del pistacho en la fertilidad masculina. Consistió en un ensayo clínico prospectivo en el que se incluyeron un total de 17 pacientes casados con disfunción eréctil (DE) durante al menos 12 meses. Los pacientes recibieron una dieta de 100g de pistachos durante tres semanas. Efectivamente se demostró que la dieta mejoró los puntajes IIEF (Índice Internacional de Función Eréctil) sin ningún efecto secundario asociado en pacientes con DE. Además, los parámetros lipídicos mostraron mejoras estadísticamente significativas después de esta dieta. 

Los pistachos son una fuente de grasa monoinsaturadas e insaturadas, las cuáles ayudan a mantener unos niveles saludables de colesterol en sangre. Esto está directamente relacionado con la función eréctil debido a que para que se produzca la erección es necesario que los vasos sanguíneos del pene están saludables y tengan la capacidad de relajarse. El selenio es otro de las sustancias que destacan en la composición del pistacho debido a que favorece que se produzca una espermatogénesis normal.

"Producción de pistachos (toneladas en 2005)"

"Pistacia vera" fue descrita por Carlos Linneo y publicado en "Species Plantarum" 2: 1025. 1753.

La palabra alfóncigo (según la Real Academia Española) deriva del árabe hispano "alfústaq", este del árabe clásico "fustuq", este del pelvi "pistag", y a su vez este del griego πιστάκη, "pistákē".

Debido al uso culinario internacional es mucho más conocido como pistacho (del italiano "pistacchio"), siendo uno de los sabores ya clásicos para las cremas heladas y de los batidos de pistacho.





</doc>
<doc id="6643" url="https://es.wikipedia.org/wiki?curid=6643" title="Panícula">
Panícula

Una panícula o panoja es una inflorescencia racimosa compuesta de racimos que van decreciendo de tamaño hacia el ápice. En otras palabras, un racimo ramificado de flores, en el que las ramas son a su vez racimos. 
Se cataloga como un racimo de racimos, posee un raquis principal que se subdivide en raquis secundarios de los cuales se desprenden flores con pedicelo.



</doc>
<doc id="6645" url="https://es.wikipedia.org/wiki?curid=6645" title="Inflorescencia">
Inflorescencia

En botánica, la inflorescencia es la disposición de las flores sobre las ramas o la extremidad del tallo; su límite está determinado por una hoja normal. 
La inflorescencia puede presentar una sola flor, como en el caso de la magnolia o el tulipán, o constar de dos o más flores como en el gladiolo y el trigo. En el primer caso se denominan inflorescencias unifloras y en el segundo se las llama plurifloras. 

Las inflorescencias unifloras pueden ser terminales como en la magnolia, o axilares como en la camelia, y constan generalmente del pedicelo y algunas brácteas. 

Los órganos constitutivos de las inflorescencias plurifloras son las flores provistas o no del pedicelo, el eje o receptáculo común, el pedúnculo y las brácteas.
El pedicelo es la parte del tallo que sostiene la flor; a veces es muy corto, y otras veces es nulo, en cuyo caso la flor se dice sentada o sésil.
El eje o raquis es la parte alargada del tallo que lleva las ramas floríferas; si es corto y está ensanchado en forma de plato se llama receptáculo común.

El pedúnculo, es la parte del tallo que soporta el raquis o el receptáculo común. El eje que sale de la base arrosetada de la planta o de un órgano subterráneo se llama escapo (por ejemplo, "amaryllis," o "Agapanthus, Taraxacum)."

Las brácteas o hipsófilos son las hojas modificadas, generalmente menores que las hojas normales, coloreadas o verdosas, que nacen sobre el ráquis o acompañan a las flores. Algunas veces faltan, como en el caso de las crucíferas, otras veces reciben nombres especiales, tales como glumas y glumelas en las poáceas y ciperáceas, o espata en las aráceas y palmeras. En otros casos las brácteas forman órganos protectores de las flores (involucros), como la cúpula de "quercus" y el erizo del "castaño".

El prófilo o bracteola es la primera bráctea de una rama axilar, está dispuesta del lado opuesto a la hoja normal. En las monocotiledóneas es bicarenada y por el dorso, cóncavo, se adosa el eje que lleva la rama. En las espiguillas de las poáceas el prófilo recibe el nombre de pálea o "glumela superior". 

Las inflorescencias plurifloras pueden ser simples, si solo constan de un eje o receptáculo común que lleva las ramitas unifloras o compuestas cuando el eje principal lleva ramas plurifloras laterales.

De acuerdo con la forma y desarrollo del eje se distinguen dos tipos diferentes de inflorescencias: las racimosas, cuyo crecimiento es indefinido, y las cimosas, de crecimiento definido.

En ambos casos pueden estar formadas por inflorescencias elementales de igual naturaleza que la inflorescencia total (por ejemplo, racimo de racimos o espiga de espigas) y se las denomina inflorescencias homogéneas. Por el contrario, pueden estar formadas de elementos de distinta naturaleza, sea del mismo tipo (por ejemplo, racimo de espigas) o de distinto tipo (por ejemplo, cima de capítulos). En el primer caso se las denomina inflorescencias heterogéneas y, en el segundo, inflorescencias mixtas.
En los casos en que la inflorescencia pluriflora simula una sola flor, tal como el capítulo de las compuestas (asteraceae), el espádice de las aráceas o el ciatio de "euphorbia," se le denomina pseudanto. 
Las inflorescencias se denominan abiertas, racimosas o racemosas cuando los meristemas apicales de los diversos ejes mantienen su actividad mientras dura el crecimiento de estas. En este tipo de inflorescencias todas las flores son laterales. El eje o raquis de la inflorescencia crece indefinidamente mientras a los costados se producen yemas florales que se abren a medida que aquel se desarrolla. Los botones apicales, o del centro de la inflorescencia, son los últimos en abrirse. La marcha de la floración es centrípeta. 
En este tipo de inflorescencias se distinguen cuatro clases diferentes que ofrecen una notable diversidad de formas: el racimo, la espiga, la umbela y el capítulo.

Las inflorescencias se denominan cerradas o cimosas cuando los meristemas apicales de los diversos ejes se consumen en la producción de flores. Por debajo de la yema terminal convertida en flor, otras yemas laterales producen nuevos ejes. En este tipo de inflorescencias todas las flores son terminales. En las inflorescencias cimosas la flor terminal del eje principal es la primera en abrirse, seguida de las flores terminales de los ejes de segundo orden, tercero, etc. 

Por su aspecto general recuerdan a inflorescencias racimosas pero el desarrollo de la floración es diferente, pues comienza por la flor central y termina en las laterales, siguiendo una marcha centrífuga. 

Es común, además, que la bráctea aparezca del lado contrario a la rama florífera; esto se debe a que cada eje que va naciendo remata en una flor y cesa de crecer, comenzando el crecimiento de otra rama en la axila formada por la hoja y la ramita floral.
El número de ramas floríferas que se desarrolla debajo de la primera flor, o donde se ha interrumpido el crecimiento vegetativo es variable entre una o más. En el caso de ser única, la inflorescencia se llama monocasio (por ejemplo el "Iris"), si son dos dicasio (por ejemplo las cariofiláceas), si son tres o más se denomina pleiocasio (por ejemplo "geranium)." Las inflorescencias cimosas comprenden muchas clases, entre las cuales las principales son la cima helicoidal, la cima circinada, la cima dicotómica, la cima umbeliforme, la cima corimbiforme y la cima capituliforme.

Se trata de inflorescencias que no siguen ningún patrón específico para la ramificación del eje floral. Son muy raras, y se presentan solo en algunos taxones.

Las inflorescencias se diferencian del resto del tallo vegetativo por algunas características como: 





</doc>
<doc id="6646" url="https://es.wikipedia.org/wiki?curid=6646" title="Ápice vegetal">
Ápice vegetal

En botánica o zoología, ápice designa el extremo superior o punta (del latín "apex", con el mismo significado) de la hoja, del fruto, del pólipo, etc. El adjetivo "apical" se puede aplicar a flores, frutos, o cnidarios, con el significado del "más distal". "Distal", a su vez, es lo que se sitúa hacia el extremo opuesto a la base o parte basal del órgano en cuestión, como la zona oral de los antozoos. <br>

En un órgano, por ejemplo una hoja, hay que distinguir entre el ápice orgánico, por donde puede crecer distalmente el órgano, dotado de tejido meristemático, y el ápice geométrico, que es simplemente el punto más distanciado de la base.


</doc>
<doc id="6649" url="https://es.wikipedia.org/wiki?curid=6649" title="Baklava">
Baklava

El baklava, baklawa o baclava (del Griego "baklava"), es un pastel turco elaborado con una pasta de pistachos o nueces trituradas, distribuida en una masa filo y bañado en almíbar o jarabe de miel. Existen diversas variedades que incorporan avellanas y almendras, entre otros frutos secos y kaymak, procedente de la cocina turca. Puede encontrarse con diferentes nombres en la gastronomía de Oriente Próximo y países árabes, de Grecia, del Subcontinente Indio y de los Balcanes.

La historia del baklava se remonta a la antigua Mesopotamia. Se cree, sin embargo, que los asirios, cerca del siglo VII a. C., fueron los primeros en colocar unas pocas capas de masa de pan junto a nueces trituradas entre esas capas, añadiendo un poco de miel y horneándolo en sus hornos de madera primitivos. Esta temprana versión del baklava se cocinaba sólo para ocasiones especiales, siendo de hecho considerado históricamente una comida para las clases acomodadas hasta mediados del siglo XIX.

Los marinos y mercaderes griegos que viajaban hacia Mesopotamia, pronto descubrieron las delicias del baklava y llevaron la receta a Atenas. 

El mayor aporte de los griegos a la elaboración de la masa, es la creación de una técnica de pastelería que hace posible amasarla hasta dejarla fina como una hoja, comparada a la áspera textura, similar a la del pan, de la elaboración asiria. Las capas de masa que lo forman son tradicionalmente 33 en referencia a los años de vida de Cristo en la tierra. En el siglo XV, los otomanos conquistaron Constantinopla, expandiendo sus territorios a gran parte de los antiguos territorios asirios y al reino Armenio por completo. Por cuatrocientos años, desde el siglo XVI hasta la declinación del Imperio otomano en el siglo XIX, las cocinas del Palacio Imperial Otomano en Constantinopla, se convirtieron en el cenit culinario del Imperio. Vryonis identifica a las antiguos "gastris", "kopte", "kopton", o "koptoplakous", mencionados en los "Deipnosofistas", como baklava y lo llama un "favorito Bizantino". Sin embargo, Perry muestra que aunque los "gastris" contenían un relleno de nueces y miel, no incluían masa, sino una mezcla de miel y sésamo similar al "pasteli" o "halva" moderno.

Perry muestra evidencias para demostrar que las capas de masa fueron creadas por los turcos en Asia central y argumenta que el "enlace perdido" entre las capas de masa (que no incluye nueces) y las moderna masa hojaldrada (o masa phyllo) con la pasta de nueces que constituyen el moderno baklava, es el alimento azerbaiyano "Baki pakhlavası". El desarrollo posterior habría ocurrido en las cocinas del Palacio de Topkapi, donde los jenízaros tenían una celebración anual llamada "Baklava Alayı".

Buell argumenta que la palabra "baklava" es de origen mongol, y menciona una receta en un libro de cocina chino escrito en 1330, bajo la dinastía Yuan.
Varias láminas de masa filo se barnizan con mantequilla derretida y se esparce el pistacho o nuez picada sobre ellas. Sobre esta se colocan otras láminas de masa filo, repitiendo el procedimiento y una vez conseguida la altura necesaria, el pastel se corta en triángulos de igual tamaño. La masa filo se humedece con agua, para luego hornear a 180 °C por 40 minutos hasta conseguir el dorado. Se retira del horno y se deja enfriar por 15 minutos. Luego se cubre con almíbar preparado con agua, canela, azúcar y jugo de limón. Se deja reposar durante una hora, hasta que se impregne. El baklava suele servirse acompañado de té o café.

"Antep baklavası" o sea "Baklava de (la ciudad de Gazi) Antep" ha sido el primer producto alimenticio de Turquía en ser reconocido y registrado como una denominación de origen por la Comisión europea.



Baklava


</doc>
<doc id="6654" url="https://es.wikipedia.org/wiki?curid=6654" title="Alfonso VI de León">
Alfonso VI de León

Alfonso VI de León, llamado «el Bravo» (1040/1041-Toledo, 1 de julio de 1109), hijo de Fernando I de León y de su esposa, la reina Sancha, fue entre 1065 y 1072 en un primer reinado, y entre 1072 y 1109 en un segundo, entre 1071 y 1072 y también entre 1072 y 1109, y entre 1072 y 1109.

Durante su reinado, se produjo la conquista de Toledo (1085) y tuvieron lugar las batallas de Sagrajas y Uclés, que constituyeron sendas derrotas para las mesnadas leonesas y castellanas. En la segunda falleció el heredero del rey, el infante Sancho Alfónsez.

Hijo del rey Fernando I y de su esposa, la reina Sancha de León, Alfonso era un «infante leonés con sangre navarra y castellana». Sus abuelos paternos fueron Sancho Garcés III, , y su esposa la reina Muniadona —hija de Sancho García, conde de Castilla— y los maternos fueron el rey Alfonso V de León y su esposa la reina Elvira Menéndez. 

El año de su nacimiento no está registrado en la documentación medieval. Un texto coetáneo del cronista anónimo de Sahagún que conoció al monarca y se halló presente cuando murió, relata que falleció con 62 años de vida y 44 de reinado, por lo tanto, habría nacido en el segundo semestre de 1047 o en la primera mitad de 1048. 

Según el Silense, la primogénita, Urraca, vino al mundo cuando sus padres aún eran condes de Castilla, antes de reinar, así que habrá nacido en 1036/37. El segundogénito, Sancho, habrá nacido en el segundo semestre de 1038 o en 1039. La infanta Elvira pudo haber nacido en 1039/40, Alfonso en 1040/41, y el más pequeño de los hermanos, García, entre 1041 y el 24 de abril de 1043 cuando el rey Fernando, en una donación a la abadía de San Andrés de Espinareda, menciona a sus cinco hijos. Todos ellos, excepto Elvira, confirman un documento en el monasterio de San Juan Bautista de Corias el 26 de abril de 1046.

Todos los hijos del rey Fernando, según el Silense, fueron educados en las artes liberales y los varones también en las armas, el «arte de correr caballos al uso español» y en la caza. El clérigo Raimundo fue el encargado del aprendizaje de Alfonso en las letras. Ya siendo rey, Alfonso le nombró y se refirió a él como "magistro nostro, viro nobile et Deum timenti". Posiblemente Alfonso pasó largas temporadas en Tierra de Campos donde aprendió el arte de la guerra y lo que se esperaba de un caballero junto con Pedro Ansúrez, hijo de Ansur Díaz y sobrino del conde Gómez Díaz de Saldaña, todos del linaje de los Banu Gómez.

Como segundo hijo varón del rey de León y conde de Castilla, Fernando I, y de la reina Sancha de León, a Alfonso no le habría correspondido heredar. A finales de 1063, probablemente el 22 de diciembre, aprovechando que numerosos magnates se habían reunido en la capital del reino para la consagración de la basílica de San Isidoro de León, Fernando I convocó una "Curia Regia" para dar a conocer sus disposiciones testamentarias, en las cuales decidió repartir su patrimonio entre sus hijos, reparto que no se haría efectivo hasta la muerte del monarca con el fin de evitar que surgieran discordias después de su muerte.


El historiador Alfonso Sánchez Candeira sugiere que, aunque no se conocen las razones que llevaron al rey Fernando a dividir los reinos, heredando Alfonso el de León que llevaba implícito el título imperial, el reparto pudo ser debido a que consideró conveniente que a cada hijo varón le entregara en herencia la región donde fueron educados y donde pasaron sus primeros años. En todo caso, la consecuencia principal de la decisión paterna fue el desencadenamiento de luchas fratricidas por el poder que duraron siete años.

Tras su coronación en la ciudad de León en enero de 1066, Alfonso tuvo que enfrentarse con los deseos expansionistas de su hermano Sancho quien, como primogénito, se consideraba el único heredero legítimo de todos los reinos de su padre. Los conflictos se inician cuando el 7 de noviembre de 1067 fallece la reina Sancha, suceso que abrirá un periodo de siete años de guerra entre los tres hermanos y cuyo primer acto tendrá lugar el 19 de julio de 1068 cuando Alfonso y Sancho se enfrentan en Llantada, en un juicio de Dios en el que ambos hermanos pactan que el que resultase victorioso obtendría el reino del derrotado. Aunque Sancho vence, Alfonso no cumple con lo acordado, a pesar de lo cual las relaciones entre ambos se mantienen como demuestra el hecho de que Alfonso acudiera, el 26 de mayo de 1069, a la boda de Sancho con una noble inglesa llamada Alberta y donde ambos decidieron unirse para repartirse el reino de Galicia que le había correspondido a García, el menor de los hijos de Fernando I.

Con la complicidad de Alfonso, su hermano Sancho entra en Galicia en 1071 y, tras derrotar a su hermano García, lo apresa en Santarém y lo encarcela en Burgos hasta que es exiliado a la taifa de Sevilla, gobernada por Al-Mutámid. Tras eliminar a su hermano, Alfonso y Sancho se titulan reyes de Galicia y firman una tregua.
La tregua se rompe con la batalla de Golpejera en 1072. Las tropas de Sancho salen victoriosas, pero este decide no perseguir a su hermano. Alfonso fue hecho prisionero y encarcelado en Burgos. Posteriormente es trasladado al monasterio de Sahagún, donde se le rasura la cabeza y se le obliga a tomar la casulla. Gracias a la intercesión de su hermana Urraca, Sancho y Alfonso llegaron a un acuerdo para que Alfonso marchara y se refugiase en la taifa de Toledo bajo la protección de su vasallo, el rey Al-Mamún y acompañado por el fiel Pedro Ansúrez, amigo de su infancia, y sus dos hermanos Gonzalo y Fernando.

Alfonso, desde su exilio en Toledo, logra el apoyo tanto de su hermana Urraca como de la nobleza leonesa que se hacen fuertes en la ciudad de Zamora, señorío que Alfonso le había otorgado anteriormente, obligando a Sancho, en 1072, a sitiar la ciudad para someterla después de que Urraca se negara a canjearla por otras plazas que le había ofrecido Sancho, deseoso de controlar la plaza fuerte de Zamora, «clave para la futura expansión al sur del Duero». En el transcurso del asedio el rey Sancho recibió la muerte en octubre de ese año. La tradición o leyenda narra el episodio con el detalle de que durante el cerco, un noble zamorano o gallego llamado Vellido Dolfos se presentó ante el rey como desertor y, con la excusa de mostrarle los puntos débiles de las murallas, lo separó de su guardia y consiguió acabar con su vida de una lanzada. Aunque no hay constancia alguna de que la muerte de Sancho se debiera a una traición más que a un engaño, ya que Dolfos era enemigo de Sancho, su asesinato fue debido a un lance bélico propio de la situación de sitio y no se produjo en las murallas sino en un bosque cercano donde Dolfos llevó al rey castellano alejándolo de su protección armada. La muerte violenta de su hermano Sancho, que no dejó descendencia, permitió a Alfonso recuperar su trono y reclamar para sí Castilla y Galicia.

Aunque Rodrigo Díaz de Vivar, hombre de confianza y portaestandarte del rey Sancho, se halló en el sitio de Zamora, no consta cual había sido su actuación. Tampoco se puede atribuir a Alfonso, que estaba desterrado y alejado de los hechos, la muerte de su hermano, «pero los juglares y el romancero rellenaron este vacío con hermosas creaciones literarias desprovistas de cualquier realidad histórica». 

En este momento, la "Leyenda de Cardeña" acerca del Cid () sitúa la jura exculpatoria de la posible participación de Alfonso en el asesinato de su hermano, que tomó El Cid en la iglesia de Santa Gadea de Burgos (Jura de Santa Gadea) y que provocaría una relación de desconfianza mutua entre ambos, aunque Alfonso intentó un acercamiento al ofrecerle en matrimonio a su sobrina Jimena Díaz junto a la inmunidad de sus heredades. Estos hechos y sus consecuencias llegarían con el tiempo a ser considerados históricos por multitud de cronistas e historiadores, aunque en la actualidad la mayor parte de estos rechazan la historicidad del episodio. 

La muerte de Sancho también fue aprovechada por García para recuperar su propio trono, pero al año siguiente, el 13 de febrero de 1073, fue llamado por Alfonso a una reunión, y fue apresado y encarcelado de por vida en el castillo de Luna, donde fallecería finalmente el 22 de marzo de 1090. Eliminados los dos hermanos, Alfonso no tuvo problema en obtener la lealtad tanto del alto clero como de la nobleza de sus territorios; para confirmar esta, pasó los dos años siguientes visitándolos.

En 1087 o 1088, estalló una revuelta en Galicia contra la concesión al yerno de Alfonso de esta región. El alzamiento fue sofocado y sirvió a Alfonso para reorganizar el episcopado del oeste del reino; el obispo de Santiago fue depuesto, junto a otros dos de los siete de la zona.

Consolidado en el trono leonés, y con el título de emperador que heredaba de la tradición neogoticista leonesa, Alfonso VI dedica los siguientes catorce años de su reinado a engrandecer sus territorios mediante conquistas como la de Uclés y los territorios de los Banu Di-l-Nun. También se tituló, desde 1072, "rex Spanie".
Alfonso se apoyó en un grupo de nobles que lo sostuvieron durante su reinado. Además del fiel Ansúrez, este grupo lo integraron su cuñado Martín Alfonso, señor de Simancas y Tordesillas y hermano de Eylo Alfonso, el álférez real Pedro González de Lara y Fernando Díaz. Otras figuras como Rodrigo Díaz de Vivar o el yerno del rey, el conde Raimundo de Borgoña, tuvieron una influencia secundaria en el círculo de la corte alfonsina.

Su primer movimiento lo realiza en 1076, cuando al fallecer asesinado el monarca navarro Sancho Garcés IV, la nobleza navarra decide que el trono no pase a su hijo menor de edad, sino a uno de los nietos de Sancho III de Pamplona: Alfonso VI o Sancho Ramírez de Aragón, que invadieron el reino navarro. Tras llegar a un acuerdo, Sancho Ramírez es reconocido como rey de Navarra y Alfonso se anexiona los territorios de Álava, Vizcaya, parte de Guipúzcoa y La Bureba, adoptando en 1077 el título de "Imperator totius Hispaniae" ('Emperador de toda España').

Pero su gran expansión territorial la hará a costa de los reinos taifas musulmanes, para lo cual Alfonso siguió con la práctica de explotación económica mediante el sistema de parias consiguiendo que la mayor parte de los reinos de taifas de la España musulmana fuesen sus tributarios, práctica a la que unió la presión militar. En el 1074 probablemente recuperó el pago de las parias de Toledo y ese mismo año, ayudado por tropas de esta ciudad, taló las tierras de la taifa granadina, que como consecuencia comenzó también a pagar tributo a Alfonso. En el 1076, el emir de Zaragoza, que deseaba apoderarse de Valencia sin que lo estorbase Alfonso, se avino a reanudar el pago de las parias. En el 1079, se adueñó de Coria.

Una de las iniciativas de estos años, que ha pasado a la historia como la traición de Rueda, terminará en fracaso. Tuvo lugar en 1083 en el castillo de Rueda de Jalón, cuando Alfonso recibe noticias de que el alcaide de dicha fortaleza, la cual pertenecía al reino Taifa de Zaragoza, pretende rendirla al rey leonés. Las tropas que envía Alfonso son emboscadas al entrar en el castillo y mueren varios de sus principales magnates.

En 1074 había fallecido envenenado en Córdoba su vasallo y amigo, el rey de la taifa de Toledo Al-Mamún a quien sucedió su nieto Al-Qádir quien, en 1084, solicitó por segunda vez la ayuda de Alfonso ante un levantamiento que pretendía derrocarlo. Alfonso aprovechó el llamamiento de ayuda del rey taifa para sitiar Toledo, ciudad que caería el 25 de mayo de 1085 y al-Qádir fue enviado como rey a Valencia bajo la protección de Álvar Fáñez. Para facilitar esta operación y recuperar el pago de las parias de la ciudad, que había dejado de pagarlas el año anterior, Alfonso asedió Zaragoza en la primavera del 1086. A comienzos de marzo, Valencia aceptó a al-Qádir; Játiva trató de resistir solicitando el socorro del reyezuelo de Tortosa y Lérida, que realizó una fallida incursión por la región antes de retirarse acosado por las huestes de Fáñez. 

Tras esta importante conquista, el monarca se tituló emperador de las dos religiones y como gesto ante la importante población musulmana de la ciudad se compromete, además de respetar las propiedades de estos, a reservarles la mezquita mayor para su culto. Esta decisión será revocada por el recién nombrado arzobispo de Toledo, Bernardo de Sedirac, aprovechando una ausencia del monarca de Toledo y valiéndose para ello del apoyo de la reina Constanza de Borgoña. El arzobispo transformó la mezquita en catedral.

La ocupación de Toledo, que permite a Alfonso VI incorporar el título de rey de Toledo a los que ya ostentaba ("victoriosissimo rege in Toleto, et in Hispania et Gallecia"), llevó a la toma de ciudades como Talavera y de fortalezas como el castillo de Aledo. También ocupa la entonces ciudad de "Maŷriṭ" en 1085 sin resistencia, probablemente mediante capitulación. La incorporación del territorio situado entre el Sistema Central y el río Tajo, servirá de base de operaciones para la corona leonesa, desde donde podía emprender un mayor hostigamiento contra las taifas de Córdoba, Sevilla, Badajoz y Granada.

La conquista de la extensa y estratégica taifa toledana, el control de Valencia y la posesión de Aledo, que aisló Murcia del resto de al-Ándalus, preocuparon a los soberanos musulmanes de la península. La presión militar y económica sobre los reinos taifas hizo que los reyes de las taifas de Sevilla, Granada, Badajoz y Almería decidiesen pedir ayuda a los almorávides que, a finales de julio del 1086, al mando del emir Yúsuf ibn Tasufín, cruzaron el estrecho de Gibraltar y desembarcaron en Algeciras.

En Sevilla, el ejército almorávide se unió a las tropas de los reinos taifas y juntos se dirigieron a tierras extremeñas donde, el 23 de octubre de 1086, se enfrentaron en la batalla de Zalaca a las tropas de Alfonso VI, que se había visto obligado a abandonar el sitio a que sometía a la ciudad de Zaragoza. A él se reunió también Álvar Fáñez, a quien se había llamado desde Valencia para unirse a las fuerzas del rey. La batalla se saldó con la derrota de las tropas cristianas, que regresaron a Toledo para defenderse, pero el emir no supo aprovechar la victoria, pues regresó apresuradamente a África a causa de la muerte de su hijo. El choque marcó el comienzo de una nueva etapa en la península que duró unas tres décadas, en las que la iniciativa militar pasó a los almorávides y el reino de Alfonso tuvo que mantenerse a la defensiva; logró en todo caso retener Toledo, objetivo principal de las acometidas almorávides. Estas no cesaron durante las dos últimas décadas del reinado de Alfonso y detuvieron la expansión leonesa por la península que había acaecido en tiempos de su padre Fernando y durante la primera parte de su propio gobierno.

Alfonso solicitó a los reinos cristianos de Europa la organización de una cruzada contra los almorávides que habían recuperado casi todos los territorios que Alfonso había conquistado, con la excepción de Toledo, ciudad en la que Alfonso se hacía fuerte. Para reforzar su posición, se reconcilió con el Cid, que acudió a Toledo a finales del 1086 o principios del 1087. Como consecuencia de la grave derrota, las taifas andalusíes dejaron de pagarle parias; esto supuso un grave menoscabo de los fondos militares de la Corona leonesa. Como consecuencia de esta falta de fondos, el rey hubo de confiar la defensa de la frontera cada vez más a grandes señores: el Cid en el este, Álvar Fáñez entre Valencia y Toledo y en esta, Pedro Ansúrez. Más al oeste, la misma tarea quedó en manos del conde Raimundo, yerno del monarca. El Cid logró volver a someter a las taifas levantinas a Alfonso a lo largo de los dos años siguientes.

Aunque la cruzada no llegó finalmente a organizarse, sí conllevó la entrada en la península de un importante número de cruzados entre los que destacaban Raimundo de Borgoña y Enrique de Borgoña, que contrajeron matrimonio con dos hijas de Alfonso, Urraca (1090) y Teresa (1094), lo que originó la implantación de la dinastía borgoñona en los reinos peninsulares. Algunos de los cruzados sitiaron infructuosamente Tudela en el invierno del 1087, antes de retirarse. Ese mismo año, el rey aplastó una revuelta en Galicia, que pretendía liberar a su hermano García.

En 1088 Yusuf ibn Tasufin cruzó por segunda vez el estrecho, pero fue derrotado en el sitio de Aledo y sufrió la deserción de muchos de los reyes de las taifas musulmanas, lo que motivó que, en su próxima venida, el emir llegase con la decisión de destituirlos a todos y quedarse él como único rey de todo al-Ándalus. Alfonso empleaba ese castillo como base de correrías por las tierras orientales de la taifa sevillana, actividad que continuó del 1087 al 1090. Gracias al fracaso musulmán ante Aledo, Alfonso había podido reanudar el cobro de las parias, mediante amenazas de talar el territorio granadino en el caso del soberano de esta ciudad y corriendo el territorio sevillano para recuperar la sumisión de la ciudad del Guadalquivir. Enemistado definitivamente Abd Allah ibn Buluggin de Granada con Ibn Tasufin, Alfonso se comprometió a socorrerlo a cambio de su sumisión. Al igual que Granada, Zaragoza y otros territorios musulmanes del este peninsulares reanudaron el pago de parias al rey leonés. Durante los cinco años siguientes, Alfonso se presentó como el defensor de la independencia de las taifas peninsulares frente a los almorávides, si bien la imposición de parias complicaba los pactos con los emires andalusíes. Estos tributos, empero, eran fundamentales para garantizar los ingresos reales, base de la munificencia real que sostenía en parte el poder y el prestigio del monarca. Los intentos de imponerlos de nuevo a la taifa sevillana mediante una incursión en su territorio resultó no solo un fracaso, sino contraproducente: el emir sevillano se negó a abonarlos y llamó en su auxilio a los almorávides. Parece que Alfonso no participó personalmente en esta campaña.

En junio del 1090, los almorávides realizaron un tercer desembarco: destituyeron al rey de Granada, vencieron a al-Mamún, gobernador de Córdoba, y tras la batalla de Almodóvar del Río, entraron en Sevilla enviando al exilio a su rey al-Mutámid. Su asedio de Toledo resultó infructuoso y, ante la tardía llegada de Alfonso en socorro de la ciudad en agosto, lo abandonaron. En la segunda mitad del año y la primera del siguiente, se apoderaron de todas las taifas sureñas; Alfonso, que se había comprometido a ayudar al soberano de Sevilla, fracasó en este propósito. El rey sufrió reveses en todos los frentes: en el este no consiguió apoderarse de Tortosa por la tardía llegada de la flota genovesa que debía participar en su toma; más al sur, al-Qádir fue depuesto en una revuelta; en el sur, su relación con Zaida, nuera del emir sevillano, no sirvió para favorecer su imagen de paladín del islam peninsular frente a los almorávides; finalmente, en el oeste, la alianza con el emir de Badajoz no bastó para librar a este de la conquista de su territorio por los magrebíes. Como precio del pacto, Alfonso había obtenido Lisboa, Sintra y Santarém, pero las perdió en noviembre del 1094, cuando su yerno Raimundo, encargado de su defensa, fue derrotado por el ejército almorávide que había tomado Badajoz poco antes. Alfonso se había apoderado fácilmente de las tres a finales de abril y comienzos de mayo del 1093 y, con ellas, se había adueñado de todo Portugal al norte del Tajo. La única buena nueva para Alfonso la proporcionó el Cid, que consiguió recuperar Valencia en junio del 1094 y vencer al ejército almorávide que avanzó contra él en octubre en la batalla de Cuarte; esta victoria fijó la frontera oriental durante aproximadamente una década.

En 1093 Raimundo había recibido el gobierno de amplios territorios: toda la costa atlántica gallega y portuguesa al norte del Tajo. No obstante, el nacimiento de Sancho Alfónsez ese año —quizá el 13 de septiembre— y la muerte de la reina Constanza supusieron serios reveses para las ambiciones del conde, pues lo alejaron del poder y menguaron su influencia en la corte. Como colofón, Alfonso decidió desposar en las Navidades del 1094 a una lombarda, Berta, en vez de escoger nuevamente a una mujer francesa, medida que debió tomar para reducir la influencia borgoñona en León. En agosto y septiembre del 1095, los reyes recorrieron las tierras de Raimundo. Seguidamente, el rey desbarató con astucia una conjura en su contra de sus yernos Raimundo y Enrique, que deseaban repartirse el reino a su muerte. Para enemistarlos, Alfonso casó a su hija Teresa con Enrique en el 1096, y concedió al matrimonio el gobierno del condado de Portugal, hasta entonces dominado por Raimundo, que comprendía las tierras desde el Miño hasta Santarém, mientras que el gobierno de Raimundo se limitaba a Galicia. La medida privó a Raimundo de la mitad de sus tierras e hizo de Enrique un nuevo rival por el trono leonés.

Otro aspecto que le permitió a Alfonso mantener su autoridad en el reino fueron las buenas relaciones que mantuvo con el papado. Los apuros del papa Urbano II por sus enfrentamientos con el emperador alemán Enrique IV y el rey francés Felipe I favorecieron el entendimiento entre el pontífice y el soberano leonés, interesado como lo habían estado sus antecesores en el trono en dominar la iglesia del reino y sus abundantes recursos.

En 1097 se produjo un cuarto desembarco almorávide. La noticia la recibió Alfonso VI cuando probablemente se dirigía a Zaragoza para prestar ayuda a su vasallo el rey al-Musta'in II en su enfrentamiento con el recién coronado Pedro I de Aragón. El objetivo almorávide era nuevamente Toledo, en cuyo camino se encontraba el castillo de Consuegra y donde, el 15 de agosto, se encontraron con las tropas cristianas que nuevamente resultaron derrotadas en la batalla homónima. Poco después, los almorávides vencieron también a Álvar Fáñez en la región de Cuenca, el otro extremo de la línea defensiva leonesa. Al poco y sin lograr expugnar Consuegra, donde se habían refugiado los restos del ejército real, los almorávides se retiraron. Las derrotas no comportaron pérdidas territoriales ni incursiones enemigas y el rey volvió a Sahagún algo después, en septiembre u octubre. Se cree que la corte pasó la Navidad en Santiago y no en León o Sahagún, como era habitual. El verano del año siguiente, Alfonso se dedicó a reforzar la frontera suroriental del reino, entre las plazas musulmanas de Atienza, Sigüenza y Medinaceli, la propia de San Esteban de Gormaz y la sierra de Guadarrama, para estorbar las comunicaciones del movimiento enemigo entre el sur y Zaragoza.

En el 1099, los almorávides conquistaron gran parte de los castillos que defendían la zona toledana —Consuegra cayó en junio— y al año siguiente trataron de apoderarse de Toledo, infructuosamente. La campaña de 1099 supuso la pérdida para los leoneses de la mitad meridional de la taifa toledana conquistada la década anterior, la fijación de la frontera aproximadamente en el Tajo y, en consecuencia, que Toledo quedase en una situación vulnerable, como plaza fronteriza. Alfonso, que se cree que dirigió la defensa de la frontera meridional en la campaña del 1099, se retiró pronto a Sahagún, en octubre; pasó allí la Navidad y la reina Berta murió poco después, a principios del 1100. La reina debía llevar tiempo enferma porque el rey tardó poco en escoger nueva consorte.

Dirigió la defensa de Toledo en el 1100 Enrique, el yerno de Alfonso, pues este había marchado a Valencia a inspeccionar sus defensas; el Cid había fallecido el año anterior y el gobierno de la ciudad recaía entonces en su viuda, Jimena. La pérdida de Valencia auguraba también la conquista almorávide de Zaragoza y con ella, la pérdida de la última fuente de parias para los leoneses y una nueva amenaza a la frontera oriental del reino.

A principios de 1101 falleció Urraca, la última de los hermanos del rey que aún quedaba con vida y que había sido una estrecha consejera del soberano. Alfonso debió permanecer en Sahagún al menos hasta la Pascua, que se año cayó a finales de abril. La principal labor militar del año fue el reforzamiento de Salamanca y Ávila como bastiones ante la posible pérdida de Toledo; las dos localidades debían servir de protección occidental de la zona al sur del Duero, aún en asimilación. La primera debía vigilar la antigua Vía de la Plata que comunicaba Mérida con Zamora y la segunda, el acceso a la región a través del puerto de Arrebatacapas en la sierra de Guadarrama. La tarea quedó encomendada al conde Raimundo.

En 1102, Alfonso envió tropas en auxilio de Valencia frente a la amenaza almorávide. La batalla entre leoneses y almorávides tuvo lugar en Cullera y terminó sin un claro vencedor, aunque Valencia cayó en manos almorávides ante lo costoso que resultaba para Alfonso defender esta plaza. Alfonso supervisó la evacuación de la ciudad en marzo y abril, y le prendió fuego antes de marcharse; en mayo, los almorávides se adueñaron de ella. La pérdida de Valencia auguraba la caída en manos almorávides de Zaragoza y, con ella, el surgimiento de una grave amenaza para la frontera oriental del reino. El emir zaragozano, ante el cariz que tomaban los acontecimientos, envió a su hijo a pactar con Ibn-Tasufin y dejó de pagar parias a Alfonso. Para proteger la zona al sur del Duero por el este, el rey leonés nombró un obispo para Osma en el 1102 y cercó y tomó Medinaceli (en julio del 1104, tras largo asedio), plaza clave que permitía el ataque hacia la región toledana desde el este a lo largo del valle del Jalón, en el 1104. La localidad, además, estaba en el camino que unía Zaragoza con Toledo y, allende el Tajo, con Córdoba y Sevilla. En el 1104, 1105 y 1106, realizó varias incursiones en el territorio andalusí; en la última alcanzó Málaga y pudo escoltar en su vuelta a mozárabes que se instalaron en su reino como repobladores. En el otoño del 1106, estuvo en el este de Castilla y luego retornó como era habitual a León. En 1107 no hubo combates destacados. El rey pasó las Navidades de ese año en Sahagún y en mayo proclamó heredero a Sancho Alfónsez en León.

En 1108 las tropas del almorávide Tamim, gobernador de Córdoba e hijo de Yúsuf ibn Tasufín, se dirigieron nuevamente contra los territorios cristianos, pero la plaza elegida no fue Toledo, sino Uclés. Alfonso se encontraba en Sahagún, recién casado, mayor y con una vieja herida que le impedía montar a caballo. Al mando del ejército se puso Álvar Fáñez, gobernador de las tierras de los Banu Di-l-Nun, y le acompañó el infante heredero Sancho Alfónsez. Los ejércitos se enfrentaron en la batalla de Uclés, donde las tropas cristianas sufrieron otra dura derrota y en la que, además, pereció el infante heredero al trono, lo que tuvo como consecuencia un parón de treinta años en la reconquista y la independencia del condado portugués. La situación militar también era grave, pues los almorávides se apoderaron casi de inmediato de toda la franja defensiva del Tajo de Aranjuez a Zorita y se produjeron levantamientos de la población musulmana de la región. Alfonso se apresuró en acudir al sur, para defender las tierras fronterizas, aunque en septiembre, ante la falta del previsto asalto enemigo a Toledo, ya había vuelto a Sahagún.

En 1067 se negoció su matrimonio con Ágata de Normandía, hija del rey Guillermo I de Inglaterra y de Matilde de Flandes, pero su muerte prematura frustró el proyecto. 

Según el obispo Pelayo de Oviedo, coetáneo del rey, en su "Chronicon regum legionensium", Alfonso VI tuvo cinco esposas y dos concubinas "nobilissimas". Las esposas fueron, según el obispo, Inés, Constanza, Berta, Isabel y Beatriz y las concubinas Jimena Muñoz y Zaida. 

En 1069 se firmó el acuerdo de esponsales con Inés de Aquitania, hija de Guido Guillermo VIII, duque de Aquitania y y de Matilde de la Marche. Inés apenas contaba con diez años de edad y hubo que esperar hasta que cumpliese los catorce años para celebrar el matrimonio que tuvo lugar a finales de 1073 o principios de 1074. Aparece en diplomas reales hasta el 22 de mayo de 1077 y a partir de esa fecha, el rey aparece solo en la documentación. Inés falleció el 6 de junio de 1078. 

Reilly sugiere que el año anterior se había anulado el matrimonio, probablemente por la falta de hijos. Sin embargo, Gambra discrepa y opina que no existen fuentes fidedignas que avalen tal aseveración. La información sobre el supuesto repudio solo aparece en un tomo de "L'art de vérifier les dates" y, según Gambra, «Se hace imposible, a falta de mejores referencias, conceder crédito a la afirmación del repudio de Inés». Además, señala que el Tudense, en su "Chronicon mundi", indica que la reina fue sepultada en Sahagún. Finalmente, señala que «si realmente se hubiese producido un hecho de dicha envergadura, carecería de sentido [...] que Alfonso VI contrajese matrimonio inmediatamente con otra princesa de la familia de Inés». Inés y la siguiente esposa del rey, Constanza, eran primas en tercer grado, ambas descendientes del duque Guillermo III de Aquitania.

Por otro lado, Orderico Vital, cronista inglés del , decía que el matrimonio de Inés y el rey Alfonso había sido anulado en 1080 por razones de consanguinidad y que Inés había vuelto a casar en 1109 con Elías de la Flèche, conde de Maine. Según Jaime de Salazar y Acha, la que casó con el conde de Maine fue Beatriz, la última esposa de Alfonso VI.

Después de la muerte de Inés, el rey mantuvo una relación con Jimena Muñoz, concubina "nobilissima", según el obispo Pelayo de Oviedo de la cual nacieron dos hijas entre 1078 y 1080.


Contrajo matrimonio por segunda vez a finales de 1079 con Constanza de Borgoña, con quien aparece por primera vez el 8 de mayo de 1080, viuda, sin hijos, del conde Hugo II de Chalon-sur-Saône, e hija de Roberto el Viejo, duque de Borgoña y Hélie de Semur, y bisnieta de Hugo Capeto, rey de Francia. También era sobrina del abad Hugo de Cluny, y tía de Enrique de Borgoña. Fue precisamente el deseo del rey de estrechar lazos con la poderosa abadía de Cluny el motivo del desposorio.

Fruto de este matrimonio, que duró hasta la muerte de la reina en 1093, nacieron seis hijos, cinco de ellos fallecidos en la niñez, y la única que sobrevivió fue:


El obispo Pelayo de Oviedo menciona a Zaida como una de las dos concubinas del rey y dice que fue hija de Al-Mu'támid rey taifa de Sevilla. Zaida, en realidad, era su nuera, casada con su hijo Abu Nasr al-Fath al-Ma'mun, rey de la taifa de Córdoba. En marzo de 1091 los almorávides sitiaron la ciudad de Córdoba. El marido de Zaida, que murió durante el asedio el 26/27 de ese mes, como medida de precaución, envió a su esposa Zaida y sus hijos a Almodóvar del Río. Después de enviudar, Zaida buscó la protección en la corte del rey leonés y ella y sus hijos se convirtieron al cristianismo, fue bautizada con el nombre de Isabel y se convirtió en la concubina del rey.

De esta relación nació entre 1091 y 1095, posiblemente en 1094: 

En la crónica "De rebus Hispaniae", del arzobispo de Toledo Rodrigo Jiménez de Rada, la mora Zaida se cuenta entre las esposas de Alfonso VI. Pero la "Crónica najerense" y el "Chronicon mundi" indican que Zaida fue concubina y no esposa de Alfonso VI.. 

Según Jaime de Salazar y Acha, seguido por otros autores, entre ellos, Gonzalo Martínez Díez, contrajeron matrimonio en 1100, quedando legitimado el hijo de ambos que se convirtió en príncipe heredero del reino cristiano. Para Salazar y Acha, Zaida y la cuarta esposa del rey, Isabel, son la misma persona, «Pese a los ímprobos esfuerzos de los historiadores posteriores por intentarnos demostrar que no era la mora Zaida», y también sería la madre de Elvira y de Sancha Alfónsez. Otra razón que esgrime el autor es el hecho que poco después de la boda del rey con Isabel, el infante Sancho comienza a confirmar diplomas regios y de no ser la nueva reina Zaida, no hubiera consentido el nuevo protagonismo de Sancho en detrimento de sus posibles futuros hijos. También cita un diploma en la catedral de Astorga del 14 de abril de 1107 donde el rey concede unos fueros y actúa "cum uxore mea Elisabet et filio nostro Sancio". Este es el único diploma donde se cita como «nuestro hijo», ya que en otros solamente figura como hijo del rey aunque también aparece la reina Isabel. 

Reilly acepta que fueron dos Isabel, la mora Zaida —bautizada Isabel— y la otra Isabel, pero argumenta que para reforzar la posición de Sancho Alfónsez, el rey Alfonso anuló el matrimonio con Isabel en marzo de 1106 y se casó con Zaida. La hipótesis de que Alfonso VI se había casado con Zaida ya había sido rechazada por Menéndez Pidal y por Lévi-Provençal.

El 27 de marzo de 1106, el rey Alfonso confirmó una donación al monasterio de Lorenzana: "(...) eiusdemque Helisabeth regina sub maritali copula legaliter aderente", una fórmula inusual que confirma un legítimo matrimonio. Salazar y Acha y Reilly interpretan esta cita como prueba de que el rey había casado con Zaida, legitimando así al hijo de ambos y la relación de concubinato. Gambra, sin embargo, se opone y dice que es «una argumentación extremadamente endeble, empezando por la referencia documental, escasamente significativa. Su carácter es más bien ornamental y literario». Montaner Frutos también dice que la hipótesis es «poco verosímil y problemática» ya que no era necesario que el rey casase con Zaida para legitimar a su hijo Alfonso y que, además, Isabel la francesa falleció en 1107 según reza en su epitafio. También menciona Montaner Frutos una donación de la reina Urraca años después, en 1115, cuando donó unas propiedades a la catedral de Toledo y solamente menciona a una Isabel como la esposa del rey.

Según Reilly, el 25 de noviembre de 1093 contrajo un tercer matrimonio con Berta, aunque en un documento del 13 de abril de 1094 no se cita lo cual «resulta extraño porque se inscribe en una época en la que ya es habitual la inclusión de la regia consorte en el tenor diplomático». Parece que la elección de Alfonso se debió a su deseo de limitar la influencia borgoñona en el reino. El genealogista Szabolcs de Vajay, por razones onomásticas, sugiere que Berta era miembro de la casa de Saboya, hija de Amadeo II de Saboya (m. 1180), hermano de Pedro I de Saboya, sobrina nieta de Berta de Saboya, bisnieta de Berta d'Este y prima hermana de otra Berta, quien fue reina por su matrimonio con Pedro I de Aragón. Su presencia en la corte se registra por primera vez el 28 de abril de 1095. Falleció entre el 17 de noviembre de 1099, fecha en que confirma un diploma real por última vez, y el 15 de enero de 1100 cuando el rey aparece solo en una donación a la catedral de Santiago de Compostela. El 25 de enero de 1100 el rey realizó una donación al monasterio de Sahagún en memoria de su difunta esposa de quien no hubo descendencia.

Su penúltimo matrimonio fue a principios de 1100 con Isabel y «la polémica ha radicado durante siglos en si esta última era la mora Zaida o un personaje distinto». Ambos aparecen juntos por primera vez el 14 de mayo de 1100 aunque el diploma es considerado sospechoso, y la segunda vez en ese mismo año pero sin fecha. Las últimas menciones de Isabel en diplomas reales fueron el 8 y el 14 de mayo de 1107 y probablemente murió a mediados de ese año. Esta es, según Salazar y Acha, Zaida que después de su bautismo se llamó Isabel. Su origen es incierto. El obispo Pelayo no se refiere a su origen. Lucas de Tuy en el siglo XIII, basándose en el epitafio de Isabel, la hace hija del rey Luis de Francia, quien por esas fechas tendría que ser Luis VI, aunque no consta que tuviera una hija llamada Isabel y, además, de ser así, esta hubiera tenido unos cinco o seis años de edad al casar. Reilly considera que su origen probablemente fue borgoñón, aunque no consta en la documentación.

Nacieron dos hijas de este matrimonio:

El rey Alfonso contrajo un quinto matrimonio, posiblemente en los primeros meses de 1108, con Beatriz. Ambos aparecen juntos por primera vez el 28 de mayo de 1108 en la catedral de Astorga y después en otros dos diplomas reales; el 1 de enero de 1109 en la catedral de León y por última vez el 25 de abril del mismo año en la catedral de Oviedo, unos tres meses antes de la muerte del rey. 
Según el obispo Pelayo de Oviedo, una vez viuda del rey, Beatriz regresó a su patria. Jaime de Salazar y Acha sugiere que fue hija de Guillermo de Poitiers, duque de Aquitania y , y de Hildegarda de Borgoña y que después de enviudar volvió a contraer matrimonio con Elías de la Flèche, conde de Maine. No hubo descendencia de este matrimonio.

Alfonso, ya anciano, tuvo que ocuparse del problema sucesorio. Berta había muerto sin darle un heredero a finales de 1099; poco después Alfonso se casó con una francesa que le dio dos hijas, pero ningún varón. Para complicar aún más la situación, en marzo del 1105 nació Alfonso Raimúndez, hijo de Urraca y Raimundo y nieto, por tanto de Alfonso. A este posible pretendiente a la corona se oponía el hijo del rey con Zaida, Sancho. Montenegro, opina que el rey legitimó a Sancho probablemente coincidiendo con la reunión de un concilio en Carrión de los Condes en enero de 1103 debido a que desde esa fecha, Sancho comienza a encabezar la lista de los confirmantes de diplomas reales, antes que sus cuñados Enrique y Raimundo de Borgoña. En mayo del 1107 Alfonso impuso el reconocimiento de Sancho como heredero, a pesar del probable disgusto de sus hijas y yernos, en el transcurso de una curia regia celebrada en León. La situación mejoró para el rey con la muerte de Raimundo en septiembre y el acuerdo con Urraca para que esta quedase como señora vitalicia de Galicia, salvo en caso de que se casase, ya que entonces pasaría a su hijo. Pocos meses más tarde, a comienzos del 1108, falleció también la esposa de Alfonso que, pese a su edad, volvió a contraer matrimonio con la francesa Beatriz, con la que no tuvo hijos y que le sobrevivió. La derrota de Uclés había resucitado el problema de la sucesión de Alfonso al fallecer en los combates el heredero, Sancho Alfónsez.

Para asegurar la sucesión, Alfonso eligió entonces a Urraca, pero decidió casarla con su rival y famoso guerrero Alfonso I de Aragón en el otoño del 1108. Parte de la nobleza y del clero sugirió que la infanta desposase al principal de los nobles castellanos, Gómez González, conde de Lara, pero el rey se opuso firmemente y escogió al rey aragonés. Aunque el matrimonio se celebró a finales del año siguiente, no condujo a la esperada estabilidad, sino a una larga guerra civil que se prolongó ocho años. El matrimonio resultó estéril y, tras la muerte de Alfonso, la mayoría de la nobleza y de los obispos del reino rehusaron someterse al Batallador.

Tras pasar el invierno probablemente en Sahagún, marchó a finales de mayo del 1109 o poco después a Toledo, para aprestarse a defender la frontera de la esperada acometida almorávide consecuencia del descalabro de Uclés. En la primavera, el enemigo se había apoderado de Alcalá de Henares. Poco antes de la marcha de Alfonso al sur, Teresa y Enrique abandonaron la corte y se dirigieron a sus tierras, ya en malas relaciones con la corte. No asistieron ni a la proclamación como heredera de Urraca en Toledo ni al posterior entierro de Alfonso en Sahagún ni a la coronación de Urraca a finales de julio.

Alfonso VI falleció en la ciudad de Toledo el día 1 de julio de 1109 El rey había acudido a la ciudad a tratar de defenderla de un inminente asalto almorávide y a proclamar Urraca heredera en la ciudad del Tajo. Su muerte acaeció tras la proclamación y desbarató los planes militares. Su cadáver fue conducido a la localidad leonesa de Sahagún, siendo sepultado en el Monasterio de San Benito de Sahagún a finales de julio o principios de agosto, cumpliéndose así la voluntad del monarca. Los restos mortales del rey fueron depositados en un sepulcro de piedra, que fue colocado a los pies de la iglesia del monasterio de San Benito, hasta que, durante el reinado de Sancho IV, pareciéndole indecoroso a este rey que su predecesor estuviese sepultado a los pies del templo, ordenó trasladar el sepulcro al interior del templo, y colocarlo en el crucero de la iglesia, donde se hallaba el sepulcro que contenía los restos de Beatriz Fadrique, hija del infante Fadrique de Castilla, quien había sido ejecutado por orden de su hermano, Alfonso X de Castilla, en 1277. Al fallecer Alfonso, el trono leonés pasó a su hija Urraca.
El sepulcro que contuvo los restos del rey, desaparecido en la actualidad, se sustentaba sobre leones de alabastro, y era un arca grande de mármol blanco, de ocho pies de largo y cuatro de ancho y alto, siendo la tapa que lo cubría lisa y de pizarra negra, y estando cubierto el sepulcro de ordinario por un tapiz de seda, tejido en Flandes, en el que aparecía el rey coronado y armado, hallándose en los lados la representación de las armas de Castilla y de León, y en la parte de la cabecera del sepulcro un crucifijo. El sepulcro que contenía los restos de Alfonso VI fue destruido en 1810, durante el incendio que sufrió el monasterio de San Benito. Los restos mortales del rey y los de varias de sus esposas, fueron recogidos y conservados en la cámara abacial hasta el año 1821, en que fueron expulsados los religiosos del monasterio, siendo entonces depositados por el abad Ramón Alegrías en una caja, que fue colocada en el muro meridional de la capilla del Crucifijo, hasta que, en enero de 1835, los restos fueron recogidos de nuevo e introducidos en otra caja, siendo llevados al archivo, donde se hallaban en esos momentos los despojos de las esposas del soberano. El propósito era colocar todos los restos reales en un nuevo santuario que se estaba construyendo entonces. No obstante, cuando el monasterio de San Benito fue desamortizado en 1835, los religiosos entregaron las dos cajas con los restos reales a un pariente de un religioso, que las ocultó, hasta que en el año 1902 fueron halladas por el catedrático del Instituto de Zamora Rodrigo Fernández Núñez. En la actualidad, los restos mortales de Alfonso VI el Bravo reposan en el monasterio de las monjas benedictinas de Sahagún, a los pies del templo, en un arca de piedra lisa y con cubierta de mármol moderna, y en un sepulcro cercano, igualmente liso, yacen los restos de varias de las esposas del rey.

En el terreno cultural, Alfonso VI fomentó la seguridad del Camino de Santiago e impulsó la introducción de la reforma cluniacense en los monasterios de Galicia, León y Castilla. En la primavera del 1073, realizó la primera concesión de un monasterio leonés a los cluniacenses. 

El monarca sustituyó la liturgia mozárabe o toledana por la romana. A este respecto cuenta la tradición popular que Alfonso tomó un breviario mozárabe y uno romano y los arrojó al fuego. Al arder solo el breviario romano, el rey volvió a arrojar al fuego el mozárabe, imponiendo así el rito romano. Es posible que esta leyenda sea el origen del refrán que afirma «Allá van las leyes, do quieran los reyes».

Alfonso VI, el conquistador de Toledo, el gran monarca europeizador, ve, en los últimos años de su reinado, cómo la gran obra política realizada se resquebraja ante el empuje almorávide y las debilidades internas. Alfonso VI había asumido plenamente la idea imperial leonesa y su apertura a la influencia europea le había hecho conocer las prácticas políticas feudales que, en la Francia de su tiempo, alcanzaban su expresión más acabada. En la conjunción de estos dos elementos, ve Claudio Sánchez-Albornoz la explicación de la concesión "iure hereditario" (reparto entre las dos hijas y el hijo del reino en lugar de legar todo el reino al único hijo varón) –más propio de la tradición navarroaragonesa– de los gobiernos de los condados de Galicia y Portugal a sus dos yernos borgoñones, Raimundo, primer marido de Urraca, y Enrique, casado con Teresa. De esa decisión, arrancó, a la vuelta de unos años, la independencia portuguesa y la perspectiva de una Galicia independiente bajo Alfonso Raimúndez, que luego no se hizo realidad al convertirse este en Alfonso VII de León.



</doc>
<doc id="6656" url="https://es.wikipedia.org/wiki?curid=6656" title="Antesis">
Antesis

La antesis () es el periodo de florescencia o floración de las plantas con flores; estrictamente, es el tiempo de expansión de una flor hasta que está completamente desarrollada y en estado funcional, durante el cual ocurre el proceso de polinización, si bien se usa frecuentemente para designar el período de floración en sí; el acto de florecer. La etapa previa a la floración suele llamarse preantesis. 

El desarrollo de la antesis es espectacular en algunas especies con flores múltiples:

En las especies de "Banksia", por ejemplo, la antesis expone los extremos de los estilos desde las partes superiores de los periantos de cada flor individual, primero en la parte baja de la inflorescencia hasta que la floración completa alcanza a las flores de la parte superior. 

La antesis de las flores de la "Banksia" es secuencial en la inflorescencia, así que cuando los estilos y los periantos se desarrollan secuencialmente con colores diferentes, el resultado es un cambio de color impactante que se desplaza gradualmente de abajo arriba a lo largo de la inflorescencia.


</doc>
<doc id="6657" url="https://es.wikipedia.org/wiki?curid=6657" title="Anemofilia">
Anemofilia

En Botánica se denomina anemofilia a la adaptación de muchas plantas espermatofitas que aseguran su polinización por medio del viento. El término se aplica también a cualquier dispersión de esporas realizado por el viento, como ocurre en muchos hongos o en los helechos.

La anemofilia es propia de especies que constituyen poblaciones densas en formaciones vegetales monoespecíficas o pauci específicas (con una o pocas especies), en donde son las especies dominantes del ecosistema. Sólo así puede ser eficaz la polinización por el viento. Las plantas que crecen dispersas, perdidas entre pies de otras especies, suelen ser polinizadas por insectos u otros animales es decir zoófilas, porque necesitan vectores especializados capaces de encontrar sucesivamente a los pocos individuos existentes en un área determinada. Las especies anemófilas tienen que producir en cambio cantidades muy grandes de polen, lo que es a veces muy perceptible en bosques de pinos, cuando el suelo se tiñe de amarillo por la gran cantidad de polen que estas especies producen durante la floración. Sólo cuando se dan estas dos condiciones puede asegurarse la polinización.

Entre los árboles la condición anemófila es propia de las coníferas, que tienden a formar bosques monoespecíficos en las latitudes frías y las montañas, o de los árboles dominantes en los bosques de las latitudes templadas, como robles o hayas. La anemofilia no se encuentra en los árboles de los bosques ricos en especies propios de las regiones tropicales. La anemofilia es también característica de las gramíneas y otras plantas próximas (como las ciperáceas) que constituyen formaciones herbáceas abiertas, donde no encuentra obstáculos el viento.

Otro grupo de polinización anemófila es la familia Juglandaceae, constituida por los géneros "Juglans", que agrupa las especies de nogal, "Carya" y "Pterocarya".<br>
En América del Sur, la familia Nothofagaceae que forma densos bosques, son polinizados por el viento. Ejemplos son la Lenga, el Coigüe y el Ñire.
Uno de los ejemplos más conocidos es el de la planta "Cannabis sativa", esta planta posee pequeñas flores las cuales son polinizadas por brisas de viento.

La anemófilia involucra una gran cantidad de adaptaciones a nivel de inflorescencias, flores y gametófitos. Así, las especies anemófilas suelen presentar flores poco vistosas, sin pétalos atractivos o, directamente, sin pétalo alguno, como en los fresnos. Esta desaparición del perianto también se aplica a otras estructuras bastante típicas tales como nectario, osmóforos y guías de néctar, los cuales se tornan innecesarios en estas especies. Además, las flores se disponen en inflorescencia frecuentemente péndulas (como por ejemplo en los amentos masculinos péndulos de "Corylus, Alnus" y "Quercus").

Asimismo, las especies anemófilas caducifolias suelen florecer temprano, antes que el follaje aparezca, de modo tal que el mismo no obstaculice la circulación del polen (ejemplo, en los robles, alisos, fresnos, sauces, álamos y olmos).

Este particular modo de polinización requiere que el polen sea pequeño o con una relación superficie/volumen muy grande, lo que reduce su velocidad de sedimentación y facilita que llegue más lejos cuando es arrastrado por el viento. Así, por ejemplo, las compuestas del género "Artemisia" presentan un polen pequeño, liso y seco, mientras que la mayoría de los miembros de la familia lo presentan más grande, ornado y cubierto de aceites, lo que facilita la aglutinación y la adherencia a vectores animales. En los pinos la ligereza se consigue por medio de dos sacos aéreos huecos, lo que reduce su densidad y aumenta la superficie de rozamiento. Asimismo, los granos de polen de las especies anemófilas carecen de cemento polínico, son secos y frecuentemente de exina lisa, por ello se separan fácilmente unos de otros.

La producción de gran cantidad de polen en estas especies se logra a través del incremento en el número de flores masculinas o de estambres (por ejemplo, en "Corylus", se producen 2 millones y medio de granos de polen por cada estambre). La expulsión del polen es facilitada por la movilidad de los filamentos de los estambres (poáceas) o por mecanismos de tensión de las anteras dentro del pimpollo floral que determinan literalmente la "explosión" de la flor y la liberación concomitante de una nube de polen en el momento de la antesis "(Urtica, Pilea").
Los estilos y estigmas de las especies anemófilas están muy agrandados con el objeto de facilitar la captura del polen que es transportado en el aire. El número de óvulos en los carpelos, por el contrario, suele ser muy reducido en relación al hecho de que la polinización usualmente la llevan a cabo granos de polen aislados.




</doc>
<doc id="6660" url="https://es.wikipedia.org/wiki?curid=6660" title="Caducifolio">
Caducifolio

Caducifolio, del latín "cadūcus" («caduco, caído», participio de "cadĕre" «caer») y "folĭum" («hoja»), hace referencia a los árboles o arbustos que pierden su hoja durante una parte del año, la cual coincide en la mayoría de los casos con la llegada de la época desfavorable, la estación más fría (invierno) en los climas templados. Sin embargo, algunos pierden el follaje durante la época seca del año en los climas cálidos y áridos.

También son llamados de hoja caduca, por oposición a los árboles llamados de hoja perenne. En Puerto Rico, por la influencia que ejerce la cultura estadounidense, es también conocido como deciduo, calco del inglés "deciduous". A su vez, la raíz de esta palabra remite al latín "dēciduus", derivada de "dēcidō", «caer, morir».

Muchos árboles y arbustos caducifolios florecen durante el período en que no tienen hojas, ya que esto aumenta la efectividad de la polinización. La ausencia de hojas beneficia la dispersión del polen por el viento o, en el caso de las plantas polinizadas por insectos, el que las flores sean más visibles por estos. Sin embargo, esta estrategia no carece de riesgos, ya que las flores pueden resultar dañadas por el hielo o, en las zonas con estaciones secas, las plantas pueden agotarse más con este esfuerzo.



</doc>
<doc id="6662" url="https://es.wikipedia.org/wiki?curid=6662" title="Polinización">
Polinización

La polinización es el proceso de transferencia del polen desde los estambres hasta el estigma o parte receptiva de las flores en las angiospermas, donde germina y fecunda los óvulos de la flor, haciendo posible la producción de semillas y frutos.

El transporte del polen lo pueden realizar diferentes agentes que son llamados vectores de polinización. Los vectores de polinización pueden ser tanto bióticos, como aves, insectos (principalmente abejas), murciélagos, etc.; como abióticos, por ejemplo agua o viento.

Existe una gran variedad de vectores bióticos, entre ellos los himenópteros (abejorros, abejas y avispas), lepidópteros (mariposas y polillas) y dípteros (moscas), así como colibríes, algunos murciélagos y en casos raros algunos ratones o monos. 

Algunas flores pueden ser polinizadas por muchos vectores, en cuyo caso se dice que son flores generalistas en cuanto a polinizadores; o, por el contrario, solo pueden ser polinizadas por un género o especie debido a que la morfología tanto de la flor como del polinizador se han acoplado a lo largo de la evolución, en cuyo caso se dice que las flores son especialistas. La especialización de la polinización genera un beneficio tanto para la planta como para el polinizador por lo cual esta se vuelve muy eficiente pues el insecto volará con seguridad a otra flor de la misma especie y depositará el polen en el estigma de esta flor. Entre las orquídeas es común encontrar una gran especialización en la interacción con los polinizadores, por ejemplo la mariposa nocturna "Xanthopan morganii praedicta" y la orquídea "Angraecum sesquipedale".

La antecología es el estudio de la polinización, así como de las relaciones entre las flores y sus polinizadores.

El transporte del polen lo pueden realizar agentes físicos como el viento (plantas anemófilas), el agua (plantas hidrófilas), o un polinizador animal (plantas zoófilas). Las características físicas y fenológicas de las flores anemófilas, hidrófilas y zoófilas, así como las de su polen, suelen ser marcadamente diferentes. Las plantas zoófilas deben llamar la atención de sus vectores con colores y olores atrayentes, así como recompensarlos con alimento o refugio. Diferentes tipos de polinizadores requieren diferentes tipos de atractivos, así las flores zoófilas han evolucionado y se han diversificado en una gran variedad de tipos los cuales pueden agruparse en síndromes florales. La belleza visual característicamente asociada a las flores es el efecto de su coevolución con insectos u otros animales polinizadores.

En los casos en que la polinización se produce como resultado de relaciones planta-animal estas relaciones son predominantemente de tipo mutualista, es decir relaciones en que ambos participantes se benefician. A diferencia de las relaciones obligatorias (propiamente simbióticas) que existen en la naturaleza, la mayoría de las relaciones de polinización son facultativas u opcionales y muy flexibles: la desaparición de un polinizador o planta no acarrea necesariamente la extinción del otro participante en la interacción, ya que cada uno de ellos posee alternativas (otras fuentes de alimento en el caso del animal, u otras especies de polinizadores en el caso de la planta). Sin embargo, existen algunos casos sumamente interesantes de relaciones simbióticas entre un polinizador y una especie de plantas, tales como la avispa de los higos y la polilla de la yuca.

El primer archivo fósil de polinización abiótica (sin ayuda de agentes orgánicos) es de plantas similares a los helechos del Carbonífero tardío. Las gimnospermas muestran clara evidencia de polinización abiótica desde el período Triásico.

Algunas gimnospermas del período Triásico ya presentan señales de polinización biótica, o sea por animales, en que los granos fosilizados tienen algunas de las características de granos de polen que son llevados por agentes polinizadores en el presente. Además el contenido intestinal, las piezas bucales y estructura de las alas de ciertos escarabajos y moscas sugieren que deben haber actuado como polinizadores. Los primeros síndromes florales de polinización surgieron entonces. Las indicaciones más tempranas de polinización por insectos son granos de pólen de hace 96 millones de años (Cretácico). Los granos están aglutinados en masas, señal de que en vez de ser transportados por el viento, se adhieren a insectos y así son llevados a otras plantas.

La primera evidencia de un insecto coleccionando polen es del Cretácico (100 millones de años) del norte de España. Los investigadores usaron tomografía de rayos-X y pudieron observar granos de polen de "Gingko" sobre un insecto conservado en ámbar.

La asociación entre escarabajos y angiospermas en el Cretácico temprano llevó a radiaciones evolutivas tanto de unos como de otros en el Cretácico tardío. La evolución de los nectarios u órganos productores de néctar señalan el comienzo de un mutualismo entre insectos himenópteros y angiospermas.

Las abejas son buenos ejemplos del mutualismo que existe entre himenópteros y angiospermas. Las flores proveen a las abejas de néctar (una fuente de energía) y polen (fuente de proteínas). Cuando las abejas van de una flor a otra colectando polen también depositan algunos granos en otras flores, causando así polinización cruzada. Si bien el polen y néctar son las recompensas más importantes para las abejas y otros polinizadores, en muchos casos hay otras recompensas, como aceites, fragancias, resinas y aun ceras. Se calcula que las abejas se originaron simultáneamente con la diversificación de las plantas con flores o angiospermas. Además hay casos de coevolución entre las plantas con flores y las especies de abejas que inducen adaptaciones especializadas. Por ejemplo las patas largas de "Rediviva neliana", una abeja que colecciona aceites de "Diascia capsularis", cuyas flores tienen un largo espolón. En el curso de la evolución los espolones de las flores y las patas de las abejas han ido haciéndose cada vez más largos.

En años recientes ha habido una pérdida de polinizadores. Tales pérdidas pueden tener un gran impacto económico. Entre las posibles causas están pérdida de hábitat, plaguicidas, parasitismo, cambios climáticos y otras. Algunos investigadores piensan que es un sinergismo de todos estos factores.

En la industria agrícola, el cambio climático está provocando una “crisis de polinizadores”. Esta crisis está afectando la producción de cultivos y los costos relacionados, debido a una disminución en los procesos de polinización. Esta alteración puede ser fenológica o espacial. En el primer caso, las especies que normalmente se encuentran en estaciones o ciclos de tiempo similares, ahora tienen diferentes respuestas a los cambios ambientales y por lo tanto dejan de interactuar. Por ejemplo, un árbol puede florecer antes de lo habitual, mientras que el polinizador puede reproducirse más tarde en el año, y por lo tanto las dos especies ya no coinciden en el tiempo. Las alteraciones espaciales ocurren cuando dos especies que normalmente compartirían la misma distribución, ahora responden de manera diferente al cambio climático y se están desplazando a diferentes regiones.

Los polinizadores silvestres, a menudo visitan un gran número de especies de plantas; éstas, a su vez, son visitadas por muchas especies de polinizadores. Todas estas relaciones forman una red de interacciones entre plantas y polinizadores. La estructura de estas redes presenta sorprendentes similitudes en diferentes ecosistemas y continentes.

La estructura de las redes planta-polinizador puede tener un gran impacto en la forma en que estas comunidades responden a los estreses ecológicos. Hay modelos matemáticos que analizan las consecuencias de la estructura de las redes en la estabilidad de las comunidades de polinizadores. Tales modelos sugieren que la forma específica en que las redes están organizadas reduce la competencia entre polinizadores y aumenta la biodiversidad. Esto incluso puede llevar a una fuerte facilitación entre polinizadores cuando las condiciones son seriamente desventajosas. Así es posible que el conjunto de especies de polinizadores pueda sobrevivir a condiciones severas. Pero también significa que las especies de polinizadores puedan llegar a un colapso simultáneo si las condiciones ambientales llegan a un punto crítico. La recuperación de la comunidad de polinizadores y plantas después de tal colapso puede ser sumamente difícil.

En agricultura, la mayoría de los cultivos, por ejemplo los cereales, son anemófilos, es decir polinizados por el viento o son autógamos (autopolinizados). Aproximadamente el 30% de los cultivos agrícolas del mundo (por ejemplo, muchos frutos y hortalizas) dependen de la polinización realizada por insectos y otros animales.

Utilizar polinizadores es crucial para la agricultura mundial. Se estima que la polinización por insectos (primordialmente abejas) se valora a aproximadamente 237-577 mil millones de dólares. Es utilizada en todo el mundo. La comida que se consume y el rol que cubren los agricultores también dependen mucho de los polinizadores. 

El 90% de las plantas con flor dependen de un insecto que las polinice. Esto significa que para la mayoría de los cultivos son un actor clave. Dependiendo de la planta, hay cierta cantidad de colonias de abejas por hectárea necesarias para un cultivo eficiente. Por ejemplo, para cultivar pepinos se necesitan 2.1 colmenas por acre. Para el melón se necesitan hasta 5 colmenas por acre. Para la calabaza y la sandía solo se necesita una colmena por acre aproximadamente.

Es un error creer que la polinización es un «servicio ecológico gratuito» de la naturaleza. Una polinización efectiva necesita algunos recursos, por ejemplo refugios de vegetación natural prístina y hábitats adecuados para los polinizadores. Cuando estos se reducen o se pierden, se limita la actividad de los polinizadores y se necesitan prácticas de gestión adaptable para mantener los medios de subsistencia.

En efecto, en todo el mundo la diversidad agrícola y de los agroecosistemas afronta el peligro de que las poblaciones de polinizadores están disminuyendo. Los principales causantes de este problema son la fragmentación de los hábitats, las sustancias químicas agrícolas e industriales, los parásitos y las enfermedades, así como la introducción de especies exóticas. En California, los productores de almendras habitualmente importan abejas melíferas de otros estados de los Estados Unidos para asegurar la polinización de sus cultivos. Este transporte puede contribuir a las epidemias.




</doc>
<doc id="6665" url="https://es.wikipedia.org/wiki?curid=6665" title="Iris (mitología)">
Iris (mitología)

En la mitología griega, Iris (en griego Ἶρις, ‘arcoíris’) es hija de Taumante y de la oceánide Electra y hermana de las Harpías y de Arce. En la "Ilíada", se la describe como mensajera de los dioses; sin embargo, en la "Odisea" este papel está reservado a Hermes. También aparece en la "Eneida" como mensajera de Hera. Eurípides la incluye en su tragedia "Heracles", Iris aparece como, otra vez, la mensajera de Hera.

Iris es la diosa del arcoíris que anuncia el pacto de unión entre el Olimpo y la tierra al final de la tormenta; al igual que Hermes, es la encargada de hacer llegar los mensajes de los dioses a los seres humanos. También es conocida como una de las diosas del mar y del cielo.

Durante la Titanomaquia, Iris fue elegida para ser la mensajera de los Olímpicos mientras que su hermana gemela, Arce, se convirtió en mensajera de los Titanes.

En el himno homérico a Apolo, los dioses presentes en el nacimiento en Delos, enviaron a Iris para que trajera a Ilitía y ayudara a Leto a dar a luz a Apolo y Artemisa.

En la Ilíada, Iris fue quien avisó a Menelao del secuestro de Helena en Esparta. Impidió a Hera y Atenea entrar en combate para ayudar a los aqueos. Ayudó a Afrodita cuando fue herida por Diomedes, donde la llevó al Olimpo conduciendo las riendas del los caballos de Ares. Además, fue la encargada de avisar a Aquiles para que liberara el cadáver de Patroclo, que estaba en poder de los troyanos. A continuación, Iris acude a la morada de los vientos para informarles de las súplicas de Aquiles; que avivaran la hoguera en la que yacía el cadáver de su amigo. Zeus le ordenó ir hasta la gruta marina de la nereida Tetis, que lloraba por el funesto destino de su hijo Aquiles, y la condujera al Olimpo.

En la Eneida, por orden de Hera, Iris corta el cabello rubio de la reina Dido que une a las personas a la vida. En otro pasaje, Iris toma la forma de la anciana Beroe, para que suscite en las mujeres troyanas el deseo de no viajar más y quemar las naves de Eneas. Por otra parte, Iris acompaña a Turno a la batalla, en donde informa al rey que los troyanos están sin su caudillo.

En la tragedia de Eurípides, "Heracles", Iris es la responsable de la locura de Heracles. En una ocasión, Iris le pide a los Argonautas alados, Calais y Zetes que no maten a sus hermanas las Harpías y promete que Fineo no será molestado por ellas nunca más.

Según el poeta romano Ovidio, después de que Rómulo fuese deificado como el dios Quirino, su esposa Hersilia suplicó a los dioses que también la dejaran inmortal para que pudiera estar con su esposo una vez más. Juno escuchó su súplica y envió a Iris hacia ella. Con un solo dedo, Iris tocó a Hersilia y la transformó en una diosa inmortal. Hersilia voló al Olimpo, donde adopta el nombre de Hora y se le permitió vivir con su esposo para siempre.

Se representa a Iris como una hermosa joven virgen con alas doradas y con una túnica multicolor, apresurándose a la velocidad del viento de un extremo a otro del mundo, a las profundidades del mar y del inframundo en donde tenía acceso libre. Es la mensajera especialmente de Hera, y está relacionada con Hermes, cuyo caduceo lleva a menudo. Por orden de Zeus, lleva un jarrón con agua del río Estigia, con la que hace dormir a todos los que perjuran. Sus atributos son el caduceo y un jarrón. También es representada suministrando a las nubes el agua que necesitan para inundar el mundo. 

Puesto que la función de Iris es transmitir los mensajes de los dioses, Platón relacionaba su etimología con "eireín", cuyo significado es «hablar». Así, Iris personificaría la dialéctica y la filosofía. Su origen sería el asombro, puesto que su padre, Taumante, está relacionado etimológicamente con la palabra "thoûma" (asombro).



</doc>
<doc id="6671" url="https://es.wikipedia.org/wiki?curid=6671" title="Alejandro Magno">
Alejandro Magno

Alejandro III de Macedonia (Pela, Grecia; 20 o 21 de julio de 356 a. C. - Babilonia; 10 o 13 de junio de 323 a. C.), más conocido como o Alejandro el Grande, fue rey de Macedonia (desde 336 a. C.), "Hegemón" de Grecia, Faraón de Egipto (332 a. C), Gran rey de Media y Persia (331 a. C), hasta la fecha de su muerte.

Hijo y sucesor de Olimpia de Epiro y Filipo II de Macedonia, su padre lo preparó para reinar proporcionándole experiencia militar y encomendando a Aristóteles su formación intelectual. Su ascenso al trono no fue fácil, su padre lo exilió junto a su madre por considerarlo un hijo adúltero. Su madre se exilió en Epiro y las amistades de Alejandro también fueron exiliadas por una posible conspiración. Filipo muere asesinado, y Alejandro se hace con el poder, eliminando adversarios que pudiesen reclamar el trono. 

Alejandro Magno dedicó los primeros años de su reinado a imponer su autoridad sobre los pueblos sometidos a Macedonia, que habían aprovechado la muerte de Filipo para rebelarse. Como hegemón de toda Grecia en concepto de sucesor de su padre, continuó el plan que habían aprobado las polis griegas: "conquistar el vasto imperio de Persia", para vengar todos los daños que les habían causado a los griegos por siglos, incluyendo la recuperación de todas las ciudades costeras de Asia Menor e islas del mar Egeo. Preparó un ejército de aliados griegos (mayormente macedonios) y en el año 334 a. C. se lanzó con su pequeño ejército, de apenas 40000 hombres, contra el poderoso Imperio persa: una guerra de venganza de los griegos —bajo el liderazgo de Macedonia— contra los persas.

En su reinado de trece años, cambió por completo la estructura política y cultural de la zona, al conquistar el Imperio aqueménida y dar inicio a una época de extraordinario intercambio cultural, en la que los griegos se expandieron por los ámbitos mediterráneo y próximoriental. Es el llamado Período helenístico (323 a. C.-30 a. C.) Tanto es así, que sus hazañas lo han convertido en un mito y, en algunos momentos, en casi una figura divina, posiblemente por la profunda religiosidad que manifestó a lo largo de su vida.

Tras consolidar la frontera de los Balcanes y la hegemonía macedonia sobre las ciudades-estado de la antigua Grecia, poniendo fin a la rebelión que se produjo tras la muerte de su padre, Alejandro cruzó el Helesponto hacia Asia Menor (334 a. C.) y comenzó la conquista del Imperio persa, regido por Darío III. Victorioso en las batallas del Gránico (334 a. C.), Issos (333 a. C.), Gaugamela (331 a. C.) y de la Puerta Persa (330 a. C.), se hizo con un dominio que se extendía por la Hélade, Egipto, Anatolia, Oriente Próximo y Asia Central, hasta los ríos Indo y Oxus. Habiendo avanzado hasta la India, donde derrotó al rey Poro en la batalla del Hidaspes (326 a. C.), la negativa de sus tropas a continuar hacia Oriente le obligó a retornar a Babilonia, donde falleció sin completar sus planes de conquista de la península arábiga. Con la llamada «política de fusión», Alejandro promovió la integración de los pueblos sometidos a la dominación macedonia promoviendo su incorporación al ejército y favoreciendo los matrimonios mixtos. Él mismo se casó con dos mujeres persas de noble cuna.

En sus treinta y dos años de vida, su Imperio se extendió desde Grecia, hasta el valle del Indo por el Este y hasta Egipto por el Oeste, donde fundó la ciudad de Alejandría (hoy Al-ʼIskandariya, الاسكندرية). Fundador prolífico de ciudades, esta ciudad egipcia habría de ser con mucho la más famosa de todas las Alejandrías fundadas por el también faraón Alejandro. De las setenta ciudades que fundó, cincuenta de ellas llevaban su nombre.

El conquistador macedonio falleció en circunstancias oscuras. Los escritos más antiguos dejan clara evidencia de una muerte lenta producto de un envenenamiento, dejando un imperio sin consolidar. El control sobre diversas regiones era débil en el mejor de los casos, y había regiones del norte de Asia Menor que jamás se hallaron bajo dominio macedonio. Al morir sin nombrar claramente un heredero, le sucedió su medio hermano Filipo III Arrideo (323-317 a. C.), que era una persona con discapacidad intelectual, y su hijo póstumo Alejandro IV (323-309 a. C.). El verdadero poder estuvo en manos de sus generales, los llamados diádocos (sucesores), que iniciaron una lucha despiadada por la supremacía que conduciría al reparto del imperio de Alejandro y su fraccionamiento en una serie de reinos, entre los cuales acabarían imponiéndose el Egipto Ptolemaico, el Imperio seléucida y la Macedonia antigónida.

Alejandro es el mayor de los iconos culturales de la Antigüedad, ensalzado como el más heroico de los grandes conquistadores. Un segundo Aquiles («soldado y semidiós»), para los griegos su héroe nacional y libertador, o vilipendiado como un tirano megalómano que destruyó la estabilidad creada por los persas. Su figura y legado han estado presentes en la historia y la cultura, tanto de Occidente como de Oriente, a lo largo de más de dos milenios y ha inspirado a los grandes conquistadores de todos los tiempos, desde Julio César hasta Napoleón Bonaparte.

Hijo de Filipo II, rey de Macedonia (dinastía de los Argéadas), y de Olimpia, hija de Neoptólemo I de Epiro, según Plutarco, el día de su nacimiento se tuvo noticia en la capital de tres triunfos: el del general Parmenión frente a los ilirios, la victoria del sitio a una ciudad portuaria por su padre y la victoria del carro del rey en competición, que fueron considerados increíbles augurios en aquel tiempo, aunque quizá fueran meras invenciones posteriores creadas bajo la aureola de grandeza de este personaje.

Existen dudas sobre la paternidad de Filipo sobre Alejandro, ya que hay otras dos versiones. Plutarco refiere que su madre Olimpia antes de quedar encinta soñó que un rayo caía sobre su vientre y que Filipo vio en un sueño que el abdomen de su esposa estaba sellado con el rostro de un león,por lo cual la acusó de adulterio. 

En tanto, Pseudo Calístenes narra que la vinculación de Alejandro con el dios Amón y la posterior visita al oráculo está relacionada con su verdadero padre, el faraón egipcio Nectanebo II, quien huyera a Grecia al ser invadido su país nuevamente por los persas. Según la leyenda, Nectanebo II fue recibido en la corte de Filipo como un «mago».

Personificado como el dios Amón, convenció a Olimpia de engendrar un hijo que pusiera a salvo a las dos naciones, a lo cual ella accedió. Se mantuvo varios años en la corte, hasta que murió en una caminata nocturna junto a «su hijo». Alejandro, según Calístenes, supo que su verdadero padre era Nectanebo II esa misma noche, razón por la que, descreyendo de él, lo empujó a un pozo y murió. 

Esta leyenda se basa en el hecho de que los sacerdotes egipcios del oráculo de Amón en Siwa, lo saludaron como hijo de aquel dios, un título que implicaba reconocerlo como faraón.

Alejandro Magno tenía el hábito de inclinar ligeramente la cabeza sobre el hombro derecho, era físicamente de hermosa presencia, de baja estatura (1,60 m), cutis blanco, la nariz algo curva inclinada a la izquierda, cabello semiondulado de color castaño claro, con un estilo de cabello denominado anastole («dentro del espíritu») y ojos heterócromos (el izquierdo marrón, y el derecho gris), se desconoce si eran así de nacimiento o como consecuencia de un traumatismo craneal. Plutarco y Calístenes citan que poseía un aroma físico agradable naturalmente, a lo que ellos llamaban "«buen humor»". Por descripciones de Plutarco, normalmente antes de dar batalla, Alejandro lanzaba un dardo hacia el cielo (Zeus) con la mano izquierda, como también se aprecian en algunas de sus esculturas, se lo ve portando objetos con el mismo brazo, por lo que sería aceptable afirmar que era zurdo.

Su educación fue inicialmente dirigida por Leónidas, un austero y estricto maestro macedonio que daba clases a los hijos de la más alta nobleza, que lo inició en el ejercicio corporal pero también se encargó de su educación. Lisímaco, un profesor de letras bastante más amable, se ganó el cariño del Magno llamándole Aquiles, y a su padre, Peleo. Sabía de memoria los poemas homéricos y todas las noches colocaba la "Ilíada" debajo de su cama. También leyó con avidez a Heródoto y a Píndaro.

Se cuentan numerosas anécdotas de su niñez, siendo la más referida aquella que narra Plutarco: Filipo II había comprado un gran caballo al que nadie conseguía montar ni domar. Alejandro, aun siendo un niño, se dio cuenta de que el caballo se asustaba de su propia sombra y lo montó dirigiendo su vista hacia el Sol. Tras domar a Bucéfalo, su caballo, su padre le dijo: «Búscate otro reino, hijo, pues Macedonia no es lo suficientemente grande para ti».Según coinciden algunos historiadores antiguos, especialmente Calístenes, quien narra la participación de Alejandro en su adolescencia de los Juegos Olímpicos (a petición de Filipo), en la cual obtuvo victorias en competencias de carros.

A los trece años fue puesto bajo la tutela de Aristóteles, el filósofo que más influyó en la filosofía y las ciencias. Durante cinco años sería su maestro, en un retiro de la ciudad macedonia de Mieza. Aristóteles le daría una amplia formación intelectual y científica en las ramas que este abordó, como filosofía, lógica, retórica, metafísica, estética, ética, política, biología, y otras tantas áreas.

Muy pronto (340 a. C.) su padre lo asoció a tareas del gobierno nombrándolo regente, a pesar de su juventud. Recibía personalmente a los enviados persas, deseosos de que Macedonia pagase los altos tributos exigidos por Darío. Les conversaba amablemente, y así obtenía información, acerca de las travesías de rutas tierra-mar, la preparación del ejército persa, valioso para las acciones que desarrolló en el futuro. En el 338 a. C. dirigió la caballería macedónica en la batalla de Queronea, siendo nombrado gobernador de Tracia ese mismo año. Desde pequeño, Alejandro demostró las características más destacadas de su personalidad: activo, enérgico, sensible y ambicioso. Es por eso que, a pesar de tener apenas 16 años, se vio obligado a repeler una insurrección armada. Se afirma que Aristóteles le aconsejó esperar para participar en batallas, pero Alejandro le respondió: «Si espero, perderé la audacia de la juventud».

Las ideas religiosas de Alejandro eran las convencionales en el tradicional politeísmo de la Grecia clásica, aunque como concepto moderno de religión, la más apropiada es la "eusébeia", definida por Platón ("Eutifrón", 12e) como «el cuidado que los hombres tienen de los dioses». Si lo que caracteriza a la religión griega son los ritos propiciatorios y sacrificiales, por medio de los que se garantiza la relación satisfactoria entre los hombres y los dioses, no hay duda de que Alejandro fue un hombre profundamente religioso que hizo sacrificios y ofrendas a los dioses olímpicos en conjunto o particularmente, como a Poseidón, a quien sacrificó un toro al cruzar el Helesponto, además de Ares y Atenea, Heracles, Asclepio, las Nereidas, Dioniso, Amón, Baal, Océano, ríos divinizados como el Indo y muchos otros.

Como dios protector de Macedonia Zeus olímpico aparecía en la mayor parte de las monedas de plata que mandó acuñar en toda su historia, en las que en el anverso aparecía la figura de Hércules con unos rasgos físicos progresivamente más parecidos a los del propio Alejandro. Ambas deidades, en efecto, le iban a ser siempre muy queridas pues, según Quinto Curcio ("Historia de Alejandro Magno", IV, 2.3), Zeus era su padre en tanto Heracles lo era de la dinastía macedónica. Sus primeros biógrafos hablan muy a menudo de los sacrificios ofrecidos por Alejandro a los dioses. Tras cada victoria sacrificaba animales a los dioses en general o a alguno en particular y les dedicaba procesiones y competiciones gimnásticas. Es célebre la ofrenda que hizo a Atenea tras la victoria sobre los persas en el Gránico, cuando envió a su templo en Atenas 300 armaduras persas completas con la inscripción: «Alejandro hijo de Filipo y los griegos, excepto los lacedemonios, de los bárbaros que habitan Asia». En Egipto se interesó por el templo de Zeus-Amón, ofreció sacrificios al dios Apis y consintió ser divinizado como Horus, «el príncipe fuerte, aquel que puso las manos en las tierras de los extranjeros, amado de Amón y elegido de Ra, hijo de Ra, Alejandro». Por lo demás nunca desdeñó incorporar a su propio panteón los dioses de los vencidos, a veces tras un proceso de sincretismo religioso mediante el cual acababan siendo identificados con las deidades griegas y macedónicas, por ejemplo cuando tras la conquista de Tiro, según Flavio Arriano ("Anábasis de Alejandro Magno", II, 24.6), consagró al mismo Heracles «el barco sagrado de Tiro dedicado a Heracles, que había sido capturado en el ataque naval» y, benevolente –tras la masacre de la conquista–, amnistió a los fenicios que se habían refugiado en el templo de Heracles, es decir, el dedicado al Melqart tirio.

Ese respeto a los templos de las ciudades conquistadas —a excepción de Tebas, donde es posible que Alejandro no tuviese aun un completo control de su ejército— es otro rasgo de su religiosidad, junto con el continuado recurso a la adivinación. Si quiso deshacer el nudo gordiano es porque había sido profetizado que quien fuese capaz de soltar el nudo gobernaría toda Asia (Arriano, II, 3, 6-8). Para Plutarco ("Al" LXX, 2-4), que habla de un palacio «lleno de sacrificadores, de expiadores y de adivinos que llenaban el ánimo de Alejandro de necesidades y de miedo», como para Quinto Curcio, la mentalidad de Alejandro podía en ese aspecto calificarse de supersticiosa.

Un nuevo matrimonio de su padre, que podría llegar a poner en peligro su derecho al trono (no conviene olvidar que el mismo Filipo fue regente de su sobrino Amintas IV —hijo de Pérdicas III—, hasta la mayoría de edad, pero se adueñó del trono), hizo que Alejandro se enemistara con Filipo. Es famosa la anécdota de cómo, en la celebración de la boda, el nuevo suegro de Filipo (un poderoso noble macedonio llamado Átalo) rogó porque el matrimonio diera un heredero legítimo al rey, en alusión a que la madre de Alejandro era una princesa de Epiro y que la nueva esposa de Filipo, siendo macedonia, daría a luz a un heredero totalmente macedonio y no mitad macedonio y mitad epirota como Alejandro, con lo cual sería posible que se relegara a este último de la sucesión. Alejandro se enfureció y le lanzó una copa, espetándole: «Y yo ¿qué soy? ¿un bastardo?». En ese momento Filipo se acercó a poner orden, pero debido a su estado de embriaguez, se tropezó y cayó al suelo, lo que le granjeó una burla de Alejandro: «Quiere cruzar Asia, pero ni siquiera es capaz de pasar de un lecho a otro sin caerse.» La historia le valió la ira de su padre, por lo que Alejandro tuvo que exiliarse a Epiro junto con su madre, Olimpia. Para evitar un complot, Filipo también ordenó el exilio de todos sus amigos, siendo Frigio uno de los más cercanos. Más tarde, Filipo terminaría por perdonarle.

Filipo muere asesinado en el año 336 a. C. a manos de Pausanias, un capitán de su guardia, como resultado de una conspiración que es generalmente atribuida a Olimpia. Después de este hecho, Alejandro hizo matar a parte de la familia de Cleopatra, su madrastra. Así, se aseguró que no quedara vivo ningún heredero que pudiese reclamar el trono y tomó las riendas de Macedonia a la edad de 20 años.

Tras suceder a su padre, Alejandro se encontró con que debía gobernar un país radicalmente distinto de aquel que heredó Filipo II 23 años antes, ya que Macedonia había pasado de ser un reino fronterizo, pobre y desdeñado por los griegos, a un territorio que tras el reinado de Filipo se consideraba como parte de la Hélade y un poderoso Estado militar de fronteras consolidadas con un ejército experimentado que dominaba indirectamente a Grecia a través de la Liga de Corinto. En un discurso, puesto en boca de Alejandro por el filósofo e historiador griego Flavio Arriano, se describía la transformación del pueblo macedonio en los siguientes términos:

La muerte del gran Filipo supuso que algunas polis griegas sometidas por él se alzasen en armas contra Alejandro ante la aparente debilidad de la monarquía macedonia. Alejandro debía resolver dos puntos importantes: mantener el control de las ciudades y reclutar mercenarios de las polis para su campaña contra Persia.

En la primavera del 335 a. C. lanza una exitosa campaña al norte, Iliria (hoy Albania y Macedonia del Norte) y Tracia (hasta las inmediaciones del río Danubio, hoy Rumania), donde es avisado que Tebas se había sublevado, tomando una guarnición macedonia.

Alejandro, con una reacción relámpago, demostró rápidamente su destreza estratégica y militar: viajó casi 600 kilómetros hasta Tesalia para reafirmar el dominio en la región (ya había sido conquistada por Filipo), y emprendió el camino hacia el Ática, reprimiendo la sublevación de Tebas, que opuso una feroz resistencia, reduciendo la ciudad a escombros. Después de ajusticiar a los sublevados, entrevistó a una parte de la población, ordenando más tarde la reconstrucción de la ciudad. Uno de los perjudicados era un deportista tebano de los Juegos Olímpicos, a quien Alejandro felicitó durante el desarrollo de estos, y otro relato cuenta que una mujer que mató a un general tracio durante la contienda, fue liberada después de haber hecho una «defensa sincera».

Camino al sur del Ática, visitó el gran oráculo de Delfos, donde un general ateniense había depuesto a la pitonisa del templo, y que luego Alejandro restableció a la misma en su puesto. Allí tuvo en dos ocasiones sus oráculos. La primera visita fue bastante errática, teniendo los sacerdotes que irrumpir en varias ocasiones. «Alejandro, no puedes entrar con espadas aquí. Y tampoco puedes llevarte las cosas». En la segunda, fue a pedir el oráculo, pero en la residencia la pitia (sacerdotisa), que forcejeando le dijo «hijo mío, eres invencible».Su paso por Atenas fue por demás totalmente atípico. Los atenienses cerraron sus puertas, no por sublevación, sino por temor por lo ocurrido en Tebas. Alejandro, que sentía un gran respeto por los filósofos, el arte y la cultura de la ciudad, envió entonces una primera carta (era su estilo), a lo que respondieron: «estamos debatiendo si presentarte batalla o dejarte entrar». Por lo que, Alejandro, a través de otra carta propuso dejar a su ejército fuera y entrar solo. Dejó que solamente lo acompañaran algunos de sus amigos, los "hetaroi". Una vez allí, Atenas reconoció su supremacía por el gesto, nombrándolo de esta manera hegemón, título que ya había ostentado su padre y que lo situaba como gobernante de toda Grecia, consolidando así la hegemonía macedónica, tras lo cual Alejandro se dispuso a cumplir su siguiente proyecto: conquistar el Imperio persa.

Una conocida historia fue, de visita en Corinto durante los Juegos Ístmicos, se encontró con el filósofo Diógenes de Sinope, que se encontraba sentado en un gran barril reflexionando, preguntándole «Diógenes, dime qué puedo hacer por ti». A lo que este le respondió con una ironía: «sí, apártate de ahí, que me tapas el sol». La elocuente respuesta le valió las bromas de sus «compañeros» allí presentes. Asombrado por la elocuencia, Alejandro exclamó «¡Si no fuera yo Alejandro, me gustaría ser Diógenes!». Esto trascendió en los manuscritos de los filósofos y sofistas de toda Grecia. En otra ocasión, encontró a Diógenes revolviendo basura, al preguntarle qué era lo que estaba buscando, Diógenes respondió «estoy buscando huesos de esclavos, pero no hallo la diferencia entre estos y los de tu padre». Era claro que Diógenes despreciaba a Alejandro, quien nunca tomó represalia alguna.

Alejandro, tras asegurar el orden en toda la región de la hélade y sureste de Europa, dejó a Antípatro al mando de todos los dominios. Preparó 160 embarcaciones, abastecimiento suficiente y armamento (y ya no contaba con tanto dinero para pagar a sus hombres), con su pequeño ejército de unos 40 000 soldados que contaba con miles de aliados griegos y mercenarios. Cruzó el Helesponto hacia Asia Menor, para iniciar la conquista del Imperio Persa, pretendiendo seguir los planes de su padre de liberar a todas las ciudades "polis" griegas de la zona de Jonia (Misia, Lidia, Licia) que se encontraban bajo dominio persa en Asia Menor (hoy Turquía). Hizo una breve parada en Troya, donde honró la tumba de Aquiles (héroe griego de la Guerra de Troya).

En la primera contienda que se libró en territorio asiático, la batalla del Gránico, a orillas del riachuelo Gránico, los sátrapas persas le hicieron frente con un ejército de igual número que los helenos, unos 40 000 hombres, comandado por Memnón de Rodas, compuesto en su mayor parte por persas en la vanguardia, y mercenarios griegos en la retaguardia, pero el ejército persa ofreció una débil resistencia y fue vencido. En este combate Alejandro estuvo cerca de la muerte, pues un persa trató de matarlo por la espalda. Finalmente salvó su vida gracias a Clito, uno de los hombres de confianza de Filipo, que mató al enemigo.

Memnón era un general mercenario griego al servicio de Persia, y poseía amplios dominios en el emplazamiento de Troya, donde se desarrolló la batalla del Gránico. En otros tiempos Filipo II (padre de Alejandro) le dio hospedaje junto a su familia en Macedonia durante la invasión persa, donde conoció a Alejandro y al filósofo Aristóteles, por lo que conocía muy bien al oponente. Con una inmensa flota bajo su mando, su objetivo fue recuperar las tierras que los persas le obsequiaron, atacando las líneas de suministros a Alejandro a través del Helesponto e islas del Egeo, y recibiendo una gran cantidad de barcos desde Chipre, Fenicia y Egipto. Memnón puso en aprietos a Alejandro en varias ocasiones con sus movimientos tácticos. Desafortunadamente para los persas, Memnón muere durante el asedio a Mitilene. Las ciudades griegas de las costas, Éfeso, Halicarnaso, Pérgamo, Mileto, y otras tantas más, lo recibieron como libertador, y otras se sometieron por temor.

Con la muerte de Memnón, la amenaza marítima estaba ya descartada, y teniendo ya el control del mar Egeo, Alejandro dispuso hacer una pausa en Jonia, nuevamente restablecida a los griegos, ya sin la amenaza persa. Allí conoció al célebre pintor Apeles.

Alejandro fue un gran amante de las artes. Era consciente del poder de propaganda que puede tener el arte y supo muy bien controlar la reproducción de su efigie, cuya realización solo autorizó a tres artistas: el célebre escultor Lisipo, un orfebre y un pintor, el jonio Apeles. Los biógrafos de Alejandro cuentan que este tenía en gran aprecio al pintor y que visitaba con frecuencia su taller y que incluso se sometía a sus exigencias. Son innumerables las representaciones de Apeles pintando sin atuendos a Alejandro, y a Campaspe, concubina del macedonio, y de aparentemente una gran belleza. Campaspe fue también modelo para representar a la diosa Afrodita (Venus) Una vez concluida esta primera etapa de conquistas, se celebraron bodas masivas de soldados griegos y mujeres de la polis liberadas. Por lo que en el otoño de 334 a. C., estando Alejandro en Caria, envió a aquellos soldados recién casados a Macedonia para que pasaran el invierno junto a sus esposas. Coeno, uno de los comandantes más capaces de Alejandro, los condujo de vuelta a Grecia. 

A finales de 334 a. C., Alejandro decidió pasar el invierno en Gordión, antigua capital de Frigia (al centro de Turquía), a la espera de refuerzos. Allí se encontraba un famoso carro real, sujeto a un nudo muy complicado de deshacer. Según el oráculo de Gordión, «quien supiera deshacerlo conquistaría Asia». Algunas fuentes indican que Alejandro desató el nudo pacientemente, mientras que otras afirman que lo cortó con su espada. En cualquier caso, la tormenta que siguió al hecho se interpretó como un claro signo de que Zeus daba su aprobación.

Coeno regresó de Grecia a encontrarse con Alejandro en Gordión, ya con refuerzos: los soldados macedonios recién casados y nuevos reclutas.

Alejandro se dirigió desde Gordion hacia la región de Cilicia, y emprendió su marcha hacia el sur, donde es avisado que desde Siria los persas, al mando del rey Darío, destruyeron un campamento macedonio, aniquilando sus guarniciones (que eran casi todos soldados heridos en batalla), por lo que tuvo que retomar el camino norte, donde los persas le hicieron frente del otro lado del río Issos, con un ejército superior a los 500 000 hombres, cuando los aliados griegos no superaban los 50 000 hombres. Aun así, prevaleció la estrategia por sobre el número. Los persas perdieron casi la mitad de sus tropas, y tal es como describen las narraciones de esta batalla, una verdadera masacre. Esta es conocida como la batalla de Isos —pequeña llanura situada entre las montañas y el mar cerca de Siria— en el 333 a. C., en la cual, el rey Darío, ante tal debacle, huyó amparado en la oscuridad de la noche dejando en el campo de batalla, abandonando sus tesoros, armas y su manto púrpura.

La familia entera de Darío III fue capturada en el interior de una lujosa tienda, haciendo prisioneras a su madre Sisigambis, su esposa Estatira, y sus dos hijas, Dripetis y Barsine. Alejandro trató a todas con gran cortesía y les manifestó que no tenía "ninguna cuestión personal contra Darío, sino que luchaba contra él para conquistar Asia". Les brindó trato real, y abundó en dotes para sus hijas. Al tiempo le propondría matrimonio a una de sus hijas, Barsine, pidiendo antes la mano a su madre. Mientras que su amigo personal y comandante Hefestión, se casó con Dripetis. Se realizó una boda en conjunto. El propósito (además de político) era eliminar diferencias entre vencedores y vencidos, mostrarse ante los persas como un referente, y lograr la mezcla de etnias, siendo él mismo parte de la propuesta. El rey Darío tomó conciencia de la amenaza y envió propuestas de negociación, que fueron todas rechazadas. Por lo que puede apreciarse en lo escrito por Calístenes, las respuestas de Alejandro eran irónicas,

Luego de Issos, y asegurarse que no había amenazas por tierra y por mar, retomó el rumbo sur, conquistando fácilmente Fenicia, siendo bien recibido en Judea (considerado un "libertador", puesto que los liberó de los persas). De su paso por este reino, existen versiones que coinciden en su buen recibimiento, pero que difieren en el diálogo que hubo. La excepción del buen trato fue la isla de Tiro, donde quiso de manera pacífica honrar a los dioses en sus templos, enviando emisarios diplomáticos. Estos fueron asesinados a traición, por lo que decidió asediar esta ciudad hasta destruirla. Con una duración de enero a agosto (332 a. C.) este asedio es conocido como el sitio de Tiro, una isla fortificada, en la que tuvo que construir muelles y vado sobre el mar, emplear torres de asedio y catapultas más modernas, como el euthytonón. 

El euthytonón era un símil a grandes ballestas lanza-cohetes, con carril de direccionamiento del proyectil. La traducción del griego al español es "adiós". Esta arma fue definitivamente la que derribó los muros, y una vez destruidos, Tiro fue arrasada. Otro sitio importante fue el de Gaza durante otro arduo enfrentamiento. Una vez conquistada, Alejandro se dirigió a Egipto.

Aparentemente Calístenes es de los pocos que se detienen en como fue el ingreso a Egipto. Este reino estaba controlado por los persas desde el año 343 a. C.. El escrito menciona que primero hubo una exhortación de Alejandro a la pequeña guarnición persa que controlaba el reino de Egipto, "«¡Abrid si no queréis desencadenar la furia de Ares!»" 

Alejandro fue bien recibido por los egipcios, quienes le apoyaron en su lucha contra los persas, cuyos reyes habían dominado Egipto en dos ocasiones: de 523 a 404 a. C. (Dinastía XXVII) y de 343 a 332 a. C. (Dinastía XXXI), hasta la entrada del macedonio.

Recibido como "salvador y libertador", e "hijo de Amón" (por la creencia de que su padre biológico fue el rey egipcio Nectanebo, representante de esta deidad), por decisión popular se concedió a Alejandro la corona de los dos reinos, siendo nombrado faraón en noviembre de 332 a. C. en Menfis.

En enero del 331 a. C. Alejandro fundó la ciudad de Alejandría en una zona costera muy fértil al oeste del delta del Nilo. Los motivos de la fundación eran tanto económicos (la apertura de una ruta comercial en el mar Egeo) como culturales (la creación de una ciudad al estilo griego en Egipto, cuya planificación se dejó en manos del arquitecto Dinócrates). La escritora inglesa Mary Renault, en su biografía de Alejandro, comenta:

Posteriormente, tras un dificultoso viaje por el desierto, llegó al oasis de Siwa, situado en pleno Sahara. Este oráculo correspondía al dios Amón. El profeta, queriendo saludarle en idioma griego le dijo «hijo mío», equivocándose en una letra; y que a Alejandro le agradó este error, por dar motivo a que pareciera le había llamado "hijo de Zeus". Le anunció que le saludaba tanto de parte del dios como de su padre. Alejandro preguntó si había quedado sin castigo alguno de los asesinos de Filipo, y si se le concedería dominar a todos los hombres. Habiéndole dado el dios favorable respuesta y asegurándole que Filipo estaba vengado, Alejandro le hizo magníficas ofrendas, y entregó ricos presentes a los hombres allí destinados. También se dice que Alejandro, en una carta enviada a su madre, le comunicó haberle sido hechos ciertos vaticinios arcanos, que solo a ella revelaría. Algunos han escrito que 

La cultura del antiguo Egipto impresionó a Alejandro desde los primeros días de su estancia en este país. Los egipcios nos han dejado testimonio, grabado en piedra, de estos hechos y apetencias. En Karnak existe un bajorrelieve donde se representa a Alejandro haciendo ofrendas al dios Amón en calidad de converso. En él, viste la indumentaria de faraón:


En los jeroglíficos del muro se distinguen además los títulos de Alejandro-faraón que se representan dentro de un serej y un cartucho egipcio:

Al cabo de un año de estadía en Egipto, y controlando la situación de rebeldía en Anatolia y el Egeo, en la primavera del 331 a. C., desde Tiro y Egipto, organizó los territorios conquistados y su ejército, para iniciar la marcha hacia la conquista de Persia

El rey persa Darío, con un ejército más numeroso, decidió hacerle frente en Gaugamela a orillas del Tigris. Esta batalla hoy en día sigue siendo analizada por expertos militares, intentando explicarse como un ejército tan inferior en número derrota a uno por lo menos cinco veces más grande. La estrategia usada por el macedonio fue una tenaza, donde el yunque era la infantería, y el martillo la caballería (los "hetairoi". sus "compañeros"), Darío apenas logró salvar su vida, y huyó -nuevamente- apabullado por el genio militar del macedonio.

Esta derrota del ejército persa fue significativa en bajas, y representó prácticamente la caída del imperio aqueménida. Así Alejandro con su ejército logró entrar en Babilonia quedando a las puertas del propio territorio persa.

En el año 331 a. C., el ejército macedonio invadió Persia entrando fácilmente a Susa, la vieja capital de Darío I, mientras que el derrotado Darío III huía hacia el interior del territorio persa en busca de fuerzas leales para enfrentar nuevamente a Alejandro.

Alejandro procedió cuidadosamente ocupando las ciudades, apoderándose de los caudales persas y asegurando las líneas de abastecimiento. Desde Susa pasó a Persépolis, capital ceremonial del Imperio aqueménida, donde quemó el palacio de la ciudad durante una fiesta, aunque otras fuentes señalan que esto no fue así. 

Se dirigieron hacia Ecbatana para perseguir a Darío. Lo encontraron asesinado por sus nobles, que ahora obedecían a Bessos. Alejandro honró a su otrora rival y enemigo, cubriéndolo con el "manto púrpura" que Darío abandonó en la batalla de Isos, y que Alejandro recogió. Le rindió un funeral real y prometió a la familia de este perseguir a sus asesinos.Bessos escapó a la zona lindera del Hindú Kush (hoy Afganistán), en las inmediaciones de Sogdiana (al este de Asia), acompañado por una resistencia formada por nobles y arqueros a caballo, autoproclamándose rey de Persia, cosa que Alejandro no toleraba, motivo también por el cual lo perseguiría.

Los extranjeros que vivían en Persia se sintieron identificados con Alejandro y se comprometieron con él para venerarle como nuevo gobernante. En su idea de conquista también estaba la de querer globalizar su Imperio mezclando distintas razas y culturas. Los sátrapas persas en su mayoría conservaron sus puestos, aunque supervisados por un oficial macedonio que controlaba las fuerzas armadas. 

Plutarco recalca «al ver Alejandro a las demás cautivas, que todas eran aventajadas en hermosura y gallardía, dijo por chiste: “¡Gran dolor de ojos son estas persas!” Con todo, oponiendo a la belleza de estas mujeres la honestidad de su moderación y continencia»

En su intento de mezclar la cultura persa y la griega se celebraron bodas masivas entre griegos y muchachas persas, y entrenó a un regimiento de soldados persas para combatir a la manera macedonia. La mayoría de los historiadores coinciden en que Alejandro adoptó el título real persa de "Shahanshah" ("Rey de Reyes").

En el 330 a. C. Filotas, hijo de Parmenión, fue acusado de conspirar contra Alejandro y asesinado junto con su padre (por temor a que este se rebelara al enterarse de la noticia). Asimismo, el primo de Alejandro, Amintas (hijo de Pérdicas III), fue ejecutado por intentar pactar con los persas para convertirse en el nuevo rey (de hecho, era el legítimo sucesor al trono macedonio). Tiempo después hubo una nueva conjura contra Alejandro, ideada por sus pajes, la cual tampoco logró su objetivo. Tras esto, Calístenes (quien hasta ese momento había sido el encargado de redactar la historia de las travesías de Alejandro) fue considerado como impulsor de este complot, por lo que fue condenado a muerte. Sin embargo, él se quitó antes la vida.

Clito, apodado «el Negro», era uno de los generales del ejército, al cual Alejandro tenía gran afecto y había nombrado sátrapa de Bactriana. Durante un banquete, Clito, irritado por la costumbre persa de la "proskynesis", y al escuchar que Alejandro se proclamaba mejor que su padre Filipo, le respondió: «Toda la gloria que posees es gracias a tu padre», agregando: «Sin mí, hubieras perecido en el Gránico.» Alejandro, ebrio, le arrojó una manzana a la cabeza, a lo que siguió una discusión en forma de versos que terminó cuando Alejandro buscó su espada para atacarlo. Según el relato, uno de los guardias la había ocultado y Clito fue sacado del lugar por varios amigos. Poco después, sin embargo, volvió a entrar por otra puerta y, mirando fijamente al conquistador, recitó un verso de Eurípides: «Qué perversa costumbre han introducido los griegos.» En ese momento, Alejandro arrebató una lanza a uno de sus guardias y atravesó con ella a Clito, quien se desplomó en medio del estupor de los presentes. Arrepentido del crimen, pasó 3 días encerrado en su tienda y algunos relatos afirman que trató de suicidarse.

Esta persecución es importante, porque es la ruta que termina llevando a Alejandro hasta la India. Tras muchos preparativos, y luego de establecer un nuevo orden en Babilonia, Alejandro partió en la persecución de Bessos, el asesino del rey Darío, y conquistar las satrapías persas de Asia Central. La mayoría de los sátrapas persas continuaron en sus cargos, dejando Alejandro en ellas pequeñas guarniciones de aliados griegos. Contaba con una expedición mediana de soldados griegos, llevando consigo soldados persas (entrenados al estilo de combate y uniformes macedonios), que conocían bien los territorios y los dialectos de las zonas a ocupar.

Los escritos antiguos dejaron testamento que este viaje fue tan exótico como penoso. Una extensa travesía, con falta de provisiones y, fundamentalmente, agua. Detalladamente se pueden encontrar en las cartas que le envió Alejandro a Aristóteles (recopiladas por Pseudo Calístenes), donde cuenta que en la expedición fueron atacados por «hombres gigantes sin inteligencia humana, que nos ocasionaron varias bajas», Incluso sacrificar parte de sus caballos por falta de alimento. Ante el cuestionamiento de sus hetaroi, respondió «estos caballos que sacrificamos nos darán el doble».

Todas las fuentes clásicas coinciden en que existió un encuentro entre las Amazonas y Alejandro Magno. Las Amazonas fue un pueblo de mujeres guerreras, cuya deidad principal era Diana, y su mito fundacional el dios olímpico Ares. Habían consolidado una sociedad matriarcal, en las inmediaciones del sur del mar Caspio, en Asia. 

El historiador Quinto Curcio Ruffo menciona que el macedonio fue visitado por la reina amazona Talestris, escoltada por una comitiva de 300 guerreras, cuyo fin fue proponerle engendrar hijos con Alejandro, para lograr herederas de estirpe guerrera y noble. Plutarco cita al menos 5 fuentes que comprueban este acontecimiento, con leves diferencias. En los escritos de Pseudo Calístenes, las cartas que Alejandro envió a Aristóteles recalcó «la belleza de esas mujeres y su gran fortaleza física». Curcio Ruffo cuenta que Alejandro ante tal propuesta "«no opuso demasiada resistencia»". Permanecieron encerrados 13 días y trece noches. 

Es sabido que Alejandro promovía la fusión de culturas y etnias a través de uniones y matrimonios mixtos, manteniendo la idea de abolir toda diferencia entre conquistadores y conquistados. Este tipo de eventos y ritos místicos-sexuales aparentemente eran comunes en la Antigüedad, como narró Calístenes, y que puso en duda la paternidad de Filipo, la unión de su madre Olimpia con Nectanebo -rey egipcio y supuesto padre biológico- para "engendrar un hijo que liberaría ambos reinos". 

Luego de todas estas exóticas experiencias, siguió la ruta trazada para perseguir a Bessos, internándose en zonas que oscilaban entre desiertos y montañas. Hasta que llegó a Sogdiana y Bactriana, donde entabló una relación de confianza con el sátrapa persa Artabazo II, cuya hija, la princesa Roxana, con quien Alejandro se casó, sería su compañía a partir de ahí en las campañas sucesivas.

En los escritos de Calístenes se narra que en un oasis en medio de la expedición, Alejandro avistó piedras preciosas en las cristalinas aguas. Luego de que unos soldados se metiesen al agua y fueran «devorados al instante por bestias acuáticas», planificó una de jaula anfibia, con tubos hechas con tripas de animales para respirar, para sumergirse él mismo y rescatarlas.

Finalmente, Bessos, el asesino del rey Darío, es arrestado por sus propios cortesanos, y entregado vivo a Ptolomeo, general y amigo de Alejandro (y futuro regente de Egipto). Es ejecutado, dando supuestamente por terminada la persecución. Alejandro dio aviso inmediatamente a la familia de Darío, que su asesino estaba vengado.

Pero ocurrió algo impensado: Espitamenes, cortesano de Bessos y principal mentor de su entrega, a cambio había pedido la independencia de Sogdiana y otras satrapías. Al tener la negativa, provocó importantes revueltas en las ciudades, aniquilando guarniciones griegas y generando caos al imperio establecido por Magno.

Espitamenes se desenvolvía en la región de "Aria", logró aliados de tribus nómades, jinetes arqueros de estepas y desiertos, y tomó las ciudades del este asiático controladas por los griegos (atacó la capital Maracanda, y Bactriana, pero Artabazo II repelió los ataques).

Alejandro ordenó fortificar todas las ciudades y satrapías, ya ahora en pasos montañosos defendibles. Pero el factor decisivo fue "fortificar todos los oasis", dejando a Espitamenes sin recursos para sus soldados y caballería. 
En diciembre de 328a.C., el comandante macedonio Coeno lo derrotó, y cuando los sogdianos y las tribus nómadas se enteraron de que el ejército principal de Alejandro se acercaba, los masagetas asesinaron a su líder y enviaron su cabeza al conquistador.

Espitamenes tenía una hija, Apama, quien se casó con uno de los generales más importantes de Alejandro, Seleuco (febrero de 324 a. C.). La pareja tuvo un hijo, Antíoco. Tras la muerte de Alejandro, Seleuco fundó la Dinastía Seleucida (todos los territorios persas desde Media, Asia Central y este), siendo Apama reconocida como la madre de la Dinastía Seléucida. Varias ciudades fueron llamadas Apamea en su honor.

Pronto llevaría a su ejército a atravesar el Hindu Kush y a dominar el valle del Indo, con la única resistencia del rey indio Poros en el río Hidaspes.

Con sus acciones militares extendió ampliamente la influencia de la civilización griega y preparó el camino para los reinos del período helenístico.

Tras la muerte de Espitámenes y su boda con Roxana ("Roshanak" en bactriano) para consolidar sus relaciones con las nuevas satrapías de Asia Central, en el 326 a. C. Alejandro puso toda su atención en el subcontinente indio e invitó a todos los jefes tribales de la anterior satrapía de Gandhara, al norte de lo que ahora es Pakistán para que vinieran a él y se sometieran a su autoridad. Āmbhi, rey de Taxila, cuyo reino se extendía desde el Indo hasta el Hidaspes, aceptó someterse pero los rajás de algunos clanes de las montañas, incluyendo los "aspasioi" y los "assakenoi" de la tribu de los "kambojas", conocidos en los textos indios como "ashvayanas" y "ashvakayanas" (nombres que se refieren a la naturaleza ecuestre de su sociedad, de la raíz sánscrita "ashva", que significa ‘caballo’), se negaron a ello.

Alejandro tomó personalmente el mando de los portadores de escudo, los compañeros de a pie, los arqueros, los agrianos y los lanzadores de jabalina a caballo y los condujo a luchar contra la tribu de los "kamboja" de la que un historiador moderno escribe que «eran gentes valientes y le fue difícil a Alejandro aguantar sus acometidas, especialmente en Masaga y Aornos».

Alejandro se enzarzó en una feroz contienda contra los "aspasioi" en la que le hirieron en el hombro con un dardo, pero en la que los "aspasioi" perdieron la batalla y 40 000 de sus hombres cayeron prisioneros. Los "assakenoi" fueron al encuentro de Alejandro con un ejército de 30 000 soldados de caballería, 38 000 de infantería y 30 elefantes, lucharon valientemente y opusieron una tenaz resistencia al invasor en las batallas de las ciudades de Ora, Bazira y Masaga, ciudad esta última cuyo fuerte fue reducido solo tras varios días de una sangrienta lucha en la que hirieron a Alejandro de gravedad en el tobillo.

Cuando el rajá de Masaga murió durante la batalla, el comandante supremo del ejército acudió a la vieja madre de este, Cleofis, la cual también parecía dispuesta a defender su tierra hasta el final y asumió el control total del ejército, lo que empujó también a otras mujeres del lugar a luchar por lo que Alejandro solo pudo controlar Masaga recurriendo a estratagemas políticas y actos de traición. Según Quinto Curcio Rufo, «Alejandro no solo mató a toda la población de Masaga, sino que redujo sus edificios a escombros». Una matanza similar ocurrió en Ora, otro bastión de los "assakenoi".

Mientras todas estas matanzas ocurrían en Masaga y Ora, varios "assakenoi" huyeron a una alta fortaleza llamada Aornos donde Alejandro los siguió de cerca y capturó la roca tras cuatro días de sangrienta lucha. La historia de Masaga se repitió en Aornos, y la tribu de los "assakenoi" fue masacrada.

En sus escritos acerca de la campaña de Alejandro contra los "assakenoi", Victor Hanson comenta: «Después de prometer a los "assakenoi", quienes estaban rodeados, que salvarían sus vidas si capitulaban, ejecutó a todos los soldados que aceptaron rendirse. Las contiendas de Ora y Aornos se saldaron de forma similar. Probablemente todas sus guarniciones fueron aniquiladas».
Sisikottos, que había ayudado a Alejandro en esta campaña, fue nombrado gobernador de Aornos. Tras reducir Aornos, Alejandro cruzó el Indo y luchó y ganó una batalla épica contra el gobernante local Poros, que controlaba la región de Panjab, en la batalla del Hidaspes del 326 a. C.

Tras la batalla, Alejandro quedó tan impresionado por la valentía de Poros que hizo una alianza con él y le nombró sátrapa de su propio reino al que añadió incluso algunas tierras que este no poseía antes. Alejandro llamó Bucéfala a una de las dos ciudades que había fundado, en honor al caballo que le había traído a la India, y que habría muerto durante la contienda del Hidaspes. Alejandro siguió conquistando todos los afluyentes del río Indo.

Al este del reino de Poros, cerca del río Ganges, estaba el poderoso Imperio de Magadha, gobernado por la dinastía Nanda. Temiendo la perspectiva de tener que enfrentarse con otro gran ejército indio y cansados por una larga campaña, el ejército macedonio se amotinó en el río Hífasis (actual río Beas), negándose a seguir hacia el este:

Alejandro, tras reunirse con su oficial Coeno, uno de sus hombres de confianza, se convenció de que era mejor regresar. Alejandro no tuvo más remedio que dirigirse al sur. Por el camino su ejército se encontró con los malios. Los malios eran las tribus más aguerridas del sur de Asia por aquellos tiempos. El ejército de Alejandro desafió a los malios, y la batalla los condujo hasta la ciudadela malia. Durante el asalto, el propio Alejandro fue herido gravemente por una flecha malia en el pulmón. Sus soldados, creyendo que el rey estaba muerto, tomaron la ciudadela y descargaron su furia contra los malios que se habían refugiado en ella, llevando a cabo una masacre, y no perdonaron la vida a ningún hombre, mujer o niño. A pesar de ello y gracias al esfuerzo de su cirujano, Critodemo de Cos, Alejandro sobrevivió a esa herida. Después de esto, los malios supervivientes se rindieron ante las fuerzas macedónicas, y estas pudieron continuar su marcha. 

Alejandro envió a la mayor parte de sus efectivos a Carmania (al sur del actual Irán) bajo el mando del general Crátero, y ordenó montar una flota para explorar el golfo pérsico bajo el mando de su almirante Nearco, mientras que él conduciría al resto del ejército de vuelta a Persia por la ruta del sur a través del desierto de Gedrosia (ahora parte del sur de Irán y de Makrán, en Pakistán). En su regreso a Babilonia, Alejandro sufre una importante pérdida: su oficial Coeno muere (326a.C.), producto de una enfermedad que había cotraído. Siendo Coeno uno de sus oficiales de infantería más destacados, Magno le rindió un funeral con todos los honores.

Alejandro dejó, no obstante, refuerzos en la India. Nombró a su oficial Peitón sátrapa del territorio del Indo, cargo que este ocuparía durante los próximos 10 años hasta el 316 a. C., y en Panyab dejó a cargo del ejército a Eudemos, junto con Poros y Āmbhi. Eudemos se convirtió en gobernador de una parte de Panyab después de que estos murieran. Él y Peitón volvieron a Occidente en el 316 a. C. con sus ejércitos. En el 321 a. C., Chandragupta Mauria fundó el Imperio mauria en la India y expulsó a los sátrapas griegos.

Tras enterarse de que muchos de sus sátrapas y delegados militares habían abusado de sus poderes en su ausencia, Alejandro ejecutó a varios de ellos como ejemplo mientras se dirigía a Susa. Como gesto de agradecimiento, Alejandro pagó las deudas de sus soldados, y anunció que enviaría a los veteranos mayores a Macedonia bajo el mando de Crátero, pero sus tropas malinterpretaron sus intenciones y se amotinaron en la ciudad de Opis, negándose a partir y criticando con amargura su adopción de las costumbres y forma de vestir de los persas, así como la introducción de oficiales y soldados persas en las unidades macedonias. Alejandro ejecutó a los cabecillas del motín, pero perdonó a las tropas. En un intento de crear una atmósfera de armonía entre sus súbditos persas y macedonios, casó en una ceremonia masiva a sus oficiales más importantes con persas y otras nobles de Susa, pero pocas de esas parejas duraron más de un año. Mientras tanto, en su regreso, Alejandro descubrió que algunos hombres habían saqueado la tumba de Ciro II el Grande, y los ejecutó sin dilación, ya que se trataba de los hombres que debían vigilar la tumba que Alejandro honraba.

Tras viajar a Ecbatana para recuperar lo que quedaba del tesoro persa, su amigo más íntimo, Hefestión, murió a causa de una enfermedad o envenenado, lo que afectó mucho a Alejandro.

El 13 de junio del 323 a. C. (10 de junio, según algunos autores), Alejandro murió en el palacio de Nabucodonosor II de Babilonia. Le faltaba poco más de un mes para cumplir los 33 años de edad. Existen varias teorías sobre la causa de su muerte, que incluyen envenenamiento por parte de los hijos de Antípatro (Casandro y Yolas, siendo este último copero de Alejandro) u otros sospechosos; enfermedad (se sugiere que pudo ser la fiebre del Nilo), o una recaída de la malaria que contrajo en el 336 a. C. Se sabe que el 2 de junio Alejandro participó en un banquete organizado por su amigo Medio de Larisa. Tras beber copiosamente, inmediatamente antes o después de su baño, lo metieron en la cama por encontrarse gravemente enfermo. Los rumores de su enfermedad circulaban entre las tropas, que se pusieron cada vez más nerviosas. El 12 de junio, los generales decidieron dejar pasar a los soldados para que vieran a su rey vivo por última vez, de uno en uno.

Plutarco hace referencia respecto a su última semana con vida, en la que "se internaba en extensos baños de inmersión para curarse" y sacrificar a los dioses, lo que sugiere la práctica de la "hidroterapia", muy común entre los griegos.

La teoría del envenenamiento deriva de la historia que sostenían en la antigüedad Justino y Curcio. Según ellos, Casandro, hijo de Antípatro, regente de Grecia, transportó el veneno a Babilonia con una mula, y el copero real de Alejandro, Yolas, hermano de Casandro y amante de Medio de Larisa, se lo administró. Muchos tenían razones de peso para deshacerse de Alejandro. Las sustancias mortales que podrían haber matado a Alejandro en una o más dosis incluyen el heléboro y la estricnina. Según el historiador Robin Lane Fox, el argumento más fuerte contra la teoría del envenenamiento es el hecho de que pasaron doce días entre el comienzo de la enfermedad y su muerte y en el mundo antiguo no había, con casi toda probabilidad, venenos que tuvieran efectos de tan larga duración.

Una de la hipótesis posibles es que sufrió una pancreatitis aguda, ya que los síntomas que sufrió, según explican los autores clásicos, encajan con los propios de esa enfermedad.

En 1865 el médico francés Émile Littré publicó el libro "La Verité sur la mort d'Alexandre le Grand" en el que basándose en el diario del secretario del rey concluyó que había muerto a causa de un mal tratamiento de una crisis de paludismo. En 2018 la doctora Katherine Hall, de la Universidad de Otago (Nueva Zelanda), afirmó que Alejandro habría muerto a causa del síndrome de Guillain-Barré, una enfermedad autoinmune cuyos síntomas son: fiebre alta, gran fatiga, dolores abdominales y parálisis de los miembros. Añadió que pudo ser embalsamado vivo porque sus médicos lo habrían creído muerto sin tomarle el pulso al paciente.

Alejandro no tenía ningún heredero legítimo. Su medio hermano Filipo Arrideo era deficiente, su hijo Alejandro nacería tras su muerte, y su otro hijo Heracles, cuya paternidad está cuestionada, era de una concubina. Debido a ello la cuestión sucesoria era de vital importancia.

Todos sus familiares y herederos, tanto su madre Olimpia, su esposa Roxana, su hijo Alejandro, su amante Barsine y su hijo Heracles, fueron mandados asesinar por Casandro, lo que llevó a la extinción de la dinastía Argéada.

Se da el nombre de "diádoco" a los pretendientes de la herencia del imperio que dejó Alejandro Magno. La mayoría de los historiadores creen que si Alejandro hubiera tenido la intención de elegir a uno de sus generales obviamente habría elegido a Crátero porque era el comandante de la parte más grande del ejército, la infantería, porque había demostrado ser un excelente estratega, y porque tenía las cualidades del macedonio ideal. Pero Crátero no estaba presente, y los otros pudieron haber elegido oír "Krat'eroi", ‘el más fuerte’. Fuera cual fuese su respuesta, Crátero no parecía ansiar el cargo. Entonces, el imperio se dividió entre sus sucesores (los diádocos).

A pesar de los intentos de mantener unificado el Imperio macedónico, este acabaría por dividirse en varios reinos independientes que fundaron sus dinastías.


En su lecho de muerte, sus generales le preguntaron a quién legaría su reino. Se debate mucho lo que Alejandro respondió: algunos creen que dijo "Krat'eroi" (‘al más fuerte’) y otros que dijo "Krater'oi" (‘a Crátero’). Esto es posible porque la pronunciación griega de ‘el más fuerte’ y ‘Crátero’ difieren solo por la posición de la sílaba acentuada. Algunos autores clásicos, como Diodoro, relatan que Alejandro dio detalladas instrucciones por escrito a Crátero poco antes de su muerte. Aunque Crátero ya había empezado a cumplir órdenes de Alejandro, como la construcción de una flota en Cilicia para realizar una expedición contra Cartago, los sucesores de Alejandro decidieron no llevarlas a cabo, basándose en que eran poco prácticas y extravagantes. El testamento, descrito en el libro XVIII de Diodoro, pedía expandir el imperio por el sur y el oeste del Mediterráneo, hacer construcciones monumentales y mezclar las razas occidentales y orientales. Sus puntos más interesantes fueron:


El cuerpo de Alejandro se colocó en un sarcófago antropomorfo de oro, que se puso a su vez en otro ataúd de oro y se cubrió con una capa púrpura. Pusieron este ataúd junto con su armadura en un carruaje dorado que tenía un techo abovedado soportado por peristilos jónicos. La decoración del carruaje era muy lujosa y fue descrita por Diodoro con gran detalle. Mary Renault resume sus palabras:

Según una leyenda, se conservó el cadáver de Alejandro en un recipiente de arcilla lleno de miel (que puede actuar como conservante) e introducido en un ataúd de cristal. Claudio Eliano cuenta que Ptolomeo robó el cuerpo mientras lo llevaban a Macedonia y lo llevó a Alejandría, donde se mostró hasta la Antigüedad Tardía. Ptolomeo IX, uno de los últimos sucesores de Ptolomeo I, reemplazó el sarcófago de Alejandro por uno de cristal y fundió el oro del original para acuñar monedas y saldar deudas, que surgieron durante su reinado. Los ciudadanos de Alejandría se mostraron horrorizados por esto y poco después Ptolomeo IX fue asesinado.

Luego que Roma ocupara Egipto definitivamente en el año 29a.C., la tumba de Alejandro ha sido saqueada, y el propio cuerpo de Magno flagelado por los mismos emperadores romanos. El emperador Octavio Augusto rompió la nariz de Alejandro. Luego Pompeyo el Grande robó su capa. Se dice que el emperador romano Calígula saqueó la tumba, robando la coraza de Alejandro para ponérsela. Alrededor del 200 d. C., el emperador Septimio Severo cerró la tumba de Alejandro al público. Su hijo y sucesor, Caracalla, admiraba mucho a Alejandro y visitó la tumba durante su reinado. Tras esto, los detalles sobre el destino de la tumba son confusos.

Ahora se piensa que el llamado «Sarcófago de Alejandro», descubierto cerca de Sidón y ahora situado en el Museo Arqueológico de Estambul, pertenecía en realidad a Abdalónimo, a quien Hefestión nombró rey de Sidón por orden de Alejandro. El sarcófago muestra a Alejandro y a sus compañeros cazando y luchando contra los persas.

Al comienzo de la campaña, su ejército era de 40000 hombres. Luego ese número se incrementó hasta 50000 al recibir refuerzos de aliados griegos. Por lo que, inicialmente, su ejército estaba compuesto de 35000 soldados de infantería, y 5000 de caballería. Es un número bastante bajo en comparación de los grandes volúmenes de ejércitos que utilizaba Darío (600000) y las ciudades de los sátrapas persas.

El ejército macedonio bajo Filipo II y Alejandro Magno consistía de diferentes cuerpos que se complementaban entre sí: caballería pesada y caballería ligera; infantería pesada e infantería ligera, armas de asedio (catapultas).

Caballería pesada formaba por izquierda (aliados griegos) y por derecha con Alejandro, que la constituían los "hetairoi" o compañeros. 

A lo largo de su vida, Alejandro se casó con varias princesas de los anteriores territorios persas:
Alejandro fue padre de al menos dos niños: Heracles de Macedonia, nacido en el 327 a. C. de la princesa Barsine, hija del sátrapa Artabazo de Frigia, y Alejandro IV de Macedonia, nacido en el 323 a. C. de la princesa Roxana, seis meses después de la muerte de Alejandro.

Una de sus concubinas más célebres fue la tesalia Campaspe, de aparentemente gran belleza, que a petición de Alejandro fue retratada por Apeles (su pintor preferido) y servido de modelo para la pintura de "Venus saliendo del mar," entre otras obras. Alejandro sentía simpatía y respeto por este pintor e incluso se sometía a sus exigencias.

Generalmente se considera que el objeto principal de los afectos de Alejandro fue su amigo, estratega de campo de batalla y comandante de caballería, Hefestión, al que probablemente se hallaba unido desde la niñez, dado que ambos se educaron en la corte de Pella. Hefestión hace su aparición en la Historia en el momento en que el conquistador alcanza Troya. Allí ambos realizaron sacrificios en los altares de los héroes de la "Ilíada", Alejandro honrando a Aquiles y Hefestión a Patroclo.

A continuación, citaremos algunos fragmentos cuestionados, enmarcados dentro de la saga de escritores denominado «vulgata», llamados así literalmente en el Anábasis de Magno (Flavio Arriano, prólogo), por la falta de rigor histórico, basado en «habladurías». Algunos de ellos son Justino, Diodoro y Curcio.

Ejemplo, la carta 24 atribuida al sofista y cínico Diógenes —de muy dudosa fiabilidad, ya que vivió en el siglo IVa.C., y esta fue escrita en el siglo I o II d.C.— expresa que amonestó a Alejandro diciendo «Si quieres ser hermoso y bueno ("kalos kai agathos"), arroja ese trapo que tienes sobre tu cabeza y ven con nosotros. Pero no serás capaz de hacerlo, dado que estás dominado por los muslos de Hefestión». Como se sabe, Diógenes despreciaba a todos por igual, y Alejandro ha sido su principal centro de ironías y burlas.

El escritor romano Curcio (siglo Id.C.) fue uno de los impulsores de introducir la idea de su ambivalencia sexual. Curcio relata que «Alejandro despreciaba los placeres sensuales a tal grado que su madre estaba ansiosa por temor de que este no le dejase descendencia». Para agudizar su apetito por las mujeres, el rey Filipo (que ya había reprochado a su hijo por cantar con voz demasiado aguda cuando Alejandro era aun pequeño) junto a su madre Olimpia, trajo a una costosa cortesana llamada Calixina, esta narración se sitúa en la época adolescente de Alejandro, etapa en la que el macedonio estaba deslumbrado por las enseñanzas de Aristóteles, cuando sus padres tenían buena relación. 

Curcio mantiene que Alejandro también tomó como amante a Bagoas, un eunuco persa que Alejandro designó como uno de sus trierarcas, hombres de capacidad administrativa y carácter que supervisaban y financiaban la construcción de barcos. Además de Bagoas, Curcio menciona otro amante de Alejandro, Euxenipo. 

Los debates sobre la identidad sexual de Alejandro son considerados anacronismos por los eruditos en ese período, quienes señalan que el concepto de homosexualidad no existía en la Antigüedad: la atracción sexual entre hombres era vista como normal y parte universal de la naturaleza humana, ya que el hombre era atraído hacia la belleza, que era un atributo de la juventud, independientemente del sexo. Si la vida amorosa de Alejandro fue transgresora lo fue no por su amor hacia jóvenes bellos, sino por su relación con hombres de su propia edad en un tiempo en el que el modelo estándar del amor masculino era el que relacionaba hombres mayores con otros mucho más jóvenes.

Principalmente en Asia, Alejandro Magno es adjetivado "Dhul-Qarnayn" (‘el de dos cuernos’), porque se hacía representar como el dios Zeus-Amón, llevando una diadema con dos cuernos de carnero (el animal que representa a Amón), y por los dos largos penachos blancos que salían de su yelmo.

La figura del rey macedonio se prestó desde la Antigüedad a todo tipo de fantasías legendarias. Así, una leyenda neogriega recogida por Nikolaos Politis presenta a Alejandro obsesionado por la inmortalidad (como Gilgamesh) y emprendiendo en vano la búsqueda del agua sagrada que podría proporcionársela.

Los zoroastristas lo recuerdan en el Arda Viraf como el «maldito Alejandro», responsable de la destrucción del Imperio persa y el incendio de su fastuosa capital, Persépolis.

Entre las culturas orientales se le conoce como Eskandar-e Maqduni (‘Alejandro de Macedonia’) en persa, "Dhul-Qarnayn" (‘el de los dos cuernos’) en las tradiciones del Medio Oriente, "Al-Iskandar al-Akbar الإسكندر الأكبر" en árabe, "Sikandar-e-azam" en urdu e hindi, "Skandar" en pashto, "Alexander Mokdon" en hebreo, y "Tre-Qarnayia" (‘el de los dos cuernos’) en arameo, debido a una imagen empleada en monedas acuñadas durante su reinado en las que aparece con los cuernos de carnero del dios egipcio Amón. Sikandar, su nombre en urdu e hindi, también se utiliza como sinónimo de ‘experto’ o ‘extremadamente hábil’.

El cráter lunar "Alexander" lleva este nombre en su honor.

Al final de la República y a principios del Imperio, los ciudadanos romanos cultos usaban el latín solo para asuntos legales, políticos y ceremoniales, empleando el griego para hablar sobre filosofía o sobre cualquier otro debate intelectual. A ningún romano le gustaba oír que su dominio de la lengua griega era pobre. En el mundo romano, la única lengua que se hablaba en todas partes era la "koiné", variante de griego que hablaba Alejandro.

Muchos romanos admiraban a Alejandro Magno y sus conquistas y querían igualar sus hazañas, aunque poco se sabe acerca de las relaciones diplomáticas que mantenían Roma y Macedonia en aquellos tiempos. Julio César lloró en Hispania con la sola presencia de una estatua de Alejandro, "lamentándose de que a su edad no había conseguido realizar tantas cosas". También Julio César honró la tumba de Alejandro Magno en su estadía en Alejandría (Egipto), siendo Cleopatra su aliada y anfitriona, y que posteriormente le daría un hijo, Cesarión (fue la última reina de la dinastía helenística de Ptolomeo). En el año 29a.C. el Egipto de Ptolomeo cae en manos de Roma definitivamente, Alejandría era el último bastión helénico en pie. Luego de la caída de Alejandría, la tumba y el cuerpo de Magno fue saqueado y arruinado poco a poco por los propios emperadores romanos. El emperador Augusto (más conocido como Octavio), luego de someter a Egipto y sus ciudades más importantes, fue a visitar su tumba en Alejandría, le preguntaron si quería ver el lugar de descanso de los faraones ptolemaicos, a lo que respondió que "Alejandro era el único líder que merecía su visita".Acto seguido, Augusto, en su empeño de honrar a Alejandro, rompió accidentalmente la nariz del cuerpo momificado mientras dejaba una guirnalda en el altar del rey. Pompeyo el Grande robó la capa de Alejandro, de 260 años de antigüedad, y se la puso como símbolo de grandeza. Calígula, el emperador desequilibrado, robó la coraza de Alejandro de su tumba para su uso personal. Los Macriani, una familia romana que ascendió al trono imperial en el siglo III d. C., llevaban siempre consigo la imagen de Alejandro, ya fuera estampada en brazaletes y anillos o cosida en sus ropas. Hasta en su vajilla estaba representada la cara de Alejandro, y la vida del rey se podía ver descrita con dibujos a lo largo de los bordes de los platos.

Se han escrito innumerables obras de Alejandro






















Historia de Grecia


</doc>
<doc id="6672" url="https://es.wikipedia.org/wiki?curid=6672" title="Enciclopedia Libre Universal en Español">
Enciclopedia Libre Universal en Español

La Enciclopedia Libre Universal en Español (EL) es un proyecto escindido de la Wikipedia en español. Fue creado el 26 de febrero de 2002.

El 27 de enero de 2018 la Enciclopedia Libre contaba con 50 240 artículos y 15 578 imágenes.

Durante un poco más de un año después de su creación en 2002, contó con un grupo de colaboradores y de visitas más numeroso que la Wikipedia en Español (antes de la actualización del software en Wikipedia en octubre de 2002, considerablemente superior). A lo largo de su trayectoria se han ido incorporando materiales que, en el caso de la Wikipedia en español, pertenecen a proyectos paralelos, como entradas de diccionario (cubiertos en Wikipedia en español por el Wikcionario) o documentos históricos (gestionados por Wikisource).

Según algunos de sus colaboradores, al ser desconectada de la enciclopedia madre, la de lengua inglesa, la Enciclopedia Libre no tendría tendencia a ser una mera traducción de la Wikipedia en inglés, y produciría más artículos "locales". Sin embargo, los mismos colaboradores piensan que el no poder enlazar con artículos de otras enciclopedias en otras lenguas es privarse de cierta riqueza cultural . También argumentan que la existencia de varias enciclopedias colaborativas en español constituirían un enriquecimiento del enciclopedismo libre en red, ya que permitirían a quien consulta tener varios puntos de vista sobre un mismo tema.

Debido a ciertas acciones tomadas en Wikipedia, consideradas lesivas, algunos cooperadores decidieron crear una web aislando el fin lucrativo ideado en la empresa Bomis Inc. El primer desarrollo fue en la Universidad de Sevilla decidido por Juan Antonio Ruiz Rivas, uno de sus empleados.

El intercambio entre la Enciclopedia Libre y la Wikipedia en español se hace fundamentalmente mediante la copia integral de artículos de una a otra (también existen wikipedistas que colaboran en EL, produciendo los mismos contenidos para ambas enciclopedias). Aunque en principio pudiera pensarse que este intercambio haría que ambas se parecieran cada día más, de hecho tanto los contenidos como, fundamentalmente, la estructura de la información van divergiendo .

El Taller es un proyecto hermano de la Enciclopedia Libre, y su principal diferencia con otros proyectos wiki es que se orienta a la iniciativa y creatividad individual. Se destina a albergar contenidos no enciclopédicos tales como imágenes, poemas, cuentos, entre otras cosas que son creaciones enteramente de cada usuario (a diferencia de Commons).



</doc>
<doc id="6674" url="https://es.wikipedia.org/wiki?curid=6674" title="Melanoma">
Melanoma

Melanoma es el nombre genérico de los tumores melánicos o pigmentados ("mélas" (μελας gr.) "negro" + -o-ma 1 (-ομα gr.) "tumor") y el melanoma maligno es una grave variedad de cáncer de piel, causante de la mayoría de las muertes relacionadas con el cáncer de piel. Se trata de un tumor generalmente cutáneo, pero también del intestino y el ojo (melanoma uveal) y altamente invasivo por su capacidad de generar metástasis. Actualmente el único tratamiento efectivo es la resección quirúrgica del tumor primario antes de que logre un grosor mayor de 1 mm.

Cerca de 160 000 casos nuevos de melanoma se diagnostican cada año mundialmente, y resulta más frecuente en hombres y personas de raza blanca que habitan regiones con climas soleados. Según un informe de la Organización Mundial de la Salud, ocurren cada año cerca de 48 000 muertes relacionadas con el melanoma. Se estima que el melanoma maligno produce un 75 % de las muertes asociadas al cáncer de piel.

Por lo general, el riesgo de un individuo de contraer un melanoma depende de dos tipos de factores: intrínsecos y ambientales. Los factores intrínsecos incluyen la historia familiar y el genotipo heredado; mientras que el factor ambiental o extrínseco más relevante es la exposición a la luz solar.

Los estudios epidemiológicos sugieren que la exposición a la radiación proveniente de la luz ultravioleta (UVA y UVB) es una de las causas principales en la aparición del melanoma. 

El melanoma es más frecuente en la espalda de los hombres y en las piernas de las mujeres. El riesgo parece estar fuertemente influido por las condiciones socioeconómicas de la persona, no tanto por el hecho de que su ocupación se desarrolle en el interior o en el exterior de un edificio. De modo que es más común ver melanomas en profesionales y personal administrativo que en trabajadores o graduados.
El uso de camas de bronceado con rayos ultravioleta penetrantes se ha asociado con la aparición del cáncer de piel, incluyendo el melanoma.

La radiación causa daño en el ADN de las células, típicamente una dimerización de la timina que, al no ser reparado por la maquinaria intracelular, crea mutación en los genes celulares.

La secuenciación masiva del genoma de muestras de melanomas metastásicos de pacientes ha permitido detectar distintas mutaciones, no solo mutaciones puntuales (transiciones C->T principalmente), sino también reordenamientos cromosómicos (deleciones, amplificaciones, translocaciones), incluyendo el fenómeno de la cromotripsis, que provocan una alta inestabilidad genómica. Cuando la célula se divide, estas mutaciones se propagan a nuevas generaciones de células. Si la mutación ocurre justo sobre un protooncogén (dará lugar a un oncogén) o si se produce en genes supresores tumorales, la velocidad de la mitosis o división celular en las células se vuelve descontrolada con las mutaciones, conllevando a la formación de un tumor.

La mayoría de los estudios sobre quemaduras sugieren una relación positiva o directa entre las quemaduras a edades tempranas y el consiguiente riesgo de padecer melanoma. Los pacientes que presentan un historial de alta exposición a la luz ultravioleta, suelen tener un porcentaje de mutaciones en genes como NRAS o BRAF (oncogenes) superior al que poseen los pacientes con una exposición normal o baja.

Según su localización se denominan:

Tiene formas compuestas:

Para prevenirse del melanoma, ante la llegada del verano, es preciso adoptar una serie de medidas de protección, como la utilización de gorras o sombreros que no dejen pasar los rayos solares, de cremas con factor protección solar mayor de 50, así como tomar el sol de una forma gradual y evitarlo en las horas de irradiación más intensa (entre las 12:00 y 16:00). Incluso debajo de las sombrillas el sol es dañino, ya que el efecto espejo de la arena puede inducir los rayos solares con mayor intensidad.

El prototipo humano con mayores posibilidades de contraer dicha enfermedad es una mujer entre 40 y 45 años, de piel y ojos claros que realice exposiciones solares intensas e intermitentes desde la infancia, con quemaduras en la etapa infantil, con un número importante de nevus congénitos o atípicos, y con antecedentes familiares de melanoma.

Algunos consejos para prevenir la aparición de melanomas son:

Para saber cuándo la apariencia es sospechosa existe una regla denominada A, B, C y D. Así, cuando un nevus es Asimétrico, tiene unos Bordes irregulares, toma una Coloración muy oscura o irregular y su Diámetro aumenta, son indicios de melanoma, por lo que se debe acudir al médico.

Se debe prestar especial atención, por ser marcadores de melanoma, a los nevus pigmentocelulares adquiridos que, a lo largo de la vida, modifican su morfología. También a los nevus atípicos o (urielosis) y congénitos, siendo los principales signos de alarma los nevus asimétricos, con bordes imprecisos, color cambiante y sangrado. Este cuadro puede darse conjuntamente.

Los melanomas se clasifican con fines pronósticos según sus características. Hay varios sistemas:

Una detección precoz permite la extirpación quirúrgica de la práctica totalidad de los melanomas. Actualmente se utilizan técnicas de diagnosis no cruentas tales como la dermatoscopia (también denominada epiluminiscencia) que permiten detectar cualquier alteración precoz de los nevus y su posible malignidad. Tras la cirugía solo los pacientes de alto riesgo necesitan inmunoterapia adicional. Si en un período de 3 a 5 años no se ha reproducido el melanoma, las posibilidades de recaída son mínimas.

En 2012 se aprobó el uso del vemurafenib, comercializado como Zelboraf, para tratar un tipo de melanoma asociado a una mutación del gen BRAF, una de las formas más malignas de esta enfermedad.

Además, y a raíz de los resultados del último estudio denominado coBRIM, en el que se ha combinado Vemurafenib con un nuevo medicamento en fase de ensayo (cobimetinib), se ha confirmado que el uso conjunto de ambas medicinas permite bloquear diferentes dianas de la célula tumoral impidiendo durante más tiempo la progresión del tumor.




</doc>
<doc id="6678" url="https://es.wikipedia.org/wiki?curid=6678" title="Código de barras">
Código de barras

El código de barras es un código basado en la representación de un conjunto de líneas paralelas de distinto grosor y espaciado que en su conjunto contienen una determinada información, es decir, las barras y espacios del código representan pequeñas cadenas de caracteres. De este modo, el código de barras permite reconocer rápidamente un artículo de forma única, global y no ambigua en un punto de la cadena logística y así poder realizar inventario o consultar sus características asociadas.

La correspondencia o mapeo entre la información y el código que la representa se denomina "simbología". Estas simbologías pueden ser clasificadas en grupos atendiendo a dos criterios diferentes:

La primera patente de código de barras fue otorgada el 7 octubre de 1952 (US Patent #2,612,994) por los inventores Joseph Woodland, Jordin Johanson y Bernard Silver en Estados Unidos. La implementación fue posible gracias al trabajo de los ingenieros Raymond Alexander y Frank Stietz. El resultado de su trabajo fue un método para identificar los vagones del ferrocarril utilizando un sistema automático. Sin embargo, no fue hasta 1966 cuando el código de barras comenzó a utilizarse comercialmente y recién tuvo éxito comercial en 1980.


Ejemplo de datos contenidos en un código de barras GTIN 13:


Los códigos de barras se imprimen en los envases, embalajes o etiquetas de los productos. Entre sus requisitos básicos se encuentran la visibilidad y fácil legibilidad por lo que es imprescindible un adecuado contraste de colores. En este sentido, el negro sobre fondo blanco es el más habitual encontrando también azul sobre blanco o negro sobre marrón en las cajas de cartón ondulado. El código de barras lo imprimen los fabricantes (o, más habitualmente, los fabricantes de envases y etiquetas por encargo de los primeros) y, en algunas ocasiones, los distribuidores.

Para no entorpecer la imagen del producto y sus mensajes promocionales, se recomienda imprimir el código de barras en lugares discretos tales como los laterales o la parte trasera del envase. Sin embargo, en casos de productos pequeños que se distribuye individualmente no se puede evitar que ocupe buena parte de su superficie: rotuladores, barras de pegamento, entre otros.

Los códigos de barras se dividen en dos grandes grupos: los códigos de barras lineales (o unidimensionales) y los códigos de barras de dos dimensiones.

Algunos de los código de barra lineales más habituales son:


Los códigos bidimensionales, en los que se lee tanto en horizontal como en vertical, nacieron como una evolución lógica de los unidimensionales para aumentar la capacidad de representar contenido.

Algunos de los código de barra lineales más habituales son:

Entre todas las primeras justificaciones de la implantación del código de barras se encontraron la necesidad de agilizar la lectura de los artículos en las cajas y la de evitar errores de digitación. Otras ventajas que se pueden destacar de este sistema son:

La información se procesa y almacena con base en un sistema digital binario donde todo se resume a sucesiones de unos y ceros. La memoria y central de decisiones lógicas es un computador electrónico del tipo universal, disponible ya en muchas empresas comerciales y generalmente compatible con las distintas marcas y modelos de preferencia en cada país. Estos equipos permiten también interconectar entre sí distintas sucursales o distribuidores centralizando toda la información.
Ahora el distribuidor puede conocer mejor los parámetros dinámicos de sus circuitos comerciales, permitiéndole mejorar el rendimiento y las tomas de decisiones, ya que conocerá con exactitud y al instante toda la información proveniente de las bocas de venta estén o no en su casa central. Conoce los tiempos de permanencia de depósito de cada producto y los días y horas en que los consumidores realizan sus rutinas de compras, pudiendo entonces decidir en qué momento debe presentar ofertas, de qué productos y a qué precios.

Entre las pocas desventajas que se le atribuyen se encuentra la imposibilidad de recordar el precio del producto una vez apartado del lineal. También hay que aclarar que el código QR no es un código de barras propiamente, sus métodos de lectura se diferencian y claramente los QR no son barras. Se incluyen aquí por ser utilizados para el mismo fin que los códigos de barras.

Para facilitar la lectura del código de barras, se aplica un contraste alto entre los componentes oscuros y claros del código.







</doc>
<doc id="6682" url="https://es.wikipedia.org/wiki?curid=6682" title="IEEE 1394">
IEEE 1394

IEEE 1394 es un tipo de conexión para diversas plataformas, destinado a la entrada y salida de datos en serie a gran velocidad. Suele utilizarse para la interconexión de dispositivos digitales como cámaras digitales y videocámaras a computadoras. Existen cuatro versiones de 4, 6, 9 y 12 pines. En el mercado doméstico su popularidad ha disminuido entre los fabricantes de hardware, y se ha sustituido por la interfaz USB en sus versiones 2.0 y 3.0, o la interfaz Thunderbolt, aunque es ampliamente utilizado en automatización industrial, industria militar y para el entorno profesional. 


Publicado en 2000. Duplica aproximadamente la velocidad del FireWire 400, hasta 786,5 Mbit/s con tecnología semi-duplex, cubriendo distancias de hasta 100 metros por cable.
Firewire 800 reduce los retrasos en la negociación, utilizando para ello 8b/10b (código que codifica 8 bits en 10 bits, que fue desarrollado por IBM y permite suficientes transiciones de reloj, la codificación de señales de control y detección de errores. El código 8b/10b es similar a 4B/5B de FDDI (que no fue adoptado debido al pobre equilibrio de corriente continua), que reduce la distorsión de señal y aumenta la velocidad de transferencia. Así, para usos que requieran la transferencia de grandes volúmenes de información, resulta muy superior al USB 2.0. Posee compatibilidad retroactiva con Firewire 400 utilizando cables híbridos que permiten la conexión en los conectores de Firewire400 de 6 pines y los conectores de Firewire800, dotados de 9 pines. En el 2003 lanzó Apple el primer dispositivo de uso comercial de Firewire800.
Anunciados en diciembre de 2007, permiten un ancho de banda de 1,6 y 3,2 Gbit/s, cuadruplicando la velocidad del Firewire 800, a la vez que utilizan el mismo conector de 9 pines. Es ideal para su utilización en aplicaciones multimedia y almacenamiento, como videocámaras, discos duros y dispositivos ópticos. Su popularidad es escasa.

Anunciado en junio de 2007. Aporta mejoras técnicas que permiten el uso de la interfaz con puertos RJ-45 sobre cable CAT 5, combinando así las ventajas de Ethernet con Firewire800.






La edición de vídeo digital con IEEE 1394 ha permitido la incorporación de FireWire en cámaras de vídeo de bajo costo y elevada calidad lo cual permite la creación de vídeo profesional en las plataformas Macintosh y PC. La interfaz IEEE 1394 permite la captura de vídeo directamente de cámaras de vídeo digital con puertos FireWire incorporados y de sistemas analógicos mediante conversores de audio y vídeo.



</doc>
<doc id="6686" url="https://es.wikipedia.org/wiki?curid=6686" title="European Article Number">
European Article Number

El número de artículo europeo (en inglés European Article Number o EAN) es un sistema de códigos de barras adoptado por más de cien países y cerca de un millón de empresas. En el año 2005, la asociación EAN se fusionó con la UCC para formar una nueva y única organización mundial identificada como GS1, con sede en Bélgica.

El código EAN más usual es EAN-13, constituido por trece (13) dígitos y con una estructura dividida en cuatro partes:


Por ejemplo, para 123456789041 el dígito de control será:


El código quedará así: codice_9.

my $ean = '750863367001'; # Valor de prueba
my $checksum = 1000; 
my $i = 0;

for my $digit (split //, reverse $ean) { # Recorremos el $ean de forma inversa, dígito por dígito

$checksum %= 10; # Ajustamos a la decena inmediatamente inferior

print "Dígito de control: $checksum\n";
print "Código EAN: $ean$checksum\n";

// Cálculo del dígito de control EAN
function ean13_checksum ($message) {

// Valor de prueba (sin dígito de control)
$ean = '931804231236';
echo 'Digito de control: ', ean13_checksum($ean);

// Cálculo del dígito de control EAN
string EAN = "123456789012";
int iSum = 0;
int iSumInpar = 0;
var iDigit = 0;

for (int i = EAN.Length; i >= 1; i--)

iDigit = (iSumInpar) + (iSum*3) ;

int iCheckSum = (10 - (iDigit % 10)) % 10;
Console.Write("Digito de control: " + iCheckSum.ToString());

EAN = "123456789041"

def ean_checksum(code: str) -> int:

print(f"Dígito de control: {ean_checksum(EAN)}")

EAN = "123456789041"

def eanCheck(ean):

print "Digito de control: %d" %eanCheck(EAN)

'Cálculo del dígito de control EAN
Dim iSum As Integer
Dim iDigit As Integer
Dim EAN As String
Dim i As Integer
iSum = 0
iDigit = 0
EAN = Text1.Text 'EAN de prueba

For i = Len(EAN) To 1 Step -1
Next
Dim iCheckSum As Integer
iCheckSum = (10 - (iSum Mod 10)) Mod 10
Text2.Text = iCheckSum
Text3.Text = Text1.Text & Text2.Text & "0000000000000"

// Cálculo del dígito de control EAN
function ean13_checksum(message) {
// Valor de prueba (sin dígito de control)
var ean = '123456789041';
console.log(ean13_checksum(ean));

function ean13_checksum() {

ean13_checksum 123456789041
 CALL FUNCTION 'MARA_EAN11'

// Cálculo del dígito de control EAN
function cc_CalcDV_Ean(Ean: String): Integer; 
var
begin
end;
 * Cálculo del código de control
private int controlCodeCalculator(String firstTwelveDigits)

DECLARE @initialBarcode VARCHAR(13), @auxBarcode VARCHAR(13), @finalBarcode VARCHAR(13)

SET @auxBarcode = REVERSE(@initialBarcode)

DECLARE @verifierCode INT = 0
SELECT 
FROM dbo.NumbersTable(1,12,1) --- FALTA ESTO EN EL SQL

SET @verifierCode = RIGHT((10 - RIGHT(@verifierCode,1)),1)

SET @finalBarcode = CONCAT(@initialBarcode, @verifierCode)

RETURN @finalBarcode

if isnumeric(ean)
end;
checksum = sum((ean(1:end-1)-'0').*(rem([0:11],2)*2+1));
if rem(10 - (rem(checksum,10)), 10) ~= ean(13)-'0'
end;

FUNCTION DIGITO_VERIFICADOR_EAN
LPARAMETERS tcCodigoBarras
LOCAL lnSuma, lnI, lnDigito, lnDigitoVerificador
ENDFUNC
El ejemplo calcula un EAN13 para la celda A1, si fuera 123456789041 añadiría un 8 como dígito de control

=SI(LARGO(A1)>12;"MAS de 13";SI(ESERROR(A1*1);"NO NUMERICO";CONCATENAR(REPETIR(0;12-LARGO(A1));A1)&RESIDUO((10-(DERECHA(((SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);2;1));"0")+(SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);4;1));"0"))+(SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);6;1));"0"))+(SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);8;1));"0"))+(SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);10;1));"0"))+(SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);12;1));"0")))*3)+(SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);1;1));"0")+(SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);3;1));"0"))+(SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);5;1));"0"))+(SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);7;1));"0"))+(SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);9;1));"0"))+(SI.ERROR(VALOR(EXTRAEB(TEXTO(A1;0);11;1));"0")));1)));10)))
El ejemplo calcula un EAN13 para la celda A6, si fuera 123456789041 añadiría un 8 como dígito de control

=IF(ISNUMBER(VALUE(A6));

// Se presupone que la variable ean contiene un código EAN válido

char ean[13] = "123456789041";
char *car = ean + 12;
int par = !0;
int control = 0;
int num;

while(car-- - ean) {

return 10 - control % 10;




</doc>
<doc id="6688" url="https://es.wikipedia.org/wiki?curid=6688" title="Cifra (matemática)">
Cifra (matemática)

Una cifra es un símbolo o carácter gráfico que sirve para representar un número. Por ejemplo, los caracteres «0», «1», «2», «3», «4», «5», «6», «7», «8» y «9» son cifras del sistema de numeración arábigo, mientras que los caracteres «», «», «», «», «», «» y «» son cifras del sistema de numeración romano.

Las cifras se usan también como identificadores en: números de teléfono, numeración de carreteras; como indicadores de orden en: números de serie; como códigos (ISBN), etc.

Un numeral es una cadena de cifras utilizada para denotar un número (no un código identificativo). A modo de ejemplo, los numerales «21», «2», «3», «4» y «500» representan en el sistema arábigo los mismos números que los respectivos numerales «», «», «», «» y » en el sistema romano.

Un número dígito es un número que puede expresarse empleando un numeral de una sola cifra. Por extensión se puede decir que un dígito es cada símbolo o guarismo de los usados para expresar un numeral o un número.

En el sistema decimal son: 0, 1, 2, 3, 4, 5, 6, 7, 8 y 9. Así, 157 se compone de los dígitos 1, 5 y 7. El nombre dígito proviene del latín dígitus dedo, porque los 10 dedos corresponden a los 10 dígitos en el sistema numérico común en base 10, esto es, un dígito decimal.

En matemáticas y ciencia de la computación, un dígito numérico es un símbolo, v.gr. «3», que usado en combinaciones, v.gr. «37», representa números (enteros o reales) en sistemas de numeración posicionales. 

Por tradición, al menos desde la época del Antiguo Egipto, se usa el sistema decimal, debido al arcaico uso de los diez dedos para ayudarse a contar, aunque no hay ninguna razón especial para que un sistema de numeración deba utilizar la base diez.

En el sistema decimal se necesitan 10 dígitos, aunque tienen diferente valor en función de su posición en el numeral, pues su valor varía de diez en diez, esto es unidades, decenas (10), centenas (10), millares (10), y así sucesivamente, de modo que un dígito a la izquierda tiene diez veces el valor de la posición dada y a la derecha la décima parte del valor de la misma. Para separar valores menores a la unidad se usa el punto decimal (en Europa la coma). Este método de notación posicional, proviene de la India y fue transmitido a Occidente por los matemáticos musulmanes durante la Edad Media.

El más simple es el sistema binario, que solo precisa dos dígitos, generalmente representados por 0 y 1; en el sistema binario varían dos en dos: unidades, parejas (2), cuartetas (2), y así sucesivamente. Es un sistema profusamente empleado en informática. 

Ejemplos de dígitos incluyen cualquiera de los caracteres decimales desde «0» hasta «9», o de los caracteres del sistema binario «0» o «1», y los dígitos «0»...«9», «A»...,«F» usados en el sistema hexadecimal. En un sistema de numeración dado, si la base (radical, en inglés ) es un entero, el número de dígitos necesarios, para la parte entera, es igual al siguiente entero del logaritmo del número a representar dividido entre el logaritmo de la base. Para la parte fraccionaria el número de dígitos dependerá de la precisión necesaria a manejar.

En los sistemas de numeración, los dígitos se combinan para representar distintos números. Si el valor viene determinado por la posición del dígito, se habla de notación posicional. Si los dígitos tienen un valor fijo, que no depende de su posición, se habla de notación aditiva, como, por ejemplo, la numeración romana. 

Cuando los árabes del siglo X adoptaron la numeración de la India, tradujeron la palabra «"sunya"», que significaba ‘vacío’ o ‘en blanco’, por «"sifr"», ‘vacío’ en árabe. Después, el sistema de numeración indo-arábigo fue introducido en Italia y la palabra «"sifr"» se latinizó como «"zephirum"». El proceso comenzó a principios del siglo XIII y con el correr del tiempo una sucesión de cambios culminó con la palabra italiana «"zero"».

Casi paralelamente se desarrolló un proceso similar en Alemania. Jordanus Nemorarius cambió la palabra «"sifr"» por «cifra». Durante un tiempo en Europa ambas palabras denotaban el cero. Como uno de los testimonios de esta etapa, la palabra inglesa «"cipher"» tiene actualmente dos significados: ‘cifra’, en el sentido moderno, y ‘cero’ en su forma arcaica, de acuerdo a su etimología.

Las palabras «cifra», «"chiffre"», «"cipher"», «"ziffer"» y «"zero"» representaban el cero para los doctos.

La historia no contempla los títulos y honores de los doctos. Los procesos sociales cambian irremediablemente algunos de los conceptos originales. Cuando la masa adopta un uso, es inútil todo esfuerzo en sentido contrario.

En la Edad Antigua y en la Edad Media los cálculos eran realizados por expertos. Hasta la adopción definitiva del sistema de posición y el cero, la multiplicación y la división se realizaban por duplicaciones y mediaciones, respectivamente. Por ejemplo, para multiplicar un número por 13 se descomponía al multiplicador en potencias de 2, en este caso, 8 + 4 + 1. El multiplicando se duplicaba dos y tres veces. Luego se sumaban la triple duplicación, la doble duplicación y la cantidad original. La división seguía un proceso análogo pero inverso. Los cálculos demandaban mucho tiempo de trabajo y el costo era elevado. Puede observarse un residuo de esto en la forma en que se subdividen las medidas antiguas, como la pulgada inglesa: medios, cuartos, octavos, dieciseisavos, treintaidosavos.

Los comerciantes de aquellos tiempos debían solventar esos gastos para tener control e información de sus negocios. Cuando llegó a ellos la noticia del nuevo sistema de numeración, vieron muy prontamente la ventaja que les daría. Los cálculos eran fáciles de realizar y ya no hacía falta una formación superior para dominar las operaciones aritméticas. No tendrían que pagar por el servicio de un experto.

Es realmente notable que estas personas se dieran cuenta del papel fundamental del cero en el nuevo sistema. La masa identificó todo el sistema con su rasgo más característico, la cifra, usando, entonces, cifra con el sentido de signo numérico que tiene hoy en nuestra civilización. Este uso era totalmente opuesto al significado de la cifra de los doctos.

Los comerciantes consideraron que era prudente reservar ese uso para ellos, como una ventaja. El sistema se utilizó en secreto. De esta forma, la palabra «cifra» era usada como un signo secreto. De esa etapa sobreviven las palabras «descifrar» y «cifrado». Un código cifrado es un texto de significado inaccesible si no se dispone de la clave. Cuando se obtiene la clave el secreto queda revelado, el código secreto se descifra, «se le quita el cero» o el secreto.

Por motivos egoístas los comerciantes guardaron para sí el sistema. Por otro lado, hubo una reacción de parte de los partidarios de las tradiciones y defensores de antiguas filosofías, a la que se sumaron quienes vivían de los cálculos difíciles de antaño. Por estas razones, el sistema tardó mucho en imponerse. La lucha duró desde el siglo XI hasta el siglo XV. En algunos lugares hasta fue prohibido. Pero hacia principios del siglo XVI ya estaba decididamente establecido y no sufrió ningún retraso en su desarrollo.

Los partidarios del sistema de posición se denominaban «algoristas» y los defensores del viejo sistema, «abacistas», porque en sus cálculos utilizaban el ábaco. En esos tiempos también «"abaci"» era sinónimo de aritmética.

Una vez que quedó completamente adoptado el nuevo sistema, el uso de la palabra «cifra» en el sentido de un signo numérico estaba tan fuertemente arraigado que fue inútil el esfuerzo de los doctos por volver al significado original de ‘cero’. No tuvieron más remedio que dejar «cifra» con ese sentido y tomar «"zero"» para designar al espacio vacío hasta llegar al uso que tiene ahora.

En astronomía un dígito astronómico es cada una de las partes iguales en que se divide el diámetro de los discos lunar y solar para expresar la importancia de un eclipse. Así, un eclipse de Luna de 8 "dígitos" afecta a los dos tercios del diámetro de nuestro planeta (ver magnitud de un eclipse).



</doc>
<doc id="6692" url="https://es.wikipedia.org/wiki?curid=6692" title="Cantidad">
Cantidad

Una Cantidad (del vocablo latino quantĭtas, -ātis, y este del griego ποσότης (posótēs) que se hace acepción a 'cuánto' o 'cuán grande' refiriéndose en términos de grandeza, extensión, cantidad, magnitud y tamaño) es un valor, componente o número, susceptible de aumento o disminución, que se obtiene de una medida u operación de uno o varios entes que pueden ser medidos de manera exacta o aproximada.

Es una propiedad medible que admite grados de comparación y representa o bien un contaje del número de elementos de un conjunto, o bien el resultado de una medición física de una magnitud. Así las cantidades pueden ser comparadas en términos de "más", "menos" o "igual" (o no ser comparables), y generalmente pueden ser representadas por diferentes sistemas de unidades (la masa se puede medir en kilogramos o en onzas).

Un gran número de propiedades pueden ser representadas por cantidades escalares, aunque algunas magnitudes físicas requieren el uso de cantidades vectoriales más complejas. Una cantidad escalar es el valor numérico que resulta de una medición (de una magnitud) que se expresa con números acompañado por unidades, de la forma siguiente:


Por ejemplo: 20 kg, 1 m, 60 s, son resultado de medir las magnitudes masa, longitud y tiempo. Igualmente ciertas magnitudes físicas como la cantidad de movimiento, o la velocidad requieren ser representadas por objetos matemáticos como vectores que no son simplemente valores numéricos.

Tradicionalmente , se han dividido las cantidades en dos grupos las referidas al contaje (magnitudes discretas) y las referidas a la comparación con una escala continua (magnitudes continuas). Las cantidades son representaciones formales de propiedades físicas que deben satisfacer algunas condiciones generales como:

La escalabilidad frecuentemente es una consecuencia de la aditividad: dada una magnitud extensiva y una propiedad medible, al dividir un sistema en dos subsistemas se obtienen dos cantidades asociadas a la propiedad medible, de tal manera que su suma es la cantidad asociada al sistema original sin dividir.

Las cantidades en general admiten siempre escalabilidad: dada una magnitud siempre es posible imaginar una cantidad mayor que ella. Esto diferencia a los resultados de una medición o contaje de otros índices numéricos como los porcentajes o las probabilidades (que por definición deben ser inferiores al 100%).

Las magnitudes de tipo contaje y las magnitudes continuas escalares tiene estructura de conjunto totalmente ordenado, así dadas dos longitudes siempre será posible decir cual de las dos longitudes es mayor que la otra. Otras magnitudes como la velocidad vectorial admiten sólo orden parcial, por ejemplo está claro que una velocidad de formula_1 representa una velocidad menor que una velocidad formula_2. Pero si ahora se considera la velocidad 
formula_3 no puede decirse si esta velocidad es mayor o menor que formula_4 (lo único que puede decirse es que son velocidades vectoriales diferentes). Esto sucede porque en un espacio vectorial no puede definirse una relación de orden total compatible con la suma.

Aunque todas las magnitudes parecen escalables, aun así puede establecerse una diferencia sobre la posibilidad de construir o no una escala continua entre dos valores cualesquiera asociados a una misma propiedad medible. Las cantidades de tipo contaje son representadas por un número natural positivo y representa el número de elementos en un cierto conjunto. Por el contrario las magnitudes de tipo continuo, se refieren a la comparación de propiedades físicas con patrones y entre dos resultados R < R siempre puede encontrarse una medición R3 tal que R < R < R.

Usualmente las cantidades discretas se representan por valores numéricos que son números enteros de formula_5 o formula_6. Mientras que las cantidades continuas se representan por números no necesariamente enteros de formula_7 o formula_8. En la práctica una medida directa con una precisión finita siempre será un contaje, que convenientemente reescalado dará como resultado un número racional. Sin embargo, para medidas indirectas calculadas a partir de mediciones directas frecuentemente se lleva a cabo una operación matemática sobre las medidas directas, y entonces el resultado no siempre es un número racional, por esa razón resulta conveniente usar representaciones sobre los números reales formula_8. Pero en ciertos casos, como en electrotecnia puede sacarse provecho de representar algunas magnitudes no como números reales, sino como números complejos formula_10. E incluso operacionalmente pueden considerarse conjuntos numéricos más complejos, en algunos ámbitos.

Las cantidades asociadas a un simple contaje de elementos frecuentemente no usan un tipo de unidades (aunque a veces se pueden usar unidades para clarificar su valor). En cambio las cantidades continuas obtenidas inicialmente comparando experimentalmente con una escala necesitan especificar la escala. Una escala definirá siempre implícitamente un sistema de unidades, de ahí que las magnitudes continuas para poder ser comparadas o interpretadas requieran especificar el valor de la unidad asociada a la escala de medición/comparación.
Las cantidades continuas posee una estructura particular que fue caracterizada explícitamente por primera vez por O. Hölder (1901) mediante un conjunto de axiomas que definen las características como "identidades" y "relaciones" entre magnitudes.

En ciencias naturales, la estructura cuantitativa está sujeta a las condiciones de la investigación empírica y no puede asumirse que existan a priori para una cierta propiedad. Algunas características fundamentales señaladas por Hölder para las cantidades asociadas a propiedades medibles son:

La unidad en cantidades continuas, es la cantidad que sirve de comparación, y en cantidades discretas es cada uno de los objetos que se cuentan. La unidad puede ser de dos tipos: libre o necesaria.


</doc>
<doc id="6693" url="https://es.wikipedia.org/wiki?curid=6693" title="Cero">
Cero

El cero (0) es un número entero de la propiedad par. Es el signo numérico de valor nulo, que en notación posicional ocupa los lugares donde no hay una cifra significativa. Si está situado a la derecha de un número entero se multiplica por 10 su valor; colocado a la izquierda, no lo modifica.

Utilizándolo como número, se pueden realizar con él operaciones algebraicas como sumas, restas, multiplicaciones, entre otras. Pero, por ser la expresión del valor nulo (nada, nadie, ninguno...), puede dar lugar a expresiones indeterminadas o que carecen de sentido.

Es el elemento del conjunto ordenado de los números enteros (, ≤) que sigue al –1 y precede al 1. Algunos matemáticos lo consideran perteneciente al conjunto de los naturales () ya que estos también se pueden definir como el conjunto que nos permite contar el número de elementos que contienen los demás conjuntos, y el conjunto vacío tiene ningún elemento. El número cero se puede representar como cualquier número más su opuesto (o, equivalentemente, menos él mismo): () 0.

Antiguas y grandes civilizaciones —como las del Antiguo Egipto, Babilonia, la Antigua Grecia y la Civilización maya— poseen documentos de carácter matemático o astronómico mostrando símbolos indicativos del valor cero; pero por diversas peculiaridades de sus sistemas numéricos, no supieron obtener el verdadero beneficio de este capital descubrimiento.

En el sistema de numeración egipcio se utilizó el signo «-nfr-»
para indicar el cero (en el "Papiro Boulaq 18", datado hacia el 1700 a. C.).

El cero apareció por primera vez en Babilonia en el , aunque su escritura en tablillas de arcilla se remonta al 2000 a. C. Los babilonios escribían en arcilla sin cocer, sobre superficies planas o tablillas. Su notación era cuneiforme. En tablillas datadas en el año 1700 a. C. se ven anotaciones numéricas en su particular forma. Los babilonios utilizaban un sistema de base 60. Con su sistema de notación no era posible distinguir el número 23 del 203 o el 2003, aunque esta ambigüedad no pareció preocuparles.

Alrededor del 400 a. C., los babilonios comenzaron a colocar el signo de «dos cuñas» en los lugares donde en nuestro sistema escribiríamos un cero, que se leía «varios». Las dos cuñas no fueron la única forma de mostrar las posiciones del cero; en una tablilla datada en el 700 a. C. encontrada en Kish, antigua ciudad de Mesopotamia al este de Babilonia, utilizaron un signo de «tres ganchos». En otras tablillas usaron un solo «gancho» y, en algunos casos, la deformación de este se asemeja a la forma del cero.

El cero también surgió en Mesoamérica e ideado por las civilizaciones mesoamericanas antes de la era cristiana, por la cultura maya. Posiblemente fue utilizado antes por la cultura olmeca.

El primer uso documentado mostrando el número cero corresponde al año 36 a. C., haciendo uso de la numeración maya. A causa de la anomalía introducida en el tercer lugar de su notación posicional, les privó de posibilidades operativas.

Claudio Ptolomeo en el "Almagesto", escrito en , usaba el valor de «vacío» o «0». Ptolomeo solía utilizar el símbolo entre dígitos o al final del número. Podría pensarse que el cero habría arraigado entonces, pero lo cierto es que Ptolomeo no usaba el símbolo como «número» sino que lo consideraba un signo de anotación. Este uso no se difundió, pues muy pocos lo adoptaron.

Los romanos no utilizaron el cero. Sus números eran letras de su alfabeto; para representar cifras usaban: I, V, X, L, C, D, M, agrupándolas. Para números con valores iguales o superiores a 4000, dibujaban una línea horizontal sobre el «número», para indicar que el valor se multiplicaba por 1000.

La civilización india es la cuna de la notación posicional, de uso casi universal en el . La palabra «cero» proviene de la traducción de su nombre en sánscrito "shunya" (vacío) al árabe "sifr" (صفر), a través del italiano. La voz española «cifra» también tiene su origen en "sifr".

Es posible que el matemático indio Brahmagupta () fuera el primero en teorizar sobre el concepto de «cero» no solo como definición de una cantidad nula, sino como posible sumando para números negativos y positivos. El primer testimonio del uso del «cero indio» está datado en el año 683: una inscripción camboyana de Angkor Wat, tallada en piedra, que incluye el número «605». Otras pruebas de uso se datan hacia el año 810. Las inscripciones de Gwalior están datados en 875-876. Abu Ja'far Mujammad ibn Musa (Al-Juarismi), en su obra titulada «Tratado de la adición y la sustracción mediante el cálculo de los indios» explica el principio de numeración posicional decimal, señalando el origen indio de las cifras. La décima figura, que tiene forma redondeada, es el «cero».

Los árabes lo transmitieron por el Magreb y Al-Ándalus, pasando posteriormente al resto de Europa. Los primeros manuscritos que muestran las cifras indias (llamadas entonces «árabes») provienen del norte de España y son del : el "Codex Vigilanus" y el "Codex Aemilianensis". El cero no figura en los textos, pues los cálculos se realizaban con ábaco, y su uso aparentemente no era necesario.

Aunque se atribuyen los primeros usos del "cero" en Francia, o al controvertido papa Silvestre II, alrededor del año 1000, la mayor parte de las referencias indican que el cero (llamado "zefhirum") fue introducido en Europa por el matemático italiano Fibonacci en el , mostrando el álgebra árabe en su "Liber abaci" ("El libro del ábaco"), aunque por la facilidad del nuevo sistema, las autoridades eclesiásticas lo tildaron de mágico o demoniaco.

La Iglesia y la casta de los calculadores profesionales —clérigos en su mayoría, que utilizaban el ábaco— se opusieron frontalmente, vetando la nueva álgebra, en algunos lugares hasta el .

El cero se representa en textos occidentales con la cifra «0». Desde el , y especialmente con el desarrollo de la informática, es frecuente que este signo aparezca cortado por una barra diagonal (/), nueva notación que evitaba la confusión con la grafía de la letra «o». Hasta hace poco, la conjunción disyuntiva «o» debía llevar tilde: «ó», cuando iba escrita entre cifras para no ser confundida con el signo numérico 0. Actualmente, dicha regla no está en vigor.

En coordenadas cartesianas el origen de coordenadas se asocia al valor 0 (cero).

El cero, por ser un concepto numérico especial, no se incluía en el conjunto de los números naturales , por convenio. Se representaba como , al conjunto de los números naturales cuando incluye al cero, por ello es posible encontrar muchos libros donde los autores no consideran al cero como número natural. De hecho, aún no hay consenso al respecto.

A algunos matemáticos les resulta conveniente tratarlo como a los otros números naturales, por eso la discrepancia. Desde un punto de vista histórico el cero aparece tan tarde que algunos no creen que sea justo llamarlo natural.

En la suma, el cero es el elemento neutro; es decir, cualquier número sumado con vuelve a dar .
Ejemplo:

En la resta, el cero es el elemento neutro; es decir, cualquier número restado con vuelve a dar , excepto cuando el cero es el minuendo, en cuyo caso resulta .
Ejemplos:

En el producto, el cero es el elemento absorbente; cualquier número operado con da .
Ejemplo:

El cero puede ser dividido por otros números, en cuyo caso es el elemento absorbente (ejemplo: ). El cero no puede dividir a ningún número.

En los números reales (incluso en los complejos) la división entre cero es una indeterminación; así, las expresiones:

carecen de sentido.

Intuitivamente, significa que no tiene 'sentido' «repartir» 8 manzanas entre niños de un aula vacía. Tampoco tiene 'sentido', distribuir 0 billetes entre cero personas: nada entre nadie.

Matemáticamente, el cero es el único número real por el cual no se puede dividir. Por eso 0 es el único real que no tiene inverso multiplicativo.

Ejemplo:

En el análisis matemático existen definiciones de distintos tipos de límites. Por ejemplo:

Sin embargo, si se analiza cada numerador y denominador por separado, el límite de todo ellos es cero. Por eso se dice que es indeterminado, pues pueden obtenerse resultados tan diferentes como infinito, uno o cero.


El valor formula_6 no está definido como potencia, pero según el contexto o por comodidad se puede elegir uno de los resultados mediante una definición. Algunas calculadoras científicas dan 1 como resultado.

En el contexto de los límites, formula_6 es una indeterminación pues los límites de potencias tales que los límites de base y exponente por separado son cero, pueden terminar dando cualquier cosa.

En el conjunto de los enteros, el es un número par; satisface la definición de paridad, así como también todas las características de los números pares.

El cero, junto con los números 1, , "", "" están relacionados en la célebre Identidad de Euler:

En otra ramas de la matemática, especialmente en el álgebra, se llama «cero» y se simboliza también con «0» a elementos de otros conjuntos muy diferentes de los reales. Es el caso del vector nulo en el conjunto de los vectores del plano o del espacio. En general se le dice cero al elemento neutro de un grupo abeliano.

El 0 se asocia con la posición de «apagado» en lógica positiva (el 1 se asocia con la posición de «encendido») y es uno de los dos dígitos (0 y 1) del sistema binario.

El cero absoluto es, en el campo de la física, la temperatura más baja que teóricamente puede alcanzar la materia. Esta temperatura da lugar a la escala Kelvin, que establece como 0 K dicha temperatura. Su equivalencia en grados Celsius es de –273,15 °C.





</doc>
<doc id="6698" url="https://es.wikipedia.org/wiki?curid=6698" title="Corán">
Corán

El Corán (del árabe , ', ‘la recitación’, [qurˈan], persa: [ɢoɾˈɒn]), también transliterado como Alcorán, Qurán o Korán"', es el libro sagrado del islam, que según los musulmanes es la palabra de Dios (del árabe "Allāh", ), revelada a Mahoma (Muhammad, ), quien se considera que recibió estas revelaciones por medio del arcángel Gabriel (Ğibrīl ). 

Durante la vida del profeta Mahoma, las revelaciones eran transmitidas oralmente o escritas en hojas de palmeras, trozos de cuero o huesos, etc. A la muerte del profeta, en 632, sus seguidores comenzaron a reunir estas revelaciones, que durante el Califato de Uthmán ibn Affán () tomaron la forma que hoy conocemos, 114 capítulos (azoras, ), cada uno dividido en versículos ("aleyas", ). 

El Corán menciona muchos personajes que aparecen en los libros sagrados del judaísmo y el cristianismo (Tanaj y Biblia) y en la literatura devota (por ejemplo, los libros apócrifos), con muchas diferencias en detalle. Personajes del mundo hebreo y cristiano muy conocidos como Adán, Noé, Abraham, Moisés, María de Nazaret, Jesús de Nazaret y Juan Bautista aparecen mencionados como profetas islámicos.

Los musulmanes creen que el Corán es la palabra «eterna e increada» de Dios. Por ello, su transmisión debería realizarse sin el menor cambio en la lengua originaria, el árabe clásico. El Corán ha sido traducido a muchos idiomas, principalmente pensando en aquellos creyentes cuyas lenguas no son el árabe. Aun así, en la liturgia se utiliza exclusivamente el árabe, ya que la traducción únicamente tiene valor didáctico, como glosa o instrumento para ayudar a entender el texto original. De hecho, una traducción del Corán ni siquiera se considera un Corán auténtico sino una interpretación del mismo.

El origen del Corán ha generado mucha controversia porque los especialistas islámicos parten de la presunción de que el Corán es un texto incorrupto y divino, mientras que los académicos lo ven como un texto humano semejante a cualquier otro. 

El texto del Corán reta a los lectores a que encuentren alguna contradicción o divergencias en él y les enfatiza que no la encontrarán, puesto que al suponerse de origen divino no debería haberlas. 

Las variedades más extendidas de la teología islámica consideran que el Corán es eterno y que no fue creado.Tomando en cuenta que los musulmanes creen que figuras bíblicas tales como Moisés y Jesús predicaron el islam, la doctrina de la revelación inmutable y no creada del Corán implica que los textos más antiguos –como el Tanaj o la Biblia– se debieron a la «degeneración humana».

No obstante, algunos islamistas de carácter liberalizador, particularmente las escuelas mutazilí e ismailí, implícita o explícitamente cuestionan la doctrina de un Corán no creado cuando realizan ciertas preguntas relacionadas con la aplicación de la "Sharía" o ley islámica. Algunos pensadores contemporáneos, como Reza Aslan o Nasr Hamid Abu Zayd, han argüido que tales leyes fueron creadas por Alá para solucionar las necesidades particulares de la comunidad de Mahoma. Otros rebaten que tales leyes no difieren en nada de la ley mosaica.

Entre las razones ofrecidas por la crítica de la doctrina del "Corán eterno" se encuentra su implicación en «la unicidad de Dios». El pensamiento de que el Corán es la palabra eterna y no creada de Alá y que siempre ha existido junto a Él podría llevar a pensar en un concepto plural de la naturaleza de dicha deidad. Preocupados de que esta interpretación parezca hacerse eco del concepto cristiano de la «palabra eterna de Yahvé» ("Logos"), algunos musulmanes, y particularmente los mutazilíes rechazaron la noción de la eternidad del Corán. Sin embargo, buena parte de los musulmanes actuales opinan que esta visión de los mutazilíes es producto de la no comprensión profunda de la naturaleza misma del Corán y de su relación con el "tawhid".

Según la tradición, Mahoma no sabía leer ni escribir sino que, simplemente, recitó lo que le era revelado para que sus compañeros lo escribieran y memorizaran. Algunos exégetas creen que esta tradición de que Mahoma no sabía leer ni escribir está en contradicción con el texto coránico mismo por doble partida: primero el Corán anuncia que el profeta «no solía leer ni escribir» es decir no era dado a la lectura o la escritura, esto, según ellos, no quiere decir que no supiera hacerlo, pero existe otra aleya susceptible de ser interpretada como indicio de que sabía leer, la número dos de la azora «La Congregación»: «Fue Dios quien levantó de entre los iletrados un Apóstol de entre ellos mismos, recitando Sus Señales, purificándoles y enseñándoles el Libro y la sabiduría...» Los simpatizantes del Islam tienen por verdad que la redacción del texto coránico existente hoy corresponde exactamente a lo que fue revelado al profeta Mahoma, es decir, las palabras textuales de Dios entregadas a Mahoma por medio del arcángel Gabriel.

Los acompañantes de Mahoma, según las tradiciones musulmanas, empezaron a registrar las azoras de forma escrita antes de que su líder muriera en el año 632. Esta práctica de escribir las «revelaciones» a medida que le llegaban al profeta era una libertad que todos los testigos de los momentos en que ocurrían las revelaciones podían tomarse, aunque se trataba de una reabundancia literaria ya que el Corán fue compilado bajo los auspicios del profeta mismo. Según la tradición islámica, entre todos los coranes que existen hoy y han existido no hay ninguna diferencia; existe solo una versión del Corán, cuyas azoras escritas se citan con frecuencia en las tradiciones. Por ejemplo, en la historia de la conversión de Úmar ibn al-Jattab (momento en que Mahoma todavía estaba en La Meca), se dice que su hermana estaba leyendo un texto de la azora Ta-Ha. En Medina, se dice que alrededor de sesenta y cinco acompañantes actuaron como escribas para él en algún momento o en otro. El profeta los llamaba para que escribieran las «revelaciones» justo después de tenerlas.

Una tradición documenta que la primera recopilación completa del Corán fue hecha durante el mandato del primer califa, Abu Bakr as-Siddiq. Zayd ibn Thábit, que había sido uno de los secretarios de Mahoma, «reuniendo el Corán a partir de varias piezas de hueso y de los pechos (es decir, ‘los recuerdos’) de los hombres». Esta recopilación fue conservada por Hafsa bint Úmar, hija del segundo califa Úmar y una de las viudas de Mahoma.

Durante el califato de Utmán ibn Affán, hubo disputas relativas a la recitación del Corán. En respuesta, Utmán decidió codificar, estandarizar y transcribir el texto. Se dice que Utmán comisionó a un comité (que incluía a Zayd y varios miembros prominentes de Quraysh) para poder producir una copia estándar del texto.

Según algunas fuentes, esta recopilación se basó en el texto conservado por Hafsa. Otras versiones indican que Utmán hizo esta recopilación de manera independiente y que el texto de Hafsa habría sido llevado adelante y que, al final, se encontró que los dos textos coincidían perfectamente. Sin embargo, otros documentos omiten por completo referencias a Hafsa.

Los eruditos musulmanes afirman que si el califa hubiera ordenado la recopilación del Corán, este nunca habría sido relegado al cuidado de una de las viudas del profeta. 

Cuando terminó el proceso de recopilación, entre los años 650 y 656, Utmán envió copias del texto final a todos los rincones del imperio islámico y ordenó la destrucción de todas las copias que difirieran de la nueva versión.

Varios de los manuscritos, incluyendo el manuscrito de Samarcanda, son reivindicados como copias originales de las enviadas por Utmán; no obstante, muchos especialistas, occidentales e islámicos, dudan que sobreviva algún manuscrito utmánico original.

En lo que respecta a las copias que fueron destruidas, las tradiciones islámicas aseguran que Abdallah Ibn Masud, Ubay Ibn Ka'b y Alí, primo y yerno de Mahoma, habían preservado algunas versiones que diferían en algunos aspectos del texto utmánico que es considerado ahora por todos los musulmanes. Los especialistas musulmanes registran determinadas diferencias entre las versiones, las cuales consisten casi totalmente en variantes léxicas y ortográficas o diferentes conteos de versos. Se ha registrado que los tres (Ibn Masud, Ubay Ibn Ka'b y Alí) aceptaron el texto utmánico como la autoridad definitiva.

La versión de Utmán se compuso según un viejo estilo de escritura árabe, que no incluía vocales, razón por la cual se puede interpretar y leer de varias formas. Este escrito utmánico básico se ha llamado "rasma" y, con algunas diferencias menores, es la base de varias tradiciones orales de recitación. Para fijar estas recitaciones y prevenir cualquier error, los escribanos y eruditos comenzaron a anotar las "rasmas" utmánicas con varias marcas diacríticas —puntos y demás— para indicar la forma en que las palabras debían ser pronunciadas. Se cree que este proceso de anotación comenzó alrededor del año 700, poco tiempo después de la compilación de Utmán, y que terminó aproximadamente en el año 900. El texto del Corán más usado en la actualidad está basado en la tradición de recitación de los Hafs, tal y como fue aprobado por la Universidad Al-Azhar de El Cairo, en 1922, (para más información relacionada con las tradiciones de recitación, refiérase a Recitación coránica, más adelante en este mismo artículo).

Aunque algunos eruditos concuerdan con varios de los aspectos señalados por las tradiciones islámicas relativas al Corán y sus orígenes, el consenso académico considera que Mahoma compuso los versos que integran el texto, las cuales fueron memorizadas por sus seguidores y puestas por escrito. Estos estudios cuestionan la creencia islámica de que todo el Corán fue enviado por Dios a la humanidad; ya que notan que en numerosos pasajes se alude a Dios en tercera persona o la voz narrativa jura por varios entes sobrenaturales, incluyendo a Dios. Se acepta también que numerosas versiones de estos textos circularon después de su muerte en el año 632; hasta que Utmán ordenó la recopilación y el ordenamiento de esta masa de material entre 650 y 656, tal cual lo describen los eruditos islámicos. Otros especialistas tienden a no atribuir el Corán entero a Mahoma, arguyendo que no hay una verdadera prueba de que el texto haya sido compilado bajo el mandato de Utmán, puesto que las más viejas copias conservadas del Corán completo datan de varios siglos después de Utmán (la más vieja copia existente del texto completo es del siglo IX). Alegan que el islam se formó lentamente, durante los siglos transcurridos tras las conquistas musulmanes y en la medida en que los conquistadores islámicos iban elaborando sus propias creencias en respuesta de los desafíos judíos y cristianos. 

La investigación literaria explica las numerosas similitudes entre el Corán y las escrituras hebreas argumentando que Mahoma enseñaba a sus seguidores lo que él pensaba que era historia universal, tal y como lo había escuchado de las bocas de judíos y cristianos que había encontrado en Arabia y durante sus viajes.

Una propuesta influyente en este punto de vista fue la del Dr. John Wansbrough, un académico inglés. Sin embargo, los escritos de Wansbrough estaban redactados en un estilo denso, complejo y casi hermético y han tenido una gran influencia en los estudios islámicos a través de sus estudiantes, Michael Cook y Patricia Crone y no tanto por sí mismos. En 1977, Crone y Cook publicaron un libro llamado "Hagarism", en el que se sostiene que:
Este libro fue extremadamente controvertido en su tiempo, pues desafiaba no solo la ortodoxia musulmana, sino las actitudes prevalecientes entre los mismos islamistas seglares. Wansbrough fue criticado por su interpretación del Corán y por la "mala" interpretación de las palabras originales en árabe. Crone y Cook se han desdicho de algunos de sus argumentos en el sentido de que el Corán evolucionó a lo largo de varios siglos, pero todavía sostienen que la tradición de lectura sunita es muy poco fiable, pues proyecta su ortodoxia contemporánea en el pasado, del mismo modo que si los exégetas del Nuevo Testamento quisieran comprobar que Jesús era católico o metodista.

Fred Donner ha argüido contra Crone y Cook, en lo relativo a la temprana fecha de la recopilación del Corán, basado en sus lecturas del propio texto. Él argumenta que si el Corán hubiera sido recopilado a lo largo de los tumultuosos siglos iniciales del Islam (con sus vastas conquistas, expansión y los sangrientos incidentes entre los rivales del califato), habría habido evidencia de esta historia en el texto. No obstante, según él, no hay nada en el Corán que no refleje las cosas conocidas de la temprana comunidad musulmana.

Algunos aseguran que los hallazgos arqueológicos de 1972 pueden arrojar luz acerca de los orígenes del Corán. En ese año, durante la restauración de la Yemen,en los obreros hallaron un «cementerio de papeles» que contenía decenas de millares de papeles en donde se leían fragmentos del Corán (los ejemplares del Corán todavía se desechan de esta manera, pues se considera impiedad tratar el «texto sagrado» como si fuera basura ordinaria). Se creyó que algunos de esos fragmentos eran los textos coránicos más antiguos que se han encontrado. El especialista europeo Gerd R. Puin ha estudiado estos fragmentos y ha publicado no solamente un corpus de textos, sino también algunos descubrimientos preliminares. Las variantes de los textos descubiertos parecen coincidir con ciertas variantes menores reportadas por algunos eruditos islámicos en sus descripciones de las variantes del Corán, que una vez estuvieron en posesión de Abdallah Ibn Masud, Ubay Ibn Ka'b y Alí, y que fueron suprimidas por órdenes de Utmán.

Los contenidos del Corán tienen que ver con las creencias islámicas básicas incluyendo la existencia de Dios y la resurrección. También aparecen en el Corán historias de los antiguos profetas, temas éticos y legales, eventos históricos de la época de Mahoma, caridad y oración. Los versos del Corán contienen exhortaciones generales en relación con el actuar bien o mal y se conectan con eventos históricos para señalar lecciones morales más generales. Los versos relativos a los fenómenos naturales han sido intepretados por los musulmanes como señal de la autenticidad del mensaje Coránico. El estilo del Corán ha sido denominado "alusivo," y se requieren comentarios para explicar a lo que se refiere. Hay referencia a los eventos, pero estos no son narrados; los desacuerdos son debatidos sin ser explicados; se mencionan personas y sitios, pero rara vez se nombran. 

̩El tema central del Corán es el monoteísmo. Dios es descrito como viviente, eterno, omnisciente y omnipotente (ver p. ej., Corán 2ː20, 2ː29, 2ː255). La omnipotencia de Dios aparece por encima de todo en su poder para crear. Es el creador de todo, de los cielos y la tierra y lo que hay entre ellos (ver, p.e.j, Corán 13ː15, 2ː253. 50ː38, etc.). Todos los seres humanos son iguales en su total dependencia de Dios y su bienestar depende de que reconozcan ese hecho y vivan de acuerdo con él.

El Corán emplea argumentos cosmológicos y de contingencia en varios versos sin referirse a los términos para demostrar la existencia de Dios. Por tanto, el universo es originado y necesita un originador, y cualquier cosa que exista debe tener una causa suficiente para su existencia. Además, el diseño del universo es referido con frecuencia como un punto de contemplación. "Fue él quien creó los siete cielos en armonía. No se puede ver falla alguna en la creación de Dios. Observad de nuevoː ¿podéis ver algún error?"

La doctrina del final de los días y la escatología (el destino final del universo) pueden considerarse la segunda gran doctrina del Corán. Se estima que aproximadamente un tercio del Corán es escatológico, relacionado con la vida después de la muerte en el otro mundo y con el día del juicio al final de los tiempos. Hay una referencia a la vida después de la muerte en la mayoría de páginas del Corán y la creencia en ello es mencionada a menudo junto con la creencia en Dios, como en la expresión común "Cree en Dios y en el último de los días." Un número de suras como el 44, 56, 75, 78, 81 y 101 están directamente relacionados con la vida después de la muerte y sus preparaciones. Algunos suras indican la cercanía del evento y advierten a la gente a estar preparados para la fecha inminente. Por ejemplo, los primeros versos del Sura 22, que tratan del poderoso terromoto y las situaciones de la gente en ese día, representan este tipo de admonisión divinaː ¡Oh, gente! Tengan temor de su Señor. El terremoto que ocurrirá cuando llegue la Hora (del Juicio) será algo terrible." 

El Corán generalmente describe de manera vívida lo que sucederá al final de los tiempos. Watt describe la perspectiva Coránica del Tiempo Finalː

"El clímax de la historia, cuando el mundo actual llegue a su fin, es llamado de distintas maneras. Es 'el Día del Juicio,' 'el Último Día,' 'el Día de la Resurrección,' o simplemente 'la Hora.' Con menos frecuencia se le llama 'el Día de la Distinción' (cuando los buenos serán separados de los malos), 'el Día de la Asamblea' (de los hombres ante la presencia de Dios) o 'el Día del Encuentro' (de los hombres con Dios). La Hora ocurre de repente. Es precedida por un grito, por trueno, o el sonido de una trompeta. Una agitación cósmica tiene lugar luego. Las montañas se disuelven en polvo, los mares hierven, el sol se oscurece, las estrellas caen y el cielo se enrrolla. Dios aparece como Juez, pero su presencia es sólo sugerida y no descrita [...] El interés central se encuentra, desde luego, en la reunión de la humanidad entera frente al Juez. Los seres humanos de todas las épocas, resurrectos, se unen a la multitud. Ante la objeción burlona de los no creyentes con respecto a que las generaciones anteriores han estado muertos mucho tiempo y son ahora sólo polvo y huesos decrépitos, la respuesta es que Dios es capaz en cualquier caso de regresarles a la vida."

El Corán no afirma una inmortalidad natural del alma humana, pues la existencia del ser humano depende de la voluntad de Diosː cuando lo desee, hace que el hombre muera, y cuando lo desea le devuelve la vida en una resurrección corporal. 

De acuerdo con el Corán, Dios se comunicó con el hombre e hizo que su voluntad fuera conocida a través de señales y revelaciones. Los profetas, o "Mensajeros de Dios," recibieron revelaciones y se las enseñaron a la humanidad. El mensaje ha sido idéntico y para la humanidad entera. "Nada se te ha dicho que no se haya dicho a los mensajeros antes de ti, que tu señor tiene a su cargo el perdón así como el castigo más severo." La revelación no viene directamente de Dios a los profetas. Los ángeles actuando como los mensajeros de Dios les entregan a ellos la revelación divina. Esto aparece en el Corán 42ː51, en el que se enunciaː "Dios no habla a las personas excepto por inspiración o tras un velo o enviando un Mensajero (el ángel Gabriel) para transmitirle por Su voluntad lo que Él quiera de la revelación."

La creencia es un aspecto fundamental de la moralidad en el Corán y los escolares han intentado determinar los contenidos semánticos de "creencia" y "creyente" en el Corán. Los conceptos y exhortaciones ético-legales que tienen que ver con la buena conducta están relacionados con una profunda conciencia de Dios, y por tanto enfatizan la importancia de la fe, responsabilidad y la creencia en el encuentro último de cada persona con Dios. Se invita a la gente a cometer actos de caridad, especialmente hacia los más necesitados. A los creyentes que "gastan su fortuna de noche y día, en secreto y en público" se les promete que "tendrán su recompensa con su Señor, no caerá miedo sobre ellos, ni se lamentarán." También afirma la vida familiar legislando sobre temas de matrimonio, divorcio y herencia. Un número de prácticas, como la usura y las apuestas, son prohibidas. El Corán es una de las fuentes fundamentales de la ley Islámica ("sharia"). Algunas prácticas religiosas formales reciben atención significativa en el Corán, incluyendo las oraciones formales ("salat") y el ayuno en el mes de Ramadán. En cuanto a la forma en que se debe conducir la oración, el Corán se refiere a la prostación. El término usado para la caridad, "zakat", significa literalmente purificación. La caridad, de acuerdo con el Corán, es una forma de auto-purificación.

El Corán ha producido un gran corpus de comentarios y explicaciones. Los musulmanes tardíos no siempre comprendían la lengua del Corán, no entendieron ciertas alusiones que parecían claras a los primeros musulmanes y estaban extremadamente preocupados en reconciliar las contradicciones y los conflictos en el Corán. Los comentadores glosaron el árabe, explicaron las alusiones y, lo que quizá sea más importante, decidieron qué versos coránicos habían sido revelados primero en la carrera profética de Mahoma (lo cual era apropiado para la naciente comunidad musulmana) y cuáles habían sido revelados después, cancelando o abrogando el texto original. Los recuerdos de las «ocasiones de revelación», es decir, las circunstancias en que Mahoma había hecho públicas las revelaciones, también fueron recopiladas, pues se pensaba que podrían explicar algunas oscuridades.

Por todas estas razones, fue extremadamente importante para los comentadores explicar cómo fue revelado el Corán, cuando y bajo qué circunstancias. Muchos comentarios o "tafsir", concernían a la historia. Los primeros "tafsir" son unas de las mejores fuentes de la historia islámica. Algunos comentadores famosos son at-Tabari, az-Zamakhshari, at-Tirmidhi e Ibn Kathir. Generalmente estos comentarios clásicos incluían todas las interpretaciones comunes y aceptadas, mientras que los comentarios de los fundamentalistas modernos, como el escrito por Sayyed Qutb tienden a dar solo una de las interpretaciones posibles.

Los comentadores se sienten muy seguros de las exactas circunstancias que motivaron algunos versos, como la azora Iqra o las aleyas 190-194, de la azora al-Baqara. Pero en algunos casos (como la azora al-Asr), lo más que se puede decir es en qué ciudad estaba viviendo Mahoma en ese momento. En otros casos, como con la azora al-Kawthar, los detalles de las circunstancias están en disputa, pues diversas tradiciones entregan versiones diferentes.

Las más importantes «ayudas exteriores» que se han usado para interpretar los significados del Corán son las "hadith", la colección de tradiciones en las que algunos eruditos musulmanes (los ulemas) basaron la historia y las leyes islámicas. Los especialistas han inspeccionado las miles de páginas de los hadices, intentando descubrir cuáles eran ciertas y cuáles eran fabricaciones. Un método muy utilizado era el estudio de la cadena de narradores, el "isnad", a través de los cuales fue transmitida la tradición.

Obsérvese que aunque se dice que ciertos hadiz —los hadiz qudsí— registran las palabras no canónicas que según la tradición Dios le dirigió a Mahoma, o el sumario de estas, los musulmanes no consideran que esos textos sean parte del Corán.

Los musulmanes creen que la Torá ("Al Tawra" - التوراة) del profeta Moisés ("Musa" - ٰمُوسیٰ) , los salmos ("Al Zabu"r - زَبُورُ) del Profeta David ("Dawud" - دَاوُوْد‎) y el Evangelio ("Injil" - إنجيل) del Profeta Jesús ("Isa Ibn Mariam" - عِيسَى ٱبْنُ مَرْيَمَ), son libros revelados por Dios, pero han sido corrompidos por judíos y cristianos. Por ello, el Corán retoma las historias de muchos de los personajes y eventos que aparecen en los libros sagrados de los judíos y los cristianos (El Tanaj, La Biblia) y la literatura devocional (Los libros apócrifos y el Midrásh), aunque difiere de estos en muchos detalles. Ciertos personajes bíblicos muy bien conocidos, como Adán, Noé, Abraham, Isaac, Jacob, Moisés, Juan el Bautista y Jesús son mencionados en el Corán como profetas del Islam. Sin olvidar a María (Maryam en árabe), madre de Jesús, quien es nombrada numerosas veces y siempre de manera elogiosa.

Los episodios son los mismos con diferencias de detalle, unas menos trascendentes que otras, y los fragmentos se encuentran dispersos entre las aleyas de las suras. Narra detalles de los episodios sobre la creación del hombre al que se da el nombre de Adán en el Jardín, la desobediencia del ángel Iblis ante el mandato de Dios Alláh de postrarse ante Adán, y cómo Dios llama Demonio (Shaytan) a Iblis; la expulsión del Jardín; una mención, indirecta, a Caín y Abel; Noé (Nuh), el arca (la nave) y el diluvio con la destrucción del pueblo de Noé y la muerte y de uno de sus hijos, así como la condenación de su mujer por traición; el arca se posa en el Chudi (los montes de Ararat según el Génesis de la Tanaj); la fecundidad de la mujer de Abraham, el nacimiento de Isaac y la prueba de Dios a Abraham (Ibrahim) pidiéndole sacrificar a Isaac; la destrucción del pueblo de Lot (Sodoma) y la condenación de su mujer por traición; la estancia de los israelitas en Egipto, el nacimiento de Moisés, su competencia con los magos del Faraón, los nueve signos (las diez plagas según la Tanaj), el paso del mar, el encuentro de Moisés con Dios al ver un fuego (en ángel del Señor en una llama de fuego en medio de una zarza, según la Tanaj), las tablas de la ley, el ternero (el becerro de oro); David, que mata a Goliat, etc.

A diferencia del Nuevo Testamento, el Corán narra el nacimiento de María (Maryam) como hija de «la mujer de Imran» y su tutela por el sacerdote Zacarías, esposo de Isabel; retiera las narraciones evangélicas del anuncio a Zacarías del nacimiento de su hijo Juan (el Bautista); la anunciación a María del nacimiento de su hijo Jesús (Isa) y su embarazo por el Espíritu de Dios. Más adelante toma elementos de los evangelios apócrifo al mencionar el retiro de María, los dolores del parto y el nacimiento de Isa/Jesús, quien habla en la cuna declarándose mortal, siervo, profeta y enviado por Dios a los hijos de Israel, e incluso anuncia la próxima llegada de Ahmad (Mahoma) a quien el Corán denomina el «Sello de los profetas». La principal discrepancia con los textos cristianos es la ascendencia de Maryam, pues en el Corán se menciona que es hija de Amram (Imran), hijo de Leví y también hermana de Aarón, quien vivió en tiempos del Éxodo, mientras que en el Nuevo Testamento se dice que es de la estirpe de David, aunque Lucas la relaciona con la familia sacerdotal de Isabel y Zacarías, sacerdote del grupo de Abías (Lc 1:5), descendiente de Aarón, siendo Aarón hijo de Amran (Imran). Es evidente que el autor del Corán confunde a la madre de Jesús con Miriam la hermana de Moisés, dificultad ya notada durante los primeros tiempos del Islam y que los eruditos musulmanes explican diciendo que se trata de otro Aarón, diferente del hermano de Moisés.

La palabra «Corán», generalmente, es traducida como 'recitación', en indicación de que no puede existir como un simple texto. Siempre ha sido transmitido oralmente al mismo tiempo que gráficamente.
Para al menos ser capaz de realizar una "salat" (oración), una obligación indispensable en el islam, un musulmán tiene que aprender al menos algunas azoras del Corán (generalmente, empezando con la primera azora, al-Fatiha, conocida como «Los siete versos repetidos», y luego avanzando hasta las más cortas que están al final del libro).

Una persona que pueda recitar todo el Corán se llama "qāri'" () o "hāfiz" (términos que se traducen como 'recitador' o 'memorizador', respectivamente). Mahoma es recordado como el primer "hāfiz". El canto ("tilāwa" ) del Corán es una de las bellas artes del mundo musulmán.

Existen diversas escuelas de recitación coránica y todas constituyen pronunciaciones permitidas del "rasm" utmánico. Hoy existen diez recitaciones canónicas y cuatro no canónicas del Corán. Para que una recitación sea canónica tiene que cumplir con tres condiciones:

Ibn Mujahid documentó siete recitaciones de este tipo e Ibn Al-Jazri agregó tres. Se trata de:

Estas recitaciones difieren en la vocalización ("tashkil" تشكيل) de unas cuantas palabras, las cuales a su vez le dan a la palabra un significado diferente, según las reglas de la gramática árabe. Por ejemplo, la vocalización de un verbo puede cambiar su voz activa y pasiva. También puede cambiar su formación, lo que implica la intensidad, por ejemplo. La vocales se pueden cambiar en su cantidad (es decir, se pueden alargar o acortar) y las pausas glotales (hamzas) pueden agregarse o elidirse, según las reglas respectivas de la recitación en particular. Por ejemplo, el nombre del Arcángel Gabriel se puede pronunciar de manera diferente en distintas recitaciones: Ŷibrīl, Ŷabrīl, Ŷibra'īl, y Ŷibra'il. El nombre Qur'ān se pronuncia sin la pausa glotal (como en Qurān) en una recitación y el nombre del profeta Ibrāhīm se puede pronunciar Ibrāhām en otra.

Las narraciones más usadas son las de Hafs (), Warsh (), Qalun () y Al-Duri a través de Abu `Amr (). Los musulmanes creen firmemente que todas las recitaciones canónicas fueron hechas por el Profeta mismo, citando la respectiva cadena de narración "isnad" canónica y las aceptan como válidas para la adoración como una referencia para las leyes de Sharía. Las recitaciones no canónicas son llamadas «explicativas» por su papel de darle diferentes perspectivas a un verso o aleya dada. Hoy varias personas poseen el título de «Memorizador de las diez recitaciones», lo cual se considera el máximo honor en las ciencias del Corán.

El Corán consiste en 114 azoras (capítulos) compuestas a su vez por un total de 6 236 aleyas (versos) dejando por fuera 112 de los 113 bizmillas o basmalas con que empiezan las azoras pues son idénticos («En el nombre de Dios, el Compasivo y Misericordioso») y, por lo general, se dejan sin enumerar. De manera alternativa, se pueden incluir los bizmillas en el recuento de los versos, lo cual arroja un número de 6348 aleyas. El número exacto de aleyas ha sido discutido, no por una disputa relativa al contenido del Corán sino debido a los métodos de conteo. Varios «musulmanes de El Corán original» han rechazado dos versos del Corán por considerarlos espurios y trabajan con la suma de 6346. Por lo general, los musulmanes no se refieren a las azoras por sus números sino por un nombre derivado del texto de cada azora. Las azoras no están dispuestas en orden cronológico (en el orden en el que los estudiosos islámicos suponen que fueron reveladas) sino que están ordenadas según el tamaño, aunque no de manera exacta; también se cree que este método es de inspiración divina. Luego de una breve introducción, aparece en el Corán la azora más larga y el texto concluye con las más cortas. Se dice que hay aproximadamente 77 639 letras en él.

Según algunos lingüistas no musulmanes, el orden decreciente de las azoras del Corán está inspirado probablemente en el tipo de ordenación de los "divanes" poéticos. El resultado final responde, más o menos, a un orden cronólogico invertido: las más largas, del periodo medinés, al principio del libro; las más cortas (correspondientes al inicio de la Revelación), del periodo mecano, al final.

Además de la división en azoras —y muy independientes de esta—, existen varias formas de dividir el Corán en secciones de similar tamaño que facilitan la lectura, la recitación y la memoria. Las siete "manzil" (estaciones) y las treinta "juz'" (partes) se pueden usar para trabajar con todo el Corán durante una semana o un mes (un mandil o un "juz<nowiki>'</nowiki>" por día). Un "juz<nowiki>'</nowiki>" se puede dividir en dos "ahzab" (grupos), y cada ahzab se puede subdividir en cuatro cuartos. Una estructura diferente ofrece el "ruku'at", en la cual aparecen unidades semánticas que se asemejan a párrafos y que se componen aproximadamente de diez aleyas.

Un "hafiz" es un hombre que ha memorizado todo el texto del Corán. Se cree que hay millones de ellos, desde niños hasta ancianos; muchos niños y adultos incluidos muchos que no pueden leer árabe, memorizan el Corán parcialmente o en su totalidad. Para realizar la salat (oración) no se necesita memorizar el texto completo , basta con dos azoras .

Todos los capítulos, con excepción de uno, empiezan con las palabras "Bismillah ir-Rahman ir-Rahim", «En el nombre de Dios, el más Misericordioso, el Compasivo». Veintinueve azoras empiezan con letras tomadas de un subconjunto restringido del alfabeto árabe; así, por ejemplo, la azora Maryam empieza «Kaf. Ha. Ya. 'Ain. Sad. (Esta es) una mención de la Misericordia de tu Señor a Su siervo Zacarías».

Aunque ha habido alguna especulación sobre el significado de estas letras, el consenso de los eruditos musulmanes es que su sentido último está más allá de la capacidad de entendimiento humano. Sin embargo, se ha observado que, en cuatro de los 29 casos, estas letras aparecen seguidas casi inmediatamente por la mención misma de la revelación coránica. Los esfuerzos de los académicos occidentales han sido provisionales; una propuesta, por ejemplo, fue que se trataba de las iniciales o los monogramas de los escribas que originalmente escribieron las azoras.

La creencia en el origen divino, directo e incorrupto del Corán es considerado fundamental por la mayoría de los musulmanes. Esto trae como consecuencia directa la creencia de que el texto no tiene errores ni inconsistencias.

A pesar de esto, a veces ocurre que unos versos prohíben una práctica determinada mientras que otros la permiten. Esto es interpretado por los musulmanes a la luz de la cronología relativa de los versos: debido a que el Corán fue revelado durante el curso de 23 años, muchos de los versos fueron clarificados o relacionados ("mansūkh") con otros versos. Los comentadores musulmanes explican esto afirmando que Mahoma fue dirigido de manera tal que pudiera liderar a un pequeño grupo de creyentes por el camino recto, en vez de revelarles de una sola vez el rigor total de la ley. Por ejemplo, la prohibición del alcohol fue llevada a cabo de forma gradual, no de inmediato. El verso más antiguo les dice a los creyentes “No se aproximen a las oraciones con una mente nublada, a menos que puedan entender todo lo que dicen” (4:43), se trata entonces de una prohibición de la ebriedad, pero no del consumo de alcohol: «Si piden consejo sobre el vino y el juego, diles: ‘Hay algún provecho en ellos para los hombres, pero el pecado es más grande que el provecho’» (2:219).

Finalmente, en algunos casos la mayoría de los académicos musulmanes aceptan la doctrina de la “abrogación” ("naskh"), según el cual los versos revelados más tarde a veces están por encima de los versos entregados anteriormente. Qué versos abrogan a otros es una cuestión generadora de controversia.

El Corán fue uno de los primeros textos redactados en árabe. Se halla escrito en una forma temprana del árabe clásico, que se conoce en español como árabe “coránico”. No hay muchos otros ejemplos de la lengua árabe de aquella época (algunos especialistas consideran que las Mu'allaqat u Odas suspendidas son ejemplos de árabe preislámico; otros consideran que fueron escritas antes de Mahoma; de cualquier manera, solo sobreviven cinco inscripciones en árabe preislámico).Poco tiempo después de la muerte de Mahoma, en 632, el islam se expandió más allá de Arabia y conquistó mucho de lo que era entonces el mundo «civilizado». Había millones de musulmanes en el extranjero con quienes los gobernadores árabes tenían que comunicarse. Por consiguiente, la lengua cambió rápidamente en respuesta a la nueva situación, perdiendo los casos y el vocabulario oscuro. Unas cuantas generaciones después de la muerte del profeta, muchas palabras usadas en el Corán ya se habían vuelto arcaísmos. Debido a que el lenguaje beduino había cambiado a un ritmo mucho más lento, los primeros lexicógrafos árabes recurrieron al beduino para explicar palabras o dilucidar cuestiones gramaticales. En buena medida debido a las necesidades religiosas de explicar el Corán al pueblo, la gramática y la lexicografía árabes se convirtieron en ciencias importantes, y el modelo para el lenguaje literario sigue siendo hasta el día de hoy el árabe usado en los tiempo coránicos, y no las variantes habladas en la actualidad.

Los musulmanes aseguran que el Corán destaca por su poesía y por su belleza y que su perfección literaria es una evidencia de su origen divino. Debido al hecho de que esta perfección solo es perceptible para los que hablan árabe, se considera que el texto original en árabe es el «verdadero Corán». En general, las traducciones a otras lenguas, aunque realizadas siempre por eminentes arabistas, son tenidas como simples glosas, en tanto interpretaciones, de las palabras directas de Dios. La lectura en otro idioma, sin la cadencia de la recitación en árabe, puede resultar confusa, tediosa y reiterativa:

Las tradiciones imperantes en la traducción y la publicación del Corán sostienen que cuando el libro es publicado simplemente debería titularse El Corán y, asimismo, debería incluir siempre un adjetivo calificativo (que evite cualquier confusión con otras «recitaciones»), este es el motivo por el cual la mayoría de las ediciones disponibles del Corán se llaman «El glorioso Corán», «El noble Corán» y otros títulos similares.

Existen numerosas traducciones del Corán a lenguas occidentales, llevadas a cabo por conocidos estudiosos islámicos. Cada traducción es un poco diferente de las otras y muestra la habilidad del traductor para verter el texto de una forma que sea al mismo tiempo fácil de entender y que mantenga el sentido original.

Prácticamente, todos los eruditos islámicos son capaces de leer y comprender el Corán en su forma original y, de hecho, la mayoría se lo sabe de memoria íntegramente.

El Corán mezcla la narrativa, la exhortación y la prescripción legal. Por lo general, las azoras combinan estos tres tipos de secuencias textuales y no siempre de maneras que resultan obvias para el lector, sino algunas veces de formas inexplicables. Los musulmanes señalan que el estilo único del Corán es un indicio más de su origen divino.

Existen muchos elementos que se repiten en el Corán: epítetos («Señor de los cielos y la tierra»), oraciones («Y cuando dijimos a los ángeles: 'Postraos ante Adán', todos se postraron»), e incluso historias, como la historia de Adán. Los especialistas musulmanes explican estas repeticiones como una forma de enfatizar y explicar diferentes aspectos de temas importantes. Asimismo, los académicos señalan que las traducciones a las lenguas occidentales demandan grandes cambios en la redacción y en el orden para poder mantener la explicación y el significado específicos.

El Corán oscila entre la rima y la prosa. Tradicionalmente, los gramáticos árabes consideran que el Corán es un género único en sí mismo. No es ni poesía (definida como palabras con métrica y rima) ni tampoco prosa (definida esta como una conversación normal, pero sin métrica ni rima, saj').

El Corán en ocasiones utiliza rima asonante entre los versos sucesivos; por ejemplo, en el inicio de la azora Al-Faǧr:

o, para dar un ejemplo menos asonante, la azora “al-Fîl”:

Obsérvese que las vocales finales de verso se dejan sin pronunciar cuando estos se pronuncian de manera aislada, se trata del fenómeno regular de las pausas en el árabe clásico. En estos casos, «î» y «û» riman a menudo y hay una cierta búsqueda de variación en las consonantes en posición final de sílaba).

Algunas azoras también incluyen un refrán que se repite varias veces, por ejemplo «"ar-Rahman"» («¿Entonces cuál de los favores de vuestro Señor negaréis?”) y «"al-Mursalat"» («¡Reproches ese día a los que repudien!»).

Los estudiosos islámicos del Corán dividen los versículos del libro en dos partes: los revelados en La Meca y los revelados en Medina después de la Hégira. En general, las azoras más viejas, de la Meca, tienden a contar con versículos más cortos, mientras que las de Medina, que lidian con cuestiones legales, son más largas. Contrástense las azoras de La Meca transcritas antes y unos versículos como los de "Al-Baqara", 229:

Del mismo modo, las azoras de Medina tienden a ser más largas; entre estas se encuentra la más larga del Corán: "Al-Baqara".

Antes de poder tocar una copia del Corán o mushaf, un musulmán debe realizar un wudu (la ablución o ritual de limpieza con agua). Esto se basa en una interpretación literal de la sura «Pues Este es en verdad el Honorable Corán, el Libro bien conservado, que nadie podrá tocar salvo quienes son limpios».

La execración del Corán significa insultar el Corán sacándolo de su contexto o desmembrándolo. Los musulmanes siempre tratan el libro con reverencia y, por consiguiente, está prohibido reciclar, reimprimir o simplemente descartar las copias viejas del texto (en este último caso, los volúmenes deben ser quemados respetuosamente, o bien, enterrados).

El respeto hacia el texto escrito del Corán es un elemento importante de la fe religiosa de muchos musulmanes. Ellos creen que insultar el Corán intencionalmente es una forma de blasfemia. De acuerdo con las leyes de algunos países musulmanes, la blasfemia se puede penar con una prisión de muchos años o incluso con la pena de muerte.

La mayoría de los musulmanes de hoy usan versiones impresas del Corán. Existen ediciones coránicas para todos los gustos, libros de bolsillo, muchos de ellos en ediciones bilingües, con el texto árabe por un lado y una glosa en lengua vernácula del otro. El primer Corán impreso se publicó en 1801 en Kazán.

Antes de que la impresión fuera común, el Corán se transmitía a través de copistas y calígrafos. Debido al hecho de que la tradición musulmana sentía que retratar directamente a los personajes sagrados podría conducir a la idolatría, se prohibió decorar el Corán con imágenes (como sí se hace con frecuencia en los textos cristianos, por ejemplo). En vez de esto, los musulmanes desarrollaron un amor y un cariño especiales por el texto en sí. Una de las consecuencias de esto es que la Caligrafía árabe es un arte que posee un honor muy alto en el mundo musulmán. Los musulmanes también decoraron sus ejemplares del Corán con figuras abstractas conocidas como arabescos, con tintas de colores y doradas. Algunas páginas de algunos de estos Coranes antiguos se han usado a lo largo de este artículo con fines ilustrativos.

El Corán ha sido traducido a muchos idiomas, pero las traducciones no son consideradas por los musulmanes como copias auténticas del Corán, sino simplemente como «glosas interpretativas» del libro; por lo tanto no se les da mucho peso en los debates relativos al significado del Corán. Además de esto, como simples interpretaciones del texto, se les trata como libros corrientes, en vez de darles todos los cuidados especiales que sí se les dan generalmente a los libros en lengua árabe. A pesar de esto, como es un Mensaje dirigido a toda la humanidad, se debe traducir el significado general de sus frases, estudiadas siglo tras siglo por multitud de sabios.

Robert de Ketton fue el primero en traducir el Corán y lo hizo al latín, en 1143 y, posiblemente, la más reciente con su correspondiente Tafsir o exégesis sea la de Ali Ünal.






</doc>
<doc id="6700" url="https://es.wikipedia.org/wiki?curid=6700" title="11 de agosto">
11 de agosto

El 11 de agosto es el 223.º (ducentésimo vigesimotercer) día del año en el calendario gregoriano y el 224.º en los años bisiestos. Quedan 142 días para finalizar el año.








</doc>
<doc id="6701" url="https://es.wikipedia.org/wiki?curid=6701" title="Música electrónica">
Música electrónica

La música electrónica es aquel tipo de música que emplea instrumentos musicales electrónicos y tecnología musical electrónica para su producción e interpretación. En general, se puede distinguir entre el sonido producido mediante la utilización de medios electromecánicos de aquel producido mediante tecnología electrónica, la cual también puede ser mezclada. Algunos ejemplos de dispositivos que producen sonido electro-mecánicamente son el telarmonio, el órgano Hammond y la guitarra eléctrica. La producción de sonidos puramente electrónica puede lograrse mediante aparatos como el theremin, el sintetizador de sonido y el ordenador.

La música electrónica se asoció en su día exclusivamente a una forma de música culta occidental, pero desde finales del año 1960, la disponibilidad de tecnología musical a precios accesibles propició que la música producida por medios electrónicos se hiciera cada vez más y más popular. En la actualidad, la música electrónica presenta una gran variedad técnica y compositiva, abarcando desde formas de música culta experimental hasta formas populares como la música electrónica de baile o shuffle dance.

La habilidad de grabar sonidos suele relacionarse con la producción de música electrónica, aunque esta no sea absolutamente necesaria para ello. El primer dispositivo conocido capaz de grabar sonido fue el fonoautógrafo, patentado en 1857 por Édouard-Léon Scott de Martinville. Podía grabar sonidos visualmente, pero no reproducirlos de nuevo. 

En 1878, Thomas A. Edison patentó el fonógrafo, que utilizaba cilindros similares al aparato de Scott. Aunque se siguieron utilizando los cilindros durante algún tiempo, Emile Berliner desarrolló el fonógrafo de disco en 1889. Otro invento significativo que posteriormente tendría una gran importancia en la música electrónica fue la válvula audión, del tipo triodo, diseñada por Lee DeForest. Se trata de la primera válvula termoiónica, inventada en 1906, que permitiría la generación y amplificación de señales eléctricas, la emisión de radio, la computación electrónica.

Con anterioridad a la música electrónica, ya existía un creciente deseo entre los compositores de utilizar las tecnologías emergentes en el terreno musical. Se crearon multitud de instrumentos que empleaban diseños electromecánicos, los cuales facilitaron el camino para la aparición de instrumentos electrónicos. Un instrumento electromecánico llamado Telharmonium (en ocasiones Teleharmonium o Dynamophone) fue desarrollado por Thaddeus Cahill entre los años 1898 y 1899. Sin embargo, como consecuencia de su inmenso tamaño, nunca se llegó a adoptar. Se suele considerar como primer instrumento electrónico el Theremin, inventado por el profesor Léon Theremin alrededor de 1919–1920. Otro primitivo instrumento electrónico fue el Ondes Martenot, que se hizo conocido al ser utilizado en la obra "Sinfonía Turangalila", por Olivier Messiaen. También fue utilizado por otros compositores, especialmente franceses, como Andre Jolivet.

En 1907, justo un año después de la invención del triodo audión, Ferruccio Busoni publicó "Esbozo de una Nueva Estética de la Música", que trataba sobre el uso tanto de fuentes electrónicas como de otras en la música del futuro. Escribió sobre el futuro de las escalas microtonales en la música, posibles gracias al Dynamophone de Cahill: «Solo mediante una larga y cuidadosa serie de experimentos, y un continuo entrenamiento del oído, puede hacerse de este material desconocido uno accesible y plástico para la generación venidera y para el arte».

Como consecuencia de este escrito, así como a través de su contacto personal, Busoni tuvo un profundo efecto en multitud de músicos y compositores, especialmente en su discípulo Edgard Varèse.

En Italia, el futurismo se acercó a la estética musical en transformación desde un ángulo diferente. Una idea de la filosofía futurista era la de valorar el "ruido", así como dotar de valor artístico y expresivo a ciertos sonidos que anteriormente no habían sido considerados como musicales. El "Manifiesto Técnico de la Música Futurista" de Balilla Pratella, publicado en 1911, establece que su credo es: «Presentar el alma musical de las masas, de las grandes fábricas, de los trenes, de los cruceros transatlánticos, de los acorazados, de los automóviles y aeroplanos. Añadir a los grandes temas centrales del poema musical el dominio de la máquina y el victorioso reinado de la electricidad».

El 11 de marzo de 1913, el futurista Luigi Russolo publicó su manifiesto "El arte de los ruidos" (original en italiano, "L'arte dei Rumori"). En 1914, organizó el primer concierto del "arte de los ruidos" en Milán. Para ello utilizó su Intonarumori, descrito por Russolo como "instrumentos acústico ruidistas, cuyos sonidos (aullidos, bramidos, arrastramientos, gorgoteos, etc.) eran manualmente activados y proyectados mediante vientos y megáfonos". En junio se organizaron conciertos similares en París.

Esta década trajo una gran riqueza de instrumentos electrónicos primitivos, así como las primeras composiciones para instrumentación electrónica. El primer instrumento, el Theremin, fue creado por Léon Theremin (nacido como Lev Termen) entre 1919 y 1920 en Leningrado. Gracias a él se realizaron las primeras composiciones para instrumento electrónico, opuestas a aquellas realizadas por los que se dedicaban a crear sinfonías de ruidos. En 1929, Joseph Schillinger compuso su "Primera Suite Aerofónica para Theremin y Orquesta", interpretada por primera vez por la Orquesta de Cleveland y Leon Theremin como solista.

Además del Theremin, el "Ondes Martenot" fue inventado en 1928 por Maurice Martenot, quien debutó en París. El año siguiente, Antheil compuso por primera vez para dispositivos mecánicos, aparatos productores de ruidos, motores y amplificadores en su ópera inacabada "Mr. Bloom".

La grabación de sonidos dio un salto cualitativo en 1927, cuando el inventor estadounidense J. A. O'Neill desarrolló un dispositivo para la grabación que utilizaba un tipo de cinta recubierta magnéticamente. No obstante, fue un desastre comercial. Dos años más tarde, Laurens Hammond abrió una empresa dedicada a la fabricación de instrumentos electrónicos. Comenzó a producir el Órgano Hammond, basado en los principios del Telharmonium, junto a otros desarrollos como las primeras unidades de reverberación.

El método foto-óptico de grabación de sonido utilizado en el cine hizo posible obtener una imagen visible de la onda de sonido, así como sintetizar un sonido a partir de una onda de el mismo.

En la misma época, comenzó la experimentación del arte sonoro, cuyos primeros exponentes incluyen a Tristan Tzara, Kurt Schwitters y Filippo Tommaso Marinetti entre otros.

Desde aproximadamente el año 1900 se utilizaba el magnetófono de alambre magnético de baja fidelidad, y a comienzos de 1930 la industria cinematográfica comenzó a convertirse a los nuevos sistemas de grabación de sonido ópticos basados en células fotoeléctricas. En esta época la empresa alemana de electrónica AEG desarrolla el primer magnetófono de cinta práctico, el Magnetophon K-1, revelado en el Berlin Radio Show en agosto de 1935.

Durante la Segunda Guerra Mundial, Walter Weber redescubrió y aplicó la técnica AC Bias, que incrementó drásticamente la fidelidad de las grabaciones magnéticas al añadir una alta frecuencia inaudible. Extendió en 1941 la curva de frecuencia del Magnetophon K4 hasta 10 kHz y mejoró la relación señal/ruido hasta 60 dB, sobrepasando cualquier sistema de grabación conocido en aquel tiempo.

En 1942, AEG ya estaba realizando pruebas de grabación en estéreo. No obstante, estos dispositivos y técnicas fueron un secreto fuera de Alemania hasta el final de la guerra, cuando varios de estos aparatos fueron requisados y llevados a Estados Unidos por Jack Mullin. Estos sirvieron de base para los primeros grabadores de cinta profesionales que fueron comercializados en Estados Unidos, entre ellos el Model 200 producido por la empresa Ampex.

La cinta de audio magnética abrió un gran campo de posibilidades sonoras para músicos, compositores, productores e ingenieros. Esta era relativamente barata y su fidelidad en la reproducción era mejor que cualquier otro medio de audio conocido hasta la fecha. Cabe señalar que, a diferencia de los discos, ofrecía la misma plasticidad que la película: puede ser ralentizada, acelerada o incluso reproducirse al revés. Puede editarse también físicamente e incluso pueden unirse diferentes trozos de cinta en loops infinitos, que reproducen continuamente determinados patrones de material pregrabado. La amplificación de audio y el equipo de mezcla expandieron aún más allá las posibilidades de la cinta como medio de producción, permitiendo que múltiples grabaciones fueran grabadas a la vez en otra cinta diferente. Otra posibilidad de la cinta era su capacidad de ser modificada fácilmente para convertirse en máquinas de eco, produciendo así de modo complejo, controlable y con gran calidad, efectos de eco y reverberación, lo que es prácticamente imposible de conseguir por medios mecánicos.

Pronto los músicos comenzaron a utilizar el grabador de cinta o magnetófono para desarrollar una nueva técnica de composición llamada música concreta. Esta técnica consiste en la edición de fragmentos de sonidos de la naturaleza o de procesos industriales grabados conjuntamente. Las primeras piezas de esta música fueron creadas por Pierre Schaeffer, con la colaboración de Pierre Henry. En 1950, Schaeffer dio el primer concierto de música concreta en la École Normale de Musique de Paris. Posteriormente, Pierre Henry colaboró con Schaeffer en la "Symphonie pour un homme seul" (1950), la primera obra importante de música concreta. Un año más tarde, RTF creó el primer estudio para la producción de música electrónica, lo que se convertiría en una tendencia global. También en 1951, Schaeffer y Henry produjeron una ópera, "Orpheus", para sonidos y voces concretos.

Karlheinz Stockhausen trabajó brevemente en el estudio de Schaeffer en 1952 y posteriormente, durante muchos años, en el Estudio de Música Electrónica de la WDR de Colonia, en Alemania.

En Colonia, el que se convertiría en el estudio de música electrónica más famoso del mundo, inició actividades en la radio de la NWDR en 1951, después de que el físico Werner Meyer-Eppler, el técnico de sonido Robert Beyer y el compositor Herbert Eimert convencieran al director de la NWDR, Hanns Hartmann, de la necesidad de dicho espacio. En el mismo año de su creación fueron transmitidos los primeros estudios de música electrónica en un programa de la propia radio y presentados en los‭ Cursos de Verano de Darmstadt.‭ ‬En‭ ‬1953‭ ‬hubo una demostración pública en la sala de conciertos de la Radio de Colonia, donde se dejaron escuchar siete piezas electrónicas.‭ ‬Los compositores de los estudios electrónicos eran Herbert ‬Eimert,‭ ‬Karel ‬Goeyvaerts,‭ ‬Paul‭ ‬Gredinger,‭ Henry ‬Pousseur y Karlheinz Stockhausen.

El programa comprendía las siguientes piezas:

En su tesis de 1949, "Elektronische Klangerzeugung: Elektronische Musik und Synthetische Sprache", Meyer-Eppler concibió la idea de sintetizar música desde señales producidas electrónicamente. De esta manera, la E"lektronische Musik" se diferenciaba a gran escala respecto a la M"usique concrète" francesa, que utilizaba sonidos grabados a partir de fuentes acústicas.

Con Stockhausen y Mauricio Kagel como residentes, el estudio de música electrónica de Colonia se convirtió en un emblema del "avant-garde o" vaguardismo, cuando se empezó ya a combinar sonidos generados electrónicamente con los de instrumentos tradicionales. Ejemplos significativos son "Mixtur" (1964) y "Hymnen, dritte Region mit Orchester" (1967). Stockhausen afirmó que sus oyentes decían que su música electrónica les daba una experiencia de "espacio exterior", sensaciones de volar, o de estar en "un mundo de ensueño fantástico".

Aunque los primeros instrumentos electrónicos como el Ondes Martenot, el Theremin o el Trautonium eran poco conocidos en Japón con anterioridad a la Segunda Guerra Mundial, algunos compositores habían tenido conocimiento de ellos en su momento, como Minao Shibata. Años después, diferentes músicos en Japón comenzaron a experimentar con música electrónica, a lo que contribuyó el apoyo institucional, lo que permitió a los compositores experimentar con el último equipamiento de grabación y procesamiento de audio. Estos esfuerzos dieron lugar a una forma musical que fusionaba la música asiática con un nuevo género y sembraría las bases del dominio japonés en el desarrollo de tecnología musical durante las siguientes décadas.

Tras la creación de la compañía Sony (conocida entonces como Tokyo Tsushin Kogyo K.K.) en 1946, dos compositores japoneses, Toru Takemitsu y Minao Shibata, de modo independiente, escribieron sobre la posibilidad de utilizar la tecnología electrónica para producir música hacia finales de los años 1940. En 1948, Takemitsu concibió una tecnología que pudiese traer "ruido" dentro de tonos musicales atemperados dentro de un pequeño y complejo tubo, una idea similar a la m"usique concrète" que Pierre Schaeffer había aventurado en el mismo año. En 1949, Shibata escribió sobre su concepto de "un instrumento musical con grandes posibilidades de actuación" que pudiera "sintetizar cualquier tipo de onda de sonido" y que sea "manejado muy fácilmente," prediciendo que con un instrumento tal, "la escena musical sería cambiada drásticamente". Ese mismo año, Sony desarrolló el magnetófono magnético G-Type.

En 1950, el estudio de música electrónica Jikken Kobo sería fundado por un grupo de músicos que querían producir música electrónica experimental utilizando magnetófonos Sony. Entre sus miembros se encontraban Toru Takemitsu, Kuniharu Akiyama y Joji Yuasa, y estaba apoyado por Sony, empresa que ofrecía acceso a la última tecnología de audio. La compañía contrató a Takemitsu para componer música electroacústica electrónica para mostrar sus magnetófonos. Más allá del Jikken Kobo, muchos otros compositores como Yasushi Akutagawa, Saburo Tominaga y Shiro Fukai también estaban experimentando con música electroacústica entre 1952 y 1953.

En Estados Unidos, se utilizaban sonidos creados electrónicamente para diferentes composiciones, como ejemplifica la pieza "Marginal Intersection" de Morton Feldman. Esta pieza está pensada para vientos, metales, percusión, cuerdas, dos osciladores y efectos de sonido.

El "Music for Magnetic Tape Project" fue formado por miembros de la Escuela de Nueva York (John Cage, Earle Brown, Christian Wolff, David Tudor y Morton Feldman) y duró tres años hasta 1954. Durante esta época, Cage completó su "Williams Mix", en 1953.

Un importante desarrollo lo constituyó la aparición de ordenadores utilizados para componer música, en contraposición de la manipulación o creación de sonidos. Iannis Xenakis comenzó lo que se conoce como "musique stochastique" o música estocástica, un método compositivo que emplea sistemas matemáticos de probabilidad estocásticos. Se utilizaban diferentes algoritmos de probabilidad para crear piezas bajo un set de parámetros. Xenakis utilizó papel gráfico y una regla para ayudarse a calcular la velocidad de las trayectorias de los glissandos para su composición orquestal "Metástasis" (1953-1954), utilizando posteriormente ordenadores para componer piezas como "ST/4" para cuarteto de cuerda y "ST/48" para orquesta.

En 1954, Stockhausen compuso "Elektronische Studie II", la primera pieza electrónica en ser publicada como banda sonora.

En 1955 aparecieron más estudios electrónicos y experimentales. Fueron notables la creación del Estudio de Fonología, un estudio en el NHK de Tokio fundado por Toshiro Mayuzumi y el estudio de Phillips en Eindhoven, Holanda, que se trasladó a la Universidad de Utrecht como Instituto de Sonología en 1960.

La banda sonora de "Forbidden Planet", producida por Louis y Bebe Barron, fue compuesta únicamente mediante circuitos caseros y magnetófonos en 1956.

El primer computador del mundo en reproducir música fue el CSIRAC, diseñado y construido por Trevor Pearcey y Maston Beard. El matemático Geoff Hill programó el CSIRAC para tocar melodías de música popular. No obstante, este computador reproducía un repertorio estándar y no pudo utilizarse para ampliar el pensamiento musical o para tocar composiciones más elaboradas.

El impacto de los computadores continuó durante 1956. Lejaren Hiller y Leonard Isaacson compusieron "Iliac Suite" para un cuarteto de cuerda, siendo la primera obra completa en ser compuesta con la asistencia de un computador utilizando un algoritmo en la composición. Posteriores desarrollos incluyeron el trabajo de Max Mathews en Bell Laboratories, quien desarrolló el influyente programa MUSIC I. La tecnología del vocoder fue otro importante desarrollo de esta época.

En 1956, Stockhausen compuso "Gesang der Jünglinge", la primera gran obra del estudio de Colonia, basada en un texto del "Libro de Daniel". Un importante desarrollo tecnológico fue la invención del sintetizador Clavivox por Raymond Scott, con ensamblaje de Robert Moog.

El sintetizador RCA Mark II Sound Synthesizer apareció en 1957. A diferencia de los primeros Theremin y Ondes Martenot, era difícil de usar, pues requería una extensa programación y no podía tocarse en tiempo real. En ocasiones denominado el primer sintetizador electrónico, el RCA Mark II Sound Synthesizer utilizaba osciladores de válvula termoiónica e incorporaba el primer secuenciador. Fue diseñado por RCA e instalado en el Columbia-Princeton Electronic Music Center, donde sigue en la actualidad. Posteriormente, Milton Babbitt, influenciado en sus años de estudiante por la "revolución en el pensamiento musical" de Schoenberg, comenzó a aplicar técnicas seriales a la música electrónica.

Estos fueron años fértiles para la música electrónica, no solo para la académica, sino también para algunos artistas independientes a medida que la tecnología del sintetizador se volvía más accesible. En esta época, una poderosa comunidad de compositores y músicos que trabajaba con nuevos sonidos e instrumentos se había establecido y estaba creciendo. Durante estos años aparecen composiciones como "Gargoyle" de Luening para violín y cinta, así como la premier de "Kontakte" de Stockhausen para sonidos electrónicos, piano y percusión. En esta última, Stockhausen abandonó la forma musical tradicional basada en un desarrollo lineal y en un clímax dramático. En este nuevo acercamiento, que él denominó como "forma momento", se recuerdan las técnicas de "cinematic splice" del cine de principios del siglo XX.

El primero de estos sintetizadores en aparecer fue el Buchla, en 1963, siendo producto del esfuerzo del compositor de música concreta Morton Subotnick.

El Theremin había sido utilizado desde los años 20, manteniendo una cierta popularidad gracias a su utilización en numerosas bandas sonoras de películas de ciencia ficción de los años 50 (por ejemplo: "The Day the Earth Stood Still" de Bernard Herrmann). Durante los años 1960, el Theremin hizo apariciones ocasionales en la música popular.

En el Reino Unido, durante este período, el BBC Radiophonic Workshop (creado en 1958) emergió como uno de los estudios más productivos y renombrados del mundo, gracias a su labor en la serie de ciencia ficción "Doctor Who". Uno de los artistas electrónicos británicos más influyentes de este período fue Delia Derbyshire. Es famosa por su icónica ejecución en 1963 del tema central de "Doctor Who", compuesto por Ron Grainer y reconocida por algunos como la pieza de música electrónica más conocida en el mundo. Derbyshire y sus colegas, entre los que se encuentran Dick Mills, Brian Hodgson (creador del efecto de sonido TARDIS), David Cain, John Baker, Paddy Kingsland y Peter Howell, desarrollaron un amplio cuerpo de trabajo que incluye bandas sonoras, atmósferas, sinfonías de programas y efectos de sonido para BBC TV y sus emisoras de radio.
En 1961, Josef Tal creó el "Centre for Electronic Music in Israel" en la Universidad Hebrea, y en 1962 Hugh Le Caine llegó a Jerusalén para instalar su "Creative Tape Recorder" en el centro.

Milton Babbitt compuso su primer trabajo electrónico utilizando el sintetizador, que fue creado mediante el RCA en el CPEMC. Las colaboraciones se realizaban superando las barreras de los océanos y continentes. En 1961, Ussachevsky invitó a Varèse al Columbia-Princeton Studio (CPEMC), siendo asistido por Mario Davidovsky y Bülent Arel. La intensa actividad del CPEMC, entre otros, inspiró a la creación en San Francisco del Tape Music Center en 1963, por Morton Subotnick junto a otros miembros adicionales, como Pauline Oliveros, Ramón Sender, Terry Riley y Anthony Martin. Un año después tuvo lugar el Primer Seminario de Música Electrónica en Checoslovaquia, organizado en el Radio Broadcast Station de Plzen.

Se siguieron desarrollando nuevos instrumentos, y uno de los más importantes avances tuvo lugar en 1964, cuando Robert Moog introdujo el sintetizador Moog, el primer sintetizador analógico controlado por un sistema integrado modular de control de voltaje. Moog Music introdujo posteriormente un sintetizador más pequeño con un teclado, llamado Minimoog, que fue utilizado por multitud de compositores y universidades, haciéndose así muy popular. Un ejemplo clásico del uso del Moog de gran tamaño es el álbum "Switched-On Bach," de Wendy Carlos.

En 1966, Pierre Schaeffer fundó el "Groupe de Recherches Musicales" (Grupo de Investigación Musical) para el estudio y la investigación de la música electrónica. Su programación está estructurada a partir de un compromiso en los procesos de difusión, investigación y creación de la música contemporánea y las tendencias en videoarte e imagen más actuales. Sus exhibiciones y conciertos son reproducidas en tiempo real mediante dispositivos electrónicos, interfaces de audio-vídeo y una plantilla de músicos y videoartistas nacionales e internacionales abiertos al uso de tecnologías punta.

CSIRAC, el primer computador en reproducir música, realizó este acto públicamente en agosto de 1951. Una de las primeras demostraciones a gran escala de lo que se conoció como "computer music" fue una emisión nacional pre-grabada en la red NBC para el programa Monitor el 10 de febrero de 1962. Un año antes, LaFarr Stuart programó el ordenador CYCLONE de la Universidad del Estado de Iowa para reproducir canciones sencillas y reconocibles a través de un altavoz amplificado adherido a un sistema originalmente utilizado para temas administrativos y de diagnóstico.

Los años 50, 60 y la década de los 70 presenciaron también el desarrollo de grandes marcos operativos para síntesis informática. En 1957, Max Mathews, de Bell Labs, desarrolló el programa MUSIC, culminando un lenguaje de síntesis directa digital.

En París, IRCAM se convirtió en el principal centro de investigación de música creada por computador, desarrollando el sistema informático Sogitec 4X, que incluía un revolucionario sistema de procesamiento de señal digital en tiempo real. "Répons" (1981), obra para 24 músicos y 6 solistas de Pierre Boulez, utilizó el sistema 4X para transformar y dirigir a los solistas hacia un sistema de altavoces.

En Estados Unidos, la electrónica en directo fue llevada a cabo por primera vez en los años 1960 por miembros del Milton Cohen's Space Theater en Ann Arbor, Míchigan, entre los que estaban Gordon Mumma, Robert Ashley, David Tudor y The Sonic Arts Union, fundada en 1966 por los nombrados anteriormente, incluyendo también a Alvin Lucier y David Behrman. Los festivales ONCE, que mostraban música multimedia para teatro, fueron organizados por Robert Ashley y Gordon Mumma en Ann Arbor entre 1958 y 1969. En 1960, John Cage compuso "Cartridge Music", una de las primeras obras de electrónica en vivo.

Los compositores y músicos de jazz, Paul Bley y Annette Peacock, fueron de los primeros en tocar en concierto utilizando sintetizadores Moog hacia finales de 1960. Peacock hacía un uso regular de un sintetizador Moog adaptado para procesar su voz tanto en el escenario como en grabaciones de estudio.

Con el transcurrir del tiempo se empezaron a formar eventos sociales que trataban de aglutinar una gran cantidad de conciertos, con varios artistas en directo. Hasta el momento son muchos los festivales que han manifestado la escena electrónica, marcando muchos de ellos récords de asistencia masiva al evento. Por mencionar solo algunos tenemos como los más representativos y destacados del género al:

A la fecha siguen surgiendo megafestivales que buscan expandir la esencia del género electrónico.

Robert Moog (también conocido como Bob Moog), a finales de 1963, conoció al compositor experimental Herbert Deutsch, quien, en su búsqueda por sonidos electrónicos nuevos, inspiró a Moog a crear su primer sintetizador, el Sintetizador Modular Moog.

El Moog, aunque era conocido con anterioridad por la comunidad educativa y musical, fue presentado a la sociedad en el otoño de 1964, cuando Bob hizo una demostración durante la Convención de la Sociedad de Ingeniería de Audio, celebrada en Los Ángeles. En esta convención, Moog ya recibió sus primeros pedidos y el negocio despegó.

La compañía Moog Music creció de forma espectacular durante los primeros años, haciéndose aún más conocida cuando Wendy Carlos editó el álbum "Switched on Bach". Bob diseñó y comercializó nuevos modelos, como el Minimoog (la primera versión portátil del Moog Modular), el Moog Taurus (teclado de pedales de una octava de extensión, con transposición para bajos y agudos), el PolyMoog (primer modelo 100% polifónico), el MemoryMoog (polifónico, equivalía a seis MiniMoog's en uno), el MinitMoog, el Moog Sanctuary, etc.

Moog no supo gestionar bien su empresa y esta pasó de tener listas de espera de nueve meses a no recibir ni un solo pedido. Agobiado por las deudas perdió el control de la empresa, la cual fue adquirida por un inversionista. Aun así continuó diseñando instrumentos musicales hasta 1977, cuando abandonó Moog Music y se mudó a un pequeño poblado en las montañas Apalaches. Sin él, la Moog Music fue a pique poco después.

En 1967, Kato se acercó al ingeniero Fumio Mieda, quien deseaba iniciarse en la construcción de teclados musicales. Impulsado por el entusiasmo de Mieda, Kato le pidió construir un prototipo de teclado y, 18 meses después, Mieda le presentó un órgano programable. La compañía Keio vendió este órgano bajo la marca Korg, hecha de la combinación de su nombre con la palabra órgano, en inglés "Organ".

Los órganos producidos por Keio fueron exitosos a finales de los años 1960 y principios de los 70 pero, consciente de la competencia con los grandes fabricantes de órganos ya establecidos, Kato decidió usar la tecnología del órgano electrónico para construir teclados dirigidos al mercado de los sintetizadores. De hecho, el primer sintetizador de Keio (MiniKorg) fue presentado en 1973. Después del éxito de este instrumento, Keio presentó diversos sintetizadores de bajo costo durante los años 1970 y 80 bajo la marca Korg.

En 1970, Charles Wuorinen compuso "Time's Encomium", convirtiéndose así en el primer ganador del Premio Pulitzer por ser una composición completamente electrónica. Los años 1970 también vieron cómo se generalizaba el uso de sintetizadores en la música rock, con ejemplos como: Pink Floyd, Tangerine Dream, Yes y Emerson, Lake & Palmer, etc.

A lo largo de los años 1970, bandas como The Residents y Can abanderaron un movimiento de música experimental que incorporaba elementos de música electrónica. Can fue uno de los primeros grupos en utilizar loops de cinta para la sección de ritmo, mientras The Residents crearon sus propias cajas de ritmos. También en esta época diferentes bandas de rock, desde Genesis hasta The Cars, comenzaron a incorporar sintetizadores en sus arreglos de rock.

En 1979, el músico Gary Numan contribuyó a llevar la música electrónica a un público más amplio con su hit pop "Cars", del álbum "The Pleasure Principle". Otros grupos y artistas que contribuyeron significativamente a popularizar la música creada exclusiva o fundamentalmente de modo electrónico fueron Kraftwerk, Depeche Mode, Jean Michel Jarre, Mike Oldfield o Vangelis.

En 1980, un grupo de músicos y fabricantes se pusieron de acuerdo para estandarizar una interfaz a través de la que diferentes instrumentos pudieran comunicarse entre ellos y el ordenador principal. El estándar se denominó MIDI (Musical Instrument Digital Interface). En agosto de 1983, la especificación 1.0 de MIDI fue finalizada.

La llegada de la tecnología MIDI permitió que con el simple acto de presionar una tecla, controlar una rueda, mover un pedal o dar una orden en un micro ordenador, se pudieran activar todos y cada uno de los dispositivos del estudio remotamente y de forma sincronizada, respondiendo cada dispositivo de acuerdo a las condiciones prefijadas por el compositor.

Miller Puckette desarrolló un software para el procesamiento gráfico de señal de 4X llamado Max, que posteriormente sería incorporado a Macintosh para el control de MIDI en tiempo real, haciendo que la composición algorítmica estuviera disponible para cualquier compositor que tuviera un mínimo conocimiento de programación informática.

En 1979, la empresa australiana Fairlight lanzó el Fairlight CMI (Computer Musical Instrument), el primer sistema práctico de sampler polifónico digital. En 1983, Yamaha introdujo el primer sintetizador digital autónomo, el DX-7, el cual utilizaba síntetis de modulación de frecuencia (síntesis FM), probada por primera vez por John Chowning en Stanford a finales de los años 1960.

Hacia finales de los años 80, los discos de música de baile que utilizaban instrumentación exclusivamente electrónica se hicieron cada vez más populares. Esta tendencia ha continuado hasta el presente, siendo habitual escuchar música electrónica en los "clubs" de todo el mundo.

En los años 1990, comenzó a ser posible llevar a cabo actuaciones con la asistencia de ordenadores interactivos. Otro avance reciente es la composición "Begin Again Again" de Tod Machover (MIT y IRCAM) para hyper chelo, un sistema interactivo de sensores que miden los movimientos físicos del chelista. Max Methews desarrolló el programa Conductor para control en tiempo real del tempo, la dinámica y el timbre de un tema electrónico.

A medida que la tecnología informática se hace más accesible y el "software" musical avanza, la producción musical se hace posible utilizando medios que no guardan ninguna relación con las prácticas tradicionales. Lo mismo ocurre con los conciertos, extendiéndose su práctica utilizando ordenadores portátiles y "live coding". Se populariza el término "Live PA" para describir cualquier tipo de actuación en vivo de música electrónica, ya sea utilizando ordenador, sintetizador u otros dispositivos.

En las décadas de 1990 y 2000 surgieron diferentes entornos virtuales de estudio construidos sobre "software", entre los que destacan productos como Reason, de Propellerhead, y Ableton Live, que se hacen cada vez más populares. Estas herramientas proveen alternativas útiles y baratas para los estudios de producción basados en "hardware". Gracias a los avances en la tecnología de microprocesadores, se hace posible crear música de elevada calidad utilizando poco más que un solo ordenador. Estos avances han democratizado la creación musical, incrementándose así masivamente y estando disponible al público en internet.

El avance del "software" y de los entornos de producción virtuales ha llevado a que toda una serie de dispositivos, antiguamente solo existentes como "hardware," estén ahora disponibles como piezas virtuales, herramientas o "plugins" de los "software". Algunos de los "softwares" más populares son Max/Msp y Reaktor, así como paquetes de código abierto tales como Pure Data, SuperCollider y ChucK.

Los avances en la miniaturización de los componentes electrónicos, que en su época facilitaron el acceso a instrumentos y tecnologías usadas solo por músicos con grandes recursos económicos, han dado lugar a una nueva revolución en las herramientas electrónicas usadas para la creación musical. Por ejemplo, durante los años 1990, los teléfonos móviles incorporaban generadores de tonos monofónicos que algunas fábricas usaron no solo para generar los "Ringtones" de sus equipos, sino que permitieron a sus usuarios algunas herramientas de creación musical. Posteriormente, los cada vez más pequeños y potentes computadores portátiles, computadores de bolsillo y PDA´s, abrieron camino a las actuales "Tablets y" teléfonos inteligentes que permiten no solo el uso de generadores de tono, sino el de otras herramientas como "samplers", sintetizadores monofónicos y polifónicos, grabación multipista, etc. que permiten la creación musical en casi cualquier lugar.





</doc>
