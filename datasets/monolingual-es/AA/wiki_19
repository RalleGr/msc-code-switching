<doc id="3401" url="https://es.wikipedia.org/wiki?curid=3401" title="Análisis numérico">
Análisis numérico

El análisis numérico o cálculo numérico es la rama de las matemáticas encargada de diseñar algoritmos para simular aproximaciones de solución a problemas en análisis matemático. Se distingue del cómputo simbólico en que no manipula expresiones algebraicas, sino números.

El análisis numérico cobra especial importancia con la llegada de los ordenadores. Los ordenadores son útiles para cálculos matemáticos extremadamente complejos, pero en última instancia operan con números binarios y operaciones matemáticas simples.

Desde este punto de vista, el análisis numérico proporcionará todo el "andamiaje" necesario para llevar a cabo todos aquellos procedimientos matemáticos susceptibles de expresarse algorítmicamente, basándose en algoritmos que permitan su simulación o cálculo en procesos más sencillos empleando números.

Definido el error, junto con el error admisible, pasamos al concepto de estabilidad de los algoritmos. Muchas de las operaciones matemáticas pueden llevarse adelante a través de la generación de una serie de números que a su vez alimentan de nuevo el algoritmo (feedback). Esto proporciona un poder de cálculo y refinamiento importantísimo a la máquina que a medida que va completando un ciclo va llegando a la solución. El problema ocurre en determinar hasta cuándo deberá continuar con el ciclo, o si nos estamos alejando de la solución del problema.

Finalmente, otro concepto paralelo al análisis numérico es el de la , tanto de los números como de otros conceptos matemáticos como los vectores, polinomios, etc. Por ejemplo, para la representación en ordenadores de números reales, se emplea el concepto de coma flotante que dista mucho del empleado por la matemática convencional.

En general, estos métodos se aplican cuando se necesita un valor numérico como solución a un problema matemático, y los procedimientos «exactos» o «analíticos» (manipulaciones algebraicas, teoría de ecuaciones diferenciales, métodos de integración, etc.) son incapaces de dar una respuesta. Debido a ello, son procedimientos de uso frecuente por físicos e ingenieros, y cuyo desarrollo se ha visto favorecido por la necesidad de estos de obtener soluciones, aunque la precisión no sea completa. Debe recordarse que la física experimental, por ejemplo, nunca arroja valores exactos sino intervalos que engloban la gran mayoría de resultados experimentales obtenidos, ya que no es habitual que dos medidas del mismo fenómeno arrojen valores exactamente iguales.

Los problemas de esta disciplina se pueden dividir en dos grupos fundamentales: 



Asimismo, existe una subclasificación de estos dos grandes apartados en tres categorías de problemas, atendiendo a su naturaleza o motivación para el empleo del cálculo numérico:


El análisis numérico se divide en diferentes disciplinas de acuerdo con el problema que resolver.

Uno de los problemas más sencillos es la evaluación de una función en un punto dado. Para polinomios, uno de los métodos más utilizados es el algoritmo de Horner, ya que reduce el número de operaciones a realizar. En general, es importante estimar y controlar los errores de redondeo que se producen por el uso de la aritmética de punto flotante.

La extrapolación es muy similar a la interpolación, excepto que ahora queremos encontrar el valor de la función desconocida en un punto que no está comprendido entre los puntos dados.

La regresión es también similar, pero tiene en cuenta que los datos son imprecisos. Dados algunos puntos, y una medida del valor de la función en los mismos (con un error debido a la medición), queremos determinar la función desconocida. El método de los mínimos cuadrados es una forma popular de conseguirlo.

Otro problema fundamental es calcular la solución de una ecuación o sistema de ecuaciones dado. Se distinguen dos casos dependiendo de si la ecuación o sistema de ecuaciones es o no lineal. Por ejemplo, la ecuación formula_1 es lineal mientras que la ecuación de segundo grado formula_2 no lo es.

Mucho esfuerzo se ha puesto en el desarrollo de métodos para la resolución de sistemas de ecuaciones lineales. Métodos directos, i.e., métodos que utilizan alguna factorización de la matriz son el método de eliminación de Gauss, la descomposición LU, la descomposición de Cholesky para matrices simétricas (o hermíticas) definidas positivas, y la descomposición QR. Métodos iterativos como el método de Jacobi, el método de Gauss-Seidel, el método de las aproximaciones sucesivas y el método del gradiente conjugado se utilizan frecuentemente para grandes sistemas.

En la resolución numérica de ecuaciones no lineales algunos de los métodos más conocidos son los métodos de bisección, de la secante y de la falsa posición. Si la función es además derivable y la derivada se conoce, el método de Newton es muy utilizado. Este método es un método de iteración de punto fijo. La linealización es otra técnica para resolver ecuaciones no lineales.

Las ecuaciones algebraicas polinomiales poseen una gran cantidad de métodos numéricos para enumerar :














Bastantes problemas importantes pueden ser expresados en términos de descomposición espectral (el cálculo de los vectores y valores propios de una matriz) o de descomposición en valores singulares. Por ejemplo, el análisis de componentes principales utiliza la descomposición en vectores y valores propios.

Los problemas de optimización buscan el punto para el cual una función dada alcanza su máximo o mínimo. A menudo, el punto también satisface cierta restricción.

Ejemplos de, problemas de optimización son la programación lineal en que tanto la función objetivo como las restricciones son lineales. Un método famoso de programación lineal es el método simplex.

El método de los multiplicadores de Lagrange puede usarse para reducir los problemas de optimización con restricciones a problemas sin restricciones.

La integración numérica, también conocida como cuadratura numérica, busca calcular el valor de una integral definida. Métodos populares utilizan alguna de las fórmulas de Newton-Cotes (como la regla del rectángulo o la regla de Simpson) o de cuadratura gaussiana. Estos métodos se basan en una estrategia de «divide y vencerás», dividiendo el intervalo de integración en subintervalos y calculando la integral como la suma de las integrales en cada subintervalo, pudiéndose mejorar posteriormente el valor de la integral obtenido mediante el método de Romberg. Para el cálculo de integrales múltiples estos métodos requieren demasiado esfuerzo computacional, siendo útil el método de Monte Carlo.

El análisis numérico también puede calcular soluciones aproximadas de ecuaciones diferenciales, bien ecuaciones diferenciales ordinarias, bien ecuaciones en derivadas parciales. Los métodos utilizados suelen basarse en discretizar la ecuación correspondiente. Es útil ver la derivación numérica.

Para la resolución de ecuaciones diferenciales ordinarias los métodos más utilizados son el método de Euler y los métodos de Runge-Kutta.

Las ecuaciones en derivadas parciales se resuelven primero discretizando la ecuación, llevándola a un subespacio de dimensión finita. Esto puede hacerse mediante un método de los elementos finitos.

Los algoritmos de los métodos numéricos suelen implementarse por medio de computadoras. Estas poseen algunas propiedades que causan fallas al emplearlas para hallar la solución numérica de problemas matemáticos, entre las que se encuentran las siguientes:
Las fallas en los cálculos intermedios realizados por una computadora para arrojar un resultado final son, con frecuencia, desconocidos para los programadores y muy difíciles de detectar: la suma y el producto de números de punto flotante son operaciones conmutativas, pero no son asociativas y tampoco distributivas. Al no verificar estas dos propiedades de los números reales, el manejo de las operaciones realizadas con números de punto flotante resulta una tarea complicada. Por otra parte, el orden de las operaciones puede incidir en la precisión de los resultados devueltos por la máquina, pues dos expresiones equivalentes en un sentido algebraico pueden dar resultados distintos en el contexto de los números de máquina.

Afortunadamente, existen algunas técnicas para prevenir y atacar el error de redondeo. En se discuten algunas de las implicaciones de estas estrategias para las operaciones básicas de suma, resta, multiplicación y división. También en se discuten algunos estándares de punto flotante de la IEEE y las conexiones entre el punto flotante y el diseño de sistemas computacionales.

El mejoramiento en la precisión de los números de punto flotante sigue siendo motivo de estudio en nuestros días. En 2015, investigadores de la Universidad de Washington desarrollaron una herramienta computacional a la que llamaron "Herbie" y que «detecta automáticamente las transformaciones necesarias para que un programa mejore su precisión». "Herbie" evalúa el error de una expresión de punto flotante e identifica qué operaciones contribuyen de forma más significativa a la acumulación de errores, luego genera alternativas para realizar estas operaciones y hace un comparativo para finalmente determinar la expresión equivalente óptima (aquella que minimiza el error) para corregir el programa.

El interés en asegurar cierto nivel de precisión en los resultados numéricos provistos una computadora se debe a sus posibles repercusiones en la práctica. Por ejemplo, en el ámbito académico se han dado casos de artículos de investigación en los que el error de redondeo ha impedido que los resultados sean reproducibles y, en ocasiones, éste ha sido incluso motivo de rechazo para su publicación ( y). Este tipo de error también ha permeado la regulación legal financiera de algunos países y distorsionado índices del mercado bursátil.

La limitante en la representación de números reales mediante el punto flotante también tiene repercusiones en las gráficas generadas por medio de una computadora. Cuando un número es menor a lo que se conoce como el épsilon de máquina, la computadora es incapaz de representarlo. Esto puede provocar que las gráficas asociadas a valores numéricos menores al épsilon presenten falsos comportamientos y afectar la toma de decisiones basadas en ellas, con consecuencias insospechadas, por ejemplo, al realizar pronósticos, área en la que la precisión juega un papel crucial.

Existen otros tipos de error en el contexto de los métodos numéricos que merecen igual atención y cuidado. Errores de truncamiento y de conversión, entre otros, han dado origen a múltiples catástrofes: la falla del misil Patriot, la explosión del cohete Ariane 5, el hundimiento de la plataforma petrolera Sleipner son solo algunos ejemplos de ello. De ahí la importancia de reconocer estas fuentes de error para anticiparse a ellas y, en su caso, detectarlas y corregirlas.





</doc>
<doc id="3403" url="https://es.wikipedia.org/wiki?curid=3403" title="Lema de Zorn">
Lema de Zorn

El lema de Zorn, también llamado de Kuratowski-Zorn, es una proposición de la teoría de conjuntos que afirma lo siguiente:

Debe su nombre al matemático Max Zorn.

Los términos se definen como sigue. Supóngase que ("P", ≤) es un conjunto parcialmente ordenado. Un subconjunto "T" de "P" es "totalmente ordenado" si para cualquier "s", "t" ∈ "T" se tiene "s" ≤ "t" o "t" ≤ "s". Tal conjunto "T" tiene una "cota superior" "u" ∈ "P" si "t" ≤ "u" para cualquier "t" ∈ "T"; no se necesita que "u" sea miembro de "T". Un elemento "m" ∈ "P" es "maximal" si el único "x" ∈ "P" tal que "m" ≤ "x" es "m" mismo.

Al igual que el teorema del buen orden, el lema de Zorn es equivalente al axioma de elección, en el sentido de que cualquiera de ellos, junto con los axiomas de Zermelo-Fraenkel, basta para probar los otros. Aparece en las demostraciones de varios teoremas importantes, tales como el teorema de Hahn-Banach en análisis funcional, el teorema de que todo espacio vectorial tiene una base, el teorema de Tychonoff en topología, y los teoremas en álgebra abstracta que afirman que todo anillo con elemento unitario tiene un ideal maximal y que todo cuerpo tiene clausura algebraica.

Se considerará una aplicación usual del lema de Zorn: la prueba de que todo anillo "R" con unidad contiene un ideal maximal. Sea "P" el conjunto de todos los ideales bilaterales de "R" excepto "R" mismo, que no es vacío pues incluye al menos al ideal trivial {0} de "R". Este conjunto está parcialmente ordenado por inclusión.

Sea entonces "T" un subconjunto totalmente ordenado de "P"; se demostrará que "T" tiene cota superior, es decir, hay un ideal "I" ⊆ "R" que contiene a todos los miembros de "T", pero que no es igual a "R" (de lo contrario no estaría en "P"). Sea "I" la unión de todos los ideales en "T". Ésta es también un ideal: para cualquier "a", "b" ∈ "I", existen "J", "K" ∈ "T" tales que "a" ∈ "J" y "b" ∈ "K". Como "T" está totalmente ordenado, "K" ⊆ "J" o "J" ⊆ "K". En el primer caso, "b" ∈ "J" y por lo tanto, como "J" es un ideal, "a" + "b", "ar", "ra" ∈ "J" ⊆ "I" para cualquier "r" ∈ "R". En el segundo caso se razona de manera similar.

Para demostrar que "I" es distinto de "R", basta con observar que un ideal es igual a "R" si y sólo si incluye a 1. Es evidente que si es igual a "R" debe incluir a 1; recíprocamente, si incluye a 1 debe incluir a 1"r" = "r" para cualquier "r" ∈ "R", y por lo tanto debe contener a "R". Ahora bien, si "I" = "R" debería incluir a 1, con lo que habría un "J" ∈ "T" tal que 1 ∈ "J", y por lo tanto "J" = "R", contradiciendo la definición de "P", que no lo incluía.

Se demostró que "T" tiene una cota superior en "P". Aplicando el lema de Zorn, se tiene que "P" debe tener un elemento maximal, y por lo tanto, "R" tiene un ideal maximal.

Es de notar que la demostración depende del hecho de que "R" tenga un elemento unitario 1. De lo contrario, no sólo la prueba fallaría, el mismo enunciado del teorema sería falso.



</doc>
<doc id="3404" url="https://es.wikipedia.org/wiki?curid=3404" title="Medalla Fields">
Medalla Fields

La Medalla Internacional para Descubrimientos Sobresalientes en Matemáticas, más conocida por el nombre de Medalla Fields, es una distinción que concede desde 1936 la Unión Matemática Internacional de forma cuatrienal, siendo el máximo galardón que otorga la comunidad matemática internacional. Su nombre le fue dado en honor del matemático canadiense John Charles Fields y solo se concede a matemáticos con edades no superiores a los 40 años, con una retribución de 15.000 dólares canadienses.

Ante la inexistencia del Premio Nobel de Matemáticas se instauró este galardón a los mejores matemáticos, siendo otorgada a una o más personas. 

La medalla está chapada en oro y fue diseñada por Robert T. McKenzie en 1933. En el anverso tiene la cabeza del matemático griego Arquímedes y la inscripción ""Transire suum pectus mundoque potiri"" ("Ir más allá de uno mismo y dominar el mundo"). En el reverso figura una esfera inscrita en un cilindro y la inscripción ""Congregati ex toto orbe mathematici ob scripta insignia tribuere"" ("Los matemáticos de todo el mundo se reunieron para dar esta medalla por escritos excelentes").

! Año !! Medallistas


</doc>
<doc id="3408" url="https://es.wikipedia.org/wiki?curid=3408" title="Geometría analítica">
Geometría analítica

La geometría analítica es una rama de las matemáticas que estudia con profundidad las figuras, sus distancias, sus áreas, puntos de intersección, ángulos de inclinación, puntos de división, volúmenes, etc. Es un estudio más profundo para saber con detalle todos los datos que tienen las figuras geométricas.

Estudia las figuras geométricas mediante técnicas básicas del análisis matemático y del álgebra en un determinado sistema de coordenadas. Su desarrollo histórico comienza con la geometría cartesiana, continúa con la aparición de la geometría diferencial de Carl Friedrich Gauss y más tarde con el desarrollo de la geometría algebraica.

Actualmente, la geometría analítica tiene múltiples aplicaciones, más allá de las matemáticas y la ingeniería, pues forma parte ahora del trabajo de administradores para la planeación de estrategias y logística en la toma de decisiones.

Las dos cuestiones fundamentales de la geometría analítica son:


La geometría analítica representa las figuras geométricas mediante la ecuación formula_1, donde formula_2 es una función u otro tipo. Así, las rectas se expresan mediante la ecuación general formula_3, las circunferencias y el resto de cónicas como ecuaciones polinómicas de grado 2 (la circunferencia, formula_4; la hipérbola, formula_5).

En un sistema de coordenadas cartesianas, un punto del plano queda determinado por dos números, llamados "abscisa" y "ordenada" del punto. Mediante ese procedimiento a todo punto del plano corresponden siempre dos números reales ordenados (abscisa y ordenada), y recíprocamente, a un par ordenado de números corresponde un único punto del plano. Consecuentemente el sistema cartesiano establece una correspondencia biunívoca entre un concepto geométrico como es el de los puntos del plano y un concepto algebraico como son los pares ordenados de números. Esta correspondencia constituye el fundamento de la geometría analítica.

Con la geometría analítica se puede determinar figuras geométricas planas por medio de ecuaciones e inecuaciones con dos incógnitas. Éste es un método alternativo de resolución de problemas, o cuando menos nos proporciona un nuevo punto de vista con el cual poder atacar el problema.

En un plano (v.g. papel milimetrado) se traza dos rectas orientadas perpendiculares entre sí (ejes) —que por convenio se trazan de manera que una de ellas sea horizontal y la otra vertical—, y cada punto del plano queda unívocamente determinado por las distancias de dicho punto a cada uno de los ejes, siempre y cuando se dé también un criterio para determinar sobre qué semiplano determinado por cada una de las rectas hay que tomar esa distancia, criterio que viene dado por un signo. Ese par de números, las coordenadas, quedará representado por un par ordenado formula_6, siendo formula_7 la distancia a uno de los ejes (por convenio será la distancia al eje vertical) e formula_8 la distancia al otro eje (al horizontal).

En la coordenada formula_7, el signo positivo (que suele omitirse) significa que la distancia se toma hacia la derecha sobre el eje horizontal (eje de las abscisas), y el signo negativo (nunca se omite) indica que la distancia se toma hacia la izquierda. Para la coordenada formula_8, el signo positivo (también se omite) indica que la distancia se toma hacia arriba sobre el eje vertical (eje de ordenadas), tomándose hacia abajo si el signo es negativo (en ningún caso se omiten los signos negativos).

A la coordenada formula_7 se la suele denominar "abscisa" del punto, mientras que a la formula_8 se la denomina "ordenada" del punto.

Los puntos del eje de abscisas tienen por lo tanto ordenada igual a formula_13, así que serán de la forma formula_14, mientras que los del eje de ordenadas tendrán abscisa igual a formula_13, por lo que serán de la forma formula_16.

El punto donde ambos ejes se cruzan tendrá por lo tanto distancia formula_13 a cada uno de los ejes, luego su abscisa será formula_13 y su ordenada también será formula_13. A este punto —el formula_20— se le denomina origen de coordenadas.

Se consideran dos rectas orientadas, (ejes) , perpendiculares entre sí, "x" e "y", con un origen común, el punto "O" de intersección de ambas rectas.

Teniendo un punto "a", al cual se desea determinar las coordenadas, se procede de la siguiente forma:

Por el punto "P" se trazan rectas perpendiculares a los ejes, éstas determinan en la intersección con los mismos dos puntos, "P"<nowiki>'</nowiki> (el punto ubicado sobre el eje "x") y el punto "P"<nowiki>"</nowiki> ( el punto ubicado sobre el eje "y").

Dichos puntos son las proyecciones ortogonales sobre los ejes "x" e "y" del punto "P".

A los Puntos "P"<nowiki>'</nowiki> y "P"<nowiki>"</nowiki> le corresponden por número la distancia desde ellos al origen, teniendo en cuenta que si el punto "P"<nowiki>'</nowiki> se encuentra a la izquierda de "O", dicho número será negativo, y si el punto "P"<nowiki>"</nowiki> se encuentra por debajo del punto "O", dicho número será negativo.

Los números relacionados con "P"<nowiki>'</nowiki> y "P"<nowiki>"</nowiki>, en ese orden son los valores de las coordenadas del punto "P".

Ejemplo 1: "P"<nowiki>'</nowiki> se encuentra a la derecha de "O" una distancia igual a 2 unidades. "P"<nowiki>"</nowiki> se encuentra hacia arriba de "O", una distancia igual a 3 unidades. Por lo que las coordenadas de "P" son (2 , 3).

Ejemplo 2: "P"<nowiki>'</nowiki> se encuentra a la derecha de "O" una distancia igual a 4 unidades. "P"<nowiki>"</nowiki> se encuentra hacia abajo de "O", una distancia igual a 5 unidades. Por lo que las coordenadas de "P" son (4 , -5).

Ejemplo 3: "P"<nowiki>'</nowiki> se encuentra a la izquierda de "O" una distancia igual a 3 unidades. "P"<nowiki>"</nowiki> se encuentra hacia abajo de "O", una distancia igual a 2 unidades. Por lo que las coordenadas de "P" son (-3 , -2).

Ejemplo 4: "P"<nowiki>'</nowiki> se encuentra a la izquierda de "O" una distancia igual a 6 unidades. "P"<nowiki>"</nowiki> se encuentra hacia arriba de "O", una distancia igual a 4 unidades. Por lo que las coordenadas de "P" son (-6 , 4).

Una recta es el lugar geométrico de todos los puntos en el plano tales que, tomados dos cualesquiera de ellos, el cálculo de la pendiente resulta siempre igual a una constante.

La ecuación general de la recta es de la forma:

formula_21

cuya pendiente es m = -"A"/"B" y cuya ordenada al origen es "b" = -"C"/"B".

Una recta en el plano se representa con la función lineal de la forma:

Como expresión general, ésta es conocida con el nombre de ecuación pendiente-ordenada al origen y podemos distinguir dos casos particulares. Si una recta no corta a uno de los ejes, será porque es paralela a él. Como los dos ejes son perpendiculares, si no corta a uno de ellos forzosamente ha de cortar al otro (siempre y cuando la función sea continua para todos los reales). Tenemos pues tres casos:



El resultado de la intersección de la superficie de un cono, con un plano, da lugar a lo que se denominan secciones cónicas, que son: la parábola, la elipse (la circunferencia es un caso particular de elipse) y la hipérbola.


Una parábola (figura A) cuyo eje de simetría sea paralelo al eje de abcisas se expresa mediante la ecuación:


Una elipse (figura B) centrada en los ejes, con longitudes de semieje "a" y "b" viene dada por la expresión:



La hipérbola (Figura C) tiene por expresión:

En coordenadas cartesianas, las cónicas se expresan en forma algebraica mediante ecuaciones cuadráticas de dos variables (x,y) de la forma:

en la que, en función de los valores de los parámetros, se tendrá:

Los razonamientos sobre la construcción de los ejes coordenados son igualmente válidos para un punto en el espacio y una terna ordenada de números, sin más que introducir una tercera recta perpendicular a los ejes "X" e "Y": el eje "Z".

Sin embargo no hay análogo al importantísimo concepto de pendiente de una recta. Una única ecuación lineal del tipo:

Representa en el espacio un plano. Si se pretende representar mediante ecuaciones una recta en el espacio tridimensional necesitaremos especificar, no una, sino dos ecuaciones lineales como las anteriores. De hecho toda recta se puede escribir como intersección de dos planos. Así una recta en el espacio podría quedar representada como:

Es importante notar que la representación anterior no es única, ya que una misma recta puede expresarse como la intersección de diferentes pares de planos. Por ejemplo los dos pares de ecuaciones:

Desde el punto de vista de la clasificación de Klein de las geometrías (el Programa de Erlangen), la geometría analítica no es una geometría propiamente dicha.

Desde el punto de vista didáctico, la geometría analítica resulta un puente indispensable entre la geometría euclidiana y otras ramas de la matemática y de la propia geometría, como son el propio análisis matemático, el álgebra lineal, la geometría afín, la geometría diferencial o la geometría algebraica.

En física se utiliza los sistemas de coordenadas para la representación de movimientos y vectores entre otras magnitudes.

El nacimiento de la geometría analítica se atribuye a Descartes, por el apéndice "La Géométrie" incluido en su "Discurso del método", publicado en 1637, si bien se sabe que Pierre de Fermat conocía y utilizaba el método antes de su publicación por Descartes. Sin embargo las ideas de Descartes eran algo oscuras y difíciles de entender y se atribuye su ampliación, desarrollo y divulgación en el mundo matemático a Frans van Schooten y colaboradores.
Sin embargo, existe una cierta controversia sobre la verdadera paternidad de este método. Omar Khayyam ya en el siglo XI, utilizó un método muy parecido para determinar ciertas intersecciones entre curvas, aunque es imposible que ni Fermat ni Descartes tuvieran acceso a su obra.

El nombre de "geometría analítica" corrió parejo al de "geometría cartesiana", y ambos son indistinguibles. Hoy en día, paradójicamente, se prefiere denominar "geometría cartesiana" al apéndice del "Discurso del método", mientras que se entiende que "geometría analítica" comprende no sólo a la geometría cartesiana (en el sentido que acabamos de citar, es decir, al texto apéndice del "Discurso del método"), sino también todo el desarrollo posterior de la geometría que se base en la construcción de ejes coordenados y la descripción de las figuras mediante funciones —algebraicas o no— hasta la aparición de la geometría diferencial de Gauss (decimos "paradójicamente" porque se usa precisamente el término "geometría cartesiana" para aquello que el propio Descartes bautizó como "geometría analítica")... El problema es que durante ese periodo no existe una diferencia clara entre geometría analítica y análisis matemático —esta falta de diferencia se debe precisamente a la identificación hecha en la época entre los conceptos de función y curva—, por lo que resulta a veces muy difícil intentar determinar si el estudio que se está realizando corresponde a una u otra rama.

La geometría diferencial de curvas sí que permite un estudio mediante un sistema de coordenadas, ya sea en el plano o en el espacio tridimensional. Pero en el estudio de las superficies, en general, aparecen serios obstáculos. Gauss salva dichos obstáculos creando la geometría diferencial, y marcando con ello el fin de la geometría analítica como disciplina. Es con el desarrollo de la geometría algebraica cuando se puede certificar totalmente la superación de la geometría analítica.

Es de puntualizar que la denominación de "analítica" dada a esta forma de estudiar la geometría provocó que la anterior manera de estudiarla (es decir, la manera axiomático-deductiva, sin la intervención de coordenadas) se terminara denominando, por oposición, geometría sintética, debido a la dualidad análisis-síntesis.




</doc>
<doc id="3411" url="https://es.wikipedia.org/wiki?curid=3411" title="Mecánica de fluidos">
Mecánica de fluidos

La mecánica de fluidos es la rama de la física comprendida dentro de la mecánica de medios continuos que estudia el movimiento de los fluidos, así como las fuerzas que lo provocan. La característica fundamental que define a los fluidos es su incapacidad para resistir esfuerzos cortantes (lo que provoca que carezcan de forma definida). También estudia las interacciones entre el fluido y el contorno que lo limita.

Nótese que los gases pueden comprimirse, mientras que los líquidos carecen de esta característica (la compresibilidad de los líquidos a altas presiones no es exactamente cero pero es cercana a cero) aunque toman la forma del recipiente que los contiene. La compresibilidad de un fluido depende del tipo de problema, en algunas aplicaciones aerodinámicas, aun cuando el fluido es aire, puede asumirse que el cambio de volumen del aire es cero. 

Como en todas las ramas de la ciencia, en la mecánica de fluidos parte de la hipótesis en función de las cuales se desarrollan todos los conceptos. En particular, en la mecánica de fluidos se asume que los fluidos verifican las siguientes leyes:

La hipótesis del medio continuo es la hipótesis fundamental de la mecánica de fluidos y en general de toda la mecánica de medios continuos. En esta hipótesis se considera que el fluido es continuo a lo largo del espacio que ocupa, ignorando por tanto su estructura molecular y las discontinuidades asociadas a esta. Con esta hipótesis se puede considerar que las propiedades del fluido (densidad, temperatura, etc.) son funciones continuas.

La forma de determinar la validez de esta hipótesis consiste en comparar el camino libre medio de las moléculas con la longitud característica del sistema físico. Al cociente entre estas longitudes se le denomina número de Knudsen. Cuando este número adimensional es mucho menor a la unidad, el fluido en cuestión puede considerarse un medio continuo. En el caso contrario los efectos debidos a la naturaleza molecular de la materia no pueden ser despreciados y debe utilizarse la mecánica estadística para predecir el comportamiento de la materia. Ejemplos de situaciones donde la hipótesis del medio continuo no es válida pueden encontrarse en el estudio de los plasmas.

Este concepto está muy ligado al del medio continuo y es sumamente importante en la mecánica de fluidos. Se llama partícula fluida a la masa elemental de fluido que en un instante determinado se encuentra en un punto del espacio. Dicha masa elemental ha de ser lo suficientemente grande como para contener un gran número de moléculas, y lo suficientemente pequeña como para poder considerar que en su interior no hay variaciones de las propiedades macroscópicas del fluido, de modo que en cada partícula fluida podamos asignar un valor a estas propiedades.
Es importante tener en cuenta que la partícula fluida se mueve con la velocidad macroscópica del fluido, de modo que está siempre formada por las mismas moléculas. Así pues un determinado punto del espacio en distintos instantes de tiempo estará ocupado por distintas partículas fluidas.

Al describir el movimiento de un fluido, existen dos puntos de vista.
Una primera forma de hacerlo es seguir a cada partícula fluida en su movimiento, de manera que buscaremos unas funciones que nos den la posición, así como las propiedades de la partícula fluida en cada instante. Esta es la descripción lagrangiana.

Una segunda forma es asignar a cada punto del espacio y en cada instante, un valor para las propiedades o magnitudes fluidas sin importar que en ese instante, la partícula fluida ocupa ese volumen diferencial. Esta es la descripción euleriana, que no está ligada a las partículas fluidas sino a los puntos del espacio ocupados por el fluido. En esta descripción el valor de una propiedad en un punto y en un instante determinado es el de la partícula fluida que ocupa dicho punto en ese instante.

La descripción euleriana es la más común, puesto que en la mayoría de casos y aplicaciones es más útil. Usaremos dicha descripción para la obtención de las ecuaciones generales de la mecánica de fluidos.

Las ecuaciones que rigen toda la mecánica de fluidos se obtienen por la aplicación de los principios de conservación de la mecánica y la termodinámica a un volumen fluido. Para generalizarlas usaremos el teorema del transporte de Reynolds y el teorema de la divergencia (o teorema de Gauss) para obtener las ecuaciones en una forma más útil para la formulación euleriana.

Las tres ecuaciones fundamentales son la ecuación de continuidad, la ecuación de la cantidad de movimiento, y la ecuación de la conservación de la energía. Estas ecuaciones pueden darse en su formulación integral o en su forma diferencial, dependiendo del problema. A este conjunto de ecuaciones dadas en su forma diferencial también se le denomina ecuaciones de Navier-Stokes (las ecuaciones de Euler son un caso particular de la ecuaciones de Navier-Stokes para fluidos sin viscosidad).

No existe una solución general a dicho conjunto de ecuaciones debido a su complejidad, por lo que para cada problema concreto de la mecánica de fluidos se estudian estas ecuaciones buscando simplificaciones que faciliten la resolución del problema. En algunos casos no es posible obtener una solución analítica, por lo que hemos de recurrir a soluciones numéricas generadas por ordenador. A esta rama de la mecánica de fluidos se la denomina mecánica de fluidos computacional. Las ecuaciones son las siguientes:




Campos de estudio:

Ecuaciones matemáticas que describen el comportamiento de los fluidos:


Tipos de fluidos:

Tipos de flujo:

Propiedades de los fluidos:


Números adimensionales:



</doc>
<doc id="3412" url="https://es.wikipedia.org/wiki?curid=3412" title="Ionización">
Ionización

La ionización es el fenómeno químico o físico mediante el cual se producen iones, estos son átomos o moléculas cargadas eléctricamente debido al exceso o falta de electrones respecto a un átomo o molécula neutra. A la especie química con más electrones que el átomo o molécula neutros se le llama anión, y posee una carga neta negativa, y a la que tiene menos electrones catión, teniendo una carga neta positiva. Hay varias maneras por las que se pueden formar iones de átomos o moléculas.

En ciertas reacciones químicas la ionización ocurre por transferencia de electrones; por ejemplo, el cloro reacciona con el sodio para formar cloruro de sodio, que consiste en iones de sodio (Na) e iones de cloruro (Cl). La condición para que se formen iones en reacciones químicas suele ser una fuerte diferencia de electronegatividad entre los elementos que reaccionan o por efectos de resonancia que estabilizan la carga. Además la ionización es favorecida por medios polares que consiguen estabilizar los iones. Así el pentacloruro de fósforo (PCl) tiene forma molecular no iónica en medios poco polares como el tolueno y disocia en iones en disolventes polares como el nitrobenceno (ONCH).

La presencia de ácidos de Lewis como en los haluros de aluminio o el trifluoruro de boro (BF) también puede favorecer la ionización debido a la formación de complejos estables como el [AlCl]. Así la adición de tricloruro de aluminio a una disolución del cloruro de tritl (Cl-CPh), un compuesto orgánico, resulta en la formación del tetracloroaluminato de tritilio ([AlCl][CPh]), una sustancia iónica y la adición de cloruro de aluminio a tetraclorociclopropeno (CCl, un líquido orgánico volátil) proporciona el tetracloroaluminato de triclorociclopropenilio ([AlCl][CCl]) como sólido incoloro.
A este proceso se le suman las sumas de los electrones compuestos por menos cargas negativas al núcleo del primer átomo consecutivo.

En el ambiente (aire, agua, suelo, etc.) existen algunos microorganismos o microbios que podrían ser dañinos para la salud humana, y sobreviven aprovechando los nutrientes a su alcance para desarrollarse o permanecer en ellos.

La esterilización es la práctica que tiene por fin destruir o eliminar todos los microbios. El efecto bactericida de las radiaciones es conocido desde tiempos antiguos, así por ejemplo se sabe que la radiación solar, o más precisamente las radiaciones ultravioletas, son agentes naturales de esterilización. Sin embargo, las radiaciones electromagnéticas infrarrojas son las menos eficaces debido a su gran longitud de onda. La esterilización mediante rayos gamma es una tecnología que ha sido identificada como una alternativa segura para reducir la carga microbiana en alimentos y en insumos que entran en contacto directo con ellos, reduciendo el riesgo de contagio de enfermedades transmitidas por alimentos, en la producción, procesamiento, manipulación y preparación de estos, todo lo cual aumenta la calidad y competitividad de los productos otorgándoles un mayor valor añadido.

La energía ionizante se puede originar a partir de tres fuentes distintas: rayos gamma, una máquina generadora de electrones y rayos X. La fuente más común de los rayos gamma es el cobalto-60.

Los rayos gamma se componen de ondas electromagnéticas de longitud de onda muy cortas que penetran en los envases y productos expuestos a dicha fuente, ocasionando pequeños cambios estructurales en la cadena de ADN de las bacterias o microorganismos, causándoles la muerte o dejándolas inviables o estériles, sin capacidad de replicarse. La tecnología permite el tratamiento de los productos en su envase final.

La energía ionizante es factible de ser aplicada a una gran variedad de productos, con el fin de esterilización o reducción de carga microbiana, eliminando patógenos que pueden ser dañinos para la salud. Entre los productos tratados se encuentran: Alimentos, cosméticos, productos médicos, hierbas medicinales, productos de laboratorio y farmacéutico, alimento animal y embalajes.

La tecnología existe en forma comercial desde la década de 1950 y está autorizada su uso en más de treinta países, para más de cincuenta productos alimentarios. Cuenta con la aprobación de importantes organismos internacionales como: la WHO, FAO y la IAEA. También cuenta con la aprobación de la FDA, que plasma su normativa en el código 21 CFR 179.26. Estas entidades pueden recomendar, regular o legislar sobre la correcta aplicación de la tecnología, estableciendo los parámetros adecuados de operación y las dosis máximas aplicables a cada tipo de producto.


Los rayos gamma no dejan ningún tipo de residuos y es efectivo contra organismos patógenos y permite la obtención de alimentos inocuos y sanos. Así lo aseguran quienes han apostado por esta alternativa, cuyo uso ha venido ampliándose en los últimos años. Diversas investigaciones han demostrado que no se producen pérdidas significativas de nutrientes en los alimentos.

Otra aplicación importante aún en fase de I+D es la de la detección de explosivos y sustancias peligrosas o prohibidas mediante la ionización por electrospray, conjuntamente con análisis de movilidad (DMA) y espectrometría de masas (MS / MS). En España, una empresa tecnológica, "SEDET" (Sociedad Europea de Detección), está desarrollando un equipo con estas características útil para la detección de explosivos, drogas o cualquier tipo de sustancias peligrosas o prohibidas que utilizaría la ionización por electrospray. El equipo se denomina "Air Cargo explosivo Screener (ACES)" y está dirigido fundamentalmente a contenedores de carga aérea o puertos. 

Sedet es una Joint Venture creada por SEADM, Morpho y el centro tecnológico CARTIF con el fin de desarrollar esta nueva generación de sistemas de detección de las trazas que dejan las sustancias explosivas.

Un electro-spray (ES) ionizador es un dispositivo que fue propuesto originalmente por Fenn. La mezcla de este aerosol cargado iónicamente con una muestra de aire que pueda contener vapores de explosivos (o partículas) conduce a la ionización de las moléculas de explosivos, ya sea por el contacto con las gotas o por intercambio de carga con los iones producidos por la evaporación de gotas ES. Esto conduce a la formación de iones moleculares que pueden ser analizados en la DMA y la MS. La ionización ES se utiliza con mayor frecuencia para las grandes especies de peso molecular biológicos, pero también es ideal para trazar la detección de explosivos de baja volatilidad por las razones siguientes:




</doc>
<doc id="3418" url="https://es.wikipedia.org/wiki?curid=3418" title="Dharma">
Dharma

Dharma es una palabra sánscrita que significa ‘religión’, ‘ley religiosa’ o ‘conducta piadosa correcta’.
Con ligeras diferencias conceptuales, se utiliza en casi todas las doctrinas y religiones de origen indio (las religiones dhármicas), como el budismo, el hinduismo, el jainismo y el sijismo.
No existe una única palabra que sirva de traducción para "dharma" en otros idiomas. 


El sustantivo "dharma" se basa en la palabra "dhara", que proviene del sánscrito "dhri", que significa ‘poseer’, ‘conservar’, ‘mantener’.

La palabra proviene de una raíz indoirania "dhar" ‘ajustar, soportar, sostener’, conectada con
Se ha sugerido, aunque permanece incierta, la identidad etimológica entre "dharma" y el latín "firmus" (de donde proviene el español «firme»).

El antónimo de "dharma" es "adharma" (‘irreligión’).

"Dharma" tiene varios significados, como
‘religión’,
‘enseñanza’,
‘ley natural’,
‘naturaleza’,
‘conducta correcta’,
‘virtud’,
‘aquello que sostiene o mantiene unido’,
‘verdad’,
‘algo establecido o firme’, figurativamente: ‘sustentador, apoyo’ (en el caso de deidades) y en sentido más abstracto, es similar al término griego "nomos", ‘norma fija, estatuto, ley’.

Todo ser humano tiene un dharma, un deber supremo cuando ejecuta sus actos, y es el utilizar la acción para realizarse interiormente. La misma naturaleza tiene también su dharma, su deber: el del Sol es iluminar y dar vida; el de los animales, entre otras funciones, es el de servir de alimento a sus depredadores, y estos a los siguientes en la cadena alimenticia, con el fin de preservar la vida, etcétera. El dharma más alto del ser humano es el conocimiento de sí mismo.

La palabra "dharma" aparece ya en el "Átharva vedá" (I milenio a. C.) y en el sánscrito clásico.
En idioma pāli toma la forma "dhamma" (como se utiliza muchas veces en el budismo).
El erudito inglés del s. XIX, Monier-Williams propone como traducciones (en el ámbito espiritual y religioso) ‘virtud, moralidad, religión, mérito religioso’. Pero también significa "propósito" o "intención"; ergo: tener un buen dharma es tener un buen propósito, una buena intención de vida. De ahí su parentesco con "virtud".

La palabra "dharma" ya estaba en uso en la religión védica histórica y su significado y alcance conceptual evolucionó a lo largo de varios milenios.

En el hinduismo, el "dharma" significa las conductas que se considera que están de acuerdo con el "rita" (es decir con el orden que hace posible la vida y el universo) e incluye deberes, derechos, leyes, conducta, virtudes y un recto modo de vivir. 

En el hinduismo, el "dharma" es la ley universal de la naturaleza, ley que se encuentra en cada individuo lo mismo que en todo el universo.
A nivel cósmico esta ley se concibe manifestada por movimientos regulares y cíclicos.
Por este motivo se simboliza al "dharma" como una rueda "(dharma-chakra: "☸")" que torna o gira sobre sí misma.
Este símbolo es el que se encuentra en la bandera de la India.

A nivel del individuo humano, el "dharma" adquiere una nueva acepción: la del deber ético y religioso que cada cual tiene asignado según su determinada situación de nacimiento.

Existen varios textos acerca del tema del deber, llamados genéricamente "Dharmasastra", entre los que se incluyen las "Leyes de Manu".

Los hinduistas no llaman «hinduismo» a su religión, sino "sanatana dharma", que se traduce como ‘religión eterna’.

En la epopeya india del "Majábharata" también aparece la figura de Dharma como un dios (Iama, el superintendente de la muerte), que encarna como un hombre, Iudistira, que es un emperador del "Majabhárata" (texto épico-religioso del siglo III a. C.). Cuando se retiró, por causa de edad, vivió en las ciudades indias para hacer meditación y encontrar el camino de la superación del ciclo de las reencarnaciones, algo que era habitual antiguamente. No murió, pues fue llevado en cuerpo y alma al Cielo de Indra, el jefe de todos los dioses, donde todavía seguiría viviendo.

En el budismo, "dharma" significa ‘ley cósmica y orden’, aunque también se aplica a las enseñanzas de Buda. En la doctrina budista, el "dharma" es también el término usado para ‘fenómenos’. 

Dentro del budismo la noción del "dharma" (entendido como doctrina) se dividió para su mejor comprensión en las llamadas "Tipitaka":

Estos tres conjuntos de escritos conforman el Canon Pali o también tal cual se ha dicho llamado "Tipitaka". El "dharma" es uno de las llamadas tres joyas "(mani)" o tesoros del budismo junto con Buda Gautama y Sangha.

Es por esto que la mención de la palabra "dharma" es frecuente entre los budistas, ya que constituye uno de los principales elementos de la llamada «fórmula del triple refugio»:

En el jainismo el "dharma" se refiere a las enseñanzas de los yinas y el cuerpo de la doctrina relativa a la purificación y transformación moral de los seres humanos. 

En el jainismo el "dharma" se entiende principalmente como ‘movimiento’ de la "dravia" o substancia universal. En tal sentido dentro del jainismo el "dharma" es una de las siete categorías de la "dravia", siendo las otras
La rueda del "dharma" que se encuentra en la bandera de la India es conocida oficialmente como "chakra" de Ashoka, aunque el emperador Ashoka fue un destacado budista, la rueda del "dharma" que se usó en sus monumentos remitía en su simbolismo principalmente a la acepción hinduista (la más antigua) del "dharma".

En el sijismo, la palabra "dharm" significa ‘el camino de la justicia’.




Dharma Capital es una compañía que identificó su propósito con el significado del Dharma y agregó la palabra capital por el capital humano que desea se integre.


</doc>
<doc id="3419" url="https://es.wikipedia.org/wiki?curid=3419" title="Karma">
Karma

Según varias religiones dhármicas, el karma () es una energía trascendente (invisible e inmensurable) que se genera a partir de los actos de las personas. También conocido como un espíritu de justicia o equilibrio.

Es una creencia central en la doctrina del hinduismo, el budismo, el jainismo, el ayyavazhi y el espiritismo.

Aunque estas doctrinas expresan diferencias en el significado mismo de la palabra karma, tienen una base común de interpretación. Generalmente, el karma se interpreta como una «ley» cósmica de retribución, o de causa y efecto. Se refiere al concepto de «acción» entendido como aquello que causa el comienzo del ciclo de causa y efecto. Según el karma, cada una de las sucesivas reencarnaciones quedaría condicionada por los actos realizados en vidas anteriores.

El karma está en contraposición con las doctrinas abrahámicas (judaísmo, cristianismo e islamismo); lo más parecido en el cristianismo es el concepto teológico de retribución. El karma explica los dramas humanos como la reacción a las acciones buenas o malas realizadas en el pasado más o menos inmediato. Según el hinduismo, la reacción correspondiente es generada por el dios Iama; en cambio, en el budismo y el jainismo ―donde no existe ningún dios controlador― esa reacción es generada como una ley de la naturaleza (como la gravedad, que no tiene ningún dios que la controle).

En las creencias indias, los efectos del karma de todos los hechos son vistos como experiencias activamente cambiantes en el pasado, presente y futuro.

Según esta doctrina, las personas tienen la libertad para elegir entre hacer el bien y el mal, pero tienen que asumir las consecuencias derivadas.

Proviene de la raíz "kri:" ‘hacer’ (según el "Unadi-sutra" 4.144).

Es errónea la etimología "karaṇa:" ‘causa’ y "manas:" ‘mente’, en boga en Occidente. Se hizo originar a partir de la palabra inexistente "karmaṇ", inventada a partir de la palabra sánscrita "karman" (declinación de "karma"). La letra "n" final de "karman" (que no es una ṇ) indica que se trata de un sustantivo neutro.
Para analizar las raíces de la palabra "karma" se debe utilizar solo el término básico "karma" (no su declinación "karman" ni el inventado "kar-maṇ").

En pali se dice "kamma" y en birmano "kan".

Tanto para el hinduismo como para el budismo, el karma no implica solamente las acciones físicas, sino habría tres factores que generan reacciones como:
Tanto el budismo como el hinduismo creen que mediante la práctica de esas respectivas religiones, las personas pueden escapar del condicionamiento del karma y así liberarse de los cuatro sufrimientos (que se enumeran igual en ambas religiones):


El concepto "karma" no solo tiene una dimensión moral sino también una dimensión existencial. En este sentido, el karma se produce cuando el sujeto que ejecuta una acción no se reconoce como la causa de los efectos que esa misma acción produce, sobre todo cuando dichos efectos le son adversos. Este no reconocimiento también ocasiona la exacerbación de los efectos nocivos, porque los movimientos que hace el sujeto para solucionar el problema solo lo agravan. Ejemplo: el caso de un sujeto que al no saber nadar, y por el instinto de querer sobrevivir, en su desesperación y con sus movimientos bruscos empeora su situación. No se da cuenta que lo que le hace hundirse cada vez más, es su propia reacción. 

Estas ideas del karma y del reconocimiento no son ajenas a la filosofía Occidental. Por ejemplo, el filósofo alemán Karl Marx en su obra "Manuscritos Económico – Filosóficos", expone la “"teoría de la alienación"” (que en este caso sería otra manera de nombrar el karma por el no-reconocimiento). Dice que en la economía capitalista el obrero no se reconoce como creador de los objetos que él mismo fabrica: "El obrero se ha convertido en una mera mercancía (…) la demanda -de la que depende la vida del obrero- depende a su vez del interés de los ricos y capitalistas". Esta situación alienante ocasiona la pérdida de autonomía  del sujeto quien no es dueño de su propia actividad. 

Usualmente se asocia el karma con la reencarnación, ya que una sola vida humana no alcanzaría para experimentar todos los efectos de las acciones realizadas («cobrar» todo el bien que se ha hecho o «pagar» todo el mal que se ha realizado en vida).

En religiones teístas (como el hinduismo o el cristianismo) existe el concepto de alma.
Bajo el punto de vista del karma, la reencarnación sería la nueva encarnación del alma en un nuevo cuerpo físico, en tiempo futuro, en el útero de una nueva madre.

En el hinduismo, el concepto de alma individual, o "yiva-atman", es una chispa del Espíritu Divino (Atman) que todos tenemos, a diferencia del budismo, en que el objeto de la reencarnación corresponde a un registro de la mente.

Se entiende que existe un estado de pureza y sabiduría original, latente pero dormido, en la vida de todos los seres humanos. En el concepto oriental, el ser humano olvida su naturaleza superior y se identifica erróneamente con el cuerpo en cada nuevo nacimiento.

La reencarnación ―o transmigración de las almas― es el paso hacia la siguiente existencia física.
El karma determina las condiciones bajo las cuales el individuo vuelve a la vida.
Sin embargo, el estado de pureza y sabiduría latente sigue intacto y desarrollándose lenta y progresivamente vida tras vida, en una especie de evolución espiritual del alma/cuerpo astral a través de numerosos cuerpos físicos y personajes, un largo viaje desde nuestra naturaleza inferior o animal hasta nuestra naturaleza superior o divina.

El gurú Paramahansa Yogananda creía que todos los seres realizados (entre quienes contaba a Jesucristo o Buda Gautama) podrían recordar sus vidas. Afirmaba también que él podía recordar a voluntad sus vidas anteriores. En cambio, al ser humano común no le ayudaría recordarlas, debido al peso emocional que le acarrearía. Por lo tanto, el recuerdo de esas vidas está oculto, pero guardado en la «memoria del alma» o en la mente hasta que la persona esté preparada para recordarlas sin daño emocional.

La mayoría de las escuelas budistas enseñan que mediante la meditación se puede llegar a un estado de superconciencia llamado nirvana ("samadhi" en yoga), que es el fin de la existencia condicionada por el karma. Por lo tanto, la práctica budista intenta que las personas alcancen un estado de paz y felicidad absoluta en esta misma vida. Algunas corrientes minoritarias, como la del budismo nichiren, entienden que no es posible escapar al ciclo de la reencarnación.

El karma y la reencarnación son la manera en que los orientales trataban de explicar el fenómeno de los niños prodigio, que serían resultado de muchas vidas de práctica en ese don particular.

Esos niños serían almas que de alguna manera podían aprovechar el talento aprendido en vidas anteriores, que estaría almacenado en una inaccesible memoria astral o registros akáshicos (listas de actividades que quedan escritas en el éter, el cual es una sustancia mítica invisible, más sutil que el aire).

En el "Rig-veda" (el texto más antiguo de la India, de mediados del II milenio a. C.) no se menciona ninguna doctrina de retribución mágica, ni tampoco la reencarnación.

En el "Rig-veda" se menciona unas 40 veces
la palabra «kárman» (cuyo nominativo es «kárma») pero solo en su acepción como ‘trabajo’ o ‘acción’, frecuentemente utiliza en el contexto de los rituales srauta (los ritos típicos de la cultura védica: sacrificios de fuego en los que se mataban animales y se bebía la droga soma).

Un himno del "Rig-veda" sugiere la creencia en la recompensa por ser dadivoso:

En el verso 1.7.1.5 del "Satapatha-bráhmana", el sacrificio es declarado como el «más grande» de los karmas.
El verso 10.1.4.1 asocia el potencial de convertirse en inmortal "(amara)" con el "karma" del sacrificio agni-chaiana.

Una cierta idea de la existencia de una «ética de la causalidad» se expresa en el "Upanishad" más antiguo:

Algunos autores
afirman que la doctrina del samsara (la transmigración de las almas) y del karma podría ser no védica, y las ideas pueden haberse desarrollado en las tradiciones shramana que en el I milenio a. C. precedieron al budismo y al jainismo.

Otros autores
afirman que algunas de las ideas de la hipótesis emergente del karma fluyeron desde los pensadores védicos a los pensadores budistas y jainistas. Las influencias mutuas entre las tradiciones no están claras. Probablemente estas ideas se desarrollaron cooperativamente a lo largo de un par de siglos (entre el VI y el V a. C.).
Muchos debates filosóficos que rodean el concepto son compartidos por las tradiciones hinduista, jainista y budista, y los primeros desarrollos en cada tradición incorporaron diferentes ideas novedosas.
Por ejemplo, los budistas permitieron la transferencia del karma de una persona a otra, y los hinduistas afirmaban la efectividad de los ritos sraddha (en los que gracias a un ritual, se podían reducir los efectos de los pecados de los antepasados), pero tuvieron dificultades para explicar por qué era esto posible.
En cambio, la religión jaina no permitió la posibilidad de transferir el karma.

La primera mención clara de la doctrina del karma se encuentra en el "Chandoguia-upanishad" (siglo VII a. C.), posiblemente el segundo "Upanishad" más antiguo (el más antiguo se considera el "Brijad-araniaka-upanishad"). Allí cuenta la historia del joven brahmán Shwetaketu, quien vuelve a su hogar después de haber aprendido todo el conocimiento védico (o sea, los rituales y las leyendas épicas contenidas en el "Rig-veda").
Sin embargo, se encuentra con su amigo de la infancia, quien pertenece a la casta chatría, quien lo interroga acerca del conocimiento que ha aprendido. ¿Sabe ya lo que nos sucede después de la muerte? Shwetaketu admite que no, que eso no era parte de su plan de estudios.
Así que se puede concluir que la doctrina central de los "Vedas" (compuestos entre el siglo XV y IX a. C.) y del primer "Upanishad" (posiblemente compuesto ―no escrito, porque los indios todavía no habían inventado un sistema de escritura― un par de siglos antes, hacia el siglo IX a. C.) no dependía de una hipótesis sobre la vida después de la muerte.

Shwetaketu corre a consultarle a su padre Uddalaka, un erudito brahmán, y le hace las mismas preguntas. Pero su padre tampoco sabe. Entonces ambos, sintiéndose engañados por no conocer la respuesta a una pregunta tan importante, recurren al rey. Resulta que él sí sabe, y les informa que los chatrías lo han sabido desde hace tiempo. Así que el rey les enseña la doctrina de la reencarnación por primera vez en la literatura védica (y por primera vez en todos los escritos más antiguos de la humanidad). El rey les informa que esta doctrina es comúnmente creída entre los guerreros chatrías.

Finalmente el rey les revela que esta creencia era el secreto del poder de los guerreros hinduistas. De hecho, aquellos que consideran sus cuerpos como simples vestidos que pueden desechar y reemplazar por otros nuevos, no tienen miedo de morir, por lo que son más intrépidos y ganan todas las batallas, y por lo tanto pueden disfrutar de todo el poder.

En Alejandría del Cáucaso (Bagram) (antigua ciudad de Afganistán fundada por Alejandro Magno, situada a unos 60 km al noroeste de Kabul) hubo una escuela de budismo con monjes budistas.
Poco más tarde, el emperador indio Asoka (304-232 a. C.) envió misioneros budistas a muchos países.

Durante los siglos XIX y XX, Occidente fue permeable a los conceptos religiosos provenientes de las antiguas colonias británicas y francesas en Asia. Así es como la creencia en la «ley del karma» ha tenido una importante difusión gracias a la penetración en Occidente del budismo, el hinduismo y el yoga, así como diversas escuelas de ocultismo, como la rosacruz (1614), y la teosofía (de Helena Blavatsky, 1831-1891).

A pesar de que Mahatma Gandhi (1869-1948) era adepto a las doctrinas del karma y la reencarnación, luchó contra la injusticia, aunque se desconoce si se apoyaba en algún basamento doctrinal.

Los creyentes en el karma sostienen que las injusticias sociales son simplemente la reacción de las malas acciones que habrían cometido las actuales víctimas en vidas pasadas. Cada víctima estaría sufriendo exactamente lo que hizo sufrir a otros (ni más, ni menos).

Según los hinduistas, el karma es una «ley» de acción y reacción: a cada acción cometida le corresponde una reacción igual y opuesta.
El encargado de hacer cumplir esta ley sería el omnisciente dios invisible Iama Rash (el ‘rey de la prohibición’) y sus monstruosos sirvientes invisibles, los "iama-dutas" (‘mensajeros de Iama’).

Después de que una persona abandona su cuerpo al momento de la muerte, los "iamadutas" le arrastrarían hasta la morada de iamarásh, donde es juzgado duramente de acuerdo con las acciones, registradas una por una en el libro de la vida, que recita Chitra Gupta, el secretario de Iamarash.

Según Yogananda, las explicaciones mitológicas serían la forma de explicar ciertas energías a personas sin educación, generalmente analfabetas, de forma que las diversas formas de energías astrales, invisibles y no registrables por los instrumentos actuales, se personalizarían y explicarían como si fueran dioses, semidioses, demonios, etc.

Para el hinduismo, el castigo de las malas acciones puede recibirse
Igualmente, el premio por las buenas acciones puede recibirse

La explicación del karma dentro de las doctrinas budistas es diferente de la hinduista.
El karma no sería una ley de causa y efecto que implicaría la existencia de dioses invisibles encargados de hacerla cumplir, sino una inercia natural.

Por ejemplo, si una persona roba un banco y tiene éxito, es muy probable que vuelva a robar, y si una persona ayuda a un anciano a cruzar la calle, entonces es muy probable que siga ayudando a otras personas. En ambos casos, si la experiencia no produjera buenos resultados, entonces la inercia se haría menor (el ladrón robaría menos y el filántropo ayudaría menos). Cuando un sujeto roba un banco, esta acción quedará registrada en su mente, alterando el flujo de esta, y provocando en él una percepción errónea de la realidad («tengo derecho a tomar sin permiso las cosas que necesito»).
Estas percepciones erróneas le condicionarán a sufrir más adelante, pues crean un estado mental propenso a la infelicidad.

El karma no sería entonces una recompensa o un castigo mágico a las acciones sino simplemente el hecho de que las acciones humanas tienen consecuencias tanto externas como mentales.

Según otra interpretación del karma más bien serían las dos cosas juntas, es decir, habría castigo y premio, pero no de forma mágica, sino mediante consecuencias automáticas de las acciones, en un concepto que implica la reencarnación, siempre unida al karma.

Según el budismo, al comportarse de acuerdo con el karma, la persona debería tomar conciencia de que la búsqueda de la venganza y el mal traerá graves consecuencias en la vida diaria y en las vidas futuras.
Esto permitiría aprender del sufrimiento, dominarlo y sacar provecho de él en términos espirituales para llegar al desarrollo de una vida más plena.

Puesto que todo acto tiene origen en la mente, el budista debe vigilar sus pensamientos y sus palabras, ya que también pueden producir bien o mal.
Cada acción y palabra, buenas o malas, sería un búmeran que a veces vuelve en la misma vida y a veces en una vida futura.

El karma puede ser explicado como un fenómeno análogo a la inercia.
Según esta visión, el individuo genera tendencias a través de sus causas.
Un pensamiento, palabra o acción intencional, si se repite, se convierte en costumbre y condicionará una tendencia en el mismo sentido.
En el futuro, las causas no necesariamente serían intencionales, sino que estarían influidas por causas previas.
En este sentido, el karma constituye una influencia inconsciente, condicionante pero no determinante, pues somos siempre libres y podemos contrarrestar nuestras influencias o tendencias negativas. Aunque sean escasos en porcentaje, tenemos numerosos ejemplos de personas que han cambiado radicalmente de vida.

En el jainismo, el karma es un principio básico de la cosmología. Para el jainismo, las acciones morales humanas son la base de la reencarnación "(yiva)". El alma se encuentra atrapada en un círculo de renacimiento y atada a un mundo temporal "(samsara)", hasta que finalmente alcanza la liberación "(moksa)". Esta liberación se consigue siguiendo el camino de la purificación.
La liberación completa del karma conduce a la omnisciencia kevala-gñana.

En la religión yaina, el karma no solo se refiere a la causalidad de la reencarnación sino que también se concibe como una materia tenue que se introduce en el alma oscureciendo sus cualidades naturales y puras. Se concibe el karma como una contaminación que tiñe el alma de diversos colores "(leshia)". En función de su karma, un alma realiza su trasmigración y se rencarna en varios estados de existencia.

Los jainistas señalan el sufrimiento, la desigualdad o el dolor como una prueba de la existencia del karma. Los textos jainistas han clasificado los tipos de karma en función de sus efectos sobre las capacidades del alma humana. La teoría jainista busca explicar los procesos del karma especificando las causas de su influjo ("asrava") y la atadura ("bandha"), mostrando el mismo interés por los actos en sí como por las intenciones detrás de los actos. La teoría jainista sobre el karma coloca toda la responsabilidad sobre las acciones individuales y elimina cualquier peso sobre una supuesta gracia divina o retribución. Además, la doctrina jaina también mantiene que es posible modificar el propio karma y también librarnos de él a través de la austeridad y la pureza de conducta.

Algunos escritores datan el origen de la doctrina del karma como anterior a la migración indoaria (mediados del II milenio a. C.) e indican que su actual forma sería el resultado del desarrollo de las enseñanzas de los sramanas, después asimilada en el hinduismo brahmánico en la época de los textos "Upanishads" (de mediados del I milenio a. C.). El concepto de karma jainista ha sido objeto de crítica por parte de las doctrinas rivales como el budismo, el hinduismo vedanta o el hinduismo samkia.

El karma sería la explicación mítica que encontraron los orientales para entender por qué ―si se supone que Dios es justo― a veces a las personas buenas les suceden cosas malas y a las personas malas les suceden cosas buenas.
Cada uno estaría pagando acciones que no recuerda, porque las cometió en vidas pasadas.

Según el "Vedanta-sutra" las reacciones del karma no se reciben en esta misma vida.

Ante la pregunta de por qué a veces sí se ve sufrir a un criminal en esta misma vida, los hinduistas sostienen que en realidad estaría sufriendo las reacciones de una vida anterior, o bien pagando el karma de acciones realmente perversas en la misma vida, pues Yogananda dice que las acciones de extrema maldad suelen recibir el castigo en la misma vida.

Si el karma que tenemos acumulado es de muchas vidas, una sola vida no bastaría para «pagarlo» y «recogerlo» todo en una sola vida, sino que también se necesitarían varias.

Si el premio o castigo viniera automáticamente poco después (a los pocos meses/días/minutos) el karma sería evidente y no seríamos libres, o no tan libres.
Por tanto castigos y premios pueden venir muchos años después o muchas vidas después, cuando las condiciones son propicias, también según Yogananda.

Según el hinduismo, Dios es neutral, y ha dejado a los semidioses la ejecución de la ley del karma, con sus premios y sus castigos.
En cambio, según Yogananda, no habría ministros para ejecutar la ley del karma, sino que esta se ejecutaría a sí misma como ley cósmica, astral o espiritual de forma automática.

Lo bueno o malo que le sucede a un ser humano no sería algo debido a la voluntad de Dios o las deidades (que es siempre amorosa), sino el resultado de los propios actos.

Según una encuesta en Internet, el 27 % de los estadounidenses creen en la reencarnación.




</doc>
<doc id="3423" url="https://es.wikipedia.org/wiki?curid=3423" title="Sonia Delaunay">
Sonia Delaunay

Sarah Ilínichna Stern (Gradizhsk, 1885 - París, 1979), más conocida como Sonia Delaunay, fue una pintora y diseñadora francesa, nacida en Ucrania. Junto con su marido Robert Delaunay fueron grandes representantes del arte abstracto y creadores del "Simultaneísmo".

Nació el 14 de noviembre de 1885 en Gradizhsk, Gobernación de Poltava, Ucrania, en una familia muy modesta que ya tenía tres hijos, ante la imposibilidad de hacerse cargo de la niña, la familia la envió a casa de sus tíos maternos Anna y Heinrich Terk que no tenían hijos, en un principio la acogieron y finalmente la adoptaron legalmente. El tío era abogado de prestigio en San Petersburgo, y amante del arte, poseía una gran colección de pinturas de la Escuela de Barbizon, y Sarah Sophie (Sonia) creció rodeada de obras de arte y disfrutando del ambiente cultural de la ciudad. Pasaba las vacaciones en Finlandia, Suiza, Alemania e Italia donde también visitaba los museos. En 1903, la familia Terk la envió a Alemania para continuar sus estudios en la Universidad de Karlsruhe, y fue allí donde descubrió la pintura contemporánea y donde estudió dibujo con Schmitd-Reuter. Dos años más tarde se trasladó a París y se matriculó en la Academie de la Palette, donde tuvo como compañeros de taller en Amédée Ozenfant, André Dunoyer de Segonzac y Jean Louis Boussingault. Durante esta época se inició en el mundo del grabado de la mano de Grossman.

A partir de 1907 vemos en sus pinturas la influencia del expresionismo alemán y de Vincent Van Gogh y Paul Gauguin, como por ejemplo Philomène, Jeune finlandaise o Desnudo amarillo. Esta etapa duró aproximadamente una año y pronto se decantó por el estilo fauve. En 1908 realizó una exposición de sus pinturas de estilo fauvista en la galería de Wilhelm Uhde, donde conoció Robert Delaunay.
Para evitar las presiones familiares que la exigían volver a Rusia, pacta un matrimonio de conveniencia con Wilhelm Uhde, y se casan en 1909 en Londres. Uhde introdujo a Sonia en los círculos artísticos de Braque, Picasso, Derain y Vlaminck. Robert Delaunay también frecuentaba estos círculos artísticos, y pronto se dieron cuenta de que compartían las mismas preocupaciones artísticas. La relación entre los dos artistas se fue consolidando, y en 1910 Sonia se divorcia de Uhde y se casa con Robert Delaunay. Él la animó a decantar sus investigaciones y obras hacia las artes aplicadas, y en cierto modo Sonia abandona la pintura como medio de expresión y se adentra en estas otras técnicas artísticas. Este cambio de dirección en su producción hará que tanto Sonia como Robert Delaunay se pasaran a la abstracción pura siguiendo caminos distintos aunque relacionados entre sí, y entraran en la historia del arte en categorías diferentes y desiguales.

La pareja se instaló en la Rue des Grands Augustins, donde mantuvieron un taller hasta 1935. Y en torno al matrimonio Delaunay-Terk se formó un círculo de pintores, músicos y escritores, entre los que estaba Apollinaire.

Coincidiendo con el nacimiento de su hijo Charles, Sonia creó su primera obra abstracta, una colcha de patchwork para la cuna de su hijo. Hecho con retales de ropa que combinaban una serie de colores contrastados. Robert identificaba la colcha con el arte popular ruso, pero los amigos de la familia e integrantes del círculo artístico que los rodeaba no dudaron a la hora de reconocer los principios del cubismo que recogía la obra textil y destacar la combinación de colores únicos que había realizado Sonia.
A partir de ese momento, y animada por la admiración que había despertado entre sus compañeros artistas comenzó a diseñar pequeños objetos de decoración con colores vivos, primero para la casa, para cubrir sus necesidades, pero poco a poco fue diseñando objetos para sus amigos y miembros del círculo de artistas que frecuentaban su casa. Pintó sus primeros "Contrastes simultáneos", y creó sus primeras encuadernaciones a base de collage para los "libros que ama" (Rimbaud, Mallarmé o Apollinaire). Inició una estrecha amistad con Blaise Cendrars, para quien hizo la encuadernación del libro " La Pascua en Nueva York" y más tarde ilustró el poema "La Prose du Transsibérien et de la Petite Jehanne de France". Durante este tiempo también realizó varias portadas para la revista "Der Sturm", al tiempo que realiza sus primeros modelos simultáneos (chalecos y trajes que lucían ella y Robert). En 1912 vuelve a la pintura, aunque no fue un retorno a las exposiciones. En efecto, su fama se había extendido rápidamente por Europa occidental, pero se la consideraba más una diseñadora comercial que una artista. Y es que Sonia aplicó los principios de simultaneidad a una amplia gama de materiales y objetos, desde colgantes, pinturas, telas, tapas de libros hasta objetos del hogar.

En 1913, participó en el Primer Salón de Otoño de Berlín, donde expuso una veintena de pinturas y objetos. Un año más tarde participó en el Salón des Indépendants en París donde expuso sus Prismas eléctricas. Diseñó varios carteles publicitarios para marcas como Dubonnet, Zèntih o Chocolat y continuó colaborando con Cendrars, quien le dedicó el poema "Sur la robe ella a un corps", inspirado por los "vestidos simultáneos" de Sonia.

El verano de 1914 la familia Delaunay estaba de vacaciones en Fuenterrabía, al estallar la guerra deciden quedarse en España, ya que Robert Delaunay había sido declarado inútil para el servicio militar, y se instalaron en Madrid. Durante este periodo Sonia comienza una serie de pinturas "Chanteurs de Flamenco". Al año siguiente marchan en Lisboa y se alquilan una casa en Vila do Conde, que compartían con Eduardo Vianna y Sam Halpert. Durante esta estancia en Portugal se reencontraron con Amadeo de Souza-Cardoso al que habían conocido en París, y también entran en contacto con otros artistas portugueses como José Pacheco y José de Almada-Negreiros. Sonia que había comenzado a estudiar la relación de la luz y el movimiento en 1913, pero descubrió en España una nueva dimensión de la luz y esta sensación se intensifica en Portugal, una luz "desembarazada de grises que exalta el color que se ha convertido una entidad en sí mismo”. Inició un periodo de actividad muy intensa, recupera los temas figurativos e imágenes cotidianas, donde expresa esta influencia de la luz en los colores de sus composiciones. Expuso en Stocolm, en la Nya Konstgalleriet y realizó la portada del catálogo. También realizó varias portadas para la revista Vogue y creó objetos de cerámica. Comenzó a trabajar con grandes composiciones con cera sobre tela como "Marché au Minho" y "Hommage au donateur", en un intento por acercarse al arte monumental. En este sentido diseñó la fachada de una capilla para un convento de jesuitas, pero el proyecto no se llevó a cabo.

En 1917, el matrimonio Delaunay abandonó Portugal y se instalaron en Barcelona. Es durante su estancia en esta ciudad en donde se enteran de que ha estallado la Revolución soviética, lo que supuso un trastorno para la economía de la familia, ya que Sonia dejó de percibir las rentas que recibía de Rusia y que hasta entonces habían supuesto su pilar económico. Deciden entonces marchar a Madrid, en donde recibieron el apoyo de Serguéi Diáguilev. Gracias a él, consiguieron colaborar con los Ballets Rusos y Sonia diseñó el vestuario para la reposición del ballet de "Cleopatra". Diáguilev, además, puso en contacto a Sonia con Alfredo Escobar y Ramírez, marqués de Valdeiglesias, director del diario "La Época" y miembro de la aristocracia española. Este contacto le abrió un mundo nuevo de posibilidades, pues las mujeres de la aristocracia española le empezaron a encomendar vestidos y objetos de decoración para sus hogares. También llevó a cabo la decoración del Pequeño Casino de Madrid. Igualmente, gracias a Diáguilev consiguió apoyo económico y abrió su primera boutique de moda (vestidos simultáneos, bordados coloreados) y complementos (bolsos, abanicos, paraguas...). Tuvo un éxito inmediato, y los principales miembros del mundo artístico madrileño se convirtieron en su clientela habitual. Esta actividad económica le permitió cubrir las necesidades de la familia y le dieron fama y prestigio. Tan es así, que abrió sucursales de su tienda en Bilbao, San Sebastián y Barcelona. Al mismo tiempo se estrenó en Londres el ballet "Cleopatra", cuyo diseño de vestuario, realizado por Sonia, causó tanto furor que le encargaron el diseño de los trajes de la ópera Aida, estrenada en el Liceo de Barcelona en 1920.

En 1921, atraídos por las nuevas ideas y las nuevas corrientes artísticas que se producen en París, los Delaunay deciden volver a la ciudad. Robert marcha unos meses antes y Sonia se queda en Madrid para liquidar sus negocios. En París pronto vuelven a integrarse en los movimientos de vanguardia y reconstruyen su ciclo de amistades, entre los que se encuentran Albert Gleizes, André Lhote, André Breton, y artistas como Tristan Tzara, Philippe Soupault o Joseph Delteil escribían poemas inspirados en las creaciones de Sonia y lucían las ropas que ella diseñaba y confeccionaba para ellos. Sonia decidió continuar financiando a la familia con el uso comercial de su talento, y con la experiencia adquirida en Madrid, acondiciona la librería "Au sans pareil" en Neully. Allí realizó las Ropas poemas en colaboración con sus amigos artistas, y retorna a la encuadernación de portadas de libros de Tristan Tzara y Iliazd.

En 1923 se encargó de realizar el vestuario de una obra de Tzara, "Le coeur à gaz". El espectáculo terminó con la intervención de la policía, pero los trajes de Sonia fueron todo un éxito, y la crítica se hizo eco de sus composiciones. A raíz de este éxito de prensa, una importante empresa textil de Lyon se puso en contacto con ella para diseñar motivos por sus tejidos. Y Sonia Delaunay se convirtió en la más conocida de un grupo de artistas del siglo XX, desde Raoul Dufy a Varvara Stepanova, y sus diseños entraron de lleno en el mundo de la moda comercial.

Al año siguiente, participó en un acto en favor de los refugiados rusos, donde presentó la "moda del futuro", maniquíes vestidos con sus diseños y acompañados por un poema de Joseph Delteil. Este nuevo éxito la llevó a asociarse con el modisto Jacques Heim, con quien abrió el "Atelier simultané", momento en que creó sus primeros abrigos bordados, que causaron un fuerte impacto en la moda del momento. Expuso sus obras en el Salon d'Automne y las presentó en movimiento, gracias a una máquina inventada por Robert. En 1925, el trabajo de diseño de Sonia constituía una ampliación del concepto de modernidad llevado a la cotidianidad, por lo que su nombre se convirtió en un sinónimo de "estilo moderno".

En 1926, participó en la exposición "Treinta años de arte independiente" en el Grand Palais. Junto con su marido, se aventuran en el mundo del cine, donde diseñaron los decorados y el vestuario de las películas "Le P'tit Parigot" de LeSomptier, y "Vertige" de Marcel de L'Herbier.

El éxito de los diseños de Sonia se traduce en la invitación que recibe de la Sorbona de París para dar una conferencia en 1927: "El influence de la peinture sur del arte vestimentaire". Pero aun así, con la recesión económica que sufre Francia, se ve obligada a cerrar su "Atelier simultané". Forma parte de la primera exposición de la Unión de los Artistas Modernos en el Museo de las Artes Decorativas y en una exposición itinerante que viajó a los Estados Unidos. A partir de 1931 el matrimonio une fuerzas para dedicarse de lleno al arte abstracto. Durante este período, que duró hasta 1934, se dedican casi exclusivamente a la pintura, pero Sonia continúa alternando su interés con las artes decorativas, escribiendo artículos en revistas de moda. Trabaja también en muchos carteles publicitarios, donde comenzó a incorporar las lámparas de "mica-tube". En esta línea crea el cartel luminoso para el papel de cigarrillos Zig-Zag, con el que gana el primer premio del concurso organizado por la Compañía Parisina de Distribución Eléctrica.
En 1937, Robert Delaunay recibe el encargo de la decoración de dos pabellones de los arquitectos Mallet Stevens y Felx aublet, pero no duda en incorporar al equipo de trabajo a Sonia, que ya había demostrado sus aptitudes para la decoración monumental con el Hommage au donateur. El equipo integrado por un total de 50 obreros y pintores como Bissière, Gleizes, Lhote, Survage... obtuvo la medalla de oro por sus paneles del Pabellón de los Ferrocarriles, y esto facilitó el reconocimiento de la crítica y del público por el trabajo que había realizado el matrimonio Delaunay-Terk. Siguiendo en esta línea de arte monumental, en 1938 Sonia realizó una puerta de cemento coloreado por la entrada de la exposición Arte Mural. A petición de Othon Friesz, el matrimonio Delaunay realizaron la decoración del vestíbulo de las esculturas del "Salon des Tulleries", con "Rhytm" de grandes dimensiones. Un año más tarde, Sonia y Robert junto con otros artistas defensores del arte abstracto (Van Doesburg, Fredo Side...) organizaron la exposición "Réalités Nouvelles", su importancia radicó en que reunía por primera vez una exposición de artistas íntegramente abstractos. También participó en una retrospectiva de los Ballets Russes de Diáguilev en el Museo de Artes Decorativas de París. Robert murió en 1941, y unos años más tarde Sonia se traslada a Toulouse donde se reencontró con viejos amigos como Tzara o Uhde, allá decoró el "Centre d'Accueil International" de la Cruz Roja, esta fue su última obra de decoración de interiores.

A partir de 1945 se instaló de nuevo en París, dedicando sus esfuerzos a que se reconozca el talento de Robert, y junto con Louis Carré organizaron la primera retrospectiva de Robert Delaunay en 1946. Conjuntamente con Fredo Side organizaron el primer Salon des Réalités Nouvelles, el único requisito era la no figuración, y en él Sonia participó en varias ocasiones.

A partir de 1950 se comienzan a publicar recopilaciones de sus obras, litografías, ilustraciones, "gouaches"... continúa trabajando en su obra, investigando sobre el color, y ampliando su producción hacia el campo de los mosaicos y los vitrales. También recupera sus investigaciones sobre alfombras y vuelve a diseñar vestuario y decorados para el teatro. La muerte de Robert liberó Sonia de la creencia de que sólo podía haber una única carrera artística por el matrimonio. Y a partir de aquí comienza el reconocimiento a su obra individual.

En 1958, el Städtischs Kunsthaus de Bielefeld (Alemania) organizó la primera gran retrospectiva de la obra de Sonia Delaunay, con un total de 250 obras. Después de esta exposición la siguieron toda una serie de exposiciones individuales en todo el mundo hasta los años 80. En 1964 Sonia y su hijo Charles, hicieron una donación al estado francés de un total de 101 obras de Sonia y Robert Delaunay, pero hasta 1987 no se pudo ver su obra junta, cuando se inauguró una destacada exposición en el Museo de Arte Moderno de París. En 1971 decora un automóvil, un Matra 530, los beneficios de la venta se destinaron a la investigación médica. Su preocupación constante por la aplicación del color más allá de la pintura, queda demostrada en la aplicación de sus principios en una amplia variedad de técnicas artísticas. En 1975 fue nombrada por el estado francés Oficial de la Legión de Honor. En 1978 publicó el libro "Nuevos Irons jusqu'au soleil", donde Sonia reflexiona sobre su trabajo y el de Robert, como el matrimonio compartía una misma visión estética, expresada en unas formas similares, pero que se materializó en técnicas diferentes.

En 1979, sufrió un accidente que redujo bastante su movilidad; sin embargo, continuó pintando. Su última obra fueron unos "gouaches" para la gran retrospectiva organizada por el Museo Albright-Knox de Búfalo. Murió en su taller el 5 de diciembre de 1979.

El Museo Nacional Thyssen Bornemisza celebró en 2017 "Sonia Delaunay. Arte, diseño y moda," la primera exposición monográfica de la artista en España "."




</doc>
<doc id="3429" url="https://es.wikipedia.org/wiki?curid=3429" title="Sátiro">
Sátiro

Los sátiros (en griego Σάτυροι, "Satyroi") son criaturas masculinas —las sátiras son una invención posterior de los poetas— que en la mitología griega acompañaban a Pan y Dioniso, vagando por bosques y montañas.

Aunque no los menciona Homero, en un fragmento de Hesíodo recogido por Estrabón se dice que los sátiros son hermanos de las ninfas de las montañas y de los curetes y que son criaturas inútiles e incapaces de trabajar. 

Los sátiros, relacionados con las ménades, forman el «cortejo dionisíaco» que acompaña al dios Dioniso, aunque algunos también aparecen solos en los bosques. Pueden estar también asociados con el dios Pan y con Sileno. Una tradición consideraba que los hijos de Sileno, llamados Marón, Leneo y Astreo, habrían sido los padres de los sátiros (de los que entonces sería Sileno su abuelo). 

Los sátiros son criaturas alegres y pícaras, aunque su carácter desenfadado y festivo puede volverse peligroso e incluso violento. Como criaturas dionisíacas, son amantes del vino, de las mujeres y disfrutan de los placeres físicos. 

En las representaciones de los integrantes del séquito de Dioniso a menudo resulta difícil distinguir entre sátiros, silenos, faunos y panes, de tal forma que a veces se usan estos términos como sinónimos.

La forma más común de representación de un sátiro es la de una criatura desnuda grotesca medio humana pero con patas y orejas de macho cabrío o de caballo, cuernos de macho cabrío, cola de caballo, nariz chata y frecuentemente itifálicos. 

A veces se los representa como graciosos jóvenes: el "sátiro en reposo", atribuido a Praxíteles, es el mejor ejemplo. En las representaciones más antiguas no se distinguen de los silenos pero a partir del periodo helenístico a Sileno se le representa más viejo y gordo que a los sátiros. 

Entre sus atributos, aparecen a menudo con un tirso en la mano, a veces con una corona vegetal, en actitud de bailar con las ninfas, a las que a menudo persiguen. Bailan al son de aulos, siringas, címbalos, castañuela y tímpanos. Tienen un baile especial llamado "sikinnis".
Debido a su gusto por el vino, a menudo aparecen embriagados o sosteniendo copas.

En el arte griego arcaico, los sátiros aparecen como criaturas con barba y feas, pero en un período posterior su fealdad es suavizada con un aspecto más grácil y juvenil. En este periodo posterior aparecen también algunas satiresas.

Esta transformación o humanización del sátiro aparece a partir del siglo IV a. C. Las representaciones compasivas y humanizadas del sátiro por Praxíteles (Sátiro en reposo y Sátiro escanciador) son ejemplos de esta evolución.

Los romanos adoraban al dios Fauno como un señor de los pastores y los bosques y se asimiló al dios griego Pan. Como Pan era representado con cuernos y patas de cabra, su representación a menudo se confunde con la de los sátiros. Además, a Fauno, que era una figura difusa, se le disgregó en una serie de "faunos". Por otra parte, también existía Fauna —consorte de Fauno—, que se representaba con la forma de satiresa. En definitiva, en el arte romano es difícil de distinguir a los sátiros de los faunos y de los "panes".

En la Edad Media el aspecto del sátiro como una criatura con patas y cuernos de cabra fue asumido por el diablo. Por otra parte, en esta época también se usó la imagen del sátiro como una alternativa al centauro en la representación del signo zodiacal de Sagitario. 

En el Renacimiento, siguieron apareciendo sátiros como híbridos de humano y cabra, pero se suavizó su aspecto negativo, volvieron a ser símbolos de la naturaleza salvaje y se mezcló su carácter erótico con su faceta festiva. En algunos casos, sin embargo, se mantuvo la relación de los sátiros con los pecados. 

Los sátiros infantiles o niños sátiros son representaciones mitológicas derivadas de los sátiros, que aparecen en el folclore popular, obras de arte clásicas, películas y distintas formas de arte.

Algunas obras clásicas muestran a sátiros infantiles cuidados por sátiros adultos, y en otras representaciones aparecen participando en bacanales y rituales dionisíacos (bebiendo alcohol, tocando instrumentos musicales y bailando).

La presencia de los sátiros infantiles en el arte clásico, como la antigua cerámica griega es simplemente una elección estética por parte del artista. Sin embargo, el papel de los niños en el arte clásico podría indicar un simbolismo más profundo para los sátiros infantiles: Eros, el hijo de Afrodita, es representado habitualmente con la forma de un niño o bebé, y Dioniso, el líder divino de los sátiros es representado en numerosas obras como un bebé, a menudo en compañía de los sátiros. Una representación de sátiros infantiles más allá de la antigua Grecia es el grabado de Alberto Durero "músico sátiro y ninfa con bebé", también conocido como "La familia del sátiro". También hay una representación victoriana que muestra a un bebé sátiro sentado al lado de un barril. 

También hay muchas obras del período rococó que muestran a niños sátiros participando en celebraciones dionisíacas. Algunas muestran a mujeres sátiro con sus hijos; otras muestran a los niños participando directamente en las bacanales. En la pintura de Jean Raoux (1677-1735) "Mademoiselle Prévost como Bacante", aparece un niño sátiro tocando un tambor, mientras la Sra. Prévost, una bailarina de la ópera, baila en medio de una fiesta en honor de Baco.

En los festivales teatrales de la antigua Atenas, a cada grupo de tragedias le seguía una especie de drama burlesco que relataba leyendas de dioses y héroes, en la que el coro estaba formado por sátiros y silenos. Estas obras se denominan dramas satíricos. Se ha conservado completo solamente uno del siglo V. a. C., "El Cíclope" de Eurípides. También se ha conservado un papiro con un fragmento extenso de un drama satírico de Sófocles, titulado "Persiguiendo sátiros" (Ichneutae), que se encontró en la antigua colonia griega de Oxirrinco, en Egipto, en 1907.

La "sátira" romana es una forma literaria que consiste en un ensayo poético utilizado para atacar o burlarse de personas o elementos sociales. Aunque la sátira romana en ocasiones ha sido vinculada con las obras satíricas griegas o romanas, se trata de dos géneros independientes, conectados por la naturaleza subversiva atribuida a los sátiros, como fuerzas opuestas al orden, el decoro y la propia civilización.

Bien por influencia externa o por desarrollo propio, otras mitologías también muestran personajes o criaturas con carácter similar a los sátiros griegos y romanos, espíritus de los bosques y de la naturaleza, como los "leszi" o "lisovik" del folclore eslavo o los "busgosus" de los bosques del noroeste de la península ibérica o los "basajaun" vascos. Estas y otras criaturas muestran rasgos muy similares con los sátiros, ya sea su carácter alegre, festivo y desenfadado, su promiscuidad sexual o su gusto por el vino.

En la mitología hebrea existen los "sh'lrlm" ("peludos"), una especie de demonio o ser sobrenatural que habita en los desiertos, y a los que se alude en el Levítico como receptores de sacrificios, y posiblemente relacionados con la simbología del chivo expiatorio. En la Biblia estos seres son traducidos como diablos, aunque en la traducción inglesa del Rey Jaime se les atribuye el término "satyr" (sátiro). En la mitología árabe y musulmana estos seres son conocidos como "azzab al-akaba" (demonios peludos de los pasos de montaña).

En el siglo XVII, las leyendas de los sátiros se asociaron con las historias del orangután, un gran simio que habitaba Insulindia. Muchas de las historias locales describían a los machos de esta especie como seres sexualmente agresivos con las hembras de su especie y con las mujeres. En aquella época en occidente algunos eruditos consideraron que estas leyendas se referían a la presencia de sátiros en la zona. De hecho, el primer nombre científico que se le dio a este simio fue el de "Simia satyrus".

Derivado de su significado mitológico, en el lenguaje popular se utiliza el término "sátiro" para denominar a un hombre dominado por el apetito sexual que se esfuerza en perseguir mujeres en contra de lo que ellas deseen. Las antiguas clasificaciones de las enfermedades sexuales denominaban "satiromanía" (en el varón) y "ninfomanía" (en la mujer) a lo que ahora se denomina "hipersexualidad".





</doc>
<doc id="3441" url="https://es.wikipedia.org/wiki?curid=3441" title="Neutrino">
Neutrino

Los neutrinos (término que en italiano significa ‘pequeños neutrones’), descubiertos por los científicos Clyde Cowman y Federick Reines, son partículas subatómicas de tipo fermiónico, sin carga y espín ½. Desde principios del siglo XXI, después de varios experimentos llevados a cabo en las instalaciones del Observatorio de Neutrinos de Sudbury (SNO) en Canadá y el Super-Kamiokande en Japón entre otros, se sabe, contrariando al modelo electrodébil, que estas partículas tienen masa, pero muy pequeña, y es muy difícil medirla. Hasta 2016, la cota superior de la masa de los neutrinos es 5,5 eV/"c", lo que significa menos de una milmillonésima parte de la masa de un átomo de hidrógeno. Su conclusión se basa en el análisis de la distribución de galaxias en el universo y es, según afirman estos científicos, la medida más precisa hasta ahora de la masa del neutrino. Además, su interacción con las demás partículas es mínima, por lo que pasan a través de la materia ordinaria sin apenas perturbarla.

La masa del neutrino tiene importantes consecuencias en el modelo estándar de la física de partículas, ya que implicaría la posibilidad de transformaciones entre los tres tipos de neutrinos existentes en un fenómeno conocido como oscilación de neutrinos.

En todo caso, los neutrinos no se ven afectados por las fuerzas electromagnéticas o nuclear fuerte, pero sí por la fuerza nuclear débil y la gravitatoria.

La existencia del neutrino fue propuesta en 1930 por el físico Wolfgang Pauli para compensar la aparente pérdida de energía y momento lineal en la desintegración β de los neutrones según la siguiente ecuación:

Wolfgang Pauli interpretó que tanto la masa como la energía serían conservadas si una partícula hipotética denominada «neutrino» participase en la desintegración incorporando las cantidades perdidas. Desafortunadamente, esta partícula hipotéticamente prevista había de ser sin masa, ni carga, ni interacción fuerte, por lo que no se podía detectar con los medios de la época. Esto era el resultado de una sección eficaz muy reducida (formula_2). Durante 25 años, la idea de la existencia de esta partícula solo se estableció de forma hipotética.

De hecho, es muy pequeña la posibilidad de que un neutrino interactúe con la materia ya que, según los cálculos de física cuántica, sería necesario un bloque de plomo de una longitud de un año luz (9,46 billones de kilómetros) para detener la mitad de los neutrinos que lo atravesaran.

En 1956 Clyde Cowan y Frederick Reines demostraron su existencia experimentalmente. Lo hicieron bombardeando agua pura con un haz de 10 neutrones por segundo. Observaron la emisión subsiguiente de fotones, quedando así determinada su existencia. A este ensayo, se le denomina experimento del neutrino.

En 1962 Leon Max Lederman, Melvin Schwartz y Jack Steinberger mostraron que existía más de un tipo de neutrino al detectar por primera vez al neutrino muónico. En el año 2000 fue anunciado por parte de la Colaboración DONUT en Fermilab el descubrimiento del neutrino tauónico. Su existencia ya había sido predicha, puesto que los resultados del decaimiento del bosón Z medidos por LEP en CERN eran compatibles con la existencia de 3 neutrinos.

En septiembre de 2011, la colaboración OPERA anunció que el análisis de las medidas para la velocidad de los neutrinos en su experimento arrojaba valores superlumínicos. En particular, la velocidad de una cierta clase de neutrino podría ser un 0,002 % mayor que la de la luz, constituyendo la anomalía de neutrinos superlumínicos, en contradicción con la teoría de la relatividad. Sin embargo, el mismo organismo reconoció meses después que a la hora de la medida de la distancia recorrida por los neutrinos hubo un fallo en el sistema de posicionamiento (GPS), al tener un cable desconectado, por lo que la medida de la velocidad superlumínica ha sido descartada.

Existen tres tipos de neutrinos asociados a cada una de las familias leptónicas (o sabores): neutrino electrónico ( formula_3 ), neutrino muónico ( formula_4 ) y neutrino tauónico ( formula_5 ) más sus respectivas antipartículas.

Los neutrinos pueden pasar de una familia a otra (es decir, cambiar de sabor) en un proceso conocido como oscilación de neutrinos. La oscilación entre las distintas familias se produce aleatoriamente, y la probabilidad de cambio parece ser más alta en un medio material que en el vacío. Dada la aleatoriedad del proceso, las proporciones entre cada uno de los sabores tienden a repartirse por igual (1/3 del total para cada tipo de neutrino) a medida que se producen sucesivas oscilaciones. Fue este hecho el que permitió considerar por primera vez la oscilación de los neutrinos, ya que al observar los neutrinos procedentes del Sol (que deberían ser principalmente electrónicos) se encontró que solo llegaban un tercio de los esperados. Los dos tercios que faltaban habían oscilado a los otros dos sabores y por tanto no fueron detectados. Esto es el llamado «Problema de los neutrinos solares».

La oscilación de los neutrinos implica directamente que estos tienen una masa no nula, ya que el paso de un sabor a otro solo puede darse en partículas masivas. Esto es debido a que, para partículas de masa nula, el tiempo propio es cero, lo que implica que desde el punto de vista de la partícula toda la trayectoria es recorrida en el mismo instante de tiempo, lo que no da margen a un cambio de estado en algún punto de ella.

En el modelo estándar se consideraba inicialmente al neutrino como a una partícula sin masa. De hecho, en muchos sentidos se la puede considerar de masa nula pues ésta es, por lo menos diez mil veces menor que la del electrón. Esto implica que los neutrinos viajan a velocidades muy cercanas a la de la luz. Por ello, en términos cosmológicos al neutrino se le considera "materia caliente", o materia relativista. En contraposición la materia fría sería la materia no relativista.

En 1998, durante la conferencia "0-mass neutrino", se presentaron los primeros trabajos que mostraban que estas partículas tienen una masa ínfima. Previamente a estos trabajos se había considerado que la hipotética masa de los neutrinos podía tener una contribución importante dentro de la materia oscura del universo. Sin embargo, resultó que la masa del neutrino era insuficiente, demasiado pequeña para ser siquiera tenida en cuenta en la ingente cantidad de materia oscura que se calcula que hay en el universo. Por otro lado, los modelos de evolución cosmológica no cuadraban con las observaciones si se introducía materia oscura caliente. En ese caso las estructuras se formaban de mayor a menor escala. Mientras que las observaciones parecían indicar que primero se formaron las agrupaciones de gas, luego estrellas, luego proto galaxias, luego cúmulos, cúmulos de cúmulos, etc. Las observaciones, pues, cuadraban con un modelo de "materia oscura fría". Por estos dos motivos se desechó la idea de que el neutrino contribuyera de forma destacada a la masa total del universo.

El Sol es la más importante fuente de neutrinos a través de los procesos de desintegración beta de las reacciones que acaecen en su núcleo. Como los neutrinos no interaccionan fácilmente con la materia, escapan libremente del núcleo solar atravesando también la Tierra. Aparte de las reacciones nucleares, hay otros procesos generadores de neutrinos, los cuales se denominan "neutrinos térmicos" ya que, a diferencia de los "neutrinos nucleares", se absorbe parte de la energía emitida por dichas reacciones para convertirla en neutrinos. De esta forma, una parte de la energía fabricada por las estrellas se pierde y no contribuye a la presión, siendo la razón por la que se dice que los neutrinos son "sumideros de energía". Su contribución a la energía emitida en las primeras etapas (secuencia principal, combustión del helio) no es significativa, pero en los colapsos finales de las estrellas más masivas, cuando su núcleo moribundo se encuentra a elevadísimas densidades, se producen muchos neutrinos en un medio que ya no es transparente a ellos, por lo que sus efectos se tienen que tener en cuenta.

Según los modelos solares, se debería recibir el triple de neutrinos que se detectan, ausencia que es conocida como el problema de los neutrinos solares. Durante un tiempo se intentó justificar este déficit revisando los modelos solares. El Sol quema el hidrógeno principalmente mediante dos cadenas de reacciones, la PPI y la PPII. La primera emite un neutrino y la segunda dos. Las hipótesis que se plantearon fueron que, quizá, la PPII tuviera una ocurrencia menor que la calculada debido a una falta de helio en el núcleo favorecido por algún tipo de mecanismo (frenado de la rotación por viscosidad) que mezclara parte del helio producido con el manto lo cual reduciría la cadencia de la PPII. Actualmente el problema va camino de resolverse al plantearse la teoría de la oscilación de neutrinos.

Las principales fuentes de neutrinos artificiales son las centrales nucleares, las cuales pueden llegar a generar unos 5·10 anti-neutrinos por segundo, y en menor medida, los aceleradores de partículas.

En las supernovas tipo II son los neutrinos los que provocan la expulsión de buena parte de la masa de la estrella al medio interestelar. La emisión de energía en forma de neutrinos es enorme y solo una pequeña parte se transforma en luz y en energía cinética. Cuando sucedió la SN 1987A los detectores captaron el débil flujo de neutrinos procedentes de la lejana explosión.

Se cree que, al igual que la radiación de microondas de fondo procedente del Big Bang, hay un fondo de neutrinos de baja energía en nuestro Universo. En la década de 1980 se propuso que estos pueden ser la explicación de la materia oscura que se piensa que existe en el universo. Los neutrinos tienen una importante ventaja sobre la mayoría de los candidatos a materia oscura: sabemos que existen. Sin embargo, también tienen problemas graves.

De los experimentos de partículas, se sabe que los neutrinos son muy ligeros. Esto significa que se mueven a velocidades cercanas a la de la luz. Así, la materia oscura hecha de neutrinos se denomina «materia oscura caliente». El problema es que, al encontrarse en rápido movimiento, los neutrinos habrían tendido a expandirse uniformemente en el Universo, antes que la expansión cosmológica los enfriara lo suficiente como para concentrarse en cúmulos. Esto causaría que la parte de materia oscura hecha de neutrinos se expandiera, siendo incapaz de formar las grandes estructuras galácticas que vemos.

Además, estas mismas galaxias y grupos de galaxias parecen estar rodeadas de materia oscura que no es lo suficientemente rápida para escapar de estas galaxias. Presumiblemente, esta materia proveyó el núcleo gravitacional para la formación de estas galaxias. Esto implica que los neutrinos constituyen solo una pequeña parte de la cantidad total de materia oscura.

De los argumentos cosmológicos, los neutrinos reliquia (del fondo de baja energía) son estimados en poseer densidad de 56 por cada centímetro cúbico, y de tener temperatura de 1.9 K (1.7×10−4 eV), esto es, si no poseen masa. En el caso contrario, serían mucho más fríos si su masa excede los 0.001 eV. Aunque su densidad es bastante alta, debido a las extremadamente bajas secciones cruzadas de neutrinos a energías bajo 1 eV, el fondo de neutrinos de baja energía aún no ha sido observado en el laboratorio. 

En contraste, neutrinos solares de boro-8, que son emitidos con una mayor energía, han sido detectados definitivamente a pesar de poseer una densidad espacial más baja que la de los neutrino reliquia, alrededor de 6 órdenes de magnitud.

Las reacciones de desintegración beta de isótopos radiactivos terrestres proporcionan una pequeña fuente de neutrinos, que se producen como consecuencia de la radiación natural de fondo. En particular, las cadenas de desintegración de 238,92U y 232,90Th, así como 40,19K, incluyen desintegración beta que emiten anti-neutrinos. Estos llamados geoneutrinos puede proporcionar información valiosa sobre el interior de la Tierra. Una primera indicación de geoneutrinos fue encontrado por el experimento KamLAND en 2005. KamLAND principales antecedentes en la medición de geoneutrino son los anti-neutrinos procedentes de los reactores. Varios experimentos futuros apuntan a mejorar la medición geoneutrino y estas necesariamente tendrá que estar lejos de los reactores.

Al conocerse con exactitud las reacciones nucleares que se dan en el Sol se calculó que un apreciable flujo de neutrinos solares tenía que atravesar la Tierra a cada instante. Este flujo es enorme pero los neutrinos apenas interactúan con la materia ordinaria. Incluso las condiciones del interior del Sol son «transparentes» a estos. De hecho, un ser humano es atravesado por miles de millones de estas diminutas partículas por segundo sin que se entere. Así pues se hacía difícil concebir algún sistema que pudiese detectarlos.

Las primeras partículas de este tipo jamás detectadas fueron los antineutrinos electrónicos emitidos por el reactor nuclear de la planta de Savannah River en Georgia (EE. UU.), que gracias al experimento de Frederick Reines y Clyde Cowan (), se pudieron observar directamente mediante el uso de dos "dianas" de cloruro de cadmio disuelto en agua. Los protones del agua eran los objetivos de los (anti)neutrinos: si poseían una energía de más de 1.8 MeV eran capaces de causar una interacción de corriente cargada (CC) llamada "decaimiento beta inverso", que daría como resultado positrones y neutrones:

formula_6

Los positrones se aniquilarían rápidamente con electrones del ambiente, dando lugar a una señal rápida consistente en dos fotones coincidentes de 511 keV. Ésta era la señal de centelleo rápida, y se podía detectar con dos detectores de centelleo colocados encima y debajo del tanque "diana". Los iones de cadmio disueltos en el agua eran el objetivo de los neutrones, que una vez termalizados tenían una gran probabilidad de ser capturados por dichos núcleos atómicos, lo que resultaba en una señal "retardada" (con respecto a la rápida de los positrones), con emisión de rayos gamma de unos 8 MeV, que venían detectados unos pocos microsegundos tras la señal de la aniquilación del positrón. El experimento probó la existencia de los neutrinos, pero no apuntaba a medir el flujo total, ya que solo en torno al 3% de los antineutrinos producidos por un reactor nuclear típico tienen suficiente energía (>1.8 MeV) como para dar lugar a una reacción de decaimiento beta inverso.

Más recientemente, detectores mucho más grandes y sofisticados utilizan el sistema de centelleo, no solo en la observación de neutrinos sino también para otros objetivos. KamLAND, por ejemplo, usa detección de centelleo para estudiar las oscilaciones de antineutrinos de 53 reactores nucleares japoneses. Borexino es un detector que utiliza el centelleador orgánico líquido (pseudocumeno con difeniloxazolo) con menor concentración de elementos radiactivos de cualquier material en el mundo. Gracias a él, es capaz de detectar y separar las componentes de neutrinos provenientes del Sol (la fuente natural más importante de neutrinos), a través de la dispersión elástica ("elastic scattering") de los neutrinos de baja energía contra los electrones deslocalizados en los orbitales de los anillos bencénicos de las moléculas aromáticas de su centelleador, mediada por las interacciones de corriente cargada (CC, mediada por los bosones W) para los neutrinos electrónicos, y en menor medida por las interacciones de corriente neutra (NC, mediada por el bosón neutro Z) para el resto de sabores de neutrinos (muónicos y tauónicos):

formula_7 (interacción de corriente cargada)

formula_8 (interacción de corriente neutra, donde x=e,µ,τ)

Borexino también es sensible a la reacción de decaimiento beta inverso para observar antineutrinos de reactores nucleares de todo el mundo, provenientes del interior de la propia Tierra, o de material radiactivo concentrado cerca del detector, como el generador de antineutrinos para el estudio de las oscilaciones a corta distancia de su programa experimental SOX.

Sin embargo, en 1967 Raymond Davis logró dar con un sistema de detección. Observó que el cloro-37 era capaz de absorber un neutrino para convertirse en argón-37 tal y como se muestra en la ecuación siguiente:

Naturalmente, ésta no era la única reacción entre los neutrinos y la materia ordinaria. Lo que tenía de especial el cloro-37 es que cumplía ciertos requisitos para poderse usar en un futuro detector.

Normalmente el cloro-37 aparece mezclado con otros isótopos. Particularmente con el cloro-35, el más abundante. Además, se puede tener mezclado con otros átomos o moléculas, siempre conociendo su proporción. Para evitar mediciones falsas debidas al argón-37 ya presente en la mezcla, el primer paso fue efectuar un limpiado del producto. Hecho esto, se debía dejar reposar la mezcla de cloro-37 durante unos meses hasta que llegaba a una situación estacionaria. Esto es cuando la cantidad de argón que se desintegra se iguala a la cantidad que se forma. El momento de equilibrio vendrá determinado por el periodo de semidesintegración.

Para proteger al detector del ruido de fondo producido por la radiación cósmica se enterró el tanque de la mezcla clorada en una mina de oro de Dakota del Sur a mucha profundidad. Sin embargo, las primeras observaciones solo dieron cotas superiores, compatibles aún con cero. Los resultados eran menores a lo esperado y se confundían con el ruido. Tras repetidos aumentos en la sensibilidad de los instrumentos y en la pureza de la mezcla de cloro-37 se logró, por fin, calcular que nos llegaba aproximadamente un tercio del flujo esperado. Estos resultados no fueron tomados muy en serio en un principio, por lo que se prosiguió experimentando con mezclas mejores pero también más caras basadas en el galio o el boro.

Las dudas acerca de los métodos utilizados por Davis incentivaron la búsqueda de alternativas para la detección de tan escurridizas partículas. Así surgió una nueva línea de detectores que se basaban en la colisión de neutrinos con electrones contenidos en un medio acuoso.

Estos detectores se basan en el hecho de que el neutrino al impactar contra un electrón le transmite parte de su momento confiriéndole a este una velocidad en ocasiones superior a la de la luz en ese mismo medio acuoso. Es en ese momento cuando se produce una emisión de luz característica, conocida como radiación de Cherenkov, que es captada por los fotomultiplicadores que recubren las paredes del recipiente. Como lo que se observa es una transmisión de momento lineal podemos inferir aproximadamente la masa de estos y la dirección de la que proceden mientras que con el anterior sistema de detección solo podíamos calcular el flujo de neutrinos.

Es el detector de neutrinos más famoso. Recibe su nombre por la mina japonesa en la que se encuentra Kamioka a 1000 metros de profundidad. Consiste en un cilindro de 39,3 metros de diámetro y 41 metros de alto cuyas paredes están cubiertas por 11 200 multiplicadores para detectar la luz del efecto Cherenkov. Esta lleno de 50 000 toneladas de agua pura que sirven para provocar la interacción con los neutrinos.
Lo primero que se hizo fue detectar los neutrinos procedentes de la supernova 1987A. Luego se midió el flujo de los neutrinos solares corroborando los resultados del detector de Davis. Fue con el experimento de la supernova con el que el laboratorio se hizo famoso al poder determinar que la masa del neutrino no era nula llegando a acotar su valor (que no medirlo con exactitud) a partir de la medición del retraso con que llegaron los neutrinos procedentes de la explosión. Si estos hubiesen carecido de masa hubiesen llegado junto a los fotones (la luz de la supernova). 
Pero lo que les ha dado la fama mundial han sido los experimentos que demuestran la oscilación de los neutrinos y por lo que su director Takaaki Kajita recibió el Premio Nobel de Física 2015 junto al director del Observatorio de Neutrinos de Subdury en Canadá.

Este detector de neutrinos consiste en una esfera de 17.8 metros de diámetro situada a 2.100 metros de profundidad en la mina Creighton, en Subdury, Ontario, Canadá. En vez de agua convencional se usa agua pesada porque ésta tiene más probabilidades de interactuar con los neutrinos, encerrada en una esfera acrílica de 12 metros de diámetro y con una capacidad para 1.000 toneladas. Alrededor de este recipiente, hasta rellenar el detector, existe agua normal pura para darle flotación y como escudo anti radiación.
Sus resultados también demuestran el fenómeno de la oscilación de los neutrinos por lo que su director Arthur B. McDonald recibió también el .




</doc>
<doc id="3443" url="https://es.wikipedia.org/wiki?curid=3443" title="1979">
1979

1979 () fue un en el calendario gregoriano.
Fue designado:










































</doc>
<doc id="3444" url="https://es.wikipedia.org/wiki?curid=3444" title="1973">
1973

1973 () fue un año común comenzado en lunes y fue designado como el "Año del Búfalo", según el horóscopo chino.




La inacción del ejército libanés lleva a la renuncia del primer ministro Saib Salam, un musulmán suní.




















































































3 de enero: Se funda en Nicoya la Asociación Deportiva Guanacasteca el cual milita en la Segunda División de Costa Rica
























</doc>
<doc id="3447" url="https://es.wikipedia.org/wiki?curid=3447" title="Ruptura espontánea de simetría electrodébil">
Ruptura espontánea de simetría electrodébil

El concepto de ruptura espontánea de simetría es uno de los ingredientes fundamentales del SM electrodébil, dando lugar a excitaciones de Goldstone que pueden ser asociadas a los términos de masa de los bosones gauge. Este procedimiento, conocido habitualmente como Mecanismo de Higgs, es uno de los posibles procedimientos para describir las interacciones débiles de rango corto mediante una teoría gauge sin destruir su invariancia.

En el SM la ruptura de simetría tiene lugar linealmente por medio de un campo escalar que adquiere un valor esperado no nulo en el vacío. Como resultado del proceso no solo adquieren masa tanto los bosones vectoriales así como los fermiones, sino que, además, aparece un nuevo campo escalar neutro físico: la partícula de Higgs.

Alternativamente la ruptura de simetría podría generarse dinámicamente por nuevas fuerzas fuertes en la escala 1 TeV. Sin embargo, aún no se ha formulado ningún modelo válido de este tipo que proporcione una descripción satisfactoria del sector fermiónico y reproduzca la elevada precisión de las medidas electrodébiles.

Son dos los conceptos básicos sobre los cuales se ha construido el “Modelo Estándar”, la teoría que unifica parcialmente las fuerzas de la naturaleza. Tales principios son:

Un ejemplo habitual en física es el de un lápiz que se mantiene en equilibrio sobre su punta. Es simétrico en el sentido de que mientras mantiene el equilibrio sobre la punta cualquier dirección es tan buena como cualquier otra; sin embargo, es inestable. Cuando el lápiz cae, algo que debe ocurrir inevitablemente, caerá al azar, en una u otra dirección, rompiendo la simetría, aunque la simetría sigue ahí, en leyes subyacentes.

Las leyes sólo describen el espacio de lo que puede ocurrir; el mundo real gobernado por esas leyes supone la elección de una realización entre muchas posibilidades. Intercambiamos la inestable libertad de las posibilidades por la estable experiencia de la realidad.

Este mecanismo de ruptura espontánea de la simetría puede ocurrirles a las simetrías entre las partículas de la naturaleza. Cuando les sucede a las simetrías que, según el principio gauge, hacen aparecer a las fuerzas de la naturaleza, conduce a diferencias en sus propiedades. Las fuerzas se vuelven diferenciadas, pueden tener diferentes alcances e intensidades.

Antes de que se rompa la simetría, las cuatro interacciones fundamentales tienen un alcance infinito, igual que el electromagnetismo, pero tras la ruptura, el alcance de alguna de ellas es finito, como las dos interacciones nucleares (fuerte y débil).

Los físicos F. Englert y R. Brout, en Bélgica, y unos meses más tarde Peter Higgs, en Escocia, propusieron, en forma independiente, combinar la ruptura espontánea de simetría con las teorías gauge. Los tres demostraron también la existencia de otra partícula consecuencia de la ruptura espontánea de simetría, y que denominamos «bosón de Higgs».

En la ruptura espontánea de simetría existe una cantidad física cuyo valor nos indica que la simetría se ha roto y cómo se ha producido esa ruptura. Esta cantidad suele ser un campo, llamado campo de Higgs.

La utilización de la ruptura espontánea de la simetría de una teoría fundamental tendría unas repercusiones muy profundas, no solo para las leyes de la naturaleza, sino también en la más amplia cuestión acerca de en qué consiste una ley de la naturaleza.

Antes se creía que las leyes eternas de la naturaleza determinaban de forma directa las propiedades de las partículas elementales, ahora bien, en una teoría con ruptura espontánea de la simetría aparece un nuevo elemento: las propiedades de las partículas elementales dependen en parte de la historia y del entorno.

La simetría puede romperse de diferentes maneras, dependiendo de condiciones como la densidad y la temperatura. Expresándolo de manera más general, las propiedades de las partículas elementales no dependen sólo de las ecuaciones de la teoría, sino también de cuál de las soluciones a estas ecuaciones es aplicable a nuestro universo.



</doc>
<doc id="3449" url="https://es.wikipedia.org/wiki?curid=3449" title="Litro">
Litro

El litro (símbolos L o l) es una unidad de volumen del sistema métrico decimal, aceptada por el SI, igual a 1 decímetro cúbico (dm³), 1000 centímetros cúbicos (cm³) o 1/1000 metros cúbicos. Un decímetro cúbico (o litro) ocupa un volumen de (véase la figura) y, por tanto, es igual a una milésima de un metro cúbico.

El sistema métrico francés original usaba el litro como unidad base. La palabra "litro" se deriva de una unidad francesa más antigua, el "litron", cuyo nombre proviene del griego, (donde era una unidad de peso, no de volumen) a través del latín, y que equivalía a aproximadamente 0,831 litros. El litro también se usó en varias versiones posteriores del sistema métrico y se acepta para su uso con el SI, aunque no sea una unidad del SI, ya que la unidad de volumen del SI es el metro cúbico (m³).

Un litro de agua líquida tiene una masa de casi exactamente un kilogramo, porque el kilogramo se definió originalmente en 1795 como la masa de un decímetro cúbico de agua a la temperatura de fusión del hielo. Las redefiniciones posteriores del metro y el kilogramo suponen que esta relación ya no es exacta.

Un litro se define como "un nombre especial para el decímetro cúbico" o 10 centímetros × 10 centímetros × 10 centímetros, (1  L ≡ 1 dm³ ≡ 1000 cm³). Por lo tanto 1 L ≡ 0,001 m³ ≡ 1000 cm³, y 1  m³ (es decir, un metro cúbico, que es la unidad SI para el volumen) es exactamente 1000 L.

Desde 1901 hasta 1964, el litro se definió como el volumen de un kilogramo de agua pura a su densidad máxima y presión estándar. El kilogramo se especificaba a su vez como la masa de un cilindro de platino/iridio que se conserva en Sèvres, en Francia, y se suponía que tenía la misma masa que un litro de agua, como se ha mencionado anteriormente. Posteriormente, se descubrió que el cilindro era aproximadamente 28 partes por millón mayor y, por lo tanto, durante este tiempo, un litro era aproximadamente 1,000028 dm³. Además, la relación masa-volumen del agua (como la de cualquier fluido) depende de la temperatura, la presión, la pureza y la uniformidad isotópica. En 1964, se abandonó la definición de litro que se refería a la masa en favor de la actual. Aunque el litro no es una unidad SI, es aceptado por el CGPM (el organismo normalizador que define el SI) para su uso con el SI, por lo que también define el litro y sus símbolos aceptables.

Un litro es igual en volumen al "milistere", una unidad métrica no-SI obsoleta usada habitualmente para medir áridos.

Los litros se usan más comúnmente para productos (fluidos y sólidos que pueden verterse) que se miden por la capacidad o el tamaño de su contenedor, mientras que los metros cúbicos (y las unidades derivadas) se usan más comúnmente para artículos medidos por sus dimensiones o sus desplazamientos. El litro también se usa en algunas mediciones calculadas, como la densidad (kg/L), que permite una fácil comparación con la densidad del agua.

Un litro de agua tiene una masa de casi exactamente un kilogramo cuando se mide a su densidad máxima, que se produce a aproximadamente 4 °C. Del mismo modo: un mililitro (1 ml) de agua tiene una masa de aproximadamente 1 g; 1000 litros de agua tienen una masa de unos 1000 kg (1 tonelada). Esta relación se mantiene porque el gramo se definió originalmente como la masa de 1 mL de agua; sin embargo, esta definición se abandonó en 1799 porque la densidad del agua cambia con la temperatura y, muy ligeramente, con la presión.

Ahora se sabe que la densidad del agua también depende de las relaciones isotópicas de los átomos de oxígeno e hidrógeno en una muestra particular. Las mediciones modernas del agua de océano media normalizada de Viena, que es agua destilada pura con una composición isotópica representativa de la media de los océanos del mundo, muestran que tiene una densidad de en su punto de máxima densidad (3,984 °C) bajo una atmósfera estándar (760 Torr, 101,325 kPa) de presión.

El litro puede ser usado con cualquier prefijo del SI. El más frecuentemente usado es el mililitro, definido como la milésima parte del litro (un centímetro cúbico) o el hectolitro (cien litros) usado por los cosecheros de vino o de producción industrial de cerveza. Otras unidades pueden verse en la tabla, las más frecuentes en negrilla.


Originalmente, el único símbolo para el litro era l (letra L minúscula), siguiendo la convención del SI de que solo los símbolos de unidad que abrevian el nombre de una persona comienzan con una letra mayúscula. Sin embargo, en muchos países de habla inglesa, la forma más común del dígito árabe 1 manuscrito es simplemente un trazo vertical; es decir, carece del trazo oblicuo agregado en muchas otras culturas. Por lo tanto, el dígito "1" puede confundirse fácilmente con la letra "l". Además, en algunas máquinas de escribir, particularmente las más antiguas, se utilizaba la tecla L minúscula para escribir el número 1. Incluso en algunas fuentes de ordenador, los dos caracteres son apenas distinguibles. Esto causó cierta preocupación, especialmente en la comunidad médica.

Como resultado, se adoptó L (letra L mayúscula ) como símbolo alternativo para el litro en 1979. El Instituto Nacional de Estándares y Tecnología de los Estados Unidos ahora recomienda el uso de la letra L mayúscula, una práctica que también se sigue ampliamente en Canadá y Australia. En estos países, el símbolo L también se usa con prefijos, como en mL y μL, en lugar de los ml y μl tradicionales utilizados en Europa. En el Reino Unido e Irlanda, así como en el resto de Europa, la letra "l" minúscula se usa con prefijos, aunque los litros enteros a menudo se escriben en su totalidad (por lo tanto, «750 ml» en una botella de vino, pero a menudo «1 litro» en un envase de zumo). En 1990, el CIPM declaró que era demasiado pronto para elegir un solo símbolo para el litro.

Antes de 1979, el símbolo ℓ (l minúscula en cursiva, U+2113), entró en uso común en algunos países; por ejemplo, fue recomendado por la publicación M33 del organismo de normalización de Sudáfrica y por Canadá, en la década de 1970. Este símbolo aún puede encontrarse ocasionalmente en algunos países de habla inglesa y europeos como Alemania, y su uso es omnipresente en Japón y Corea del Sur. Las fuentes que incluyen los caracteres CJK, por lo general, incluyen no solo la cursiva ℓ sino también cuatro caracteres precompuestos: ㎕, ㎖, ㎗ y ㎘ (U+3395 a U+3398) para el microlitro, mililitro, decilitro y kilolitro.

El primer nombre del litro era ""cadil""; los patrones se muestran en el Museo de Artes y Oficios de París. 

El litro se introdujo en Francia en 1795 como una de las nuevas "unidades de medida republicanas" y se definió como un decímetro cúbico. El decímetro original tenía una longitud de 44,344 "lignes", que se revisó en 1798 a 44,3296 "lignes". Esto hizo que el litro original 1,000974 decímetros cúbicos de hoy. Fue con este litro como referencia con el que se definió el kilogramo.

En 1879, el CIPM adoptó la definición del litro, con el símbolo l (letra minúscula L).

En 1901, en la 3.ª conferencia del CGPM, el litro se redefinió como el espacio ocupado por 1 kg de agua pura a la temperatura de su densidad máxima (3,98 °C) bajo una presión de 1 atm.

En 1964, en la 12.ª conferencia CGPM, la definición original se revirtió, y así el litro se definió una vez más en relación exacta con el metro, como otro nombre para el decímetro cúbico, es decir, exactamente 1 dm³.

En 1979, en la 16.ª conferencia de la CGPM, se adoptó el símbolo alternativo L (letra mayúscula L).

La abreviatura cc (para centímetro cúbico, igual a un mililitro o mL) es una unidad del sistema CGS, que precedió al sistema MKS, que luego evolucionó hacia el sistema SI. La abreviatura cc todavía se usa comúnmente en muchos campos, incluida la dosificación médica y el tamaño para pequeñas Cilindradas de motores de combustión, como los que se usan en las motocicletas.

El microlitro (μL) ha sido conocido en el pasado como lambda (λ), pero ahora se desaconseja este uso. En el campo médico, el microlitro a veces se abrevia como mcL en los resultados de las pruebas. 

En el sistema SI, se prefiere el uso de prefijos para potencias de 1000 y todos los demás múltiplos no se recomiendan. Sin embargo, en los países donde el sistema métrico se estableció mucho antes de la adopción del estándar SI, ya se establecieron otros múltiplos, su uso sigue siendo común. En particular, el uso de los prefijos centi (10 ), deci (10 ), deca (10 ) y hecto (10 ) todavía son comunes. Por ejemplo, en muchos países europeos, el hectolitro es la unidad típica para la producción y exportación de volúmenes de bebidas (leche, cerveza, refrescos, vino, etc.) y para medir el tamaño de las capturas y las cuotas de los barcos de pesca; los decilitros son comunes en Suiza y Escandinavia y algunas veces se encuentran en libros de cocina; los centilitros indican la capacidad de los vasos y de las botellas pequeñas. En neerlandés coloquial en Bélgica, un "vijfentwintiger" y un "drieëndertiger" (literalmente "veintiocho" y "treinta y tres") son los vasos de cerveza comunes, las botellas correspondientes citan 25 cL o 33 cL. Las botellas también pueden ser de 75 cL o "media botella", 37,5 cL para cervezas 'artesanales' o 70  cL para vinos o licores. Las latas vienen en 25 cL, 33 cL y 50 cL.

En los países donde se adoptó el sistema métrico como el sistema de medición oficial después de que se estableció el estándar SI, el uso común sigue más de cerca las convenciones contemporáneas del SI. Por ejemplo, en Canadá, Australia y Nueva Zelanda, las bebidas de consumo se etiquetan casi exclusivamente con litros y mililitros. Los hectolitros a veces aparecen en la industria, pero los centilitros y decilitros rara vez, si es que se usan. Una excepción es la patología, donde, por ejemplo, el nivel de plomo en sangre puede medirse en microgramos por decilitro. Los volúmenes más grandes se dan generalmente en metros cúbicos (equivalente a 1 kL) o miles o millones de metros cúbicos.

Aunque los kilolitros, megalitros y gigalitros se usan comúnmente para medir el consumo de agua, la capacidad de los reservorios y los caudales de los ríos, para volúmenes más grandes de líquidos, como el consumo anual de agua del grifo, camiones cisterna o piscinas, el metro cúbico es la unidad general. Lo mismo para todos los volúmenes de naturaleza no líquida.

Los campos donde se utilizan el litro y el mililitro como medida para volúmenes no líquidos, donde se indica la capacidad del contenedor, incluyen: 





</doc>
<doc id="3454" url="https://es.wikipedia.org/wiki?curid=3454" title="Interpretaciones de la mecánica cuántica">
Interpretaciones de la mecánica cuántica

Una interpretación de la mecánica cuántica es un conjunto de afirmaciones que tratan sobre la completitud, determinismo o modo en que deben entenderse los resultados de la mecánica cuántica y los experimentos relacionados con ellas. Aunque las predicciones básicas de la mecánica cuántica han sido confirmadas extensivamente por experimentos muy precisos, algunos científicos consideran que algunos aspectos del entendimiento que ésta proporciona son insatisfactorios y requieren explicaciones o interpretaciones adicionales que permitan un reconocimiento más cercano a la intuición de los resultados de los experimentos.

Los problemas sobre cómo deben entenderse ciertos aspectos de la mecánica cuántica son tan agudos que existen una serie de escuelas alternativas, que difieren por ejemplo en cuanto a si la teoría es subyacentemente determinista, o si algunos elementos tienen o no realidad objetiva, o si la teoría proporciona una descripción completa de un sistema físico.

El gran problema lo constituye el proceso de medición. En la física clásica, "medir" significa revelar o poner de manifiesto propiedades que estaban en el sistema desde antes de que midamos.

En mecánica cuántica el proceso de medición altera de forma incontrolada la evolución del sistema. Constituye un error pensar dentro del marco de la física cuántica que medir es revelar propiedades que estaban en el sistema con anterioridad. La información que nos proporciona la función de onda es la distribución de probabilidades, con la cual se podrá medir tal valor de tal cantidad. Cuando medimos ponemos en marcha un proceso que es indeterminable a priori, lo que algunos denominan azar, ya que habrá distintas probabilidades de medir distintos resultados. Esta idea fue y es aún objeto de controversias y disputas entre los físicos, filósofos y epistemólogos. Uno de los grandes objetores de esta interpretación fue Albert Einstein, quien a propósito de esta idea dijo su famosa frase ""Dios no juega a los dados"".

Independientemente de los problemas de interpretación, la mecánica cuántica ha podido explicar esencialmente todo el mundo microscópico y ha hecho predicciones que han sido probadas experimentalmente de forma exitosa, por lo que es una teoría unánimemente aceptada.

El problema de la medida se puede describir informalmente del siguiente modo:

Eso plantea un problema serio, si las personas, los científicos u observadores son también objetos físicos como cualquier otro, debería haber alguna forma determinista de predecir cómo tras juntar el sistema en estudio con el aparato de medida, finalmente llegamos a un resultado determinista. Pero el postulado de que ""una medición destruye la coherencia de un estado inobservado e inevitablemente tras la medida se queda en un estado mezcla impredecible"", parece que sólo nos deja 3 salidas:


El enunciado anterior, ""una medición destruye la coherencia de un estado inobservado e inevitablemente tras la medida se queda en un estado mezcla impredecible, parece que sólo nos deja 3 salidas"", es demasiado arriesgado y no probado. Si partimos de que las entidades fundamentales que constituyen la materia, precisamente, y al contrario de lo que deduce (B) no tienen consciencia de sí mismas, y sin preferencia alguna por el determinismo o el caos absoluto, sólo pueden encontrar el equilibrio comportándose según leyes de probabilidad o lo que es lo mismo por leyes de "caos determinado". En la práctica cualquier defensa o negación de la teoría cuántica no responde a razonamientos matemáticos deductivos sino a impresiones o sugestiones con origen en axiomas filosóficos totalmente arbitrarios. Cabe notar que p.ej, la palabra "equilibrio" en este párrafo puede o no tener sentido y el valor de realidad que se conceda al mismo no está sujeto a demostración matemática alguna.

Comúnmente existen diversas interpretaciones de la mecánica cuántica, cada una de las cuales en general afronta el problema de la medida de manera diferente. De hecho si el problema de la medida estuviera totalmente resuelto no existirían algunas de las interpretaciones rivales. En cierto modo la existencia de diferentes interpretaciones refleja que no existe un consenso sobre cómo plantear precisamente el problema de la medida. Algunas de las interpretaciones más ampliamente conocidas son las siguientes:




</doc>
<doc id="3457" url="https://es.wikipedia.org/wiki?curid=3457" title="Meteorología">
Meteorología

La meteorología (del griego μετέωρον "metéōron" ‘alto en el cielo’, ‘meteoro’; y λόγος "lógos" ‘conocimiento’, ‘tratado’) es la ciencia interdisciplinaria, de la física de la atmósfera, que estudia el estado del tiempo, el medio atmosférico, los fenómenos producidos y las leyes que lo rigen.

La Tierra está constituida por tres partes fundamentales: una parte sólida llamada litosfera, otra cubierta por agua llamada hidrosfera y una tercera, que envuelve a las dos anteriores, conformada por una capa gaseosa denominada atmósfera. Éstas se relacionan entre sí produciendo modificaciones profundas en sus características. La ciencia que estudia estas características, las propiedades y los movimientos de las tres capas fundamentales de la Tierra, es la geofísica. En ese sentido, la meteorología es una rama de la geofísica que tiene por objeto el estudio detallado de la envoltura gaseosa de la Tierra y los fenómenos que en ella ocurren.

Se debe distinguir entre las condiciones actuales y su evolución (lo cual constituye el tiempo atmosférico) y las condiciones medias durante un largo período (que se conoce como clima de un lugar o una región). En este sentido, la meteorología es una ciencia auxiliar de la climatología ya que los datos atmosféricos obtenidos en múltiples estaciones meteorológicas durante largo tiempo se usan para definir el clima, predecir el tiempo, comprender la interacción de la atmósfera con otros subsistemas, etc. El conocimiento de las variaciones meteorológicas y el impacto de las mismas sobre el clima ha sido siempre de suma importancia para el desarrollo de la agricultura, la navegación, las operaciones militares y la vida en general.

Desde la más remota antigüedad se tiene constancia de la observación de los cambios en la atmósfera y de otros componentes asociados con el movimiento de los astros, con las estaciones del año y con fenómenos relacionados. Los antiguos egipcios asociaban los ciclos de crecida del Nilo con los movimientos de las estrellas explicados por los movimientos de los dioses, mientras que los babilonios predecían el tiempo guiándose por el aspecto del cielo. Pero el término «meteorología» proviene de "Meteorologica", título del libro escrito alrededor del año 340 a. C. por Aristóteles, quien presenta observaciones mixtas y especulaciones sobre el origen de los fenómenos atmosféricos y celestes. Una obra similar, titulada "Libro de las señas", fue publicada por Teofrasto, un alumno de Aristóteles; se centraba en la observación misma de los fenómenos más que en la previsión del tiempo.

Los progresos posteriores en el campo meteorológico se centraron en que nuevos instrumentos, más precisos, se desarrollaran y pusieran a disposición. Galileo construyó un termómetro en 1607, seguido de la invención del barómetro por parte de Evangelista Torricelli en 1643. El primer descubrimiento de la dependencia de la presión atmosférica con relación a la altitud fue realizado por Blaise Pascal y René Descartes; la idea fue profundizada luego por Edmund Halley. El anemómetro, que mide la velocidad del viento, fue construido en 1667 por Robert Hooke, mientras que Horace de Saussure completa el elenco del desarrollo de los más importantes instrumentos meteorológicos en 1780 con el higrómetro a cabello, que mide la humedad del aire. Otros progresos tecnológicos, que son conocidos principalmente como parte del progreso de la física, fueron la investigación de la dependencia del volumen del gas sobre la presión, que conduce a la termodinámica, y el experimento de Benjamin Franklin con la cometa y el rayo. Franklin fue asimismo el primero en registrar de modo preciso y detallado las condiciones del tiempo en base diaria, así como en efectuar previsiones del tiempo sobre esa base.
El primero en definir de modo correcto la circulación atmosférica global fue George Hadley, con un estudio sobre los vientos alisios efectuado en 1735. En los inicios, ésta fue una comprensión parcial de cómo la rotación terrestre influye en la cinemática de los flujos de aire. Más tarde (en el siglo XIX), fue comprendida la plena extensión de la interacción a larga escala tras la fuerza del gradiente de presión y la deflexión causada por el efecto de Coriolis, que en forma conjunta dan origen al complejo movimiento tridimensional del viento. La fuerza de deflexión debe su nombre Gaspard-Gustave Coriolis, quien en una publicación de 1835 describió los resultados de un estudio sobre la energía producida por la máquina con partes en rotación, como la ruta del agua de los molinos. En 1856, William Ferrel hipotetizó la existencia de una «célula de circulación» en latitudes medias, en las cuales el aire se deflecta por la fuerza de Coriolis creando los principales vientos de los oestes. La observación sinóptica del tiempo atmosférico era aún compleja por la dificultad de clasificar ciertas características climáticas como las nubes y los vientos. Este problema fue resuelto cuando Luke Howard y Francis Beaufort introdujeron un sistema de clasificación de las nubes (1802) y de la fuerza del viento (1806), respectivamente. El verdadero punto de cambio fue la invención del telégrafo en 1843, lo cual permitió comenzar a intercambiar información sobre el tiempo meteorológico a velocidades inigualables.

A inicios del siglo XX, los progresos en la comprensión de la dinámica atmosférica llevaron al nacimiento de la previsión del tiempo llevada a cabo a partir de cálculos matemáticos. En 1922, Lewis Fry Richardson publicó "Weather prediction by numerical process", que describía cómo eliminar las variantes menos importantes de las ecuaciones de la dinámica de fluidos que regulaban los fluidos atmosféricos, permitía encontrar fácilmente soluciones numéricas, a pesar de que el número de los cálculos necesarios era muy grande. En el mismo periodo, un grupo de meteorólogos noruegos conducido por Vilhelm Bjerknes desarrolló un modelo para explicar la generación, la intensificación y la disolución de los ciclones en niveles medios de la atmósfera, introduciendo la idea del frente meteorológico y de las subdivisiones de las masas de aire. El grupo incluía a Carl-Gustaf Rossby (que fue el primero en explicar el flujo atmosférico a gran escala en términos de fluidodinámica), Tor Bergeron (el primero en comprender el mecanismo de formación de la lluvia) y Jacob Bjerknes.

En los años 1950, los experimentos de cálculo numérico con computador mostraron ser factibles. La primera previsión del tiempo realizada con este método usaba modelos barotrópicos (es decir, representaban a la atmósfera como una única capa) y podía prever con éxito los movimientos a gran escala de las ondas de Rossby. En los años 1960, la naturaleza caótica de la atmósfera fue comprendida por Edward Lorenz, fundador del campo de la teoría del caos. Los avances matemáticos obtenidos en este campo fueron retomados por la meteorología y contribuyeron a estabilizar el límite de predictibilidad del modelo atmosférico.

En los años recientes, se han estado desarrollando modelos climáticos a alta resolución, usados para estudiar los cambios a largo plazo, sobre todo el actual cambio climático. Sin embargo, hay que ser cuidadosos en este sentido: el clima es el promedio estadístico a largo plazo de los datos meteorológicos obtenidos en estaciones meteorológicas ubicadas en una zona determinada que presentan características similares y que definen un clima determinado. Esto se hace en todos los tipos climáticos de todo el mundo. Pero estos tipos climáticos no pueden condensarse en determinados modelos porque las variaciones a largo plazo de los mismos deben ser obtenidas a posteriori de dichas variaciones producidas a largo plazo. Dicho en otros términos: la información meteorológica obtenida en multitud de estaciones meteorológicas de todo el mundo sirve, de manera inductiva, para establecer las características climáticas con sus variantes en toda la superficie terrestre y una vez que las obtenemos podemos estudiar los cambios climáticos ocurridos en el pasado hasta el momento en el que se analizan, pero no podríamos usar esta información hacia el futuro porque la meteorología y la climatología trabajan a escalas distintas, como señala una institución científica tan cuidadosa en sus análisis como es la NASA al señalar la posible relación existente entre la cruda ola de frío en Europa y América del Norte en los primeros tres meses de 2014 (con extremos de temperaturas tan bajas que nunca se habían registrado en muchos lugares) y los modelos climáticos que nos hablan de un calentamiento global en el seno de la atmósfera.

Así, en el análisis hecho por la NASA de la ola de frío tan intensa que ha vivido el hemisferio norte (Europa y América del Norte) se señala que debemos ser muy cautos a la hora de especular la relación entre meteorología y climatología ya que las dos ciencias operan en escalas de tiempo distintas. En este análisis se señala que:

El desarrollo tecnológico obtenido en el perfeccionamiento de instrumentos y aparatos de detección y procesamiento de datos ha revolucionado la ciencia de la meteorología, especialmente en lo que respecta al empleo de los satélites meteorológicos, aviones de los denominados cazahuracanes, drones con fines también meteorológicos, satélites que recogen información sobre las corrientes marinas, temperatura superficial de mares y océanos y, sobre todo la recopilación, procesamiento de datos y proyección y pronósticos meteorológicos. Desde luego, todos estos avances se iniciaron en las últimas décadas del siglo XX (recordemos lo que significó el lanzamiento del satélite artificial TIROS I (Television Infra-Red Observation Satellite) en 1960 pero ello no fue sino el punto de partida de una nueva era, que ha dejado muy atrás el estado de la ciencia (en este caso de la meteorología) que sigue difundiéndose en las escuelas y en la bibliografía especializada. Y no solo nos vamos quedando atrás en el campo de la formación científica y técnica, sino también en los programas de investigación y desarrollo, aunque en esto último exista una gran diversidad de situaciones a escala mundial. []

La meteorología incluye el estudio (descripción, análisis y predicción) de las variaciones diarias de las condiciones atmosféricas a gran escala o Meteorología sinóptica, el estudio de los movimientos en la atmósfera involucrados en la dinámica atmosférica y su evolución temporal basada en los principios de la mecánica de fluidos (Meteorología dinámica, muy relacionada actualmente con la meteorología sinóptica), del estudio de la estructura y composición de la atmósfera, así como las propiedades eléctricas, ópticas, termodinámicas, radiactivas y otras (Meteorología física), la variación de los elementos meteorológicos cerca de la Tierra en un área pequeña (Micrometeorología), el estudio específico de los fenómenos meteorológicos de la zona intertropical (Meteorología tropical) y otros muchos fenómenos. El estudio de las capas más altas de la atmósfera (superiores a los 20 o 25 km) acostumbra a implicar el uso de técnicas y disciplinas especiales, y recibe el nombre de aeronomía. El término aerología se aplica al estudio de las condiciones atmosféricas a cualquier altura.

La meteorología aplicada tiene por objeto acopiar constantemente un máximo de datos sobre el estado de la atmósfera y, a la luz de los conocimientos y leyes de la meteorología teórica, analizarlos, interpretarlos y obtener deducciones prácticas, especialmente para prever el tiempo con la máxima antelación. Como la atmósfera es una inmensa masa gaseosa sujeta a variaciones constantes, que la mayoría de las veces se producen en el ámbito regional, su estado en un momento dado sólo puede ser conocido si se dispone de una red suficientemente densa de puestos de observación o estaciones meteorológicas, distribuidas por todas las regiones del globo, que a horas fijas efectúan las mismas mediciones (temperatura, presión, humedad, viento, precipitaciones, radiación solar, nubosidad, etc.) y transmiten los resultados a los centros encargados de utilizarlos.

Los concernientes a la climatología y la previsión del tiempo. Su campo de estudios abarca, por ejemplo, las repercusiones en la Tierra de los rayos solares, la radiación de energía calorífica por el suelo terrestre, los fenómenos eléctricos que se producen en la ionosfera, los de índole física, química y termodinámica que afectan a la atmósfera, los efectos del tiempo sobre el organismo humano, etc.

Los temas de la meteorología teórica se fundan, en primer lugar, sobre un conocimiento preciso de las distintas capas de la atmósfera y de los efectos que producen en ella los rayos solares. En particular, los meteorólogos establecen el balance energético que compara la energía solar absorbida por la Tierra con la energía irradiada por ésta y disipada en el espacio interestelar. Todo estudio ulterior implica, por lo demás, un conocimiento de las repercusiones que tienen los movimientos de la Tierra sobre el tiempo, los climas, la sucesión de las estaciones. También dan lugar a profundos estudios teóricos los dos parámetros principales relativos al aire atmosférico: la presión y la temperatura, cuyos gradientes y variaciones han de ser conocidos con la mayor precisión.

En lo concerniente a la evolución del tiempo, tiene especial importancia el estudio del agua atmosférica en sus tres formas: (gaseosa, líquida y sólida), así como las condiciones y circunstancias que rigen sus cambios de estado (calor latente de evaporación, de fusión, etc.), de la estabilidad e inestabilidad del aire húmedo, de las nubes y las precipitaciones.

Otra rama fundamental se esfuerza en determinar las leyes que rigen la circulación general de la atmósfera, la formación y los movimientos de las masas de aire, el viento y las corrientes en general, la turbulencia del aire, las condiciones en que se forman y mueven los frentes, anticiclones, ciclones y otras perturbaciones, así como los procesos que dan lugar a los meteoros.

En general, cada ciencia tiene su propio equipamiento e instrumental de laboratorio. Sin embargo, la meteorología es una disciplina corta en equipos de laboratorio y amplia en los equipos de observación en campo. En algunos aspectos esto puede parecer bueno, pero en realidad puede hacer que simples observaciones se desvíen hacia una afirmación errónea.

En la atmósfera, hay muchos objetos o cualidades que pueden ser medidos. La lluvia, por ejemplo, ha sido observada en cualquier lugar y desde siempre, siendo uno de los primeros fenómenos en ser medidos históricamente.

Una estación meteorológica es una instalación destinada a medir y registrar regularmente diversas variables meteorológicas. Estos datos se utilizan tanto para la elaboración de predicciones meteorológicas a partir de modelos numéricos como para estudios climáticos. Está equipada con los principales instrumentos de medición, entre los que se encuentran los siguientes:

Estos instrumentos se encuentran protegidos en una casilla ventilada, denominada abrigo meteorológico o "pantalla de Stevenson", la cual mantiene la luz solar directa lejos del termómetro y al viento lejos del higrómetro, de modo que no se alteren las mediciones de estos.

Cuanto más numerosas sean las estaciones meteorológicas, más detallada y exactamente se conoce la situación. Hoy en día, gran cantidad de ellas cuentan con personal especializado, aunque también hay un número de estaciones automáticas ubicadas en lugares inaccesibles o remotos, como regiones polares, islotes deshabitados o cordilleras. Además existen "fragatas meteorológicas", barcos que contienen a bordo una estación meteorológica muy completa y a los cuales se asigna una posición determinada en pleno océano. Sin embargo, es necesario recalcar que, con el gran crecimiento de la población urbana desde fines del siglo XIX, la mayor parte de las estaciones meteorológicas están actualmente situadas en zonas urbanas, bien porque se ubican en ciudades nuevas o bien porque se encuentran en poblaciones rurales absorbidas por los grandes núcleos urbanos en su proceso de expansión, con lo que existe un sesgo introducido por los microclimas urbanos que dan pie para corroborar, de manera errónea, el aumento de las temperaturas a escala mundial (lo que sería una prueba del calentamiento global).

Los satélites meteorológicos son un tipo de satélite artificial utilizados para supervisar el tiempo atmosférico y el clima de la Tierra, aunque también son capaces de ver las luces de la ciudad, incendios forestales, contaminación, auroras, tormentas de arena y polvo, corrientes del océano, etc. Otros satélites pueden detectar cambios en la vegetación de la Tierra, el estado del mar, el color del océano y las zonas nevadas.

El fenómeno de El Niño y sus efectos son registrados diariamente en imágenes satelitales. El agujero de ozono de la Antártida es dibujado a partir de los datos obtenidos por los satélites meteorológicos. De forma agrupada, los satélites meteorológicos de China, Estados Unidos, Europa, Canadá, India, Japón y Rusia proporcionan una observación casi continua del estado global de la atmósfera, aunque a una escala muy detallada en la que pueden identificarse los patrones nubosos y la circulación de los vientos, así como los flujos de energía que generan los fenómenos meteorológicos.

Varias veces por día, a horas fijas, los datos procedentes de cada estación meteorológica, de los barcos y de los satélites llegan a los servicios regionales encargados de centralizarlos, analizarlos y explotarlos, tanto para hacer progresar a la meteorología como para establecer previsiones sobre el tiempo clave que hará en los días venideros. Como las observaciones se repiten cada 3 horas (según el horario sinóptico mundial), la sucesión de los mapas y diagramas permite apreciar la "evolución sinóptica": se ve cómo las perturbaciones se forman o se resuelven, si están subiendo o bajando la presión y la temperatura, si aumenta o disminuye la fuerza del viento o si cambia éste de dirección, si las masas de aire que se dirigen hacia tal región son húmedas o secas, frías o cálidas, etc. Parece así bastante fácil prever la trayectoria que seguirán las perturbaciones y saber el tiempo que hará en determinado lugar al cabo de uno o varios días. En realidad, la atmósfera es una gigantesca masa gaseosa tridimensional, turbulenta y en cuya evolución influyen tantos factores que uno de estos puede ejercer de modo imprevisible una acción preponderante que trastorne la evolución prevista en toda una región. Así, la previsión del tiempo es tanto menos insegura cuanto menor es la anticipación y más reducido el espacio a que se refiere. Por ello la previsión es calificada de "micrometeorológica", "mesometeorológica" o "macrometeorológica", según se trate, respectivamente, de un espacio de 15 km, 15 a 200 km o más de 200 km. Las previsiones son formuladas en forma de boletines, algunos de los cuales se destinan a la ciudadanía en general y otros a determinados ramos de la actividad humana y navegación aérea y marítima, agricultura, construcción, turismo, deportes, regulación de los cursos de agua, ciertas industrias, prevención de desastres naturales, etc.




</doc>
<doc id="3461" url="https://es.wikipedia.org/wiki?curid=3461" title="Geomorfología">
Geomorfología

La geomorfología es una rama de la geografía y de la geología que tiene como objetivo el estudio de las formas de la superficie terrestre enfocado en describir, entender su génesis y su actual comportamiento.

Por su campo de estudio, la geomorfología tiene vinculaciones con otras ciencias. Uno de los modelos geomorfológicos más popularizados explica que las formas de la superficie terrestre son el resultado de un balance dinámico —que evoluciona en el tiempo— entre procesos constructivos y destructivos, dinámica que se conoce de manera genérica como ciclo geográfico.

La geomorfología se centra en el estudio de las formas del relieve, pero dado que estas son el resultado de la dinámica litosférica que en general integra, como insumos, conocimientos de otras ramas de la Geografía física, tales como la climatología, la hidrografía, la pedología, la glaciología, y también de otras ciencias, para abarcar la incidencia de fenómenos biológicos, geológicos y antrópicos, en el relieve. La geomorfología es una ciencia relacionada tanto con la geografía humana (por causa de los riesgos naturales y la relación hombre medio) como con la geografía matemática (por causa de la topografía).

En un comienzo inseparable de la geografía, la geomorfología toma forma a finales del siglo XIX de manos de quien fue su padre, el renombrado geógrafo William Morris Davis, quien también es considerado el padre de la geografía estadounidense. En su época la idea predominante sobre la creación del relieve se explicaba a través del catastrofismo como si fuera el supuesto de la gran inundación bíblica. Davis y otros geógrafos comenzaron a creer que otras causas eran responsables del modelamiento de la superficie de la Tierra y no eventos catastróficos. Davis, dentro del marco del uniformismo, desarrolló una teoría de la creación y destrucción del paisaje, a la que llamó ciclo geográfico. Trabajos tales como "The Rivers and Valleys of Pennsylvania", "The Geographical Cycle" y "Elementary Physical Geography", dieron un primer y fuerte impulso seguido por sus numerosos sucesores tales como Mark Jefferson, Isaiah Bowman, Curtis Marbut, quienes fueron consolidando la disciplina, sin dejar de participar en el contexto de la geografía y también profundizando en otras ciencias.

El relieve terrestre va evolucionando en la dinámica del ciclo geográfico mediante una serie de procesos constructivos y destructivos que se ven permanentemente afectados por la fuerza de gravedad que actúa como equilibradora de los desniveles; es decir, hace que las zonas elevadas tiendan a caer y colmatar las zonas deprimidas. Estos procesos hacen que el relieve transite por diferentes etapas. Los desencadenantes de los procesos geomorfológicos pueden categorizarse en cuatro grandes grupos:


Aunque los distintos factores que influyen en la superficie terrestre se ven incluidos en la dinámica del ciclo geográfico, solo los factores geográficos contribuyen siempre en dirección al desarrollo del ciclo y a su fin último; la penillanura. Mientras que el resto de los factores (biológicos, geológicos y antrópicos) interrumpen o perturban el normal desarrollo del ciclo. De la interacción de estos elementos resultan los procesos morfogenéticos o modelado, dividido en 3 etapas o tres procesos sucesivos, a saber, la erosión, el transporte y la sedimentación. Este proceso es, en gran parte, causante del modelado de la superficie terrestre teniendo en cuenta una serie de circunstancias.

Se puede subdividir, a su vez, en tres grandes enfoques: geomorfología estructural que trata de la caracterización y génesis de las “formas del relieve”, como unidades de estudio. La geomorfología dinámica, sobre la caracterización y explicación de los procesos de erosión y meteorización por los principales agentes (gravedad y agua). Y la geomorfología climática, sobre la influencia del clima sobre la morfogénesis (dominios morfoclimáticos).

De carácter descriptivo y clasificatorio en sus orígenes, la geomorfología fue evolucionando, como toda ciencia, hacia una disciplina exploratoria de las causas e interrelaciones entre procesos y formas. Desde la última mitad del siglo XX, gran sector de los geomorfólogos se ha enfocado particularmente en encontrar relaciones entre procesos y formas. Este enfoque, conocido como geomorfología dinámica, se ha visto beneficiado enormemente con el avance tecnológico paralelo y reducción de costos en equipos de medición y el incremento exponencial de la capacidad de procesamiento de las computadoras. La geomorfología dinámica trata de procesos elementales de erosión, de los agentes de transporte, del ciclo geográfico y de la naturaleza de la erosión.

Otras ramas de la geomorfología estudian diversos factores que ejercen una marcada influencia en las formas de la tierra como por ejemplo el efecto predominante del clima o la influencia de la geología en el relieve. Las principales son:

El éxito de la capacidad predictiva de algunos modelos y potenciales aplicaciones en los campos de planificación urbana, ingeniería civil, estrategias militares, desarrollo costero, entre varios más, da inicio en las últimas décadas a la geomorfología aplicada muy destacada en la geografía francesa, en especial gracias al instituto de Geografía Aplicada, fundado por Jean Tricart. Esta aplicación se centra básicamente en la interacción entre acciones humanas y las formas de la tierra, en particular enfocándose en el manejo de riesgos causados por cambios en la superficie de la tierra (naturales o inducidos) conocidos como georriesgos. Estudios de este tipo incluyen movimientos en masa, erosión de playas, mitigación de inundaciones, tsunamis entre otros.



</doc>
<doc id="3463" url="https://es.wikipedia.org/wiki?curid=3463" title="Geografía física">
Geografía física

La geografía física (conocida en un tiempo como fisiografía, término ahora en desuso) es la rama de la geografía que estudia en forma sistémica y espacial, la superficie terrestre considerada en su conjunto y específicamente, el espacio geográfico natural.

Constituye uno de los tres grandes campos del conocimiento geográfico; los otros son la geografía humana cuyo objeto de estudio comprende el espacio geográfico humanizado y la geografía regional que ofrece un enfoque unificador, estudiando los sistemas geográficos en forma integrada.

La geografía física se preocupa (según Strahler) de los procesos que son el resultado de dos grandes flujos de energía: el flujo de radiación solar que dirige las temperaturas de la superficie junto al movimiento de los fluidos, y el flujo de calor desde el interior de la Tierra que se manifiesta en los materiales de los estratos superiores de la corteza terrestre. Estos flujos interactúan en la superficie terrestre que es el campo de estudio del geógrafo físico. Son diversas las disciplinas geográficas que estudian en forma específica las relaciones de los componentes de la superficie terrestre. La geografía física enfatiza el estudio y la comprensión de los patrones y procesos geográficos del ambiente natural, haciendo abstracción por razones metodológicas del ambiente cultural que es el dominio de la Geografía humana. Ello significa que, aunque las relaciones entre estos dos campos de la Geografía existen y son muy importantes, cuando se estudia uno de dichos campos, es necesario excluir al otro de alguna manera, con el fin de poder profundizar el enfoque y los contenidos.

La metodología geográfica tiende a relacionar estos campos al proporcionar un marco seguro para la localización, distribución y representación del espacio geográfico además de emplear herramientas tales como los sistemas de información geográfica o el desarrollo de mapas que sirven a ambas especialidades.

Por otra parte, las ciencias con las que se relaciona y los métodos empleados suelen ser diferentes en los tres campos, aunque tienen en común el interés humano en conocer cada vez más y mejor el mundo en que vivimos.

Estos dos conceptos equivalen a los de estructuras y sistemas en la teoría de sistemas, siendo el de patrones un concepto similar al de estructuras y el de procesos uno similar al de sistemas. De nuevo solemos separar estos dos conceptos de manera individual por razones metodológicas ya que no suelen estar separados en la naturaleza. La diferencia entre procesos y patrones es que en el primer caso, resulta fundamental la escala temporal y en el segundo no es tan importante: cuando estamos estudiando los efectos de la erosión fluvial en los márgenes de un río consideramos a la erosión como un proceso, es decir, un fenómeno que ocurre a lo largo del tiempo. Por el contrario, cuando nos referimos a las características de la cuenca de un río, estamos haciendo un estudio de patrones espaciales, es decir, nos estamos refiriendo a un área determinada, con una extensión, relieve, clima, caudal, vegetación, etc., sin referirnos en detalle a cómo estos patrones han venido siendo modificados a lo largo del tiempo por los procesos geográficos. En el caso del glaciar de Aletsch pueden verse las lenguas de hielo, las morrenas intermedias y otras características estructurales del glaciar. Pero su lento movimiento y evolución constituyen la culminación actual de un proceso que es necesario analizar a través del tiempo.

Lo mismo sucede en el campo de la geología: la Geología histórica hace referencia a procesos que han ocurrido en el tiempo geológico, mientras que la Geología física hace referencia al presente: patrones estratigráficos o geológicos de la época actual y en la forma como se localizan en la superficie terrestre. En el caso de los estratos descubiertos por la erosión fluvial en el margen izquierdo de un río en los Cárpatos podemos ver los patrones estratigráficos típicos de las rocas sedimentarias, por ejemplo, una disposición estratigráfica normal sería cuando los estratos más recientes son los más altos de una sucesión estratigráfica.
La investigación científica progresa básicamente mediante el análisis: el estudio detallado de un fenómeno precede al de la comprensión general del mismo (síntesis), si bien no hay consenso total sobre esto. Así muchos geógrafos que consideran que la Geografía es una ciencia de síntesis han pretendido comenzar con la metodología holística apoyándose en investigaciones temáticas previas.
De este modo se puede ver las definiciones.


Las ciencias geográficas que estudian un componente específico del espacio natural en su relación con los demás son numerosas y entre las más importantes pueden citarse:


Debido al campo de estudio tan amplio de la geografía física, existen numerosas ciencias que están relacionadas con ella, entre las cuales podemos citar a:

A partir del nacimiento de la geografía como ciencia durante la época clásica griega y hasta fines del siglo XIX con el nacimiento de la antropogeografía o geografía humana, la geografía fue en parte una ciencia natural: el estudio descriptivo de localización y la toponimia de todos los lugares del mundo conocido. Diversas obras entre las más conocidas durante ese largo período podrían citarse como ejemplo, desde las de Estrabón ("Geografía"), Eratóstenes ("Geografía") o Dionisio Periegeta ("Periegesis Oiceumene") en la Edad Antigua hasta las de Alejandro de Humboldt ("Cosmos") en el siglo XIX, en las cuales se consideraba a la Geografía como una ciencia físico-natural; desde luego, pasando por la obra Summa de Geografía de Martín Fernández de Enciso () de comienzos del siglo XVI, donde aparece señalado por primera vez el Nuevo Mundo.

Entre los siglos XVIII y XIX, una polémica tomada de la Geología, entre los partidarios de James Hutton (Tesis del uniformismo) y de Georges Cuvier (catastrofismo) influyó poderosamente en el campo de la Geografía.

Dos procesos desarrollados durante el siglo XIX tuvieron una gran importancia en el desarrollo posterior de la geografía física: el primero se trata del imperialismo colonial europeo en Asia, África, Australia e incluso América, en busca de las materias primas exigidas por la Revolución industrial, que contribuyó a que se crearan y se invirtiera en los departamentos de geografía de las universidades de las potencias coloniales y al nacimiento y desarrollo de las sociedades geográficas nacionales, dando origen así al proceso identificado por Horacio Capel como la institucionalización de la Geografía (). Uno de los imperios más prolíficos en este sentido fue el Ruso. A mediados del siglo XVIII numerosos geógrafos son enviados por el almirantazgo ruso en diferentes oportunidades a realizar levantamientos geográficos en la zona del ártico siberiano. Entre estos se encuentra quien es considerado el patriarca de la geografía rusa: Mijaíl Lomonósov quien a mediados del decenio de 1750 comienza a trabajar en el Departamento de Geografía de la Academia de Ciencias para realizar investigaciones en Siberia; sus aportes en este sentido son notables: demuestra el origen orgánico de los suelos, desarrolla una ley general sobre el movimiento de los hielos que aún rige en lo básico, fundando así una nueva rama geográfica: la glaciología. En 1755 se funda, por iniciativa suya, la Universidad de Moscú, que ahora lleva su nombre, donde promueve el estudio de la geografía y la formación de geógrafos. En 1758 es nombrado director del Departamento de Geografía de la Academia de Ciencias, puesto desde el cual elaboraría una metodología de trabajo para el levantamiento geográfico, que guiaría por mucho tiempo las más importantes expediciones y estudios geográficos en el extenso territorio de Rusia. De esta manera la línea de Lomonosov siguió y los aportes de la escuela rusa se fueron multiplicando a través de sus discípulos; ya en el siglo XIX tenemos grandes geógrafos como Vasili Dokucháyev quien realizó obras de gran relevancia como “principio del análisis integral del territorio” y "Los Chernozem rusos", esta última la más importante, donde introduce el concepto geográfico de suelo, distinguiéndolo de un simple estrato geológico, y fundando de esta manera una nueva área de estudio geográfico: la Pedología o Edafología. La climatología recibiría también un fuerte impulso desde la escuela rusa a través de Wladimir Peter Köppen cuyo principal aporte, su clasificación climática, sigue vigente hoy en día, aunque con algunas modificaciones y mejoras. Por otra parte, este gran geógrafo físico también contribuyó a la Paleogeografía a través de su trabajo ""Los climas del pasado geológico"" por el cual se le considera el padre de la Paleoclimatología. Otros geógrafos rusos que realizaron grandes aportes a la disciplina en este periodo fueron N.M. Sibirtsev, Piotr Semiónov, K. D. Glinka, Neustrayev, entre otros.

El segundo proceso de importancia se trata de la teoría de la evolución formulada por Darwin a mediados de siglo (lo que influyó decisivamente en la obra de Ratzel, quien poseía una formación académica como zoólogo y era seguidor de las ideas de Darwin) lo que significó un importante impulso en el desarrollo de la Biogeografía.

Otro importante suceso a finales del siglo XIX y principios del siglo XX dará un importante impulso al desarrollo de la geografía y tendrá lugar en Estados Unidos. Se trata del trabajo del afamado geógrafo William Morris Davis quien no solo realizó importantes aportes al establecimiento de la disciplina en su país, sino que revolucionó el campo al desarrollar la teoría del ciclo geográfico la cual propuso como paradigma para la geografía en general, aunque en realidad sirvió sólo como paradigma para la geografía física. Su teoría explicaba que las montañas y demás accidentes geográficos están modelados por la influencia de una serie de factores que se manifiestan en el ciclo geográfico. Él explicó que el ciclo comienza con el levantamiento del relieve por procesos geológicos (fallas, vulcanismo, levantamiento tectónico, etc.). Factores geográficos tales como ríos y el escurrimiento superficial comienzan a crear los valles de forma de V entre las montañas (la etapa llamada "juventud"). Durante esta primera etapa, el relieve es más escarpado y más irregular. En un cierto plazo, las corrientes pueden tallar valles más anchos ("madurez") y después comenzar a serpentear, sobresaliendo solamente suaves colinas ("senectud"). Finalmente, todo llega a lo que es un llano plano, llano en la elevación más baja posible (llamado el "nivel de base"). Este llano fue llamado por Davis "peneplanicie" que significa "casi un llano". Entonces, el "rejuvenecimiento" ocurre y hay otro levantamiento de montañas y el ciclo continúa. Aunque la teoría de Davis no es enteramente exacta, era absolutamente revolucionaria y excepcional en su tiempo y ayudaba a modernizar la geografía y a crear el subcampo de la geomorfología. Sus implicaciones impulsaron un sinnúmero de investigaciones en distintas ramas de la geografía física. Para el caso de la paleogeografía esta teoría aportó un modelo para comprender la evolución del paisaje. Para la hidrología, la glaciología y la climatología fue un impulso en cuanto se investigó como los factores geográficos que estudian, modelan el paisaje e influyen en el ciclo. El grueso de los trabajos de William Morris Davis condujeron al desarrollo de una nueva rama de la geografía física: la geomorfología, cuyos contenidos hasta entonces no se diferenciaban del resto de la geografía. Al poco tiempo esta rama presentaría un gran desarrollo. Algunos de sus discípulos realizaron importantes aportes a diferentes ramas de la geografía física y humana tales como Curtis Marbut con su invaluable legado para la pedología, Mark Jefferson, Isaiah Bowman, entre otros.

La geografía física integra el conocimiento que han desarrollado las ciencias geográficas más especializadas como la Geomorfología, climatología, hidrografía e hidrología, oceanografía, pedología y muchas otras. Todas estas ciencias suelen tener una perspectiva aplicada. Por otra parte, nuevas disciplinas aún más específicas han venido a desarrollar campos aplicados dentro de la geografía física. Unos ejemplos: la geocriología que se ha desarrollado en Rusia y Canadá se especializa en el estudio del permahielo. La geografía litoral se aboca al estudio específico de la dinámica costera, la geografía de los riesgos, enfoque resumido en un artículo de Francisco Calvo García-Tornel (), estudia las implicaciones de los riesgos naturales sobre los seres humanos.

Por lo general, las mayores aplicaciones de la geografía física tienen lugar en el desarrollo de las materias específicas de esta disciplina como son la geomorfología, en especial, la geomorfología fluvial; la climatología, la geomorfología litoral e incluso la oceanografía entendida como una geografía del mar y no como una física o una geología del mar, y muchas otras ciencias más específicas.





</doc>
<doc id="3464" url="https://es.wikipedia.org/wiki?curid=3464" title="Modos de organización del espacio terrestre">
Modos de organización del espacio terrestre

Según Pierre George la organización del espacio es un acontecimiento para responder a las necesidades de la comunidad local, del mosaico constituido por el espacio bruto diferenciado. Para Olivier Dollfus, a cada tipo de sociedad, y a cada etapa de la evolución histórica, corresponde unas formas de organización del espacio que es posible reunir en familias, a veces un tanto arbitrarias. Es conveniente, para cada familia, analizar la función de los limitadores naturales en las diferentes escalas, y las relaciones jerárquicas que se establecen entre los elementos constitutivos del espacio.

Pueden distinguirse tres modos básicos de organización:

 Espacio


</doc>
<doc id="3465" url="https://es.wikipedia.org/wiki?curid=3465" title="Espacio natural">
Espacio natural

Un espacio natural, paisaje natural o ambiente natural, es una parte del territorio de la tierra que no se encuentra modificado por la acción del ser humano. El término se utiliza más específicamente para designar alguna de las categorías que sirven, de acuerdo con las diferentes legislaciones, para la protección de determinadas zonas de la naturaleza de especial interés.

El paisaje natural es aquello que no está modificado por el hombre. Son las tierras que no pertenecen a la ecúmene o sea que no están habitadas, como: las regiones polares, la alta montaña y alguna selva tropical que es recorrida por cazadores y recolectores que no utilizan el fuego.

El paisaje natural será un espacio recorrido pero no organizado, y con densidades de población bajas. Se trata de los espacios ocupados por sociedades de recolectores, pastores, cazadores y pescadores que tienen un conocimiento muy íntimo y especializado del medio. El área necesaria para procurarse los recursos debe ser muy amplia ya que dependen de lo que ofrece la naturaleza.

En la actualidad el paisaje natural esta en proceso de desaparición por la actividad humana ya que los humanos destruyen los paisajes para obtener recursos tales como: madera, piedras, etc

Existen dos tipos de paisajes naturales: el paisaje costero y el paisaje de interior. El paisaje costero, como su nombre bien indica, es el que está más próximo al mar. El paisaje de interior es el que está más alejado de la costa. En él podemos estudiar distintos tipos de paisajes: el de montaña, el del valle, y el de la llanura.

Para representar los paisajes y para poder estudiarlos utilizamos mapas y croquis. Estos están regidos por signos convencionales.

En resumen el paisaje natural es un paisaje que no fue modificado por el hombre, es lo contrario a los paisajes ordenados (ciudades, megalopolis, represas, etc.).

Se define como ambiente natural «lo que no ha sido alterado por el hombre». Pero esta definición no es dogmática; puesto que, supongamos, si un hombre se interna en una selva y toca o afecta un árbol, ello no la transforma automáticamente en un ambiente artificial o antropizado. De este modo, la definición dada tiene sólo un sentido relativo. 

Por oposición se encuentra el ambiente antropizado (artificial), que es el que ha sido afectado por la presencia humana («o ha tocado la mano del hombre»).

Es un término usualmente utilizado en planeamiento físico por arquitectos e ingenieros civiles. También se utiliza en la teoría del impacto ambiental, en la evaluación del impacto ambiental y "EA" (educación ambiental).

El ambiente natural puede describirse por su naturaleza: sus cambios siempre ocurren porque el hombre los ha transformado. En la teoría general de sistemas, un ambiente es un complejo de factores externos que actúan sobre un sistema, y determinan su curso y su forma.

El ambiente es un elemento vital de la humanidad, ya que sin él no podríamos vivir: todos necesitamos de las plantas, de los animales y de aquellos elementos que componen el ambiente natural.

Para ser un espacio natural o un paisaje natural, se deben cumplir los siguientes requisitos: 

Son áreas naturales, poco transformadas por la explotación u ocupación humana que, en razón de la belleza de sus paisajes, la representatividad de sus ecosistemas o la singularidad de su flora, de su fauna o de sus formaciones geomorfológicas, poseen valores ecológicos, estéticos, educativos y científicos cuya conservación merece una atención preferente. Tres categorías:

Aquellas áreas en las que existan ecosistemas, no sensiblemente alterados por el hombre y de máxima relevancia dentro del contexto del medio natural de la nación que hace necesaria su protección.

Aquellas áreas en las que existan ecosistemas no sensiblemente alterados por el hombre y de máxima relevancia dentro del contexto del medio natural de la región que hacen necesarias su protección.

Espacios de relativa extensión, notable valor natural y de singular calidad biológica, en los que se compatibiliza la coexistencia del hombre y sus actividades con el proceso dinámico de la naturaleza, a través de un uso equilibrado y sostenible de los recursos. Un espacio natural es una parte del territorio de la tierra que se encuentra escasamente modificado por la acción del hombre, el término se utiliza más específicamente para designar algunas de las categorías que sirven, de acuerdo con las diferentes legislaciones, para la protección de determinadas zonas de la naturaleza de especial interés.

Espacios naturales, cuya declaración tiene como protección de ecosistemas, comunidades o elementos biológicos que, por su rareza, fragilidad, importancia o singularidad, merecen una valoración especial.
Los espacios naturales protegidos son demarcaciones administrativas establecidas con la finalidad de favorecer la conservación de la naturaleza. En la mayoría de los casos se trata de conservar una porción de la naturaleza que por sus condiciones se perfila como privilegiada. En otras ocasiones, se intenta mantener al margen las actividades industriales por parte del ser humano para preservar dichas zonas.

Las que por la especificidad de sus características o elementos tengan un valor científico concreto para mejor desarrollo

Las que contengan ecosistemas o comunidades en perfecto estado de conservación y que por ello deberán gozar de una protección absoluta. Desde el punto de vista urbanístico conlleva la prohibición de cualquier tipo de aprovechamiento, de modo que el sistema deberá funcionar con la mínima intervención exterior posible siendo el acceso de personas muy restringido.
Espacios o elementos de la Naturaleza constituidos básicamente por formaciones de notoria singularidad, rareza o belleza, que merecen ser objeto de una "protección especial". También se pueden considerar dentro de esta categoría las formaciones geológicas, los yacimientos paleontológicos y demás elementos de la gea que reúnan un interés especial por la singularidad o importancia de sus valores científicos, culturales o paisajísticos.

Aquellas áreas del medio natural que, por sus valores estéticos y culturales, necesitan protección especial.
Los espacios naturales protegidos son demarcaciones administrativas establecidas con la finalidad de favorecer la conservación de la naturaleza. En muchos casos se trata de preservar un enclave singular o una porción de naturaleza privilegiada; en otros se pretende además mantener ciertas actividades humanas finamente ajustadas a las condiciones naturales. Actualmente se ha comenzado a plantear el objetivo de mantener los procesos ecológicos.



</doc>
<doc id="3466" url="https://es.wikipedia.org/wiki?curid=3466" title="Paisaje modificado">
Paisaje modificado

Un paisaje modificado es una región en la que las prácticas humanas (agrícolas, industriales o urbanas) y el 
fuego u otras fuerzas naturales han modificado el medio ambiente de manera irreversible, aunque las huellas de esa transformación no sean perceptibles. Esta transformación no tiene por qué ser degradatoria y puede encontrar un nuevo equilibrio ecológico estable. En la mayor parte de los casos es la transición a un paisaje ordenado. Este es el paisaje que encontramos en las regiones menos pobladas de los países subdesarrollados y el que hubo en todo el mundo antes de la revolución industrial.

En el medio modificado la persona depende menos de las condiciones naturales, aunque aún marcan su vida y sus ciclos, sobre todo si están relacionadas con el clima, pero provoca endemismos no necesariamente buenos para su salud.

El paisaje modificado es un espacio acondicionado para las sociedades no industrializadas que los producen, pero sin comprometer el equilibrio ecológico. Los paisajes modificados pueden estar aislados entre sí por paisajes naturales. La sociedad explota diferentes medios ecológicos de su entorno para procurarse todo lo que necesita. Existe una red que pone en comunicación los diferentes ámbitos ecológicos. La velocidad de circulación en la red es reducida. Según el modelo de explotación del territorio que tiene cada sociedad, el paisaje modificado puede tener una densidad de población mayor o menor. La producción varía en función de las diferencias climáticas.



</doc>
<doc id="3467" url="https://es.wikipedia.org/wiki?curid=3467" title="Paisaje ordenado">
Paisaje ordenado

Llamamos paisaje ordenado al que refleja la acción meditada, concentrada y continua de una sociedad sobre el medio. Es, pues, producto de una comunidad con un tipo de economía y unos medios jurídicos y técnicos, que realiza la transformación en conjunto, a lo largo del tiempo y con perspectivas de futuro. Se trata de una opción entre las condiciones naturales y las técnicas.

En el medio ordenado la lucha contra los elementos de la naturaleza ha llegado al extremo de crear un entorno artificial de grandes dimensiones donde se desarrolla la vida humana, con las limitaciones que impone su propia biología, pero en gran parte al margen de las condiciones ambientales. Sin embargo, este medio artificial no es independiente de la naturaleza ya que necesita de ella para proveerse de los elementos naturales que son necesarios para la subsistencia, si bien se puede recurrir a ellos aunque se encuentren en lugares muy lejanos. El ser humano no puede sustraerse a su condición de ser natural.

El paisaje ordenado es un espacio organizado por una sociedad industrial con la capacidad técnica suficiente para modificar el medio de manera drástica. Esto sólo ha ocurrido tras el triunfo de la revolución industrial. La red de comunicaciones es muy densa y permite intercambios a grandes velocidades. Los recursos que utiliza no dependen de las condiciones ecológicas del entorno inmediato, ya que los flujos de la red permiten intercambios internacionales entre ámbitos ecológicos lejanos y diferentes. La red posee una jerarquía en función de la importancia de los intercambios, y una serie de nodos, las ciudades, en las que se distribuyen los productos. Esta forma de ordenación del espacio puede entrar en conflicto con el medio y con las otras dos formas de organizar el espacio, evitando su funcionamiento. Esto es lo que pasa en los países subdesarrollados, que el espacio ordenado impide el funcionamiento del espacio modificado. Todas las relaciones que se establecen en el espacio organizado están intercaladas entre sí, de manera que forman un sistema y unas afectan a las otras. 

El espacio ordenado está dividido, de forma generalizada en: espacio rural y espacio urbano; cada uno de los cuales tiene una morfología y unas funciones diferentes y hasta opuestas. Aunque en las sociedades desarrolladas modernas cada vez es más difícil establecer los límites. Los modos y las formas de vida urbanas invaden el campo y son asumidos por la población rural. Pocas cosas diferencian lo rural de lo urbano, aunque algunas son radicales, como la densidad de población, la presencia de actividades agrícolas y con tierra, las actividades extractivas, las actividades industriales con necesidades de espacio, las zonas de desechos, etc. Muchas de estas actividades, sobre todo las agrícolas, todavía dependen en alto grado de las condiciones ecológicas en las que se desarrollan. 

Generalizando, se puede decir que el espacio rural está especializado en el sector primario y energético, y la ciudad en el terciario. El sector secundario, según las actividades se localiza en el mundo rural, en el urbano o en el rururbano. 

La fluidez y la especialización de estos espacios dependen del nivel de desarrollo. En un país subdesarrollado la ciudad es una atractor de población, actividades y funciones, esquilmando, en buena medida, su entorno y anulando la jerarquía que se establece en un país desarrollado.



</doc>
<doc id="3468" url="https://es.wikipedia.org/wiki?curid=3468" title="Paisaje">
Paisaje

El concepto de paisaje se utiliza de manera diferente por varios campos de estudio, aunque todos los usos del término llevan implícita la existencia de un sujeto observador (el que visualiza) y de un objeto observado (el terreno), del que se destacan fundamentalmente sus cualidades visuales, espaciales.

El "paisaje", desde el punto de vista geográfico, es el objeto de estudio primordial y el documento geográfico básico a partir del cual se hace la geografía. En general, se entiende por paisaje cualquier área de la superficie terrestre producto de la interacción de los diferentes factores presentes en ella y que tienen un reflejo visual en el espacio. El paisaje geográfico es por tanto el aspecto que adquiere el espacio geográfico.
El "paisaje", desde el punto de vista artístico, sobre todo pictórico, es la representación gráfica de un terreno extenso. Con el mismo significado se utiliza el término "país" (no debe confundirse con el concepto político de "país"). El paisaje también puede ser el objeto material a crear o modificar por el arte mismo.

En literatura, la "descripción del paisaje" es una forma literaria que se denomina "topografía" (término que también da nombre a la "topografía" como ciencia y técnica que se emplea para la representación gráfica de la superficie terrestre). En construcciones literarias y ensayísticas es habitual comparar el "paisaje" con el "paisanaje" (de "paisano"), es decir, el medio con los grupos humanos.

El paisaje no es un componente del medio ambiente, pero puede ser objeto de protección para preservar un posible patrimonio, por parte de diversas leyes e instituciones nacionales e internacionales (Unesco y Consejo de Europa).

Desde el año 2000 existe el ELC ("European Landscape Convention" o, en español, Convenio Europeo del Paisaje (CEP), también llamado Convención de Florencia , cuyo documento fundacional entró en vigor el 1 de marzo de 2004 y ya ha sido firmado y ratificado (20-08-2008) por 29 de los 46 países miembros del Consejo de Europa (y firmado por otros seis). España lo ratificó en el año 2008 y entró en vigor el 1 de marzo de ese año.

Su propósito general es establecer un marco para la protección, gestión y planificación de los paisajes europeos. Su objetivo último es conservar y mejorar su calidad. Las estrategias que plantea animan a la implicación del público, las instituciones, autoridades y agentes locales, regionales, nacionales e internacionales en procesos de toma de decisiones públicas.

El Convenio reconoce todas las formas de los paisajes europeos: naturales, rurales, urbanos y periurbanos, y tanto los emblemáticos como los ordinarios y los deteriorados. Este recurso, no renovable, se define según el CEP como:

“"cualquier parte del territorio, tal como la percibe la población, cuyo carácter sea el resultado de la acción y la interacción de factores naturales y/o humanos" (Art. 1)”

El paisaje se define como un espacio geográfico con características morfológicas y funcionales similares en función de una escala y una localización. La escala vendría definida por el tamaño del paisaje o, lo que es lo mismo, el tamaño de la "visión" del observador. Por ejemplo, un paisaje regional como un gran desierto puede esconder paisajes diferenciales a escala local.

La localización es la posición del volumen del paisaje respecto a un sistema de referencia, modelizado por la cartografía.

En la tradición de ciencias del paisaje se han establecido tres elementos o subsistemas principales que componen los paisajes: abióticos (elementos no vivos), bióticos (resultado de la actividad de los seres vivos) y antrópicos (resultado de la actividad humana). Determinar estos elementos es lo que constituye el primer nivel del análisis geográfico. Las posibilidades combinatorias, prácticamente infinitas, que se pueden dar entre ellas determina las características de un paisaje en particular.

El paisaje surge de la interacción de los diversos agentes geográficos. Estos agentes son materiales y energéticos de los que derivan formas y procesos. Se clasifican en Litosfera, Atmósfera, Hidrosfera y Biosfera. De esta última se diferencia la Antroposfera (Tecnosfera, ecosistema novel) formada por las poblaciones humanas y que juega un papel diferenciado como agente del paisaje, generando incluso una nueva época en la historia de la Tierra (el Antropoceno).

La interacción de estos agentes forma el amplio espectro de paisajes definidos por sus características geográficas. La relación que existe entre todos sus elementos constitutivos es multicausal y dinámica. Los cambios son tanto producto como condicionante de la dinámica de los paisajes, en los cuales el ser humano cumple un papel específico.

La biosfera se asienta sobre la superficie, que es la zona de contacto entre las diferentes esferas, y de manera especial en la hidrosfera. La biosfera transforma el paisaje superficial pero luego limitada según sus características funcionales a los relieves litológicos, a las características atmosféricas (climas) y a la disponibilidad de agua.

De manera especial destaca en la biosfera la antroposfera formada por los seres humanos en su organización social y en su poblamiento y uso sobre el territorio. Ya que su influencia abarca casi todos los rincones del planeta, el paisaje ya no está definido por sus agentes naturales, los paisajes naturales sólo son espacios marginales y residuales.

En la definición de paisaje que nos da la geógrafa física española María de Bolós, queda de manifiesto otra teoría del paisaje de carácter geofísico, en la cual se aprecia la existencia de tres elementos fundamentales: las características del geosistema que las define, el tamaño referido a una escala espacial (epigeósfera, es decir, sistema abierto desde el cosmos como hacia el interior de la tierra) y el período de tiempo considerado en la escala temporal (métodos de datación – absoluta y relativa – y las escalas de tiempo cronológico – megaescala, macroescala, mesoescala y microescala).

La edad de un paisaje se mide de acuerdo a la autora, en cuanto este comienza a funcionar como sistema, como el geosistema actual que es. Los paisajes antiguos son aquellos en cuya formación aparecen en un mismo momento todos los elementos en forma dinámica desde hace mucho tiempo parecida a la actual dinámica que presentan. Los paisajes nuevos no nacen de la nada, sino en que su mayoría son antropizaciones radicales o extensivas de los antiguos, estos pueden aparecer por: “"las causas antrópicas, los cambios climáticos, los movimientos tectónicos recientes, modificaciones en la línea de costa, emersión de tierras o formación de islas nuevas"” [entre las principales].

Un paisaje cultural es transformado del paisaje natural. El paisaje cultural es el resultado de esa transformación.

Se da en muy pocas comunidades que sus bases conozcan sus paisajes culturales y los protejan como tales, pues no le ven ningún valor tangible:

“"La sociedad al contemplar un paisaje, le asignará un valor positivo o negativo según la percepción que éste le proporcione (bonito, agradable, etc.), pero con mayor dificultad será capaz de reconocerle un significado histórico relacionado con su dilatado proceso de configuración. Es necesario, por tanto, sensibilizar a la sociedad, pero también instruirla acerca del valor del paisaje cultural como elemento patrimonial. Ello requiere conocer esos paisajes (génesis, interrelación entre estructuras, etc.) y este proceso, a su vez, facilitará la protección real del paisaje como elemento ambiental, pero también social, cultural y patrimonial más allá de un mero amparo legal"”

Según desde que interés sea usada, la producción simbólica y cultural – ya sea en paisajes culturales, historias culturales o de reconstrucción de la memoria colectiva – ésta puede ser también un recurso de las clases dominantes para distinguirse y transmitir información distorsionada. Cuando se advierte que las relaciones simbólicas entre los hombres son asimismo relaciones de poder, comprendemos que el estudio académico de las representaciones debe acompañarse con el análisis de otra región de la superestructura: la política

En síntesis, los paisajes culturales son esencialmente construcciones multidimensionales, resultado de la interacción de estructuras históricamente determinadas y de procesos contingentes. Como marco de la actividad humana y escenario de su vida social, los paisajes humanos en general, son una construcción histórica resultante de la interacción entre los factores bióticos y abióticos del medio natural. Cualquier interpretación histórica debe partir de la comprensión de esta dinámica. Es necesario, por tanto, que se consideren todos los paisajes como consecuencia de la coevolución socio-natural a largo plazo. Por otra parte, desde el punto de vista evolutivo, los paisajes son resultado de la dependencia histórica de sentido, es decir, que con frecuencia, emergen elementos arbitrarios, no previstos, que determinan el posterior desarrollo histórico

Una de las formas en que las organizaciones globales han decidido proteger y conservar ciertos paisajes culturales que poseen cualidades importantes para el género humano es mediante las Declaraciones de Patrimonio de la Humanidad realizadas cada cierto tiempo por Unesco.

Desde las pinturas rupestres hasta el siglo XVIII, la naturaleza aparecía muy pocas veces en las obras pictóricas como paisaje valorable por sí mismo.

Se atribuye a los artistas chinos, a partir del siglo V, el mérito de 'descubrir' el paisaje como elemento pictórico, por influencia del budismo y su concepción de la naturaleza. En Europa el paisaje no aparece hasta el Renacimiento, aumentando progresivamente su presencia en las obras de arte y convirtiéndose en objeto de interés por sí mismo y no como fondo de una composición religiosa o de un retrato. Pero no ganó categoría de género pictórico hasta el siglo XVII en Holanda, país que desarrolló una importante escuela paisajística, representada por artistas como Jacob van Ruysdael.

En el siglo XIX, el ejemplo holandés se universaliza, convertido en uno de los objetivos del realismo pictórico, y en especial en Francia a través de la Escuela de Barbizon y el "plenairismo" (los pintores pintan al aire libre y no en sus gabinetes). Este nuevo interés por plasmar un instante fugaz de luz o una anécdota, en plena naturaleza, impulsó el uso de técnicas como la acuarela, con una mayor rapidez de ejecución, y la pincelada suelta en busca de conseguir una impresión más que un dibujo, una de las claves del impresionismo.

En momentos cronológicamente diferentes de oriente y occidente, la geografía y naturaleza dejaron de ser objeto de temor o espacio simbólico de los poderes míticos o de los espíritus de la región para convertirse en objeto estético, y por tanto objetivo de la obra de arte.





</doc>
<doc id="3470" url="https://es.wikipedia.org/wiki?curid=3470" title="Escorrentía">
Escorrentía

Se llama escorrentía o escurrimiento a la corriente de agua que se vierte al rebasar su depósito o cauce naturales o artificiales. En hidrología la escorrentía hace referencia a la lámina de agua que circula sobre la superficie en una cuenca de drenaje, es decir, la altura en milímetros del agua de lluvia escurrida y extendida. Normalmente se considera como la precipitación menos la evapotranspiración real y la infiltración del sistema suelo. Según la teoría de Horton, se forma cuando las precipitaciones superan la capacidad de infiltración del suelo. Esto solo es aplicable en suelos de zonas áridas y de precipitaciones torrenciales. Esta deficiencia se corrige con la teoría de la saturación, aplicable a suelos de zonas de pluviosidad elevada y constante. Según dicha teoría, la escorrentía se formará cuando los compartimentos del suelo estén saturados de agua.

La escorrentía superficial es una de las principales causas de erosión a nivel mundial. Suele ser particularmente dañina en suelos poco permeables, como los arcillosos, y en zonas con una cubierta vegetal escasa.

La proporción de agua que sigue cada uno de estos caminos depende de factores como el clima, el tipo de roca o la pendiente del terreno. De modo similar, en lugares en los que hay abundantes materiales sueltos o muy porosos, es muy alto el porcentaje de agua que se infiltra.

Los principales parámetros que afectan la escorrentía son:

La comparación entre estas variables permite obtener información sobre los procesos que se pueden presentar bajo diferentes situaciones. Las condiciones en las que se encuentra el suelo en el momento en que se produce la precipitación, afectará de forma sustancial el escurrimiento o escorrentía. Se pueden distinguir los siguientes casos:


</doc>
<doc id="3471" url="https://es.wikipedia.org/wiki?curid=3471" title="Índice de escorrentía">
Índice de escorrentía

El índice de escorrentía es un término usado en hidrología. Si conocemos el caudal relativo (módulo relativo) de un río, en una sección determinada, podemos obtener el índice de escorrentía multiplicándolo por un valor constante: 31,557. 

Ie = índice de escorrentía expresado en [mm/(km·año)]

Mr = caudal relativo (módulo relativo) expresado en [l/(s·km²)]



</doc>
<doc id="3472" url="https://es.wikipedia.org/wiki?curid=3472" title="Coeficiente de escorrentía">
Coeficiente de escorrentía

Se conoce como coeficiente de escorrentía a la relación entre el índice de escorrentía y la precipitación anual. Indica qué porcentaje de la precipitación anual circula, de media. La fórmula de este índice es 

Expresado en tantos por ciento es

Siendo:

Ce = Coeficiente de escorrentía

Ie = Índice de escorrentía

Pmm = Precipitaciones anuales en milímetros

Cuando hablamos de la cantidad de lluvia que resbala sobre un material determinado lo llamamos factor de impermeabilidad, que es diferente para cada uno de ellos; por ejemplo: pizarra (0,70-0,95); grava de carretera (0,15-0,30); césped (0,05-0,03).



</doc>
<doc id="3473" url="https://es.wikipedia.org/wiki?curid=3473" title="Índice de evaporación">
Índice de evaporación

Se conoce como índice de evaporación, referido a una cuenca hidrográfica, considerando una base de tiempo anual, de manera a que se puede considerar que no hay variación del volumen almacenado en la cuenca, a la diferencia entre la altura de la lluvia y el índice de escorrentía (lámina de agua que circula en una cuenca de drenaje).

Iev = Índice de evaporación

Ie = Índice de escorrentía

Pmm = Precipitaciones anuales en milímetros


</doc>
<doc id="3474" url="https://es.wikipedia.org/wiki?curid=3474" title="Biogeografía">
Biogeografía

La biogeografía es una disciplina científica que estudia la distribución de los seres vivos sobre la Tierra, así como los procesos que la han originado, que la modifican y que la pueden hacer desaparecer. Es una ciencia interdisciplinar, que es tanto una rama de la geografía (Clasificación UNESCO 250501), como de la biología, recibiendo sus fundamentos de especialidades como la botánica, la zoología, la ecología y la biología evolutiva y de otras ciencias como la geología.

La distribución de los seres vivos es el resultado de la evolución biológica y de la dispersión de las estirpes, de la evolución climática global y regional, y de la evolución de la distribución de tierras y mares, debida sobre todo a los avatares de la orogénesis y la tectónica de placas. La biogeografía es una ciencia histórica, es decir, que se ocupa del estudio de sistemas cuya evolución ha seguido una trayectoria única, que debe estudiarse en concreto, no pudiendo obtenerse su conocimiento deductivamente a partir de principios generales. En particular, los seres vivos presentes en una región no pueden deducirse de los factores geográficos, sino que deben ser examinados empíricamente.

La superficie de la Tierra no es uniforme, no se dan las mismas condiciones en diferentes lugares. La primera distinción, y fundamental, es entre el medio acuático y el medio aéreo o terrestre. En ambos casos un primer factor fundamental es la disponibilidad de energía primaria, la que entra en el ecosistema por los productores primarios, que es generalmente luz solar. La distribución de este factor sigue un gradiente latitudinal, en el que la energía y la temperatura son máximas en las regiones ecuatoriales y disminuyen en dirección a las polares. Varía a la vez la estacionalidad, que se va haciendo más marcada cuanto más nos alejamos del ecuador. En ambientes terrestres el segundo gran factor es la distribución de las precipitaciones, o más bien del balance entre precipitaciones y evapotranspiración, con una franja intertropical y dos templadas caracterizadas por la máxima humedad. En los océanos el segundo gran factor es la distribución de nutrientes, muy desigual, con ecosistemas más productivos y diversos en aguas relativamente frías, pero abonadas por afloramientos de nutrientes desde el fondo.

La biogeografía no estudia sólo la distribución de especies y taxones de categoría superior, sus áreas, de lo que se ocupa la especialidad llamada corología, sino también de la distribución de ecosistemas y biomas. Aunque la realidad es siempre compleja, la ciencia debe realizar operaciones de simplificación para hacerla accesible al estudio y, sobre todo, para lograr descripciones útiles. Para la biogeografía la tarea es definir áreas relativamente homogéneas y distintas de las circundantes, que estén caracterizadas por valores más o menos uniformes de los factores, y por una biota y unos ecosistemas igualmente homogéneos. Estas áreas, más o menos idealizadas, son susceptibles de ser presentadas cartográficamente. Por otra parte el estudio geográfico de la diversidad ambiental y ecológica debe contemplar las diferencias de escala; puesto que el área que en un mapa continental se presenta homogénea, por ejemplo como bosque mediterráneo, es en realidad a una escala inferior un mosaico de situaciones, con ambientes especiales como bosques de galería, en las orillas de los ríos, o saladares en cuencas endorreicas salinizadas; o diferencias debidas un relieve marcado, como la que hay entre solanas (en las laderas que miran al ecuador) y umbrías (en las opuestas).

La biogeografía tiene que tener en cuenta, para la interpretación de su objeto de estudio, el factor humano. La humanidad ha alterado significativamente los ambientes terrestres, y ahora también los oceánicos, desde el Paleolítico Superior, desde el final del último período glacial. Ya antes de la actual explosión demográfica e industrial, era imposible encontrar en los continentes un solo rincón que no guardara memoria de la alteración humana, si bien la conciencia de este hecho es reciente. Actualmente es ya muy pequeña la proporción de áreas que merezcan ser llamadas naturales, y lo que encontramos en su lugar son ambientes antropizados en diverso grado.

A la biogeografía se le ha dividido en dos ramas, la conocida como la biogeografía histórica y la biogeografía ecológica. La biogeografía ecológica estudia la biodiversidad en el tiempo y el espacio, y cada una de estas ramas se apoya más en uno de estos elementos, la biogeografía histórica se enfoca más en el tiempo, buscando como se fueron dando las distribuciones de especies hasta su estado actual. La biogeografía ecológica usando técnicas, como la teoría de la tolerancia ecológica, se basa más en la distribución espacial de los seres vivos en el momento actual. Algunos consideran a estas dos ramas irreconciliables, sin embargo cada una es el complemento de la otra.

La primera pregunta que nos plantea la historia de esta disciplina es en qué medida la religión influyó o continúa influyendo en las ideas que en ella se han planteado. Desde un punto de vista, la idea de un centro de creación de las especies y a partir de ahí su dispersión al resto del planeta fue el eje de las primeras ideas sobre la distribución de los seres vivos, pero aun cuando aparentemente esas ideas quedaron atrás con la aparición de los naturalistas, se tenía una noción de que el eje principal de la distribución era la dispersión, la idea estaba influida indirectamente por las ideas religiosas y filosóficas.

No fue sino hasta la introducción de las ideas vicariancistas de Alfred Russel Wallace en el siglo XIX cuando el enfoque empezó a cambiar verdaderamente. Es en ese punto donde se marca una nueva etapa en la historia de la biogeografía, acompañada por el nuevo paradigma de la biología, la teoría de la evolución, aunque algunos autores ya habían planteado ideas evolucionistas antes que Darwin, pero sin haberlas concretado o solo como ejemplos aislados. Y sin duda la evolución cambió a la biogeografía como cambió a todas las demás ramas de la biología. “La biogeografía de Charles Darwin y Wallace predominaría por casi un siglo, aniquilando la idea de la dispersión en esta ciencia y circunscribiéndola básicamente a aspectos ecológicos” El fin de la llamada biogeografía darwinista termina en la etapa de la biogeografía contemporánea, donde se buscan los factores que anteriormente se dejaban como productos del azar, además como en todas las ciencias, se ven cambiadas por el desarrollo tecnológico y del pensamiento, en este caso se toma en cuenta la teoría tectónica de placas, se tiene la tecnología para el análisis filogenético, y se rechazan algunas teorías que se consideran obsoletas. Es para la biogeografía una revolución científica, que conlleva a un cambio de paradigma. Los resultados son, numerosos enfoques distintos, basados en diferentes criterios de búsqueda y análisis. Entre los que destacan la panbiogeografía y la biogeografía cladista. Esta última basa su método en tres pilares: el método cladista, la tectónica de placas, y la crítica al modelo dispersionista hecha por León Croizat y se considera una de las principales escuelas actuales de la biogeografía histórica. En parte por el impacto que ha tenido el cladismo en la sistemática, la cual está íntimamente relacionada con la biogeografía, ya que incluso son áreas de los mismos autores.



</doc>
<doc id="3475" url="https://es.wikipedia.org/wiki?curid=3475" title="Geografía humana">
Geografía humana

La geografía humana constituye la segunda gran división de la geografía general. Como disciplina se encarga de estudiar las sociedades humanas desde una perspectiva espacial, la relación entre estas sociedades y el medio físico en el que habitan, así como los paisajes culturales y las regiones humanas que éstas construyen. Según esta idea, la Geografía humana podría considerarse como una geografía regional de las sociedades humanas, un estudio de las actividades humanas desde un punto de vista espacial, una ecología humana y una ciencia de los paisajes culturales. Analiza la desigual distribución de la población sobre la superficie terrestre, las causas de dicha distribución y sus consecuencias políticas, sociales, económicas, demográficas y culturales en relación a los recursos existentes o potenciales del medio geográfico a distintas escalas.
Parte de la premisa de que el ser humano siempre forma parte de agrupaciones sociales amplias. Estas sociedades crean un entorno social y físico mediante procesos de transformación de sus propias estructuras sociales y de la superficie terrestre en la que se asientan. Su accionar modifica ambos aspectos en función de las necesidades e intereses que los agentes sociales que las forman, especialmente de los agentes sociales dominantes. Estas transformaciones se deben a procesos económicos, políticos, culturales, demográficos, etc.

El conocimiento de estos sistemas geográficos formados por la sociedad y su medio físico (regiones humanas, paisajes culturales, territorios etc), es el objeto de estudio de la geografía humana. Podemos considerar como iniciador de la geografía humana a Elisée Reclus en Francia, teniendo como antecedente la obra de Karl Ritter en Alemania.

Fue Vidal de la Blache quien definió la geografía como una ciencia de síntesis que estudia la interacción entre el ser humano y su medio, definición que ha perdurado hasta nuestros días en la escuela francesa de Geografía .

Aunque la primera obra de "Geografía humana" apareció en Alemania en el siglo XIX con el nombre de "Antropogeografía", obra de Friedrich Ratzel, fueron varios geógrafos franceses los que le dieron un gran impulso a esta rama de la geografía a fines de dicho siglo y en la primera mitad del siglo XX a nivel de investigación empírica. Más recientemente, la Geografía humana a nivel universitario ha venido siendo dividida en subdisciplinas más específicas y aplicadas. En algunas universidades, aparece con el nombre de Geografía simplemente al desaparecer la Geografía física como disciplina o pasar a otras escuelas y facultades, y lo mismo podemos decir de otras ramas geográficas como es el caso de la Geografía Regional en este caso por absorción o confluencia a un punto de vista común. Entre los geógrafos franceses que han desarrollado obras sobre Geografía humana podemos citar a Vidal de la Blache, Albert Demangeon y Max Derruau, además de Eliseo Reclus, cuya obra "El hombre en la Tierra" constituye la primera obra de Geografía Humana de orientación ecológica cuidadosa y exhaustivamente desarrollada y que constituye el punto de partida de la geografía francesa que se desarrolló posteriormente.

Paul Vidal de la Blache fue el verdadero impulsor de la escuela francesa de geografía. Presenta una visión distinta de la Geografía humana a la desarrollada por Ratzel. Bastantes historiadores de la geografía coinciden en atribuir a Ratzel la visión determinista de la Geografía humana desarrollada con mayor intensidad por su discípula Ellen Churchill Semple en EE.UU. Vidal de la Blache en cambio es conocido como el fundador del posibilismo geográfico. Sus aportes más importantes al campo de la geografía humana fueron los conceptos de género de vida o modos de vida y el desarrollo del enfoque regional de la geografía.

Max Sorre fue uno de los discípulos de Vidal de la Blache que más contribuyó al desarrollo de la Geografía humana en Francia. En "El hombre en la Tierra" se presentan algunos enunciados que sirven para definir a la geografía humana francesa desde una óptica ecológica y paisajística:


Aunque el objetivo de la geografía humana no se centra en el conocimiento del medio físico, estudiado por la geografía física, es necesario cierto conocimiento del paisaje natural para adentrarnos en la geografía ambiental, un campo de estudio emergente dentro de la geografía humana.

Los métodos de la geografía humana, lo mismo que sucede con la geografía física, son sumamente diversos, y podemos citar procedimientos tanto cuantitativos como cualitativos, incluyendo entre los primeros, los estudios de casos, las encuestas, el análisis estadístico, y la formulación de modelos, todo lo cual se ha venido agrupando como la geografía cuantitativa, desarrollada en la década de los 60 del siglo XX, con los trabajos iniciales de David Harvey y otros. Y entre los procedimientos de investigación cualitativos podemos señalar todos aquellos utilizados por las ciencias sociales en general, como los que se emplean en demografía, antropología, historia, sociología y muchas otras ciencias.

En resumen, la metodología empleada en geografía humana es aproximadamente la misma que la que se emplea en la geografía general y en muchas otras ciencias (aunque con énfasis distinto en cuanto al empleo de dichos métodos), tal vez con la excepción del método regional aunque, en sentido estricto, este método siempre ha sido empleado por numerosas ciencias sistemáticas: no hay muchas limitaciones en el empleo de diferentes metodologías en cualquier ciencia. Y al referirnos a la metodología en las ciencias sociales no podemos olvidar las críticas de Paul Karl Feyerabend en su obra "Contra el método" (1975, edic. española), donde critica la simplicidad metodológica con que se venían abordando los estudios de historia y de otras ciencias sociales.

Aunque en un principio, el objeto de la geografía humana era el estudio de las regiones humanas y de las relaciones mutuas entre el hombre y el medio natural, el desarrollo progresivo del conocimiento de los procesos sociales obligó a la sucesiva aparición de diversas ramas que enfatizaban algunos de ellos considerándolas como ciencias o ramas relativamente autónomas. Todo ello vino a sustituir el concepto original de la geografía humana por una integración de una serie de conocimientos sistemáticos estudiados con más detalle por ciencias como:





</doc>
<doc id="3477" url="https://es.wikipedia.org/wiki?curid=3477" title="Geografía rural">
Geografía rural

Geografía rural es el estudio geográfico del paisaje rural: los asentamientos rurales, las actividades y modos de vida desarrollados en el medio rural.

La tecnología, o mejor aún, el desarrollo tecnológico, ha originado una transformación tan importante, tanto en las ciudades como en el medio rural, como para considerar que ya no podemos hablar de los conceptos que tienen que ver con esta rama de la geografía de la misma manera que hace unas cuatro o cinco décadas. Esto es debido a que desde mediados del siglo XX, en muchas zonas geográficas consideradas de baja densidad de población, hay servicios hasta hace no mucho tiempo se consideraban plenamente urbanas. Por otra parte, las ciudades actuales tienden a invadir el espacio antes claramente rural, mediante la construcción de residencias, la dedicación a la agricultura a tiempo parcial, etc., creándose así, una zona intermedia de difícil delimitación. A pesar de ello, se puede aún describir una geografía rural, como un espacio donde predominan básicamente las actividades humanas relacionadas principalmente con el sector primario de la economía ("Geografía agraria").

La presencia ineludible de actividades agropecuarias es lo más característico dentro del mundo rural, siendo estas actividades las que definen y dan carácter a la gran mayoría de los distintos espacios rurales del mundo y a sus respectivos paisajes. 

Por y para su carácter, y su dedicación a la agricultura, los espacios rurales sufren una serie de condicionantes geográficos; ya que no todos los climas, ni todos los suelos son aptos para cualquier tipo, ni técnica, de cultivo. Además, tienen unos condicionamientos demográficos; ya que tiende al equilibrio entre la población y los recursos. Esta ponderación favorece la modificación de las técnicas de cultivos, en caso de superpoblación o subpoblación. En las situaciones más graves se puede pasar de una agricultura intensiva a una agricultura extensiva: intensificar el uso del suelo, roturar territorio de bosque e, incluso, se reorganizará la estructura social; o se asumirá una nueva tecnología de cultivo. En la actualidad del medio rural se demandan servicios, por lo que encontramos en el campo personas que no viven de la agricultura ni la ganadería. El medio rural también ha de someterse a ciertos condicionamientos jurídicos que afectan a la estructura de la propiedad y a las formas de explotación. Por último, el mundo rural sufre los avatares económicos y políticos, sobre todo en los países donde la agricultura está subvencionada. 

La agricultura actual ha tratado de superar los condicionamientos climáticos cultivando las especies bajo plástico: en invernadero.

El cultivo de una determinada especie durante años en un mismo lugar termina por agotar los mineral es de los que se alimenta la planta. Para evitar esto se deja descansar la tierra, sin cultivar, durante al menos un año. A esta técnica se le llama barbecho. No obstante, hay varios tipos de barbecho: el corto, en las tierras sobre las que se vuelve a cultivar en uno o dos años, antes de que se recupere el bosque; y el largo en el que se permite la recuperación total del bosque. 

Las técnicas de regadío han cambiado mucho. La técnica tradicional es el regadío por inundación en el que se hacen unos surcos entre las plantas, se desvía parte de la corriente del río o pozo y se inunda toda la superficie. Este sistema es poco eficaz, ya que se emplea mucha más agua de la necesaria. Modernamente se ha empleado el riego por aspersión, que si se hace en horas nocturnas necesita mucha menos agua. El riego por aspersión consiste en un mecanismo que esparce el agua por toda la superficie como si fueran gotas de lluvia. La técnica de riego más eficaz es el gota a gota. Consiste en canalizar el agua con pequeños tubos hasta el pie de cada planta y dejar caer una gota cada cierto tiempo, hasta completar las necesidades de cada planta. Se controla por ordenador y se suele practicar en los cultivos de invernadero. 

En muchas ocasiones, es la estructura de la propiedad de la tierra y la estructura agraria, lo que define los paisajes rurales. La propiedad puede ser colectiva y de aprovechamiento común: con bienes propios, comunes, etc., pero también puede haber gran propiedad y pequeña propiedad. En España, la gran propiedad tiene su origen en la Reconquista: durante la Edad Media. Esta gran propiedad ha podido evolucionar hasta la pequeña propiedad, si el sistema de herencia favorece la partición, o si se vendió a quienes trabajaban las explotaciones. Por el contrario, la pequeña propiedad puede evolucionar hacia la gran propiedad, si el sistema hereditario favorece el mayorazgo, por ejemplo, o si el precio del suelo es bajo y hay un capitalista rural que compra las tierras contiguas. 

Pero una cosa es el tamaño de la propiedad y otra el de las explotaciones. Una explotación es la unidad técnico-económica de la que se obtiene los productos agrarios. Estas explotaciones, según las técnicas de aprovechamiento, pueden ser un latifundio, si son grandes o un minifundio, si son pequeñas. No tiene porqué coincidir gran propiedad con latifundio, ni pequeña propiedad con minifundio: la gran propiedad puede estar dividida hasta el minifundio y la pequeña concentrada, por arrendamiento, hasta el latifundio. No obstante ambos extremos suelen quedar obsoletos y tienden a no ser funcionales. Además, tienen diferentes consecuencias económicas y sociales. Los desequilibrios han propiciado, en todos los países, reformas agrarias, bien sean técnicas o bien políticas. 

Al mismo tiempo, los condicionamientos técnicos han supuesto un aumento progresivo de la productividad de la tierra, con lo que el tamaño de la explotación se ha relativizado. Esta tendencia ha alcanzado su máximo grado en la revolución verde, o la aplicación de todos los avances técnicos que puede ofrecer la ciencia moderna, en la agricultura. 

Por último, en general podremos distinguir dos grandes conjuntos de paisajes agrarios: 

En el mundo rural distinguimos dos tipos de poblamiento: el concentrado y el disperso. El poblamiento concentrado en el agrupamiento de las viviendas de la aldea en un lugar en concreto, dejando el resto para que pueda ser cultivado. El poblamiento disperso se caracteriza porque no existe un núcleo de viviendas sino que están esparcidas por todo el territorio, normalmente cerca de las explotaciones de cada familia.

Actualmente se definen como espacios rurales: otras zonas alejadas de la ciudad o de baja densidad poblacional, como:

La producción agrícola en el mundo, en términos generales, ha ido decreciendo su producción en comparación a las cifras alcanzadas en años anteriores, tanto en las regiones desarrolladas como en las regiones en desarrollo, consecuencia producida principalmente por la incidencia de las condiciones atmosféricas catastróficas y de gran magnitud, como lo son las sequías y las lluvias torrenciales, principales factores que dejan una huella profunda en la producción agrícola impidiendo su normal desarrollo, principalmente en las regiones en desarrollo las cuales fueron notablemente afectadas como por ejemplo la región del Lejano Oriente y el Pacífico, estas cifras podrían representar las más bajas de la producción agropecuaria desde 1972; sin embargo no todas las regiones fueron igualmente perjudicadas, vemos el caso de Vietnam que será uno de los países con una de las tasas de crecimiento más altas próximas o superiores al 5 % y el caso África Subsahariana donde la producción agrícola tuvo un crecimiento recuperatorio a un ritmo de 4,3 % gracias a la expansión de la producción en Nigeria o también el caso de Vietnam en Asia.

Respecto a las economías en transición, también se registra un descenso por la contracción proveniente de la antigua Comunidad de Estados Independientes (procedentes de la antigua Unión Soviética), donde no se estima un mayor crecimiento para los próximos años salvo en Kazajistán.

Se encuentra la región de África Oriental (Somalia, Etiopía, Sudán, etc.), cuyas principales causas de escasez alimentaria son la falta de precipitaciones que conllevan a las sequías que afectaron a la mayor parte de la zona de pastoreo sumada a los enfrentamientos civiles pasados y actuales que han provocado la perturbación en la producción, mala distribución de alimentos y a la migración de miles de personas en busca de éstos generando graves crisis sociales por la carencia de aprovisionamiento en que se encuentra la población.

En África Occidental, las zonas afectadas por sequías e inundaciones en el Sahel fueron las principales carentes de un aprovisionamiento alimentario adecuado y por otra parte los enfrentamientos civiles de Liberia y Sierra Leona hacen a estos países continuos dependientes de la asistencia alimentaria internacional. 

En la región de los grandes lagos africanos (Burundi, Ruanda, R.D. Congo, Katanga), existen carencias en la producción de alimentos debido a las sequías y a los persistentes enfrentamientos civiles donde la población desplazada no tiene acceso a sus tierras llevándolos a estados de malnutrición y haciéndolas particularmente vulnerables. 

En el África Austral, las épocas prolongadas de sequía y/o las grandes inundaciones provocaron daños considerables en infraestructura y viviendas (Madagascar, Mozambique, etc.), dejando a decenas de miles de personas con urgente necesidad de asistencia alimentaria de emergencia, mientras que en Angola los conflictos civiles dejan a miles de refugiados carentes de alimentación.

En el Cercano Oriente, la escasez de insumos agrícolas, los desplazamientos forzosos de población (guerras internas o internacionales), las sequías y, sobre todo, los problemas de unas deficientes políticas agrarias y las guerras casi permanentes, son los principales causantes de los déficits en la producción agrícola y en la alimentación, 

En Asia las catástrofes naturales como los ciclones de la India nororiental y grandes inundaciones que afectan anualmente al sureste del continente con las lluvias monzónicas han provocado dificultades considerables a la producción agrícola. En otros países, la superpoblación y los regímenes políticos han dado origen a una grave diiisminución de la seguridad alimentaria y de la producción agrícola.

En América Latina, las malas condiciones atmosféricas han provocado prolongadas sequías en América central dañando las cosechas de frijoles y de cereales, mientras que en el Caribe los destrozos causados por los huracanes Lily, George y Match causaron la pérdida de todas las cosechas, innumerables vidas y daños serios en viviendas e infraestructura. Los desmanes causados por el fenómeno del niño también causaron graves problemas a lo largo de toda la región.

En los países de la antigua Unión Soviética, los enfrentamientos registrados en Chechenia, por ejemplo, han provocado una situación crítica de la agricultura en especial el sector vitivinícola y el sector de la ganadería, no esperándose tampoco buenas condiciones para las cosechas de forraje y cereales de invierno; por otro lado las personas desplazadas se encuentran económicamente vulnerables y necesitan con urgencia asistencia alimentaria.





</doc>
<doc id="3479" url="https://es.wikipedia.org/wiki?curid=3479" title="Geografía industrial">
Geografía industrial

La geografía industrial es una rama de la geografía que estudia los usos industriales en el paisaje geográfico. Forma parte de la geografía económica y la geografía humana.

Pretende explicar la relación que se establece entre los grupos humanos y el medio ambiente en los paisajes industriales, es decir, los paisajes humanizados en los que las actividades del sector secundario son las predominantes. 

Las consecuencias de los procesos de industrialización están entre las más transformadoras del espacio geográfico y con más . 

Desde el siglo XIX, la Primera Revolución Industrial, basada en el uso del carbón, dio origen a los paisajes industriales tradicionales o "paisajes negros" ("Pays Noir"). En el siglo XX, con la Segunda Revolución Industrial, se desarrollaron los paisajes industriales urbanos, caracterizados por los polígonos industriales en torno a las ciudades, y las grandes instalaciones petroquímicas en zonas portuarias. A finales del siglo XX y en el siglo XXI, con la Tercera Revolución Industrial, Revolución científico-tecnológica o Revolución digital, caracterizada por la economía del conocimiento y las tecnologías de la información y la comunicación (TIC, que "terciarizan" la industria), aparecen las modernas "tecnópolis".



</doc>
<doc id="3480" url="https://es.wikipedia.org/wiki?curid=3480" title="Geografía cultural">
Geografía cultural

La geografía cultural es un concepto consustancial a la Geografía humana.

El término aparece en los EE. UU. a comienzos del siglo XX, aunque con un sentido diferente. Se trataba de la contraposición en los mapas de la representación de la naturaleza y de los elementos creados por el hombre: poblaciones, vías de comunicación, cultivos, etc. Tras la Primera Guerra Mundial en Alemania aparecerían ideas muy similares, con una concepción más acusada de la transformación humana del medio. La geografía cultural deja de lado los condicionamientos biológicos para considerar únicamente los que proceden de la actividad humana.

En Estados Unidos sus máximos representantes, en los años 20 y 30, fueron Carl O. Sauer y sus alumnos de la escuela californiana de Berkeley. En 1931 Sauer publica el ensayo: "Cultural Geography", donde define que; «La geografía cultural se interesa, por tanto, por las obras humanas que se inscriben en la superficie terrestre y le imprimen una expresión característica… la geografía cultural implica, por tanto, un programa que está unificado con el objetivo general de la geografía: esto es, un entendimiento de la diferenciación en áreas de la Tierra. Sigue siendo en gran parte observación directa de campo basada en la técnica sencilla del análisis morfológico”». 

En el siglo XX, sobre todo tras la Segunda Guerra Mundial, la idea de la Geografía Cultural se asume con naturalidad. Los máximos representantes son el alemán Schultze y el austriaco Bobek. En Italia destacan Biasutti y Sestini, en Francia desde Max Sorre a Paul Michotte, Philippe Pinchemel y Paul Claval. Pero ya Max Sorre superaba los conceptos de Geografía cultural para apostar decididamente por una Geografía humana. 

Un texto universitario norteamericano que tuvo una decidida importancia en lo que se refiere a la Geografía Cultural es el de George F. Carter de la Universidad Johns Hopkins de Baltimore:

Carter, George F. "Man and the Land. A Cultural Geography". Nueva York: Holt, Rinehart & Winston, 1964


</doc>
<doc id="3482" url="https://es.wikipedia.org/wiki?curid=3482" title="Escala (cartografía)">
Escala (cartografía)

La escala es la relación de proporción entre las dimensiones reales de un objeto y las del dibujo que lo representa. Ejemplo: si una escala indica una proporción 1:15000 significa que un centímetro del mapa representa 15000 en la vida real.

Las escalas se escriben en forma de razón donde el antecedente indica el valor del plano y el consecuente el valor de la realidad. Por ejemplo, la escala 1:500 significa que 1cm del plano equivale a 500cm (5m) en el original.

Si lo que se desea medir del dibujo es una superficie, habrá que tener en cuenta la relación de áreas de figuras semejantes, por ejemplo un cuadrado de 1cm de lado en el dibujo o plano.

Existen tres tipos de escalas:


Según la norma UNE EN ISO 5455:1996. "Dibujos técnicos. Escalas" se recomienda utilizar las siguientes escalas normalizadas:
Si, para aplicaciones especiales, se estima necesaria una escala de ampliación mayor o una escala de reducción menor que las que se dan en la tabla, la gama de escalas recomendadas puede ampliarse por su parte superior e inferior, a condición de que la escala deseada se derive de una escala recomendada mediante multiplicación por una potencia de 10.




</doc>
<doc id="3488" url="https://es.wikipedia.org/wiki?curid=3488" title="Router">
Router

Un rúter, enrutador, (del inglés router) o encaminador, es un dispositivo que permite interconectar computadoras que funcionan en el marco de una red. Su función: se encarga de establecer la ruta que destinará a cada paquete de datos dentro de una red informática.

El primer dispositivo o hardware que tenía fundamentalmente la misma funcionalidad que lo que el día de hoy entendemos por encaminador, era el "Interface Message Processor" o IMP. Los IMP eran los dispositivos que formaban la ARPANET, la primera red de conmutación de paquetes. La idea de un encaminador (llamado por aquel entonces puerta de enlace ) vino inicialmente de un grupo internacional de investigadores en redes de computadoras llamado el "International Network Working Group" (INWG). Creado en 1972 como un grupo informal para considerar las cuestiones técnicas que abarcaban la interconexión de redes diferentes, se convirtió ese mismo año en un subcomité del "International Federation for Information Processing".

Esos dispositivos se diferenciaban de los conmutadores de paquetes que existían previamente en dos características. Por una parte, conectaban tipos de redes diferentes, mientras que por otra parte, eran dispositivos sin conexión, que no aseguraban fiabilidad en la entrega de datos, dejando este rol enteramente a los anfitriones. Esta última idea había sido ya planteada en la red CYCLADES.

La idea fue investigada con más detalle, con la intención de crear un sistema prototipo como parte de dos programas. Uno era el promovido por DARPA, programa que creó la arquitectura TCP/IP que se usa actualmente, y el otro era un programa en Xerox PARC para explorar nuevas tecnologías de redes, que produjo el sistema llamado "PARC Universal Packet". Debido a la propiedad intelectual que concernía al proyecto, recibió poca atención fuera de Xerox durante muchos años.

Un tiempo después de 1974, Xerox consiguió el primer encaminador funcional, aunque el primer y verdadero enrutador IP fue desarrollado por "Virginia Stazisar" en BBN, como parte de ese esfuerzo promovido por DARPA, durante 1975-76. A finales de 1976, tres encaminadores basados en PDP-11 entraron en servicio en el prototipo experimental de Internet.

El primer encaminador multiprotocolo fue desarrollado simultáneamente por un grupo de investigadores del MIT y otro de Stanford en 1981. El encaminador de Stanford se le atribuye a William Yeager y el del MIT a Noel Chiappa. Ambos estaban basados en PDP-11.
Como ahora prácticamente todos los trabajos en redes usan IP en la capa de red, los encaminadores multiprotocolo son en gran medida obsoletos, a pesar de que fueron importantes en las primeras etapas del crecimiento de las redes de ordenadores, cuando varios protocolos distintos de TCP/IP eran de uso generalizado. Los encaminadores que manejan IPv4 e IPv6 son multiprotocolo, pero en un sentido mucho menos variable que un encaminador que procesaba AppleTalk, DECnet, IP, y protocolos de XeroX. Desde mediados de los años 1970 y en los años 1980, los miniordenadores de propósito general servían como enrutadores.

Actualmente, los encaminadores de alta velocidad están altamente especializados, ya que se emplea un hardware específico para acelerar las funciones de encaminamiento más específicas, como son el encaminamiento de paquetes y funciones especiales como la encriptación IPsec.

El funcionamiento básico de un enrutador o encaminador, como se deduce de su nombre, consiste en enviar los paquetes de red por el camino o ruta más adecuada en cada momento. Para ello almacena los paquetes recibidos y procesa la información de origen y destino que poseen. Con arreglo a esta información reenvía los paquetes a otro encaminador o bien al anfitrión final, en una actividad que se denomina 'encaminamiento'. Cada encaminador se encarga de decidir el siguiente salto en función de su tabla de reenvío o tabla de encaminamiento, la cual se genera mediante protocolos que deciden cuál es el camino más adecuado o corto, como protocolos basado en el algoritmo de Dijkstra.

Por ser los elementos que forman la capa de red, tienen que encargarse de cumplir las dos tareas principales asignadas a la misma:
Por tanto, debemos distinguir entre reenvío y encaminamiento. Reenvío consiste en coger un paquete en la entrada y enviarlo por la salida que indica la tabla, mientras que por encaminamiento se entiende el proceso de hacer esa tabla.

En un enrutador se pueden identificar cuatro componentes:

Tanto los enrutadores como los anfitriones guardan una tabla de enrutamiento. El daemon de enrutamiento de cada sistema actualiza la tabla con todas las rutas conocidas. El núcleo del sistema lee la tabla de enrutamiento antes de reenviar paquetes a la red local. La tabla de enrutamiento enumera las direcciones IP de las redes que conoce el sistema, incluida la red local predeterminada del sistema. La tabla también enumera la dirección IP de un sistema de portal para cada red conocida. El portal es un sistema que puede recibir paquetes de salida y reenviarlos un salto más allá de la red local.

Hosts y redes de tamaño reducido que obtienen las rutas de un enrutador predeterminado, y enrutadores predeterminados que sólo necesitan conocer uno o dos enrutadores.

La información de enrutamiento que el encaminador aprende desde sus fuentes de enrutamiento se coloca en su propia tabla de enrutamiento. El encaminador se vale de esta tabla para determinar los puertos de salida que debe utilizar para retransmitir un paquete hasta su destino. La tabla de enrutamiento es la fuente principal de información del enrutador acerca de las redes. Si la red de destino está conectada directamente, el enrutador ya sabrá el puerto que debe usar para reenviar los paquetes. Si las redes de destino no están conectadas directamente, el encaminador debe aprender y calcular la ruta más óptima a usar para reenviar paquetes a dichas redes. La tabla de enrutamiento se constituye mediante uno de estos dos métodos o ambos:


Las rutas estáticas se definen administrativamente y establecen rutas específicas que han de seguir los paquetes para pasar de un puerto de origen hasta un puerto de destino. Se establece un control preciso de enrutamiento según los parámetros del administrador.

Las rutas estáticas por defecto especifican una puerta de enlace de último recurso, a la que el enrutador debe enviar un paquete destinado a una red que no aparece en su tabla de enrutamiento, es decir, se desconoce.

Las rutas estáticas se utilizan habitualmente en enrutamientos desde una red hasta una red de conexión única, ya que no existe más que una ruta de entrada y salida en una red de conexión única, evitando de este modo la sobrecarga de tráfico que genera un protocolo de enrutamiento. La ruta estática se configura para conseguir conectividad con un enlace de datos que no esté directamente conectado al enrutador. Para conectividad de extremo a extremo, es necesario configurar la ruta en ambas direcciones. Las rutas estáticas permiten la construcción manual de la tabla de enrutamiento.

El enrutamiento dinámico le permite a los encaminadores ajustar, en tiempo real, los caminos utilizados para transmitir paquetes IP. Cada protocolo posee sus propios métodos para definir rutas (camino más corto, utilizar rutas publicadas por pares, etc.).
RIP (Protocolo de Información de Enrutamiento) es uno de los protocolos de enrutamiento más antiguos utilizados por dispositivos basados en IP. Su implementación original fue para el protocolo Xerox a principios de los 80. Ganó popularidad cuando se distribuyó con UNIX como protocolo de enrutamiento para esa implementación TCP/IP. RIP es un protocolo de vector de distancia que utiliza la cuenta de saltos de enrutamiento como métrica. La cuenta máxima de saltos de RIP es 15. Cualquier ruta que exceda de los 15 saltos se etiqueta como inalcanzable al establecerse la cuenta de saltos en 16. En RIP la información de enrutamiento se propaga de un enrutador a los otros vecinos por medio de una difusión de IP usando protocolo UDP y el puerto 520.
El protocolo RIP versión uno es un protocolo de enrutamiento con clase que no admite la publicación de la información de la máscara de red. El protocolo RIP versión 2 es un protocolo sin clase que admite CIDR, VLSM, resumen de rutas y seguridad mediante texto simple y autenticación MD5.

Los enrutadores pueden proporcionar conectividad dentro de las empresas, entre las empresas e internet, y en el interior de proveedores de servicios de Internet (ISP). Los enrutadores más grandes interconectan los ISP y a esos dispositivos se les suele llamar "metroenrutador", y pueden ser utilizados en grandes redes de empresas

Los rúteres se usan en los hogares para conectar a servicios de banda ancha, tales como IP sobre cable, ADSL o fibra óptica. Un enrutador usado en una casa puede permitir la conectividad a una empresa a través de una red privada virtual.

Si bien son funcionalmente similares a los enrutadores, los equipos residenciales usan traducción de direcciones de red en lugar de direccionamiento. Esto es debido a que en lugar de conectar ordenadores locales directamente a la red del proveedor, un rúter residencial debe hacer que los ordenadores locales parezcan ser un solo equipo.

En las empresas se pueden encontrar encaminadores de todos los tamaños. Si bien los más poderosos tienden a ser encontrados en ISP, instalaciones académicas y de investigación, pero también en grandes empresas. 

El modelo de tres capas es de uso común, no todos de ellos necesitan estar presentes en otras redes más pequeñas.

Los encaminadores de acceso, incluyendo SOHO, se encuentran en sitios de clientes como sucursales que no necesitan de encaminamiento jerárquico de los propios. Normalmente, son optimizados para un bajo costo.

Los encaminadores de distribución agregan tráfico desde encaminadores de acceso múltiple, ya sea en el mismo lugar, o de la obtención de los flujos de datos procedentes de múltiples sitios a la ubicación de una importante empresa. Los encaminadores de distribución son a menudo responsables de la aplicación de la calidad del servicio a través de una WAN, por lo que deben tener una memoria considerable, múltiples interfaces WAN, y transformación sustancial de inteligencia.

También pueden proporcionar conectividad a los grupos de servidores o redes externas. En la última solicitud, el sistema de funcionamiento del encaminador debe ser cuidadoso como parte de la seguridad de la arquitectura global. Separado del encaminador puede estar un cortafuegos o VPN concentrador, o el encaminador puede incluir estas y otras funciones de seguridad. Cuando una empresa se basa principalmente en un campus, podría no haber una clara distribución de nivel, que no sea tal vez el acceso fuera del campus.

En tales casos, los encaminadores de acceso, conectados a una red de área local (LAN), se interconectan a través del enrutador de núcleo.

En las empresas, el enrutador de núcleo puede proporcionar una "columna vertebral" interconectando la distribución de los niveles de los encaminadores de múltiples edificios de un campus, o a las grandes empresas locales.Tienden a ser optimizados para ancho de banda alto.

Cuando una empresa está ampliamente distribuida sin ubicación central, la función del enrutador de núcleo puede ser asumido por el servicio de WAN al que se suscribe la empresa, y la distribución de encaminadores se convierte en el nivel más alto.

Los encaminadores de borde enlazan sistemas autónomos con las redes troncales de Internet u otros sistemas autónomos, tienen que estar preparados para manejar el protocolo BGP y si quieren recibir las rutas BGP, deben poseer una gran cantidad de memoria.

A pesar de que tradicionalmente los encaminadores solían tratar con redes fijas (Ethernet, ADSL, RDSI...), en los últimos tiempos han comenzado a aparecer encaminadores que permiten realizar una interfaz entre redes fijas y móviles (Wi-Fi, GPRS, Edge, UMTS, Fritz!Box, WiMAX...)
Un encaminador inalámbrico comparte el mismo principio que un encaminador tradicional. La diferencia es que este permite la conexión de dispositivos inalámbricos a las redes a las que el encaminador está conectado mediante conexiones por cable.
La diferencia existente entre este tipo de encaminadores viene dada por la potencia que alcanzan, las frecuencias y los protocolos en los que trabajan.

En Wi-Fi estas distintas diferencias se dan en las denominaciones como clase a/b/g/ y n.

Los equipos que actualmente se le suelen vender al cliente como enrutadores no son simplemente eso sino que son los llamados "Equipos locales del cliente" (CPE). Los CPE están formados por un módem, un enrutador, un conmutador y opcionalmente un punto de acceso WiFi.

Mediante este equipo se cubren las funcionalidades básicas requeridas en las 3 capas inferiores del modelo OSI.

En el modelo OSI se distinguen diferentes niveles o capas en los que las máquinas pueden trabajar y comunicarse para entenderse entre ellas. En el caso de los enrutadores encontramos dos tipos de interfaces:


Estas posibilidades de configuración están únicamente disponibles en los equipos modulares, ya que en los de configuración fija, los puertos de un enrutador actúan siempre como interfaces encaminadas, mientras que los puertos de un conmutador como interfaces conmutadas. Además, la única posible ambigüedad en los equipos configurables se da en los módulos de conmutamiento, donde los puertos pueden actuar de las dos maneras, dependiendo de los intereses del usuario.

Un conmutador, al igual que un encaminador es también un dispositivo de conmutación de paquetes de almacenamiento y reenvío. La diferencia fundamental es que el conmutador opera en la capa 2 (capa de enlace) del modelo OSI, por lo que para enviar un paquete se basa en una dirección MAC, al contrario de un encaminador que emplea la dirección IP.





</doc>
<doc id="3489" url="https://es.wikipedia.org/wiki?curid=3489" title="Dirección IP">
Dirección IP

La dirección IP es un conjunto de números que identifica, de manera lógica y jerárquica, a una Interfaz en la red (elemento de comunicación/conexión) de un dispositivo (computadora, laptop, teléfono inteligente) que utilice el protocolo ("Internet Protocol") o , que corresponde al nivel de red del modelo TCP/IP. La dirección IP no debe confundirse con la dirección MAC, que es un identificador de 48 bits expresado en código hexadecimal, para identificar de forma única la tarjeta de red y no depende del protocolo de conexión utilizado en la red.

La dirección IP puede cambiar a menudo debido a cambios en la red, o porque el dispositivo encargado dentro de la red de asignar las direcciones IP, decida asignar otra IP (por ejemplo, con el protocolo DHCP). A esta forma de asignación de dirección IP se le denomina también "dirección IP dinámica" (normalmente abreviado como "IP dinámica").Los sitios de Internet que por su naturaleza necesitan estar permanentemente conectados, generalmente tienen la necesidad de una "dirección IP fija" (comúnmente, "IP fija" o "IP estática"). Esta no cambia con el tiempo. Los servidores de correo, DNS, FTP públicos y servidores de páginas web necesariamente deben contar con una dirección IP fija o estática, ya que de esta forma se permite su localización en la red.

Los dispositivos se conectan entre sí mediante sus respectivas direcciones IP. Sin embargo, para las personas es más fácil recordar un nombre de dominio que los números de la dirección IP. Los servidores de nombres de dominio DNS, "traducen" el nombre de dominio en una dirección IP. Si la dirección IP dinámica cambia, es suficiente actualizar la información en el servidor DNS. El resto de las personas seguirán accediendo al dispositivo por el nombre de dominio.

Las direcciones IPV4 se expresan mediante un número binario de 32 bits permitiendo un espacio de direcciones de hasta 4.294.967.296 (2) direcciones posibles.

Las "direcciones IP" se pueden expresar como números de notación decimal: se dividen los 32 bits de la dirección en cuatro octetos. El valor decimal de cada octeto está comprendido en el intervalo de 0 a 255 [el número binario de 8 bits más alto es 11111111 y esos bits, de derecha a izquierda, tienen valores decimales de 1, 2, 4, 8, 16, 32, 64 y 128, lo que suma 255].

En la expresión de direcciones IPv4 en decimal se separa cada octeto por un carácter único ".". Cada uno de estos octetos puede estar comprendido entre 0 y 255.


En las primeras etapas del desarrollo del Protocolo de Internet, los administradores de Internet interpretaban las direcciones IP en dos partes, los primeros 8 bits para designar la dirección de red y el resto para individualizar la computadora dentro de la red. Este método pronto probó ser inadecuado, cuando se comenzaron a agregar nuevas redes a las ya asignadas. En 1981 el direccionamiento internet fue revisado y se introdujo la arquitectura de clases. (classful network architecture). En esta arquitectura hay tres clases de direcciones IP que una organización puede recibir de parte de la Internet Corporation for Assigned Names and Numbers (ICANN): clase A, clase B y clase C.



El diseño de redes de clases ("classful") sirvió durante la expansión de internet, sin embargo este diseño no era escalable y frente a una gran expansión de las redes en la década de los noventa, el sistema de espacio de direcciones de clases fue reemplazado por una arquitectura de redes sin clases Classless Inter-Domain Routing (CIDR) en el año 1993. CIDR está basada en redes de longitud de máscara de subred variable (variable-length subnet masking VLSM), lo que permite asignar redes de longitud de prefijo arbitrario. Permitiendo por tanto una distribución de direcciones más fina y granulada, calculando las direcciones necesarias y "desperdiciando" las mínimas posibles.

Existen ciertas direcciones en cada clase de dirección IP que no están asignadas y que se denominan direcciones privadas. Las direcciones privadas pueden ser utilizadas por los "hosts" que usan traducción de dirección de red (NAT) para conectarse a una red pública o por los "hosts" que no se conectan a Internet. Se reservan tres rangos no superpuestos de direcciones IPv4 para redes privadas. En una misma red no pueden existir dos direcciones iguales, pero sí se pueden repetir en dos redes privadas que no tengan conexión directa entre sí o que se conecten a través de un tercero que haga NAT. Las direcciones privadas son:

Muchas aplicaciones requieren conectividad dentro de una sola red, y no necesitan conectividad externa. En las redes de gran tamaño a menudo se usa TCP/IP. Por ejemplo, los bancos pueden utilizar TCP/IP para conectar los cajeros automáticos que no se conectan a la red pública, de manera que las direcciones privadas son ideales para estas circunstancias. Las direcciones privadas también se pueden utilizar en una red en la que no hay suficientes direcciones públicas disponibles.

Las direcciones privadas se pueden utilizar junto con la traducción de direcciones de red (NAT) para suministrar conectividad a todos los "hosts" de una red que tiene relativamente pocas direcciones públicas disponibles. Según lo acordado, cualquier tráfico que posea una dirección destino dentro de uno de los intervalos de direcciones privadas no se enrutará a través de Internet.

La máscara de red permite distinguir dentro de la dirección IP, los bits que identifican a la red y los bits que identifican al host. En una dirección IP versión 4, de los 32 bits que se tienen en total, se definen por defecto para una dirección "clase A", que los primeros ocho (8) bits son para la red y los restantes 24 para host, en una dirección de "clase B", los primeros 16 bits son la parte de red y la de host son los siguientes 16, y para una dirección de "clase C", los primeros 24 bits son la parte de red y los ocho (8) restantes son la parte de host. Por ejemplo, de la dirección de "clase A" 10.2.1.2 sabemos que pertenece a la red 10.0.0.0 y el anfitrión o "host" al que se refiere es el 2.1.2 dentro de la misma.

La máscara se forma poniendo en 1 los bits que identifican la red y en 0 los bits que identifican al host. De esta forma una dirección de "clase A" tendrá una máscara por defecto de 255.0.0.0, una de "clase B" 255.255.0.0 y una de "clase C" 255.255.255.0 :los dispositivos de red realizan un AND entre la dirección IP y la máscara de red para obtener la dirección de red a la que pertenece el host identificado por la dirección IP dada. Por ejemplo:

Dirección IP: 196.5.4.44

Máscara de red (por defecto): 255.255.255.0

AND (en binario):

11000100.00000101.00000100.00101100 (196.5.4.44) Dirección ip
11111111.11111111.11111111.00000000 (255.255.255.0) Máscara de red

11000100.00000101.00000100.00000000 (196.5.4.0) Resultado del AND

Esta información la requiere conocer un "router" ya que necesita saber cuál es la red a la que pertenece la dirección IP del datagrama destino para poder consultar la tabla de encaminamiento y poder enviar el datagrama por la interfaz de salida. La máscara también puede ser representada de la siguiente forma 10.2.1.2/8 donde el /8 indica que los 8 bits más significativos de máscara que están destinados a redes o número de bits en 1, es decir /8 = 255.0.0.0. Análogamente (/16 = 255.255.0.0) y (/24 = 255.255.255.0).

Las máscaras de red por defecto se refieren a las que no contienen subredes, pero cuando estas se crean, las máscaras por defecto cambian, dependiendo de cuántos bits se tomen para crear las subredes.

El espacio de direcciones de una red puede ser subdividido a su vez creando subredes autónomas separadas. Un ejemplo de uso es cuando necesitamos agrupar todos los empleados pertenecientes a un departamento de una empresa. En este caso crearíamos una subred que englobara las direcciones IP de estos. Para conseguirlo hay que reservar bits del campo "host" para identificar la subred estableciendo a uno los bits de red-subred en la máscara. Por ejemplo la dirección 173.17.1.1 con máscara 255.255.255.0 nos indica que los dos primeros octetos identifican la red (por ser una dirección de clase B), el tercer octeto identifica la subred (a 1 los bits en la máscara) y el cuarto identifica el "host" (a 0 los bits correspondientes dentro de la máscara). Hay dos direcciones de cada subred que quedan reservadas: aquella que identifica la subred (campo host a 0) y la dirección para realizar "broadcast" en la subred (todos los bits del campo "host" en 1).

Las redes se pueden dividir en redes más pequeñas para un mejor aprovechamiento de las direcciones IP que se tienen disponibles para los hosts, ya que estas a veces se desperdician cuando se crean subredes con una sola máscara de subred.

La división en subredes le permite al administrador de red contener los broadcast que se generan dentro de una LAN, lo que redunda en un mejor desempeño del ancho de banda.

Para comenzar la creación de subredes, se comienza pidiendo “prestados” bits a la parte de host de una dirección dada, dependiendo de la cantidad de subredes que se deseen crear, así como del número de hosts necesarios en cada subred.

Una dirección IP dinámica es una IP asignada al usuario, mediante un servidor DHCP (Dynamic Host Configuration Protocol). La IP que se obtiene tiene una duración máxima determinada. El servidor DHCP provee parámetros de configuración específicos para cada cliente que desee participar en la red IP. Entre estos parámetros se encuentra la dirección IP del cliente.

DHCP apareció como protocolo estándar en octubre de 1993. El estándar RFC 2131 especifica la última definición de DHCP (marzo de 1997). DHCP sustituye al protocolo BOOTP, que es más antiguo. Debido a la compatibilidad retroactiva de DHCP, muy pocas redes continúan usando BOOTP puro.

Las IP dinámicas son las que actualmente ofrecen la mayoría de operadores. El servidor del servicio DHCP puede ser configurado para que renueve las direcciones asignadas cada tiempo determinado.



Dependiendo de la implementación concreta, el servidor DHCP tiene tres métodos para asignar las direcciones IP:

Una dirección IP fija o IP estática es una dirección IP asignada por el usuario de manera manual, o por el servidor de la red, con base en la dirección MAC del cliente. Muchas personas confunden IP fija con IP pública e IP dinámica con IP privada.

Una IP puede ser privada ya sea dinámica o fija como puede ser IP pública dinámica o fija.

Una IP pública se utiliza generalmente para montar servidores en internet y necesariamente se desea que la IP no cambie. Por eso la IP pública se la configura, habitualmente, de manera fija y no dinámica.

En el caso de la IP privada es, generalmente, dinámica y está asignada por un servidor DHCP, pero en algunos casos se configura IP privada fija para poder controlar el acceso a internet o a la red local, otorgando ciertos privilegios dependiendo del número de IP que tenemos. Si esta cambiara (si se asignase de manera fuera dinámica) sería más complicado controlar estos privilegios (pero no imposible).

La función de la dirección IPv6 es exactamente la misma que la de su predecesor IPv4, pero dentro del protocolo IPv6.
Está compuesta por 128 bits y se expresa en una notación hexadecimal de 32 dígitos. IPv6 permite actualmente que cada persona en la Tierra tenga asignados varios millones de IP, ya que puede implementarse con 2 (3.4×10 hosts direccionables). La ventaja con respecto a la dirección IPv4 es obvia en cuanto a su capacidad de direccionamiento.

Su representación suele ser hexadecimal y para la separación de cada par de octetos se emplea el símbolo ":". Un bloque abarca desde 0000 hasta FFFF. Algunas reglas de notación acerca de la representación de direcciones IPv6 son:

Ejemplo: "2001:0123:0004:00ab:0cde:3403:0001:0063 -> 2001:123:4:ab:cde:3403:1:63"
Ejemplo: "2001:0:0:0:0:0:0:4 -> 2001::4".

Ejemplo no válido: "2001:0:2001::2:0:0:1" o "2001:0:0:0:2::1".

Cuando se realiza una búsqueda en Google, se registra la dirección IP del dispositivo con el que se está buscando (PC, laptop, tableta, teléfono inteligente, etc) de esta manera Google sabe dónde enviar la respuesta.

Cuando se realiza un cambio en Wikipedia, se registra la dirección IP en el historial del artículo.

Dependiendo del sistema operativo en el que usted se encuentre, puede saber fácilmente cuál es su dirección IP:

En Windows, se puede averiguar la IP del equipo en cada interfaz de red con el siguiente comando:
El cual informará de la dirección IP, de la máscara de red usada y de la dirección de la puerta de enlace de cada interfaz de red conectada.

En GNU/Linux y demás subsistemas UNIX y UNIX-like, se tienen dos comandos:
IFCONFIG

"Nota: "ifconfig" puede no estar disponible en nuevas versiones de algunos subsistemas UNIX, está siendo sustituido por "ip""

IP
O también su versión abreviada:



</doc>
<doc id="3491" url="https://es.wikipedia.org/wiki?curid=3491" title="Mahoma">
Mahoma

Mahoma (La Meca, c. 26 de abril de 570-Medina, 8 de junio de 632) fue el fundador del islam. Su nombre completo en lengua árabe es Abū l-Qāsim Muḥammad ibn ‘Abd Allāh ibn ‘Abd al-Muttalib ibn Hāšim al-Qurayšī (), que se castellaniza como «Mahoma».

En la religión musulmana, se considera a Mahoma «el último de los profetas» ("jātim al-anbiyā<nowiki>'</nowiki>" ), el último de una larga cadena de mensajeros enviados por Dios para actualizar su mensaje, entre cuyos predecesores se contarían Abraham, Moisés y Jesús de Nazaret. A su vez, el bahaísmo lo venera como uno de los profetas o "Manifestación de Dios", cuyas enseñanzas habrían sido actualizadas por las de Bahá'u'lláh, fundador de esta religión.

Árabe de la tribu de Quraysh, nació en La Meca () alrededor del 570. La Meca se encuentra en la región de Hiyaz en la actual Arabia Saudí. Fue hijo póstumo de Abd Allah ibn Abd al-Muttalib, miembro del clan de los hachemíes.

La costumbre de los más honorables de la tribu de Quraysh era enviar a sus hijos con niñeras beduinas con el propósito de que crecieran libres y saludables en el desierto, para poder también robustecerse y aprender de los beduinos, que eran reconocidos por su honradez y la carencia de numerosos vicios, y Mahoma fue confiado a Bani S’ad.

El primer milagro que se narra sobre Mahoma en la compilación de los hadices es que el arcángel Gabriel descendió y abrió su pecho para sacar su corazón. Extrajo un coágulo negro de este y dijo «Esta era la parte por donde Satán podría seducirte». Después lo lavó con agua del pozo de Zamzam en un recipiente de oro y devolvió el corazón a su sitio. Los niños y compañeros de juego con los que se encontraba corrieron hacia su nodriza y dijeron: «Mahoma ha sido asesinado»; todos se dirigieron a él pero descubrieron que estaba vivo. Los musulmanes ven este acontecimiento como una protección para que él se apartara desde su infancia de la adoración de los ídolos y probablemente la razón por la que fue devuelto a su madre.

Quedó huérfano a temprana edad y, debido a una costumbre árabe que dice que los hijos menores no pueden recibir la herencia de sus progenitores, no recibió ni la de su padre ni la de su madre. Se dice que ella murió cuando él tenía seis años, por lo que fue acogido y educado primero por su abuelo Abd al-Muttálib y luego por su tío paterno Abu Tálib, un líder de la tribu Quraysh, la más poderosa de La Meca, y padre de su primo y futuro califa Alí.

En aquella época La Meca era un centro comercial próspero, principalmente porque existían varios templos que contenían diferentes ídolos, lo cual atraía a un gran número de peregrinos. Mercaderes de diferentes tribus visitaban La Meca en la época del peregrinaje, cuando las guerras tribales estaban prohibidas y podían contar con un viaje seguro. En su adolescencia, Mahoma acompañó a su tío por sus viajes a Siria y otros lugares. Por tanto, pronto llegó a ser una persona con amplia experiencia en las costumbres de otras regiones.

A los doce años se dirigió a Bosra con su tío Abu Tálib y tuvieron un encuentro con un monje llamado Bahira. Algunos orientalistas dicen que esto demuestra que Mahoma aprendió de él los libros sagrados, pero los estudiosos musulmanes refutan esta opinión alegando que no pudo haber aprendido en la hora de la comida ese conocimiento y que además no se registra un segundo encuentro con este monje. En los hadices se narra que Bahira reconoció algunas señales de la profecía de Mahoma y le advirtió a su tío sobre llevarlo a Siria por temor de los judíos y romanos (en aquel entonces los bizantinos).

Mahoma no tuvo un trabajo específico en su juventud, pero se ha reportado que trabajó como pastor para Bani Sad y en La Meca como asalariado. A la edad de 25 años, Mahoma trabajó como mercader en la ruta caravanera entre Damasco y La Meca a las órdenes de Jadiya, hija de Juwáylid (), una rica comerciante viuda, a quién impresionó y esta le propuso matrimonio en el año 595. Ibn Ishaq presenta que la edad de Jadiya era de veintiocho años, y Al-Waqidi presenta cuarenta. Algunos dicen que al engendrar Jadiya dos varones y cuatro mujeres de Mahoma, hace que la opinión más fuerte sea la de Ibn Ishaq, pues es sabido que la mujer llega a la edad de la menopausia antes de los cincuenta años, a pesar de que estas informaciones no están establecidas en un hadiz sino que circularon entre los historiadores. Jadiya tuvo seis hijos con Mahoma, dos varones y cuatro mujeres. Todos nacieron antes de que Mahoma recibiera la primera revelación. Sus hijos Al-Qásim y Abdullah murieron en la infancia en La Meca. Sus cuatro hijas se llamaban Záinab, Ruqayyah, Umm Kulthum y Fátima. Jadiya sería posteriormente la primera persona en aceptar el islam después de la revelación.

Mahoma era de carácter reflexivo y rutinariamente pasaba noches meditando en la cueva de Hira, cerca de La Meca. Los musulmanes creen que en 610 a los cuarenta años de edad, mientras meditaba, Mahoma tuvo una visión del arcángel Gabriel. Las primeras revelaciones hicieron que Mahoma llegase a pensar que estaba bajo el influjo de una presencia demoníaca, llevándolo cerca del suicidio. La mediación de su esposa evitó tal desenlace y animó a Mahoma a escuchar las revelaciones. Mahoma describió luego esta visita como un mandato para memorizar y recitar los versos enviados por Dios. Durante su vida, Mahoma confió la conservación de la palabra de Dios (Allah ), trasmitida por Gabriel (Yibril, ), a la retentiva de los memoriones, quienes la memorizaban recitándola incansablemente que después de su muerte serían recopilados por escrito en el Corán debido a la primordial importancia de conservar el mensaje original en toda su pureza, sin el menor cambio ni de fondo ni de forma. Para ello emplearon materiales como las escápulas de camello, sobre las que grababan los versículos del Corán. El arcángel Gabriel le indicó que había sido elegido como el último de los profetas y como tal predicó la palabra de Dios sobre la base de un estricto monoteísmo, prediciendo el Día del Juicio Final.

De acuerdo con el Corán y las narraciones, Mahoma era analfabeto ("ummi"), hecho que la tradición musulmana considera una prueba que autentifica al Corán (Al-Qur'ān, ), libro sagrado de los musulmanes, como portador de la verdad revelada. Sin embargo, hay al menos dos hadices que muestran que Mahoma no era analfabeto Bujari
1305. ‘Abdullah bin ‘Abbâs dijo: «La enfermedad del Profeta empeoró un jueves». Entonces el Profeta dijo: «Traedme algo para escribir que os redactaré un escrito y no os perderéis después de ello».

Al-Bara' dijo: «Así el Mensajero de Allah, tomó el documento, y aunque no podía escribir bien escribió: “Esto es lo que Muhammad ibn 'Abdullah concluye...”» (Esto sucedió durante las negociaciones del tratado de Hudaybiyyah).

A medida que los seguidores de Mahoma comenzaban a aumentar en número, su acción crítica con el politeísmo lo convirtió en una amenaza para los jefes de las tribus locales. La riqueza de estas tribus se basaba en la Kaaba, el recinto sagrado de los ídolos de los árabes y el punto principal religioso de La Meca. Si rechazaban a dichos ídolos, tal como Mahoma predicaba, no habría peregrinos hacia La Meca, ni comercio, ni riqueza. El repudio al politeísmo que denunciaba Mahoma era particularmente ofensivo a su propia tribu, la qurayshí, por cuanto ellos eran los guardianes de la Kaaba. Es por esto que Mahoma y sus seguidores se vieron perseguidos.

En el año 619 fallecieron Jadiya, la esposa de Mahoma, y su tío Abu Tálib. Este año se conoce como el "año de la tristeza". El clan al que pertenecía Mahoma lo repudió y sus seguidores sufrieron hambre y persecución.

En el año 620 DC , según relata el hadiz Mahoma hizo un viaje en una noche que es conocido como "Isra y Miraŷ". "Isra" es la palabra en árabe que se refiere a un viaje milagroso desde La Meca a Jerusalén, específicamente al lugar conocido como Masyid al-Aqsa. "Isra" fue seguida por el "Mi'rāŷ", su ascensión al Cielo, donde según el hadiz recorrió los siete cielos y se comunicó con profetas que le precedieron, como Abraham, Moisés o Jesús.

La vida de la pequeña comunidad musulmana en La Meca no solo era difícil, sino también peligrosa. Las tradiciones árabes afirman que hubo varios atentados contra la vida de Mahoma, quien finalmente decidió trasladarse a Yazrib (la actual Medina) en el 622, un gran oasis agrícola donde había seguidores suyos. Rompiendo sus vínculos con las lealtades tribales y familiares, Mahoma demostraba que estos vínculos eran insignificantes comparados con su compromiso con el islam, una idea revolucionaria en la sociedad tribal de Arabia. Esta migración a Medina marca el principio del año en el calendario islámico. El calendario islámico cuenta las fechas a partir de la Hégira (), razón por la cual las fechas islámicas llevan el prefijo AH (año de la Hégira).

Mahoma llegó a Medina como un mediador, invitado a resolver querellas entre los bandos árabes de Aws y Khazraj. Logró este fin absorbiendo a ambas facciones en la comunidad musulmana y prohibiendo el derramamiento de sangre entre los musulmanes. Sin embargo, Medina era también el lugar donde vivían varias tribus judías. Mahoma esperaba que estas tribus lo reconocieran como profeta, lo cual no ocurrió. Algunos académicos afirman que Mahoma abandonó la esperanza de ser reconocido como profeta por todas las tribus judías. La alquibla, es decir, la dirección en la que rezan los musulmanes, fue cambiada del antiguo templo de Jerusalén a la Kaaba en el año 624 A.D. .

Mahoma emitió un documento que se conoce como "La Constitución de Medina" (en 622-623), en la cual se especifican los términos en que otras facciones, particularmente los judíos, podían vivir dentro del nuevo estado islámico. De acuerdo con este sistema, a los judíos y cristianos les era permitido mantener su religión mediante el pago de un tributo: Jizya o jizyah (no así a los practicantes de religiones consideradas paganas). Este sistema vendría a tipificar la relación entre los musulmanes y los dhimmis, y esta tradición es la razón de la relativa estabilidad que normalmente existía en los califatos árabes. La principal tribu judía de Medina (Banu Qurayza o Banu Nadir) no fue citada por La Constitución de Medina debido a su traición, posterior desintegración y retirada de La Carta de Medina .

Las relaciones entre La Meca y Medina se deterioraron rápidamente. Todas las propiedades de los musulmanes en La Meca fueron confiscadas, mientras que en Medina Mahoma lograba alianzas con las tribus vecinas.

Los seguidores de Mahoma comenzaron a asaltar las caravanas que se dirigían a La Meca. En marzo de 624, Mahoma condujo a trescientos guerreros en un asalto a una caravana de mercaderes que se dirigía a La Meca. Los integrantes de la caravana lograron esquivar el ataque cambiando la ruta habitual por otra más cercana a la costa , gracias a cierta información que le llego a Abu Sufyan ibn Harb el cual era jefe de la caravana. Posteriormente los jefes de Meca decidieron dirigir una represalia contra los musulmanes, enviando un pequeño ejército a invadir Medina. El 15 de marzo de 624 A.D., en un lugar llamado Badr, ambos bandos chocaron. Si bien los seguidores de Mahoma eran numéricamente tres veces inferiores a sus enemigos (trescientos contra mil), los musulmanes ganaron la batalla. Este fue el primero de una serie de logros militares por parte de los musulmanes.

Para los musulmanes, la victoria de Badr resultaba una ratificación divina de que Mahoma era un legítimo profeta. Después de la victoria, y una vez que el clan judío de Banu Qainuqa fue expulsado de Medina, la mayoría de los ciudadanos de este lugar adoptaron la fe musulmana mientras que algunos conservaron su antigua fe monoteísta anterior al Islam y Mahoma se estableció como el regente de la ciudad.

Después de la muerte de su esposa, Mahoma contrajo matrimonio con Aisha, la hija de su amigo Abu Bakr (quien posteriormente se convertiría en el líder de los musulmanes tras la muerte de Mahoma). En Medina también se casó con Hafsah, hija de Úmar (quien luego sería el sucesor de Abu Bakr). Estos casamientos sellarían las alianzas entre Mahoma y sus principales seguidores.

La hija de Mahoma, Fátima, se casó con Ali, primo de Mahoma. Otra hija, Ruqayyah, contrajo matrimonio con Uthmán pero ella falleció y después Uthmán se casó con su hermana Umm Kulthum. Estos hombres surgirán en los años subsiguientes como los sucesores de Mahoma ("califas") y líderes políticos de los musulmanes. Por tanto, los cuatro primeros califas estaban vinculados a Mahoma por los diferentes matrimonios. Los musulmanes consideran a estos califas como los "rāshidūn" (), que significa «guiados».

En 625 un jefe de La Meca, Abu Sufyan, marchó contra Medina con 3000 hombres. En la batalla de Uhud que se libró el 23 de marzo, no salió victorioso ninguno de los dos bandos. El ejército de La Meca afirmó haber ganado la batalla, pero quedó demasiado diezmado como para perseguir a los musulmanes de Medina y ocupar la ciudad.

En abril de 627, Abu Sufyán emprendió otro ataque contra Medina, pero Mahoma había cavado trincheras alrededor de la ciudad y pudo defenderla exitosamente en lo que se conoce como la batalla de la Trinchera. En esta batalla, la tribu judía de Banu Qurayza se había aliado con el ejército de La Meca, . por lo que los musulmanes emprendieron guerra contra ellos, derrotándolos. 

Tras la victoria de la batalla de las Trincheras, los musulmanes expandieron su influencia a través de conversiones o conquistas de varias ciudades y tribus, aplicando el mismo concepto bélico de Yihad.

En el año 628, la posición de Mahoma era lo suficientemente fuerte para decidir su retorno a La Meca, esta vez como un peregrino. En marzo de ese año, se dirigió a La Meca seguido de 1600 hombres. Después de diversas negociaciones, se firmó un tratado en un pueblo cercano a La Meca llamado al-Hudaybiyyah. Si bien a Mahoma no se le permitió ese año entrar en La Meca, las hostilidades cesaron y a los musulmanes se les autorizó el acceso a la ciudad en el año siguiente. El tratado duró solo dos años, ya que en 630 los regentes de La Meca lo rompieron. Como consecuencia de esto, Mahoma marchó hacia esa ciudad con un ejército de más de 10 000 hombres y la conquistó sin encontrar resistencia Mahoma amnistió a los habitantes de la ciudad salvo a quienes lo habían injuriado y a los musulmanes apóstatas. Mandó matar a éstos “incluso si eran hallados bajo las cortinas de la Kaaba”. Muchos habitantes se convirtieron al islam. Mahoma destruyó los 360 ídolos colocados alrededor de la Kaaba e hizo borrar las pinturas paganas de sus muros interiores, aunque preservó las de Jesús y la Virgen María. Asimismo, prohibió a los no musulmanes peregrinar a la Meca, convirtiéndola así en el lugar sagrado del islam y principal sitio de peregrinaje de la nueva religión. A pesar de que Mahoma no estuvo presente en el asalto a la ciudad , administraba la quinta parte del botín para repartirlo entre los más necesitados. Los cuatro quintos restantes pertenecían siempre a los combatientes. Cobró un rescate 45 onzas de plata por cada prisionero, rescate que fue repartido entre los necesitados. Mahoma no llegó nunca a saciarse de comida alguna, en su casa no había sino lo necesario para pasar el día y para los invitados que a ella acudían.

La capitulación de La Meca y la derrota de las tribus enemigas Hunayn permitió a Mahoma imponer su dominio sobre toda Arabia. Sin embargo, Mahoma no formó ningún gobierno, sino que prefirió gobernar a través de las relaciones personales y los tratados con las diferentes tribus.

Desde 595 hasta 619, Mahoma solo tuvo una esposa, Jadiya, una rica mujer de La Meca que contaba veintisiete años (cuarenta según otras fuentes) cuando se casó.

Después de su muerte contrajo matrimonio con Sawdah, y al poco tiempo con Aisha, hija de Abu Bakr —quien posteriormente sucedería a Mahoma—. Según algunos hadices, Aisha tenía seis años de edad cuando fue prometida al profeta, que tenía cincuenta y cuatro, aunque el matrimonio se consumó cuando ella tenía nueve años. Hay, sin embargo, estudiosos musulmanes del que creen que dichos datos son erróneos y que Aisha era considerablemente mayor. Pese a estas reinterpretaciones modernas de los hadices que adjudicarían a Aisha una edad más madura, una gran parte de los musulmanes siguen aceptando actualmente las interpretaciones tradicionales. Esto último ha sido utilizado por críticos del islam, como Ibn Warraq, para sostener que los matrimonios infantiles que se siguen practicando en la actualidad en los países islámicos encuentran un argumento favorable en estos posibles relatos del hadiz.

Más tarde se casó con Hafsa, con Zaynab bint Jahsh (quien fue mujer menos de una año de su hijo adoptivo Zaid del cual se divorcio ), Ramlah, hija de un líder que combatió a Mahoma, y con Umm Salama, viuda de un combatiente musulmán.

También se casó con una cristiana de nombre "Mariyah Al-Qibtía" (Mariyah, la copta), tuvo otro hijo con ella después de mudarse a Medina. Ese séptimo y último hijo se llamaba Ibrahim ibn Muhammad. Al igual que sus hermanos varones, Ibrahim falleció en su niñez; se dice que murió a los 17 o 18 meses de edad. Una de las sunas o hadices, la 153 del Libro 18 de los Eclipses, narra que el sol se eclipsó el día en el que Ibrahim murió aunque Mahoma recuerda que un eclipse de Sol no es señal de la muerte (ni del nacimiento) de alguien. Ibrahim es el mismo nombre que el del patriarca de judíos y cristianos (y musulmanes), Abraham, del cual una de las sunas o hadices, la 314 del Libro 1 Musulmán de Fe, narra que fue encontrado por Mahoma en el séptimo cielo durante su viaje por los cielos, e Ibrahim es el nombre del séptimo hijo de Mahoma.

Se casó con una judía de nombre Safiyya bint Huyayy. Tuvo varias otras esposas, de número impreciso entre estas nueve reseñadas, que afirman casi todos los expertos como seguras, y las más de veinte que algunos le estiman. Algunas de estas mujeres eran esposas de seguidores de Mahoma muertos en batalla, mientras que otras eran hijas de sus aliados.

Mahoma prescribió un máximo de cuatro esposas por musulmán, por lo que su casamiento con al menos nueve mujeres constituye la única excepción dentro de la fe que se estaba desarrollando , hasta la venida de la azora An-Nisa en el año 628 d.C. que sentaría las bases legales del matrimonio , divorcio , herencia y orfandad .


Después de una corta enfermedad, Mahoma falleció el 8 de junio de 632 en la ciudad de Medina a la edad de sesenta y tres años. La dolencia es tradicionalmente atribuida a la ingestión de una pieza de carne envenenada, preparada por una mujer perteneciente a una población judía de Jáibar. Esto se produjo tres años antes de su muerte, tras la caída y represión de los líderes de Jáibar frente a las tropas islámicas.

Abu Bakr, el padre de Aisha, la tercera esposa de Mahoma, fue elegido por los líderes de la comunidad musulmana como el sucesor de Mahoma, pues éste era el favorito de Mahoma. Cualesquiera que hayan sido los hechos, lo cierto es que Abu Bakr se convirtió en el nuevo líder del islam. La mayor parte de su corto reinado la pasó combatiendo tribus rebeldes en lo que se conoce como las Guerras Ridda.

A la fecha de la muerte de Mahoma, había unificado toda la península arábica y expandido la religión islámica en esta región, así como en parte de Siria y Palestina.

Posteriormente los sucesores de Mahoma extendieron el dominio del imperio árabe a Palestina, Siria, Mesopotamia, Persia, Egipto, el norte de África y al-Ándalus.

A Mahoma le sobrevivieron su hija Fátima y los hijos de ésta y también su última esposa. Los chiíes afirman que el esposo de Fátima, Alí y sus descendientes, son los verdaderos líderes del islam. Los suníes no aceptan esta afirmación, si bien respetan a los descendientes de Mahoma.

Los descendientes de Mahoma son conocidos por diferentes nombres, tales como "sayyid" y "sharif". Muchos líderes y nobles de los países musulmanes, actuales y pasados, afirman ser descendientes de Mahoma con variables grados de credibilidad, tales como la dinastía fatimí del norte de África, los idrisíes, la actual familia real de Marruecos y Jordania y los imanes ismaelitas que usan el título de Aga Jan.

Antes de su muerte en 632, Mahoma había establecido al islam como una fuerza social, militar y religiosa y había unificado Arabia. Algunas décadas después de su muerte, sus sucesores conquistaron Persia, Egipto, Palestina, Siria, Armenia y gran parte del norte de África, y cercaron dos veces Constantinopla, aunque no pudieron hacerse con ella, lo que les impidió avanzar hacia el este de Europa.

Entre 711 y 716 comienza la lucha por la conquista árabe, de casi ocho siglos, de la península ibérica, y en 732, cien años después de la muerte de Mahoma, el avance árabe en la conquista de Europa Occidental es detenido en el corazón de Francia en la batalla de Poitiers.

Bajo los gaznavíes, el islam se extendió en el a los principales Estados hindúes al este del río Indo, en lo que es actualmente el norte de la India. La expansión del islam continuó sin invasiones militares por diversas regiones de África y del sudeste de Asia. El islam cuenta actualmente con más de mil seiscientos millones de seguidores, siendo la segunda mayor religión del mundo, después del cristianismo. No obstante, el número de fieles es difícil de determinar, ya que según la ley islámica la apostasía debe ser castigada con la muerte. Este hecho puede inhibir a aquellos que manifiestan su identidad religiosa en zonas de mayoría musulmana.

Tras la Reconquista de España, finalizada en 1492, y las conversiones diversas al catolicismo en los años posteriores (véase su extensión), las imágenes musulmanas fueron rechazadas de manera oficial y absoluta. El erudito Sebastián de Covarrubias, en su Tesoro de la lengua castellana o española, comienza la entrada de Mahoma con el comentario «"(q nunca huviera nacido en el mundo)"».

Los musulmanes profesan amor y veneración por Mahoma:


El Corán no prohíbe explícitamente las imágenes de Mahoma pero hay unos pocos hadices (tradiciones complementarias) que han prohibido directamente a los musulmanes crear representaciones visuales de figuras humanas en cualquier circunstancia. La mayoría de los musulmanes sunníes contemporáneos creen que las imágenes visuales de los profetas en general deberían prohibirse, y muy especialmente las imágenes de Mahoma. El concepto clave es que el islam considera que el uso de imágenes fomenta la idolatría, porque la imagen tiende a volverse más importante que el concepto que representa. En el arte islámico Mahoma suele aparecer con el rostro cubierto por un velo, o simbólicamente representado como una llama, sin embargo otras imágenes, especialmente de Persia o realizadas durante el gobierno del Imperio otomano, entre otros ejemplos, lo muestran por completo.

La perspectiva islámica es diversa y algunos musulmanes mantienen una visión más flexible. Algunos, especialmente los chiíes de Irán, aceptan las imágenes respetuosas, y utilizan ilustraciones de Mahoma en libros y decoración arquitectónica, como los sunníes en varios momentos y lugares del pasado, aunque estos últimos actualmente tienden hacia posturas iconoclastas y al rechazo de cualquier imagen de Mahoma, incluyendo las creadas y publicadas por no musulmanes.

Desde el , el nombre del profeta del islam ha conocido varios estereotipos. Muchas fuentes mencionan estereotipos exagerados y a veces equivocados. Estos estereotipos nacen en Oriente, pero serán adoptados o desarrollados en las culturas occidentales. En estas referencias, se desempeñan un papel principal en la introducción de Mahoma y su religión en Occidente como el falso profeta, príncipe sarraceno, deidad de sarracenos, la bestia bíblica, cismático del cristianismo y una criatura satánica, el autor del Corán, y el Anticristo.

En el Pabellón del Santo Manto y las Reliquias Sagradas, del Palacio de Topkapi, en Estambul, se exhiben objetos que habrían pertenecido a Mahoma, como el Santo Manto, el arco y la espada del profeta, tierra de la tumba de Mahoma y una huella de su pie enmarcada en bronce, así como un pelo de su barba y el relicario donde se conserva uno de sus dientes.





</doc>
<doc id="3494" url="https://es.wikipedia.org/wiki?curid=3494" title="I milenio a. C.">
I milenio a. C.

El I milenio a. C. comenzó el 1 de enero de 1000 a. C. y terminó el 31 de diciembre del 1 a. C.

Este periodo coincide con el fin de la Edad del Hierro en el Viejo Mundo.




</doc>
<doc id="3495" url="https://es.wikipedia.org/wiki?curid=3495" title="III milenio">
III milenio

El tercer milenio es el milenio actual. Comprende el período de tiempo entre el 1 de enero de 2001 al 31 de diciembre del año 3000.

El siglo actual de este milenio es el siglo XXI, la década actual de este milenio es los años 2020 y el año actual de este milenio es el 2020.

Como este milenio está en curso, se han desarrollado acontecimientos muy importantes en las 2 primeras décadas del siglo XXI (años 2000 y años 2010) que han sido el tema de atención para los historiadores, los siguientes años (2021-3000), las siguientes décadas (2030 a 2990) y los siglos restantes (XXII a XXX) del Milenio son o se están investigando en estudios de futuros.





</doc>
<doc id="3496" url="https://es.wikipedia.org/wiki?curid=3496" title="II milenio a. C.">
II milenio a. C.

El II milenio a. C. comenzó el 1 de enero de 2000 a. C. y terminó el 31 de diciembre de 1001 a. C.





</doc>
<doc id="3498" url="https://es.wikipedia.org/wiki?curid=3498" title="Siglo XXI a. C.">
Siglo XXI a. C.

El siglo veintiuno antes de Cristo comenzó el 1 de enero de 2100 a. C. y terminó el 31 de diciembre de 2001 a. C.






</doc>
<doc id="3499" url="https://es.wikipedia.org/wiki?curid=3499" title="Siglo XX a. C.">
Siglo XX a. C.

El siglo veinte antes de Cristo comenzó el 1 de enero del 2000 a. C. y terminó el 31 de diciembre del 1901 a. C.



</doc>
<doc id="3500" url="https://es.wikipedia.org/wiki?curid=3500" title="Siglo XIX a. C.">
Siglo XIX a. C.

El siglo XIX antes de Cristo comenzó el 1 de enero de 1900 a. C. y terminó el 31 de diciembre de 1801 a. C.







</doc>
<doc id="3501" url="https://es.wikipedia.org/wiki?curid=3501" title="Siglo XVIII a. C.">
Siglo XVIII a. C.

Formalmente el siglo XVIII antes de Cristo comenzó el 1 de enero de 1800 a. C. y terminó el 31 de diciembre de 1701 a. C.

En Oriente Próximo este siglo estuvo marcado por el ascenso (en 1792 a. C.) de Hammurabi al trono de la ciudad de Babilonia a partir de la cual comenzará una política de expansión. En primer lugar se liberó de la tutela de Ur para, en 1786 a. C., enfrentarse al vecino rey de Larsa, Rim-Sin, arrebatándole Isin y Uruk. Con la ayuda de Mari (Siria) en 1762 a. C. venció a una coalición de ciudades de la ribera del Tigris, para, un año después, conquistar la ciudad de Larsa. Tras esto se proclamó rey de Sumer y Acad, título que había surgido en tiempos de Sargón de Acad y que se había venido utilizando desde entonces por los monarcas que conseguían el dominio de toda la región de Mesopotamia. Tras un nuevo enfrentamiento con una nueva coalición de ciudades conquistó Mari, tras lo cual, en 1753 a. C., completó su expansión con las conquistas de Asiria y Ešnunna, al norte de Mesopotamia. 
En Egipto comienza el segundo periodo intermedio. Hacia 1800 a. C. llega al poder la dinastía XIII que se ve incapaz de controlar las extensas tierras dominadas. A finales del siglo hacen aparición los hicsos, que conquistaron la región norte del país.




</doc>
<doc id="3503" url="https://es.wikipedia.org/wiki?curid=3503" title="Siglo XVI a. C.">
Siglo XVI a. C.

El siglo XVI a. C. comenzó el 1 de enero de 1600 a. C. y terminó el 1 de enero de 1501 a. C.







</doc>
<doc id="3504" url="https://es.wikipedia.org/wiki?curid=3504" title="Siglo XV a. C.">
Siglo XV a. C.

Formalmente, el siglo XV antes de Cristo comenzó el 1 de enero de 1500 a. C. y terminó el 31 de diciembre de 1401 a. C.




</doc>
<doc id="3505" url="https://es.wikipedia.org/wiki?curid=3505" title="Siglo XIV a. C.">
Siglo XIV a. C.

El siglo XIV antes de Cristo comenzó el 1 de enero de 1400 a. C. y terminó el 31 de diciembre de 1301 a. C.


Akenatón, faraón de Egipto


</doc>
<doc id="3506" url="https://es.wikipedia.org/wiki?curid=3506" title="Siglo XIII a. C.">
Siglo XIII a. C.

Formalmente, el siglo XIII antes de Cristo empezó el 1 de enero de 1300 a. C. y terminó el 31 de diciembre de 1201 a. C.




</doc>
<doc id="3507" url="https://es.wikipedia.org/wiki?curid=3507" title="Siglo XII a. C.">
Siglo XII a. C.

El siglo XII antes de nuestra era comenzó el 1 de enero del año 1200 a. C. y terminó el 31 de diciembre de 1101 a. C.



</doc>
<doc id="3508" url="https://es.wikipedia.org/wiki?curid=3508" title="Siglo XI a. C.">
Siglo XI a. C.

El siglo XI antes de cristo comenzó el 1 de enero de 1100 a. C. y terminó el 31 de diciembre de 1001 a. C.




</doc>
<doc id="3509" url="https://es.wikipedia.org/wiki?curid=3509" title="Siglo IX a. C.">
Siglo IX a. C.

El siglo IX a. C. o a.e.c. comenzó el 1 de enero de 900 a. C. y terminó el 31 de diciembre de 801 a. C.











</doc>
<doc id="3510" url="https://es.wikipedia.org/wiki?curid=3510" title="Siglo VIII a. C.">
Siglo VIII a. C.

El siglo VIII a. C. comenzó el 1 de enero del 800 a. C. y terminó el 31 de diciembre del 701 a. C.

El siglo VIII antes de cristo fue un periodo de grandes cambios en las civilizaciones.

En Egipto corresponde a las dinastías XXII, XXIII, XXIV y XXV cuyos soberanos (excepto los de la XXIV) eran de origen extranjero; libio (dinastías XXII y XIII) y kushita o etíope (dinastía XXV). 
El Imperio neoasirio alcanza el pico de su poder, conquistando varios países vecinos, entre ellos el reino de Israel.

Grecia coloniza otras regiones del mar Mediterráneo y el mar Negro.
La civilización etrusca se expande por Italia.
Convencionalmente se toma este siglo octavo como el principio de la Antigüedad clásica, con la primera Olimpiada festejada en el 776 a. C. En esta época se cree que fueron compuestos los textos épicos de Homero (la "Ilíada" y la "Odisea").
En China se registra un eclipse solar histórico en el año 780 a. C.

En la India —en plena Edad de Hierro— comienza la cultura de la cerámica negra pulida norteña, de baja tecnología y producción de armas de hierro. Esta cultura es una de las candidatas de haber sido la cultura védica.











</doc>
<doc id="3511" url="https://es.wikipedia.org/wiki?curid=3511" title="Siglo VII a. C.">
Siglo VII a. C.

El siglo VII a. C. comenzó el 1 de enero del 700 a. C. y terminó el 31 de diciembre del 601 a. C.












</doc>
<doc id="3512" url="https://es.wikipedia.org/wiki?curid=3512" title="Siglo VI a. C.">
Siglo VI a. C.

El siglo VI a. C. comenzó el 1 de enero del 600 a. C. y terminó el 31 de diciembre del 501 a. C.














</doc>
<doc id="3513" url="https://es.wikipedia.org/wiki?curid=3513" title="Siglo V a. C.">
Siglo V a. C.

El siglo V a. C. comenzó el 1 de enero del 500 a. C. y terminó el 31 de diciembre del 401 a. C. En Occidente es llamado el «Siglo de Pericles».





</doc>
<doc id="3514" url="https://es.wikipedia.org/wiki?curid=3514" title="Siglo IV a. C.">
Siglo IV a. C.

El siglo IV a. C. comenzó en el 400 a. C. y terminó en el 301 a. C., también es llamado el «Siglo de Alejandro Magno».









</doc>
<doc id="3515" url="https://es.wikipedia.org/wiki?curid=3515" title="Siglo III a. C.">
Siglo III a. C.

El siglo III a. C. comenzó el 1 de enero de 300 a. C. y terminó el 31 de diciembre del 201 a. C. Es parte de la denominada, época clásica. 

En la cuenca mediterránea, las primeras décadas de este siglo se caracterizaron por un equilibrio de poder entre los reinos helenísticos en el este y el gran poder mercantil de Cartago en el oeste. Este equilibrio finalizó cuando surgió un conflicto entre la antigua Cartago y la República romana. En las décadas siguientes, la República cartaginesa fue humillada y luego destruida por los romanos en la Primera y Segunda guerra púnica. Después de la Segunda guerra púnica, Roma se convirtió en el poder más importante en el Mediterráneo occidental y junto con la Antigua Grecia y el Antiguo Egipto en una de las potencias mundiales.

En el Mediterráneo oriental, el Imperio seléucida y el Reino ptolemaico, estados sucesores del imperio de Alejandro Magno, libraron una serie de guerras sirias por el control del Levante. En Grecia, la dinastía antipátrida de corta duración de Macedonia fue derrocada y reemplazada por la dinastía antigónida en 294 a. C., una casa real que dominaría los asuntos de la Grecia helenística durante aproximadamente un siglo hasta el punto muerto de la Primera Guerra de Macedonia contra Roma.. Macedonia también perdería la guerra de Creta contra la pólis griega de Rodas y sus aliados.

En la India, Ashoka gobernó el Imperio maurya. Las dinastías Pandya, Chola y Chera de la época clásica florecieron en el antiguo país tamil.

El periodo de los Reinos Combatientes en China llegó a su fin, con Qin Shi Huang conquistando a los otros seis estados nacionales y estableciendo la dinastía Qin de corta duración el primer imperio de China, que fue seguido en el mismo siglo por el Han de larga duración. dinastía. Sin embargo, existió un breve interregno y una guerra civil entre los períodos Qin y Han conocidos como la disputa Chu-Han, que duró hasta 202 a.C. con la victoria final de Liu Bang sobre Xiang Yu.

En la península de Corea comenzó el período protohistórico. En el siglo siguiente, la dinastía Han conquistaría el reino Gojoseon del norte de Corea. Los Xiongnu estaban en el apogeo de su poder en Mongolia. Derrotaron a los chinos Han en la Batalla de Baideng en 200 a. C., marcando el comienzo del acuerdo tributario forzado "Heqin" y la alianza matrimonial que duraría varias décadas.






</doc>
<doc id="3516" url="https://es.wikipedia.org/wiki?curid=3516" title="Siglo II a. C.">
Siglo II a. C.

El siglo II a. C. comenzó el 1 de enero del 200 a. C. y terminó el 31 de diciembre de 101 a. C. Este siglo verá la decadencia del Egipto Ptolemaico y, en general, de todos los países helenísticos, favoreciendo la política expansionista de Roma por el Mediterráneo, que consiguió conquistar parte de la Galia, Grecia, Asia Menor, los territorios africanos de Cartago y gran parte de Hispania.








</doc>
<doc id="3517" url="https://es.wikipedia.org/wiki?curid=3517" title="Siglo I a. C.">
Siglo I a. C.

El siglo a. C. comenzó el 1 de enero del año 100 a. C. y terminó el 31 de diciembre del año 1 a. C. Se encuentra dentro del periodo histórico de la Edad Antigua. Es llamado el «Siglo Imperial».

En el transcurso de este siglo, todos los territorios circundantes al mar Mediterráneo quedaron bajo el control de Roma, siendo dirigidos directamente por gobernadores romanos, o a través de reyes vasallos nombrados por Roma. El Estado romano atravesó una época de crueles guerras civiles, que finalizaron con un retroceso político: la disolución de la Antigua república romana, tras unos 500 años de existencia, y la concentración de todo el poder del Estado en un solo hombre, el emperador, originándose así el Imperio romano, uno de los más importantes (sino el "más" importante) de la historia occidental.

La turbulencia interna que asoló Roma en este tiempo puede ser vista como las últimas convulsiones antes de la final muerte de la Antigua república romana, que finalmente tomó el camino del gobierno autocrático de hombres poderosos como Julio César, Marco Antonio y Octavio. A finales de este siglo se estima que habría nacido Jesús de Nazaret, la figura central del cristianismo.

Mientras tanto, en Asia, el Imperio Chino de los Han continúa floreciendo. Los chinos establecen la hegemonía en la región tras los enfrentamientos con los nómadas Xiongnu, quienes terminan (temporalmente) sometidos a finales del siglo.







</doc>
<doc id="3518" url="https://es.wikipedia.org/wiki?curid=3518" title="Siglo I">
Siglo I

El siglo d. C. o de la era común (habitualmente leído "siglo primero") comenzó el 1 de enero del año 1 y terminó el 31 de diciembre del año 100.
Durante este siglo el Imperio Romano completó el dominio de Europa meridional y occidental, el Norte de África, Asia Menor y el Levante mediterráneo. Las reformas introducidas por Augusto durante su principado estabilizan finalmente el mundo romano tras la agitación política y militar que había caracterizado gran parte del siglo anterior, dando inicio al periodo de paz relativa conocido como "Pax Romana".







</doc>
<doc id="3519" url="https://es.wikipedia.org/wiki?curid=3519" title="Siglo II">
Siglo II

El siglo d. C. (siglo segundo después de Cristo) o siglo e. c. (siglo segundo de la era común) comenzó el 1 de enero del año 101 y terminó el 31 de diciembre de 200. Es llamado el «Siglo de los Santos» también es llamado el siglo de oro o de los Antoninos por la asombrosa prosperidad que se vivió en la llamada "Pax romana".
El siglo se considera parte de la Antigüedad clásica de Occidente. Mesoamérica se encuentra en el inicio de su Período clásico.







</doc>
<doc id="3520" url="https://es.wikipedia.org/wiki?curid=3520" title="Siglo III">
Siglo III

El ' (siglo tercero después de Cristo) o ' (siglo tercero de la era común), comenzó el 1 de enero del año 201 y terminó el 31 de diciembre del 300.
En Europa y el Mediterráneo, el siglo III fue un periodo de grandes convulsiones políticas en el seno del Imperio romano. Un total de 28 emperadores se sucedieron en el poder durante de todo el siglo, la mayor parte de las veces mediante conspiraciones y asesinatos. En grandes regiones del Imperio se iniciaron aventuras secesionistas, como en el caso del Imperio Galo o Palmira. A este periodo se le conoce como Crisis del siglo III, y no fue hasta el año 274 cuando Aureliano puso fin a los separatismos y restauró la unidad del Imperio. Finalmente, Diocleciano trató de descentralizar el Imperio con el establecimiento de la Tetrarquía. Mientras se sucedían todos estos acontecimientos en el seno del Imperio, los pueblos godos penetraban hasta los balcanes, asolando Grecia y estableciendo finalmente un reino en el noreste.

En el Imperio romano se produjo una crisis demográfica que obligó al estado y a los terratenientes a asegurar la permanencia de los campesinos en los campos. El pequeño propietario, acosado por impuestos y deudas, se veía obligado en multitud de ocasiones a ceder su propiedad a cambio de la protección de los grandes latifundistas, quedando bajo el gobierno de estos. Se sientan así las bases de la futura sociedad feudal en Europa.

En Asia, China también sufrió un periodo de gran agitación. El principio del siglo trajo consigo el fin de la dinastía Han y la división de China en los llamados Tres Reinos, lo que inició un periodo de guerras que se prolongaron a lo largo de todo el siglo no concluyeron hasta el año 280 con la reunificación bajo la dinastía Jin.

En América, concluyó el período Preclásico de Mesoamérica, que dio lugar al florecimiento de Teotihuacan como el más importante centro de poder y nodo comercial en la región.









</doc>
<doc id="3521" url="https://es.wikipedia.org/wiki?curid=3521" title="Siglo IV">
Siglo IV

El ' (siglo cuarto después de Cristo) o ' (siglo cuarto de la era común) comenzó el 1 de enero de 301 y terminó el 31 de diciembre de 400. En Occidente, la primera parte del siglo fue conformada por Constantino el Grande, que se convirtió en el primer emperador romano en convertirse al cristianismo. Recuperó la unidad del imperio. También se destacó por restablecer una sola capital imperial, eligiendo el emplazamiento de la antigua Bizancio en el año 330 (desechando las capitales contemporáneas, que habían sido establecidas por las reformas de Diocleciano en Milán para el Oeste, y Nicomedeia para el este) para construir la nueva capital, pronto llamada Nova Roma (Nueva Roma); más tarde fue renombrada Constantinopla en su honor. Es llamado el «siglo de los Padres de la Iglesia».
Siglo en que se oficializa el cristianismo (313) y más tarde se prohíben las otras religiones (380). En este siglo continúa la construcción de grandes obras monumentales romanas como el Arco de Constantino (conservado magníficamente) y la grandiosa basílica de Massenzio (conservada parcialmente). Empieza por otra parte un nuevo fenómeno arquitectónico que dará una impronta definitiva a la ciudad, eso es la transformación tanto de casas privadas de cristianos como de basílicas romanas en iglesias cristianas. Por otra parte, el arte romano irá lentamente incorporando elementos cristianos o bien se fusionan como puede apreciarse en obras conservadas en los museos: así los pequeños dioses que acompañan Baco se van transformando en angelillos, los dioses del averno en demonios, los dioses mismos en arcángeles, etc. 

Muchos mitos romanos y aún los de otras religiones son absorbidos por el cristianismo y a veces transformados en dogmas. El politeísmo se mantiene disfrazado en santerías, el mito de la madre que debe ser virgen para parir dioses o semidioses es el mismo, el horror del infierno cristiano ya estaba en Virgilio. El alma romana cambiará y aunque las causas son muchas el cristianismo es una de las principales.

Filosofías como la aristotélica, la estoica y la epicúrea desaperecerán de la cultura occidental por siglos. La más dramáticamente opuesta a la sensibilidad cristiana es el epicureismo: Epicuro, filósofo griego del IV siglo a.C. afirmaba que no existe un Dios providencial, que no hay Más Allá y que la vida debe gozarse siendo el único principio y fin de la vida el placer entendido principalmente como ausencia de dolor. En este siglo habrán 23 emperadores, incluidos los simultáneos: 9 fueron asesinados, 3 murieron en batalla (incluso Majencia, ahogado), 2 o 3 suicidas, algunos por fiebres o enfermedades sospechosas, y 4 por muerte natural (Constantino, Teodosio, Arcadio y Onorio). El siglo anterior había terminado con un imperio gobernado por 4 augustos, pero en el año 305 dos de ellos, Diocleciano y Maximiano, renuncian. A la renuncia de Diocleciano (quien, además, rechazaría una segunda coronación años después) quedan en plaza Constantino y Massensio (Majencio) y se produce una guerra entre ellos con la victoria de Constantino entre el Puente Milvio y una zona llamada Saxa Rubra. 

Constantino, que sería llamado el Grande, simpatizó con los cristianos por influencia de su hija Constanza quien era cristiana y en la batalla decisiva este emperador llevó la cruz como insignia. Pero Constantino pasa a la historia por varios otros motivos : trasladó la capital del imperio a Bizancio, una antigua colonia griega, que cambia su nombre por Constantinopla (la actual Estambul) y por lo tanto Roma pasa a ser solo una ciudad importante; representa el apogeo del absolutismo divino formalizado por la solemnidad del ceremonial y del vestuario y concretado con medidas tales como la reducción de los Senados de Roma y de Constantinopla a simples consejos municipales ; dividió el imperio en 4 prefecturas y varias diócesis y provincias, designando la ciudad de Milán como capital de Italia con lo que Roma disminuye aún más su status ; organizó un Consejo de la Corona con varios ministros ( justicia, cancillería, cuestiones religiosas, finanzas, tesorería ) para aumentar la concentración del poder en sus manos ; y, finalmente, lo más importante, autorizó el libre ejercicio de la religión católica (edicto de Milán, año 313 ) con lo cual se inicia el irrefrenable poder de la iglesia y los cristianos, en breve, pasan de perseguidos a perseguidores (fot. Maqueta de Roma en esta época).

A su muerte le sucede el hijo Constancio II, cristiano, quien impone el arrianismo (doctrina según la cual Cristo es distinto al Padre aunque ambos tienen una sustancia semejante porque Cristo es “creado” y Dios siempre ha “sido ”) lo que conlleva algunos problemas para la doctrina del incipiente cristianismo como, por ejemplo, la designación de un papa de tal doctrina que posteriormente sería descalificado y declarado antipapa, situación esta de los antipapas que se repetiría mucho a lo largo de la historia de la iglesia. El emperador siguiente es Juliano, llamado el apóstata, por privar de algunos privilegios a los cristianos. Pero todos estos emperadores y los otros en este siglo IV, han de vérselas no solo con problemas internos, sino con grandes guerras en las fronteras presionadas por Godos. Algunos de estos pueblos son empujados por otros que vienen del Asia, como los hunos. Otros parecen conocer la debilidad del Imperio y aspiran a independizarse y terminar su situación de pueblos de frontera. El emperador siguiente, Teodosio, pasa a la historia por unas decisiones que cambiarán la cultura occidental: el año 380 prohíbe el arrianismo y en el 391 declara que la religión oficial del imperio será el cristianismo y se prohíben todos los otros cultos “paganos”. Desde este momento la religión cristiana ejercerá su poderosa influencia en todos los sucesos políticos y culturales de Roma, Italia, Europa y, más adelante, de buena parte del mundo. A la muerte de Teodosio, el año 395, el imperio se divide formal y definitivamente entre sus hijos Arcadio, quien recibe el de oriente siendo Constantinopla la capital, y Honorio quien recibe el de occidente pero traslada la capital a Ravenna también en el norte de Italia : el Imperio Romano se ha terminado. Ahora tenemos dos imperios, el Imperio Romano de Occidente (capital Ravenna) que durará apenas un siglo y el Imperio Romano de Oriente (capital Constantinopla) que, en cambio, durará 10 siglos. Roma es sólo la sede del obispo de Occidente.

El cristianismo, que desde este siglo IV será parte y esencia de la Urbe, realiza dos importantes Concilios: el de Nicea y el de Éfeso. En el primero, entre otros temas, se proclama la igualdad de los 4 patriarcados: Antioquía, Alejandría, Jerusalén y Roma tema que será siempre de controversia en la historia de la Iglesia. El otro, fundamental para los dogmas, es el problema de la sustancia de Cristo que viene a considerarse divina. Hay importantes ideólogos como Jerónimo, quien tradujo la Biblia al latín, Ambrosio, obispo de Milán, y Agustín, autor de libros de gran sensibilidad como Confesiones, el más íntimo, y La Ciudad de Dios. Se construyen iglesias. Hay 10 papas, con 2 mártires, y 2 antipapas en este siglo IV. Algunos mártires y hombres ilustres empezarán a ser declarados beatos o bien santos: el proceso de beatificación es correspondiente a la divinización en época romana, como Rómulo, un hijo del emperador Majencio cuyo templo del 309 d.C. está en el Foro Romano. Rómulo murió muy joven afectando gravemente al padre, quien perdería la gran batalla ante Constantino poco tiempo después.








</doc>
<doc id="3522" url="https://es.wikipedia.org/wiki?curid=3522" title="Siglo V">
Siglo V

El ' (siglo quinto después de Cristo) o ' (siglo quinto de la era común) comenzó el 1 de enero del año 401 y terminó el 31 de diciembre de 500. También es llamado el «Siglo de los Bárbaros».
A la muerte del emperador Teodosio I el Grande, sus dos hijos, Honorio y Arcadio, heredan las dos mitades del Imperio romano, que queda oficialmente dividido, mostrándose ambos jóvenes como emperadores incapaces. El imperio occidental comienza a sufrir problemas cuando un éxodo masivo de tribus bárbaras, cruzan el río Rin y penetran por la Galia e Hispania, el imperio occidental sufre continuos ataques y sublevaciones, y en el año 410, la propia Roma sufre un saqueo a manos de los visigodos, dirigidos por su rey Alarico I, tras el saqueo, ambos imperios gozan de una estabilidad aparente, pero el imperio occidental sufre otro duro golpe, cuando los vándalos toman Cartago, la capital de la provincia romana de África, por su parte, el imperio oriental tiene que hacer frente a un nuevo enemigo, desconocido para los romanos, los hunos, dirigidos por su caudillo Atila, atacan el imperio oriental, y posteriormente el occidental, siendo detenido en la Batalla de los Campos Cataláunicos por una coalición de romanos y bárbaros, dirigidos por Flavio Aecio y Teodorico I, a la muerte de Atila, los hunos se disgregan, pero Roma habría de sufrir un nuevo saqueo, esta vez a manos de los vándalos del rey Genserico, en el año 455, la agonía final de Roma acabó en el año 476, cuando el emperador Rómulo Augústulo es depuesto por un jefe bárbaro. Este año marca el comienzo de la Edad Media, el imperio oriental, habría de sobrevivir casi 1000 años más. Distintas tribus bárbaras, se disgregan por los antiguos territorios del Imperio romano de Occidente.

Distintos autores consideran que los dos sucesos que marcaron este siglo fueron la caída de Roma y la destrucción de la biblioteca de Alejandría, dando origen al oscurantismo europeo. 






</doc>
<doc id="3524" url="https://es.wikipedia.org/wiki?curid=3524" title="Comercio justo">
Comercio justo

El comercio justo (también denominado comercio equitable, comercio equitativo, o comercio alternativo) es una forma alternativa de comercio promovida por varias ONG (organizaciones no gubernamentales), por la Organización de las Naciones Unidas y por los movimientos sociales y políticos (como el pacifismo y el ecologismo) que promueven una relación comercial voluntaria y justa entre productores y consumidores. 

El comercio justo es una iniciativa para crear canales comerciales innovadores, dentro de los cuales la relación entre las partes se orienta al logro del desarrollo sustentable y sostenible de la oferta. El comercio justo se orienta hacia el desarrollo integral, con sustentabilidad económica, social y ambiental, respetando la idiosincrasia de los pueblos, sus culturas, sus tradiciones y los derechos humanos básicos.
El comercio justo puede ser considerado una versión humanista del comercio libre, que al igual que este es voluntario entre las dos partes y no tendría lugar si ambas partes no creyeran que iban a salir beneficiadas.

Los principios que defiende el comercio justo son:

"Los Principios de Comercio Justo publicado por" Fair Trade Federation "de los Estados Unidos (fundado en 1995)"

Principio de Justo Comercio #1 "“Los miembros colocan los intereses de los productores y sus comunidades como la principal preocupación de su empresa.”"


Es favorable a la libertad de comercio en iguales condiciones, es decir, abolir las restricciones discriminatorias a productos provenientes de países en desarrollo, desde materia prima a manufacturas o tecnología. Así se evita la discriminación y el proteccionismo. Intenta también evitar las grandes diferencias entre el precio que pagan por un producto los consumidores del primer mundo y el dinero que se les paga a sus productores en el tercer mundo, además de evitar la explotación de los trabajadores. Esto contribuye a compensar los efectos de la obsesión consumista por el precio más barato, sin otra consideración, y sus consecuencias:


La filosofía del comercio justo es que la mejor ayuda de los países centrales a los países en vías de desarrollo es el establecimiento de relaciones comerciales éticas y respetuosas, con crecimiento sostenible de las naciones y de los individuos. Más que por las entidades oficiales o estatales, el comercio justo es impulsado y practicado por millones de personas solidarias en diversas partes del mundo. Aquí las llamadas Tiendas del Tercer Mundo cumplen un rol decisivo, a través de voluntarios que en sus horas libres apoyan en la venta de productos como Café de Colombia, Ron de Cuba, Miel de Chiapas, Quinua de Bolivia y Perú, etc.


En 1964, comienza el sistema de Comercio Justo, (FT por sus siglas en inglés: "Fair Trade"), con la conferencia de la UNCTAD: Conferencia de las Naciones Unidas sobre Comercio y Desarrollo. Allí, algunos grupos plantearon suplantar la ayuda económica hacia los países pobres por un régimen de apertura comercial de los mercados de alto poder adquisitivo. Sólo unos pocos grupos de habitantes de los países desarrollados promovieron la creación de tiendas "UNCTAD", que comercializarían productos del llamado Tercer Mundo en Europa, evitando las barreras arancelarias de entrada. A partir de ese momento, se inició una cadena de tiendas "Solidarias", en Países Bajos y luego Alemania, Suiza, Austria, Francia, Suecia, Gran Bretaña y Bélgica.

En 1967, además, la organización católica SOS Wereldhandel, de los Países Bajos, comenzó a importar productos artesanales desde países subdesarrollados, con un sistema de ventas por catálogo. La formación de la red de tiendas Solidarias le otorgó a la SOS Wereldhandel un canal de comercialización estable. Las tiendas Solidarias gozaron de éxito de ventas, donde las sucursales se transformaron en organizaciones autónomas importando productos en forma directa.

En 1973, entró en este sistema de comercio el primer producto alimentario importante: el café FT, producido por cooperativas guatemaltecas bajo la marca común Indio Solidarity Coffee. El café FT constituye un hito importante, dando un gran impulso al crecimiento del sistema.

En los años 1980, las transacciones y su frecuencia permitió que muchos productores encarasen la mejora de la calidad y el diseño de productos, apoyados en una red que les permitía ingresar a los mercados más importantes. La lista de productos involucrados creció con la incorporación de mezclas de café, té, miel, azúcar, cacao, nueces. Las artesanías crecieron en cantidad y calidad, con técnicas de marketing.

En 2006, hay organizaciones de comercio justo en Europa, Canadá, Estados Unidos, Japón; con ventas por más de 3.000 tiendas solidarias, por catálogos, por representantes, por grupos. También es considerable la participación en la red de las diferentes organizaciones religiosas. La aparición de los Sellos identificatorios ha dado un gran impulso al sistema. La primera marca de calidad comercio justo fue en Holanda en 1988. A partir de ese ejemplo, surgieron varias iniciativas de "Etiquetado Justo". En 1997, varias de ellas se organizaron formando la Fairtrade Labelling Organizations International —Organización Internacional de Etiquetado Justo— (FLO). El miembro español de la FLO es Fairtrade España, anteriormente conocido como Asociación del Sello de Productos de Comercio Justo.

Como curiosidad histórica, aunque no debe entenderse como algo directamente relacionado con el movimiento actual, el anarcoindividualista y mutualista estadounidense Josiah Warren, enunciador del «principio del costo» escribió un manifiesto en 1841 en el que se encuentra una reivindicación del comercio justo, con una filosofía no muy diferente a la del actual movimiento por el comercio justo. 

También durante la revolución española de 1936 los sindicatos anarquistas colocaban sellos a los productos elaborados en fábricas colectivizadas por sus propios trabajadores (principalmente a los de exportación, véase UCLEA) para que sus consumidores finales supieran que tal producto era elaborado en una empresa en que sus mismos productores eran los propietarios.
En la actualidad hay un consenso sobre el comercio justo entre tres partidos políticos españoles: PP, PSOE e IU que acostumbran a apoyar las diferentes iniciativas sobre comercio justo en el ámbito local.

El Sello de Comercio Justo o Sello Fairtrade es el sello que, impreso en un producto, garantiza que este proviene de «comercio justo» y se ha producido y comercializado siguiendo los criterios internacionales del comercio justo establecidos por Fairtrade Labelling Organizations (FLO) International. 

La certificación de los productos de comercio justo con un sello permite su fácil identificación y su venta en los canales de distribución habituales. Además la certificación Fairtrade abre el comercio justo a todas aquellas empresas dispuestas a seguir los criterios de comercio justo en la elaboración de uno o más productos. De este modo la certificación Fairtrade ha contribuido de forma significativa al crecimiento global del volumen de los productos de comercio justo vendidos en todo el mundo. 

Fairtrade España se encarga dentro del Estado español de dar la licencia para el uso de este sello y fomenta el que consumidoras y consumidores lo conozcan y opten por los productos Fairtrade. La Asociación del Sello Fairtrade es miembro de FLO International, el organismo internacional que apoya a los productores que trabajan con Fairtrade o desean trabajar con Fairtrade y que establece los criterios o estándares de comercio justo para cada producto.

El sello Madera Justa es el primer sello de comercio justo para el sector forestal y la industria de la madera.

Es el sistema de certificación más completo, puesto que garantiza que se cumplen criterios ambientales como la legalidad de la madera, reducción de emisiones de gases efecto invernadero y la gestión sostenible de los montes, criterios sociales con los principios del comercio justo y criterios económicos como garantizar que al menos los gastos de producción se cubren.

Las empresas certificadas muestran a sus clientes y proveedores un gran nivel de responsabilidad social, ambiental y económica con su entorno.

El sello Madera Justa certifica que la empresa que cumple 81 requisitos de su estándar es responsable ambientalmente, socialmente y económicamente. Esos requisitos se dividen en siete partes que los definen por su categoría:

El comercio justo ha sido relacionado también con la soberanía alimentaria. La activista y autora Esther Vivas considera que el comercio justo debe asumir las premisas y postulados de la soberanía alimentaria y que la aplicación de los principios de la soberanía alimentaria al comercio justo supone “hablar de un comercio justo de proximidad, exceptuando aquellos productos que no se elaboran en nuestro territorio; de un comercio justo respetuoso con el medioambiente y controlado por las comunidades; de un comercio justo que combate las políticas neoliberales y a las multinacionales".

WFTO es otro sello que garantiza la procedencia bajo criterios de comercio justo.



El comercio justo ha recibido críticas por traducirse en un precio final más caro que los productos tradicionales y por exigir a los productores complejos sistemas organizativos, que muchas veces no se corresponden con el sistema productivo originario y con las tradiciones de los agricultores locales.

El comercio justo ha recibido también críticas desde ambos lados del espectro político. Algunos economistas lo consideran un tipo de subsidio que entorpece el crecimiento. Defensores del libre mercado argumentan que el término es falaz, puesto que las transacciones comerciales sólo puede ocurrir si las partes implicadas las aceptan libremente, haciendo que sea intrínsecamente justo. Por otro lado, grupos de izquierda critican que el comercio justo no supone ningún cambio real sobre el actual sistema comercial.
También han surgido críticas de algunos comerciantes que dicen que, debido a que los locales donde se colocan están subvencionados por organismos públicos y los dependientes suelen ser voluntarios de las ONG es un sistema injusto para los comerciantes autóctonos, que no pueden competir en régimen de igualdad contra estos establecimientos.

El tratamiento de los medios de comunicación del comercio justo, si atendemos a la teoría de Auguste Comte de las funciones de los medios de comunicación para el cambio social vemos dos funciones sociales de los medios de comunicación muy por encima del resto. Estas serían la función de cohesión o consenso y la de otorgar estatus o reconocimiento. Por tanto, podemos comprobar que los medios están participando activamente en la concienciación al respecto de este tipo de comercio, informando de la mayor parte de iniciativas que se llevan a cabo al respecto.





</doc>
<doc id="3525" url="https://es.wikipedia.org/wiki?curid=3525" title="OpenBSD">
OpenBSD

OpenBSD es un sistema operativo libre tipo Unix multiplataforma, basado en 4.4BSD. Es un descendiente de NetBSD, con un foco especial en la seguridad y la criptografía.

Este sistema operativo se concentra en la portabilidad, cumplimiento de normas y regulaciones, corrección, seguridad proactiva y criptografía integrada.
OpenBSD incluye emulación de binarios para la mayoría de los programas de los sistemas SVR4 (Solaris), FreeBSD, Linux, BSD/OS, SunOS y HP-UX.

Se distribuye bajo la licencia BSD, aprobada por la OSI.

OpenBSD se creó como una variante de NetBSD debido a las diferencias filosóficas y personales entre Theo de Raadt y los demás miembros fundadores de NetBSD. Dejando aparte el hecho de que la seguridad sea la principal razón para que OpenBSD exista, el proyecto también tiene otras metas. Siendo un descendiente de NetBSD, es un sistema operativo muy portable. Actualmente funciona sobre 17 plataformas distintas de hardware.

Hasta junio de 2002, el sitio web de OpenBSD ostentaba el eslogan: 

Esto debió ser cambiado por: 

después de que se encontrara un agujero en OpenSSH y posteriormente por: 

al encontrase un fallo en el módulo de IPv6. 

Algunas personas han criticado este lema ya que casi nada está activado en la instalación por defecto de OpenBSD, y las versiones estables han incluido software en el que posteriormente se encontraron agujeros de seguridad. El equipo de programadores de OpenBSD mantiene que el eslogan se refiere una instalación por defecto del sistema operativo, y que es correcto ajustándose a su definición. 

Uno de las innovaciones fundamentales del proyecto OpenBSD es introducir el concepto del sistema operativo "Seguro por Defecto". Según la ciencia de la seguridad informática es estándar y además fundamental activar la menor cantidad posible de servicios en máquinas que se encuentren en producción. Incluso sin tener en cuenta esta práctica, OpenBSD es considerado un sistema seguro y estable.

Como parte de una "limpieza de cadenas", todas las apariciones de strcpy, strcat, sprintf y vsprintf en el código han sido sustituidas por variantes más seguras, tales como strlcpy, strlcat, snprintf, vsnprintf y asprintf. Adicionalmente a sus permanentes auditorías de código, OpenBSD contiene criptografía fuerte. 

Más recientemente, muchas nuevas tecnologías han sido integradas en el sistema, incrementando aún más su seguridad. Desde la versión 3.3, ProPolice está activado por defecto en el compilador GCC, garantizando protección adicional ante ataques de desbordamiento de pila. En OpenBSD 3.4, esta protección fue activada también en el núcleo. OpenBSD también implementa el sistema W^X (pronunciado W XOR X), que es un esquema de gestión de memoria de gran detalle, que asegura que la memoria es editable o ejecutable, pero jamás las dos, proveyendo así de otra capa de protección contra los desbordamientos de búfer. Separación de privilegios, revocación de privilegios y carga de librerías totalmente aleatoria también contribuyen a aumentar la seguridad del sistema.

En mayo de 2004, OpenBSD/sparc fue más allá en la protección de la pila, añadiendo StackGhost.

Un analizador estático de dimensiones fue añadido al compilador, que intenta encontrar fallos comunes de programación en tiempo de compilación. Se puede usar Systrace para proteger los puertos del sistema.

OpenBSD usa un algoritmo de cifrado de contraseñas derivado del Blowfish de Bruce Schneier. Este sistema se aprovecha de la lentitud inherente del cifrado del Blowfish para hacer la comprobación de contraseñas un trabajo muy intensivo para la CPU, dificultando sobremanera el procesamiento paralelo. Se espera que así se frustren los intentos de descifrado por medio de fuerza bruta. 

Debido a todas estas características, OpenBSD se usa mucho en el sector de seguridad informática como sistema operativo para cortafuegos y sistemas de detección de intrusos. El filtro de paquetes de OpenBSD, pf es un potente cortafuegos desarrollado a causa de problemas con la licencia de ipf. OpenBSD fue el primer sistema operativo libre que se distribuyó con un sistema de filtrado de paquetes incorporado.

La filosofía de OpenBSD puede ser reducida a 3 palabras:

"Libre" hace referencia a su licencia, "funcional" se refiere al estado en el cual se decide finalizar el versionado de los programas, y "seguro" por su extrema revisión y supervisión del código incluido en sus versiones.




</doc>
<doc id="3526" url="https://es.wikipedia.org/wiki?curid=3526" title="FreeBSD">
FreeBSD

FreeBSD es un sistema operativo de código abierto para computadoras basado en las CPU de arquitectura x86, Intel 80386, Intel 80486 (versiones SX y DX), y Pentium. En la actualidad se ejecuta en once arquitecturas distintas como Alpha, AMD64, IA-64, MIPS, PowerPC y UltraSPARC.

FreeBSD está basado en BSD-Lite versión 4.4 del Computer Systems Research Group (CSRG) de la Universidad de California en Berkeley siguiendo la tradición que ha distinguido el desarrollo de los sistemas BSD. Además del trabajo realizado por el CSRG, el proyecto FreeBSD ha invertido miles de horas en ajustar el sistema para ofrecer las máximas prestaciones en situaciones de carga real.

Es un derivado de código abierto y gratuito de BSD (Berkeley Software Distribution) con un enfoque en velocidad, estabilidad, seguridad y consistencia, entre otras características. Ha sido desarrollado y mantenido por una gran comunidad desde su lanzamiento inicial el 1 de noviembre de 1993.

BSD es la versión de UNIX desarrollada en la Universidad de California en Berkeley, y "Free" es el sufijo para BSD, al ser una versión de código abierto y gratuito.

FreeBSD ofrece una gran cantidad de funciones avanzadas e incluso cuenta con algunas no disponibles en algunos sistemas operativos comerciales. Es un excelente servidor de Internet e Intranet gracias a sus robustos servicios de red que le permiten maximizar la memoria y trabajar con cargas pesadas para entregar y mantener buenos tiempos de respuesta para miles de procesos de usuario simultáneos.

Ejecuta una gran cantidad de aplicaciones con facilidad. Por el momento, tiene más de 24.000 aplicaciones y bibliotecas portadas con soporte para escritorio, servidor y entornos integrados. FreeBSD es excelente para trabajar con plataformas integradas avanzadas, está disponible para instalar de varias maneras y hay instrucciones a seguir para cualquier método que desees usar, ya sea a través de CD-ROM, a través de una red usando NFS o FTP, o DVD.

Es fácil de contribuir y todo lo que tienes que hacer es ubicar la sección de la base de código de FreeBSD para modificar y hacer un trabajo ordenado. Los potenciales contribuyentes también son libres de mejorar sus ilustraciones y documentación, entre otros aspectos del proyecto, FreeBSD es una organización sin fines de lucro con la que puedes contribuir financieramente.

La licencia de FreeBSD permite a los usuarios incorporar el uso de software propietario que es ideal para las empresas interesadas en generar ingresos. Netflix, por ejemplo, podría citar esto como una de las razones para usar servidores FreeBSD.

La mascota del sistema operativo es Beastie.

FreeBSD es un sistema operativo multiusuario, capaz de efectuar multitarea con apropiación y multiproceso en plataformas compatibles con múltiples procesadores; el funcionamiento de FreeBSD está inspirado, como ya se dijo, en la variante BSD-Lite 4.4 de UNIX. Aunque FreeBSD no puede ser propiamente llamado UNIX, al no haber adquirido la debida licencia de The Open Group, FreeBSD sí está hecho para ser compatible con la norma POSIX, al igual que varios otros sistemas "clones de UNIX". 

El sistema FreeBSD incluye el núcleo, la estructura de ficheros del sistema, bibliotecas de la API de C, y algunas utilidades básicas. La versión 6.1 
trajo importantes mejoras como mayor apoyo para dispositivos Bluetooth y controladores para tarjetas de sonido y red.

La versión 7.0, lanzada el 27 de febrero de 2008, incluye compatibilidad con el sistema de archivos ZFS de Sun y a la arquitectura ARM, entre otras novedades.

Los instaladores, código fuente y paquetes del sistema operativo FreeBSD se distribuyen de manera libre al público, en forma de archivos e imágenes ISO disponibles en servidores FTP y a través de la WWW. También es posible comprarlos en forma de CD-ROM o DVD.

La instalación del sistema FreeBSD puede ser iniciada de varias formas. La más común es la utilización de un CD-ROM o DVD auto-arrancable, o utilizando un juego de 2 o 3 disquetes (en función de la versión que se desea instalar), o incluso mediante red utilizando el estándar PXE.

Todas ellas arrancan la computadora con un sistema FreeBSD abreviado, y llevan a la misma utilidad "sysinstall". La utilidad sysinstall es la encargada de instalar realmente el sistema operativo, y posee varias alternativas. A saber, instalar el sistema utilizando los datos disponibles en un dispositivo de almacenamiento local (CD-ROM, DVD, directorio en un sistema de archivos FAT, etc.), u obteniéndolos desde un sitio remoto a través de un protocolo de transferencia de archivos (HTTP, FTP, NFS, etc.).

FreeBSD al igual que varios otros sistemas inspirados en BSD, provee de manejo semi-automatizado de paquetes distribuidos en formato comprimido (en formato tar.bz o .tbz). Además de eso, y al igual que NetBSD y OpenBSD, FreeBSD provee para conveniencia del usuario, de un eficiente sistema de gestión de paquetería llamado ports. Los ports son un conjunto de comandos por lotes, que especifican exactamente los requisitos, lo que se debe hacer para compilar el código fuente y lo necesario para instalar la versión ejecutable de un determinado paquete de software en el sistema. Existen miles de programas libres y comerciales hechos para sistemas como GNU/Linux, que también tienen versiones en FreeBSD. Debido a que muchos de los paquetes están ya compilados y preparados por los participantes del proyecto FreeBSD, estos pueden ser instalados simplemente seleccionándolos en una interfaz provista por el sistema operativo, y copiados directamente desde un servidor HTTP o FTP.

FreeBSD es compatible con binarios de varios sistemas operativos de tipo Unix, incluyendo GNU/Linux. La razón de esto es la necesidad de ejecutar algunas aplicaciones desarrolladas para ser ejecutadas en sistemas con el núcleo Linux en las que el código fuente no se distribuye públicamente y, por tanto, no pueden ser portadas a FreeBSD.

Algunas de las aplicaciones usadas bajo esta compatibilidad son la versión de GNU/Linux de Adobe Flash Player, Linux-Opera, Netscape, Adobe Acrobat, RealPlayer, VMware, Oracle, WordPerfect, Skype, Doom 3, Quake 4, Unreal Tournament y varias más. 

Si bien algunas aplicaciones funcionan perfectamente, otras se ven limitadas debido a que la capa de compatibilidad solo incluye las llamadas de sistema del núcleo Linux 2.4.2, una versión antigua. Una emulación incompleta del núcleo Linux 2.6 está incluida en FreeBSD 7.x, aunque todavía no viene activada por defecto. FreeBSD 8.x implementa compatibilidad con las llamadas nativas del núcleo Linux 2.6 y el conjunto de librerías base de Fedora 10.




</doc>
<doc id="3527" url="https://es.wikipedia.org/wiki?curid=3527" title="NetBSD">
NetBSD

NetBSD es un sistema operativo de la familia Unix de código abierto y libre, y, a marzo de 2019, disponible para 58 plataformas de hardware. Su diseño y sus características avanzadas lo hacen ideal para multitud de aplicaciones. 

NetBSD ha surgido como resultado del esfuerzo de un gran número de personas que tienen como meta producir un sistema operativo tipo Unix accesible y libremente distribuible.

La primera versión de NetBSD (0.8) data de 1993 y surge del sistema operativo BSDLite 4.3, una versión de UNIX desarrollada en la Universidad de California Berkeley, y del sistema 386BSD, el primer BSD portado al CPU Intel 386.

NetBSD toma su nombre de la versión 4BSD/Tahoe-Net/1 de los BSD, pues sobre ellos se desarrolló el protocolo TCP/IP, el protocolo más importante en Internet. NetBSD, al igual que FreeBSD, se deriva de la última versión de los BSD, la 386BSD 0.1. El primer release de NetBSD (la versión 0.8) vio el mundo el 20 de abril de 1993.

NetBSD está basado en una gran variedad de software de libre distribución que incluye entre otros, a 4.4BSD Lite de la Universidad de California-Berkeley, a Net/2 (Berkeley Networking Release 2) el sistema de ventanas X del MIT y software de GNU.

Actualmente NetBSD se centra en ofrecer un sistema operativo estable, multiplataforma, seguro.Está diseñado teniendo como prioridad escribir código de calidad y bien organizado, y teniendo muy en cuenta también el cumplimiento de estándares (POSIX, X/Open y otros más relevantes): prueba de este buen diseño es su amplia portabilidad.

Se trata de un sistema operativo maduro, producto de años de desarrollo (los orígenes de BSD están sobre el año 1977), y partiendo del sistema UNIX sexta edición.

Algunas ventajas sobre otros sistemas operativos: 


NetBSD ha sido portado a un gran número de arquitecturas de computadores, desde minicomputadores VAX a PDAs Pocket PC; el lema de NetBSD es «"Of course it runs NetBSD"» (por supuesto que corre NetBSD). El núcleo y el espacio de usuario para todas las plataformas soportadas (que comprenden alrededor de una veintena de diferentes procesadores) se compilan desde un árbol de código central y unificado gestionado con CVS.

Debido a la gestión de código fuente centralizada y a un diseño altamente portable, las adiciones de funcionalidad general (no específicas de un hardware en concreto) benefician a todas las plataformas inmediatamente sin necesidad de «portarlas».

El desarrollo de controladores de dispositivos es también con frecuencia independiente del hardware. Es decir, el controlador para un dispositivo PCI funcionará independientemente de que tal dispositivo esté instalado en un i386, Alpha, PowerPC, SPARC o cualquier otra plataforma con buses PCI. Muchos controladores de NetBSD también tienen el código específico de un cierto bus dividido en subcontroladores de bus, permitiendo a un mismo controlador para un dispositivo específico operar vía diferentes buses (por ejemplo ISA, PCI, PCMCIA...).

Esta independencia de plataforma ayuda gratamente al desarrollo de sistemas embebidos, especialmente desde la aparición en NetBSD 1.6 de la compilación cruzada:

Empezando en NetBSD 1.6, el juego de herramientas completo de compiladores, ensambladores, enlazadores y otras soportan completamente la compilación cruzada, permitiendo compilar un sistema NetBSD completo para una arquitectura desde otro sistema de diferente arquitectura (usualmente más potente), incluso de diferente sistema operativo (el framework de compilación cruzada soporta cualquier sistema POSIX).

La portabilidad de NetBSD es debida a su única capa modular de portabilidad (MPL por sus siglas en inglés, "Modular Portability Layer"). Con la MPL el controlador de dispositivo se aísla completamente de la plataforma hardware, instrucciones E/S, interbloqueo, recuperación de errores, incluso periféricos que usan una pseudo-DMA para escribir un buffer RAM con copy-in y copy-out de la CPU local son transparentemente manejados en la capa de controladores. Por otra parte, varios dispositivos empotrados usando NetBSD no han requerido de software de desarrollo adicional otro que el juego de herramientas.

En otros sistemas como GNU/Linux, en contraste, el código del controlador debe ser readaptado para cada nueva arquitectura. Como consecuencia, en esfuerzos recientes por parte de desarrolladores de NetBSD y Linux para portar el sistema, NetBSD ha tomado un 10% del tiempo del de Linux para ser portado al nuevo hardware. Los ingenieros que portaron NetBSD al procesador SuperH tardaron sólo seis semanas; para portar Linux se tardó tres meses. NetBSD fue portado a la plataforma AMD64 en aproximadamente un mes, mientras Linux se tomó unos seis meses.

En 2005, como demostración de la portabilidad y conveniencia de NetBSD para aplicaciones empotradas, Technologic Systems, un vendedor de sistemas hardware empotrados, diseñó y demostró un tostador de cocina funcionando con NetBSD.

El logotipo de NetBSD, una gran bandera ondeante, fue diseñado por Grant Bisset luego de que varios miembros del equipo de desarrollo de NetBSD, señalaron al viejo logo de 1994 como inadecuado para un proyecto internacional pues estaba inspirado en el levantamiento de la bandera estadounidense en Iwo Jima.

Todo el código fuente de NetBSD está liberado bajo la licencia BSD y sus cláusulas 1,2,3 y 4. Esto hace posible que cualquiera pueda usar, modificar e incluso vender NetBSD siempre y cuando mantenga los reconocimientos. 

El 20 de junio de 2008, la Fundación de NetBSD anunció una transición a la licencia BSD de dos cláusulas, citando algunas preocupaciones con el soporte de UCB de la cláusula 3 y aplicabilidad industrial de la cláusula 4.

NetBSD también incluye las herramientas de desarrollo de GNU y otros paquetes que están cubiertos por la licencia GPL y otras licencias de código abierto.

Uno de los proyectos más interesantes de NetBSD es su sencillo y poderoso sistema de paquetes, pkgsrc. Dado que el kernel de NetBSD es portable a muchas arquitecturas, pkgsrc es un meta sistema, esto es, descarga código fuente y compila para producir los binarios. Este sistema de paquetes funciona de manera similar a emerge, de la distribución Gentoo Linux. Pkgsrc es una manera sencilla de tener las últimas versiones de software como Openoffice.org, KDE o Gnome, entre otros muchos programas. 

Recientemente Sun Microsystems ha financiado parte del desarrollo de pkgsrc. Actualmente pkgsrc está disponible para diferentes sabores de Unix como Irix, Solaris, FreeBSD, OpenBSD, en la lista, además, se incluye a Slackware Linux, aunque en principio es posible instalarlo en cualquiera de las distribuciones de GNU/Linux. DragonFlyBSD, otra distribución de BSD, también ha adoptado a pkgsrc como su sistema de paquetes. Este sistema creó su última actualización de un sistema el 25 de septiembre de 2005, el cual correspondería a la versión 5.1.




</doc>
<doc id="3528" url="https://es.wikipedia.org/wiki?curid=3528" title="PHP">
PHP

PHP es un lenguaje de programación de uso general que se adapta especialmente al desarrollo web. Fue creado inicialmente por el programador danés-canadiense Rasmus Lerdorf en 1994. En la actualidad, la implementación de referencia de PHP es producida por The PHP Group. PHP originalmente significaba "Personal Home Page" (Página personal), pero ahora significa el inicialismo recursivo "PHP: Hypertext Preprocessor".

El código PHP suele ser procesado en un servidor web por un intérprete PHP implementado como un módulo, un daemon o como un ejecutable de interfaz de entrada común (CGI). En un servidor web, el resultado del código PHP interpretado y ejecutado —que puede ser cualquier tipo de datos, como el HTML generado o datos de imágenes binarias— formaría la totalidad o parte de una respuesta HTTP. Existen diversos sistemas de plantillas, sistemas de gestión de contenidos y "frameworks" que pueden emplearse para organizar o facilitar la generación de esa respuesta. Por otra parte, PHP puede utilizarse para muchas tareas de programación fuera del contexto de la web, como aplicaciones gráficas autónomas y el control de drones. También se puede interpretar y ejecutar un código PHP cualquiera a través de una interfaz de línea de comandos (CLI).

El intérprete estándar de PHP, impulsado por Motor Zend, es un software libre publicado bajo Licencia PHP. PHP ha sido ampliamente portado y puede ser desplegado en la mayoría de los servidores web en casi todos los sistemas operativos y plataformas, de forma gratuita.

El lenguaje PHP evolucionó sin una especificación formal escrita o un estándar hasta 2014, con la implementación original actuando como el estándar "de facto" que otras implementaciones intentaban seguir. Desde 2014, se ha trabajado para crear una especificación formal de PHP.

Para abril de 2020, más de la mitad de los sitios en la web que usan PHP siguen en la versión descontinuada 5.6 o anterior. y con la versión 7.0 y 7.1 más del 68%, que no están oficialmente soportadas por The PHP Development Team, mientras que el soporte de seguridad es proporcionado por terceros, como Debian (hasta junio de 2020 para PHP 5). Adicionalmente, la versión 7.2, la versión con soporte más popular, dejará de recibir actualizaciones de seguridad el 30 de noviembre de 2020.

PHP puede ser desplegado en la mayoría de los servidores web y en todos los sistemas operativos y plataformas sin costo alguno. El lenguaje PHP se encuentra instalado en más de 20 millones de sitios web y en un millón de servidores. Migrar los servicios basados en PHP hacia las nuevas tecnologías que aparecen, supone un costo a justificar monetariamente (sobre todo, cuando hablamos de hardware y rendimiento), por ello, hablar si el número de sitios basados en PHP se ha visto reducido progresivamente en los últimos años, con la aparición de nuevas tecnologías como Node.js, Golang, ASP.NET, etc., o no, supone abrir un debate no carente de falacias y argumentos demagógicos. Es un hecho constatado que, en el mundo empresarial, solo se cambian las cosas cuando va a suponer una ventaja estratégica en el mercado. Las empresas carecen de un sentido que les polaricen los 'sentimientos', emergiendo una pauta de posicionamiento mercantilista a favor o en contra de algo solo por un sentido despectivo hacia ciertas tecnologías, tal como sucede con el mundo linux, windows y/o mac en el ámbito de los usuarios.

Poderse hacer una visión general de como PHP está evolucionando a día de hoy, es hablar sobre las nuevas librerías disponibles, el uso de matrices, manejo de números enteros, tratamiento de la criptografía cuando se usa una Base64, Web scraping, indexación de webs, precocinado de datos, Macrodatos, set de datos, supervisión de datos, set de proposiciones dimensionales. Como vemos, una visión general de este lenguaje, es mucho más compleja que afirmar que es un lenguaje para desarrollo web. Dicha realidad, subyace en la aparición de dominios xyz, cuya finalidad y especialidad es el cacheado de los servicios PHP para servir contenido con una orientación al desempeño vinculada a servicios para consumo de otros servicios que necesitan datos, tales como I.A.s. donde PHP está viendo una acogida con un sentimiento de confianza en los resultados a obtener (sin pretender servir de lenguaje en sí mismo orientado a la I.A., dado que no fue su conceptualización origen). Aunque el marketing que está recibiendo este tipo de dominios, está relacionado con las nuevas generaciones de personas educadas en las nuevas tecnologías Como parte de la visión general, habría que incluir un último aspecto, y es la capacidad de generar capas de seguridad autodependientes, en un paradigma solo emergente gracias a las librerías de las que dispone PHP (En proceso de desarrollo).

El sitio web de Wikipedia está desarrollado en PHP. Es también el módulo Apache más popular entre las computadoras que utilizan Apache como servidor web.

El gran parecido que posee PHP con los lenguajes más comunes de programación estructurada, como C y Perl, permiten a la mayoría de los programadores crear aplicaciones complejas con una curva de aprendizaje muy corta. También les permite involucrarse con aplicaciones de contenido dinámico sin tener que aprender todo un nuevo grupo de funciones.

Aunque todo en su diseño está orientado a facilitar la creación de sitios webs, es posible crear aplicaciones con una interfaz gráfica de usuario, utilizando alguna extensión como puede ser PHP-Qt, PHP-GTK, WxPHP, WinBinder, Roadsend PHP, Phalanger, Phc o HiP Hop VM. También puede ser usado desde la línea de comandos, de la misma manera como Perl o Python pueden hacerlo; a esta versión de PHP se la llama PHP-CLI ("Command Line Interface").

Cuando el cliente hace una petición al servidor para que le envíe una página web, el servidor ejecuta el intérprete de PHP. Este procesa el script solicitado que generará el contenido de manera dinámica (por ejemplo obteniendo información de una base de datos). El resultado es enviado por el intérprete al servidor, quien a su vez se lo envía al cliente.

Mediante extensiones es también posible la generación de archivos PDF, Flash, así como imágenes en diferentes formatos.

Permite la conexión a diferentes tipos de servidores de bases de datos tanto SQL como NoSQL tales como MySQL, PostgreSQL, Oracle, ODBC, DB2, Microsoft SQL Server, Firebird, SQLite o MongoDB.

PHP también tiene la capacidad de ser ejecutado en la mayoría de los sistemas operativos, tales como Unix (y de ese tipo, como Linux o Mac OS X) y Microsoft Windows, y puede interactuar con los servidores de web más populares ya que existe en versión CGI, módulo para Apache, e ISAPI.

PHP es una alternativa a las tecnologías de Microsoft ASP y ASP.NET (que utiliza C# y Visual Basic .NET como lenguajes), a ColdFusion de la empresa Adobe, a JSP/Java, CGI/Perl y a Node.js/Javascript. Aunque su creación y desarrollo se da en el ámbito de los sistemas libres, bajo la licencia GNU, existe además un entorno de desarrollo integrado comercial llamado Zend Studio. CodeGear (la división de lenguajes de programación de Borland) ha sacado al mercado un entorno de desarrollo integrado para PHP, denominado 'Delphi for PHP. También existen al menos un par de módulos para Eclipse, uno de los entornos más populares.

Fue originalmente diseñado en Perl, con base en la escritura de un grupo de CGI binarios escritos en el lenguaje C por el programador danés-canadiense Rasmus Lerdorf en el año 1994 para mostrar su currículum vítae y guardar ciertos datos, como la cantidad de tráfico que su página web recibía. El 8 de junio de 1995 fue publicado "Personal Home Page Tools" después de que Lerdorf lo combinara con su propio "Form Interpreter" para crear PHP/FI.

Dos programadores israelíes del Technion, Zeev Suraski y Andi Gutmans, reescribieron el analizador sintáctico ("parser", en inglés) en 1997 y crearon la base del PHP3, y cambiaron el nombre del lenguaje por "PHP: Hypertext Preprocessor". Inmediatamente comenzaron experimentaciones públicas de PHP3, y se publicó oficialmente en junio de 1998. Para 1999, Suraski y Gutmans reescribieron el código de PHP, y produjeron lo que hoy se conoce como motor Zend. También fundaron Zend Technologies en Ramat Gan, Israel.

En mayo del 2000, PHP 4 se lanzó bajo el poder del motor Zend 1.0. El 13 de julio de 2007 se anunció la suspensión del soporte y desarrollo de la versión 4 de PHP, y, a pesar de lo anunciado, se ha liberado una nueva versión con mejoras de seguridad, la 4.4.8, publicada el 13 de enero del 2008, y posteriormente la versión 4.4.9, publicada el 7 de agosto del 2008. Según esta noticia, se le dio soporte a fallos críticos hasta el 9 de agosto del 2008.

El 13 de julio del 2004, se lanzó PHP 5, utilizando el motor Zend Engine 2.0 (o Zend Engine 2). Incluye todas las ventajas que provee el nuevo Zend Engine 2, como:


La sintaxis de PHP, se fundamenta en los principios de programación de C.

El intérprete de PHP solo ejecuta el código que se encuentra entre sus delimitadores. Los delimitadores más comunes son codice_1 para abrir una sección PHP y codice_2 para cerrarla. El propósito de estos delimitadores es separar el código PHP del resto de código, como por ejemplo el HTML. En los archivos que contienen solo código PHP, el delimitador codice_2 se puede omitir. De hecho, PHP-FIG a través de sus PHP Standard Recommendation ("recomendaciones estándar para PHP") recomienda omitir el delimitador codice_2, ya que así no se envía contenido HTML de manera accidental. Por ejemplo, si se envía un carácter "no PHP" (que no es procesado por el intérprete de PHP), no se podrán ejecutar ciertas acciones como enviar encabezados HTTP a través de la función codice_5, ya que el proceso de respuesta ya ha comenzado.

Las variables se prefijan con el símbolo del dólar (codice_6) y no es necesario indicar su tipo. Las variables, a diferencia de las funciones, distinguen entre mayúsculas y minúsculas. Las cadenas de caracteres pueden ser encapsuladas tanto en dobles comillas como en comillas simples, aunque en el caso de las primeras, se pueden insertar variables en la cadena directamente, sin necesidad de concatenación.

Los comentarios se pueden escribir bien con dos barras al principio de la línea, o con una almohadilla. También permite comentarios multi-línea encapsulados en codice_7.

En cuanto a las palabras clave, PHP comparte con la mayoría de otros lenguajes con sintaxis C las condiciones con codice_8, los bucles con codice_9 y codice_10 y los retornos de funciones. Como es habitual en este tipo de lenguajes, las sentencias deben acabar con punto y coma (codice_11).

Programa Hola mundo con PHP embebido en código HTML:
<!DOCTYPE html>
<html lang="es">

<head>
</head>

<body>
</body>

</html>
Programa Hola mundo con PHP en forma de app monolítica:

<?php

// Area de cabeceras, constantes e includes (inicialización de la aplicación)
// Encabezado solo para demostración, no es necesario
header('Content-Type: text/html; charset=UTF-8');

$respuesta = ";

// Lógica de la aplicación, como obtener usuario de la sesión, contenido, etc.

// Este código podría venir de una plantilla externa para una fácil reutilización
$respuesta .= '<!DOCTYPE html>';
$respuesta .= '<html lang="es">';
$respuesta .= ";
$respuesta .= '<head>';
$respuesta .= ' <meta charset="UTF-8" />';
$respuesta .= ' <meta name="viewport" content="width=device-width, initial-scale=1.0" />';
$respuesta .= ' <title>Ejemplo básico de PHP</title>';
$respuesta .= '</head>';
$respuesta .= ";
$respuesta .= '<body>';
$respuesta .= ' <h1>Hola mundo!</h1>';
$respuesta .= '</body>';
$respuesta .= ";
$respuesta .= '</html>';

// Envía respuesta
echo $respuesta;

// Notar que no se cierra la "etiqueta" <?php. PHP-FIG recomienda esta práctica en
// archivos que contienen solo código PHP para así no enviar contenido HTML de
// manera accidental.

Programa Hola mundo con PHP respondiendo en formato JSON:
<?php

// Archivo respuesta_json.php
// Le comunica al navegador que la respuesta será JSON
header('Content-Type: application/json; charset=UTF-8');

$respuesta = [

echo json_encode($respuesta);

// No se cierra la "etiqueta" <?php ya que el archivo contiene solo código PHP

El lado del cliente que realizará petición al archivo PHP:

<!DOCTYPE html>
<html lang="es">

<head>
</head>

<body>

</body>

</html>

<!DOCTYPE html>
<html lang="es">

<head>
</head>

<body>
</body>

</html>

PHP puede combinarse con MySQL para trabajar con bases de datos, aunque también se pueden utilizar otros motores de base de datos como Microsoft SQL Server, PostgreSQL, MongoDB, entre otros.

En el siguiente ejemplo se muestra un simple inicio de sesión con usuario y contraseña utilizando MySQL y PHP con el estilo orientado a objetos:

<?php

// Conexión a la base de datos local 'sitio'. Normalmente esto vendría de un
// servicio externo que se reutiliza en todo el sitio.
$conexion = new PDO('mysql:host=127.0.0.1;dbname=sitio', 'root', ", [

// Se obtienen los datos enviados por el formulario
// Para la simplicidad de la demostración se utiliza el operador coalesce (??)
$nombre_usuario = trim($_POST['nombre_usuario'] ?? "); // Se eliminan los espacios al inicio y al final
$contrasena = $_POST['contrasena'] ?? "; // La contraseña se deja tal cual, sin trim()

// Solo actuar si el formulario ha sido enviado con datos
if (isset($_POST['enviar']) && !empty($nombre_usuario) && !empty($contrasena)) {

// No se cierra la "etiqueta" <?php ya que el archivo contiene solo código PHP

El siguiente ejemplo hace lo mismo que el anterior, pero con la seguridad de, sea cual sea el estado medio del servicio, si no hay forma de conectar, destruida la clase se cierran los hilos.

<?php //Archivo BD_access.class.php

<?php //Archivo cabecera.h.php
// Solo mostrar errores fatales, de parseo y advertencias para el ejemplo
error_reporting(E_ERROR | E_PARSE | E_WARNING);

//Inclusion de clases para instancias de gestion
$seHaIncluido=include_once(BD_access.class.php);
if($seHaIncluido){

Los servicios REST se fundamentan en tres conceptos fundamentales:



XAMPP es un servidor independiente de plataforma, software libre, que consiste principalmente en la base de datos MySQL, el servidor Web Apache y los intérpretes para lenguajes de script: PHP y Perl. El nombre proviene del acrónimo de X (para cualquiera de los diferentes sistemas operativos), Apache, MySQL, PHP, Perl. El programa está liberado bajo la licencia GNU y actúa como un servidor Web libre, fácil de usar y capaz de interpretar páginas dinámicas. Actualmente XAMPP está disponible para Microsoft Windows, GNU/Linux, Solaris, y MacOS X.

Es un software "liviano" que se puede utilizar en cualquier PC. No necesita muchos recursos.

LAMP presenta una funcionalidad parecida a XAMPP, pero enfocada en Linux, WAMP lo hace enfocado en Windows, y MAMP para MacOS X. UwAmp es muy idéntico a WAMP y se destaca en que se puede ejecutar desde una memoria USB.

Se utiliza PHP en millones de sitios; entre los más destacados se encuentran:



Diseño web en Madrid


</doc>
<doc id="3529" url="https://es.wikipedia.org/wiki?curid=3529" title="Patrick Volkerding">
Patrick Volkerding

Patrick Volkerding (1966), de ascendencia germana, también conocido por muchos como El Hombre (""The Man""), es el fundador y principal desarrollador de la distribución Linux Slackware. Obtuvo su BS en "Computer Science en la Moorhead State University" en el año 1993, mismo año en el cual se lanzó la primera versión de Slackware.

Junto a Chris Lumens, David Cantrell y otros voluntarios inicio el Proyecto Slackware Linux. Debido a la falta de un flujo de ingresos a raíz de la venta de su editor, Walnut Creek CDROM, a BSDi (que fue finalmente vendida a "Wind River Systems"), esas personas dejaron dicho proyecto. En los últimos años Patrick Volkerding manejó Slackware con la ayuda de varios voluntarios y testeadores. Volkerding saca nuevas versiones de Slackware generalmente una vez al año.

Patrick es un miembro/afiliado de La iglesia de los subgenios (Church of SubGenius). El uso de la palabra "Slack" en Slackware es un homenaje a :

""..I'll admit that it was SubGenius inspired. In fact, back in the 2.0 through 3.0 days we used to print the "Bob" Dobbs head on each CD.""

Que en español se traduce por "Voy a admitir que fue inspirado de SubGenius. De hecho, en los tiempos entre 2.0 y 3.0 solíamos imprimir la cabeza de "Bob" Dobbs en cada CD".

En el año 2004 enfermó de una rara infección pulmonar la cual afectó seriamente su rendimiento físico, causando que el desarrollo de Slackware se detenga en su mayoría. Afortunadamente se recuperó y volvió al proyecto.

Patrick Volkerding está actualmente casado con su esposa Andrea, y son padres desde el 22 de noviembre de 2005, de su primera hija llamada Cecilia. También se lo conoce por ser un gran bebedor de cerveza: En las primeras versiones de Slackware rogaba a sus usuarios que le mandaran una botella de cerveza de su ciudad local en aprecio a su trabajo.




</doc>
<doc id="3533" url="https://es.wikipedia.org/wiki?curid=3533" title="GTK">
GTK

GTK (conocido hasta febrero de 2019 como GTK+) o "The GIMP Toolkit" es una biblioteca de componentes gráficos multiplataforma para desarrollar interfaces gráficas de usuario (GUI). Esta licenciado bajo los términos de la GNU LGPL, por lo que permite la creación de tanto software libre como software privativo. GTK es parte del proyecto GNU, siendo usado por proyectos tan importantes como GIMP, Inkscape, GNOME, Xfce, entre otros.

Junto a Qt, es uno de los kit de herramientas de widgets más popular para el sistema operativo GNU/Linux, teniendo un amplio soporte para Wayland y XOrg. Además, se puede emplear para desarrollar aplicaciones gráficas que funcionen sobre otros sistemas Unix-like o sistemas operativos como Microsoft Windows, Mac OS, entre otros.

Fue desarrollada inicialmente por Peter Mattis, Spencer Kimball y Josh MacDonald para implementar la interfaz gráfica del programa de Manipulación de Imágenes de GNU (GIMP) como reemplazo del kit de herramientas Motif; en algún momento Peter Mattis se desencantó con Motif y comenzó a escribir su propio kit de herramientas GUI llamado GIMP toolkit, reemplazando con éxito Motif para la versión 0.60 de GIMP. Finalmente, GTK fue reescrito para ser orientado a objetos y pasó a llamarse GTK+, haciéndose uso de esto por primera vez en la versión 0.99 de GIMP. Siendo la primera versión estable de GTK lanzada el 14 abril de 1998.

GTK está escrito en lenguaje C, y es principalmente desarrollado por el Proyecto GNOME, ya que en 1997 este proyecto escoge GTK como base sobre la cual desarrollar el entorno de escritorio GNOME. Otras aplicaciones gráficas no directamente relacionadas con GNOME también han empleado GTK. Además de otros entornos gráficos para Linux como XFCE y ROX han elegido también GTK como su biblioteca de componentes gráficos.

El desarrollo de GTK se maneja libremente. La discusión ocurre principalmente en varias listas de correo públicas. Los desarrolladores y usuarios de GNOME se reúnen en una reunión anual "GNOME Users And Developers European Conference" GUADEC para discutir el estado actual y la dirección futura de GNOME. GNOME incorpora estándares y programas de freedesktop.org para interactuar mejor con otros escritorios.

Esta biblioteca contenía algunas rutinas de ayuda para resolver tareas básicas de programación, como el almacenamiento de diversos datos. Estos requieren mucho tiempo para el programador, especialmente cuando se trata del desarrollo repetido de programas. Las primeras versiones de GTK incluían estructuras de datos para listas vinculadas, árboles binarios o cadenas "en crecimiento". Además, ya que fue reescrito para ser orientado a objetos, GTK permite usar este paradigma de programación dentro del lenguaje C. Estas herramientas también resultaron útiles para programas sin una interfaz gráfica de usuario y por lo tanto más tarde se trasladaron a una biblioteca separada, llamada "biblioteca" "GLib de funciones C" . El sistema orientado a objetos en la siguiente versión se relevo a la biblioteca "GObject", los métodos de dibujo orientados al sistema a la biblioteca GDK (GTK + Kit de dibujo). Este último permite que GTK se ejecute de forma idéntica en plataformas en Windows, X Window System, macOS , entre otros.

La versión GTK 2 recibió nuevas funciones mejoradas para renderizar texto usando la biblioteca "Pango", la creación de GObject, un nuevo motor de temas, una API más flexible y una biblioteca novedosa ("ATK") para mejorar la accesibilidad del kit de herramientas para personas discapacitadas, por ejemplo, se puede abordar el software de lectura, las herramientas de aumento y los dispositivos de entrada alternativos. GTK 2 no es compatible con GTK 1, por lo que las aplicaciones existentes tuvieron que ser portadas.

A partir de la versión 2.8, GTK + usa la biblioteca Cairo basada en vectores, que usa aceleración de hardware para renderizar si es posible. GTK 2.24 es la última versión de la serie 2.x.

La versión 3.0 fue lanzada el 10 de febrero de 2011. Muchas funciones que se clasificaron como obsoletas se han eliminado y se han mejorado las interfaces existentes. GTK 3 no es compatible con versiones anteriores de GTK 2.x. Sin embargo, las bibliotecas de ambas versiones se pueden instalar en paralelo.

La serie GTK 3 incluye una nueva interfaz para dibujar widgets, que ahora está completamente basada en Cairo, un motor de temas basado en CSS que también permite transiciones de estado animadas, opciones de diseño mejoradas, un nuevo widget de interruptor deslizante, una clase de aplicación, soporte para múltiples dispositivos señaladores "(punteros múltiples)" e iconos simbólicos eso puede cambiar de color según la condición. Las estructuras internas se han separado y ocultado de la interfaz pública, por lo que será más fácil realizar cambios en el futuro sin tener que romper la interfaz. Además, se admiten múltiples backends GDK simultáneamente dentro de la misma biblioteca. Hasta ahora, varias bibliotecas eran necesarias.

Se introdujeron dos nuevos backends: uno para el servidor de visualización Wayland y un back-end HTML5 llamado "Broadway", con el que las aplicaciones GTK pueden operarse de forma remota en el navegador a través de la red. Se admite el desplazamiento múltiple y suave, que también está disponible para Windows, además, se admite animaciones CSS y difuminación de sombras. La versión 3.6.4 es la última versión oficial para Windows de 32 bits y 64 bits.

Existen más cambios como la mejora de la visualización de los monitores de alta resolución. La presentación de Popover (burbujas de discurso cómicas que se pueden usar como ayuda), se admite gestos multitáctiles. Además, se admite la representación de ventanas a través de OpenGL. El nuevo elemento de control codice_1 también permitió integrar objetos 3D directamente en las interfaces del programa. Desde GTK 3.18 (base de Gnome 3.18), la biblioteca es oficialmente compatible con Wayland, el sucesor del sistema X Windows. GTK 3.20 integra el corrector ortográfico con gspell y mejora la temática CSS. Mientras que GTK 3.22 se presenta como una versión LTS y añade soporte a portales Flatpak para instalaciones de software simples.

El 1 de septiembre de 2016, una publicación en el blog de desarrollo de GTK denotó, entre otras cosas, el futuro esquema de numeración de GTK. La versión 3.22 de GTK de otoño de 2016 será la última versión 3.x (o al menos eso era lo planeado). Después de eso, todos los recursos se trasladarán a la serie de desarrollo GTK 4 con los nombres de versión 3.90, 3.92, etc. Incluso cuando la serie 4.x entre en desarrollo, las aplicaciones notables aún usan GTK 2.x y no se han portado a 3.22. Con respecto al futuro del software heredado que usa GTK, no existe un proyecto colectivo para portar el software GTK 2.x a 3.22.

GTK 4.0 se está preparando con la serie "inestable" 3.9x. En marzo de 2017, se agregó el "Scene Graph Kit" (GSK) con la versión 3.90 . Implementa un gráfico de escena y se encarga de la síntesis de imágenes. En febrero de 2019 se anunció que GTK 4.0 eliminará el signo más ("+") de la parte del nombre.

Una de las principales novedades implementadas durante el ciclo de desarrollo de GTK 4 (es decir, GTK 3.92, etc.) ha sido la delegación de funcionalidad a objetos auxiliares en lugar de codificarla en las clases base proporcionadas por GTK.


En el 26 de enero de 2018 en DevConf.cz, Matthias Clasen ofreció una visión general del estado actual del desarrollo de GTK 4, incluida una explicación de alto nivel sobre cómo funcionaba el renderizado y la entrada en GTK 3, qué cambios se estaban realizando en GTK 4, y las razones de esos cambios. También se dieron ejemplos de cosas que se han hecho posibles con GTK 4.


Sobre el backend Quartz, un puerto del sistema X Window, GTK puede usarse en macOS .

Anteriormente, GTK (y GNOME, GLib, etc.) utilizaban el sistema de compilación GNU (denominado Autotools) como el sistema de automatización de compilación elegido.

Desde el 14 de agosto de 2017, la rama maestra de GTK compila con Meson, y los archivos del sistema de compilación de Autotools se han eliminado.

GTK se basa en varias bibliotecas desarrolladas por el equipo de GTK y de GNOME:

Es un conjunto de bibliotecas de bajo nivel, que comprende, GLib Core, GObject y GIO. Estas bibliotecas se desarrollan todas bajo un mismo repositorio de Git, denominado glib. Por lo que "GLib" puede referirse a "GLib core" o todo el conjunto de bibliotecas.

GLib core proporciona el manejo de estructura de datos para C (Listas enlazadas, árboles, tablas hash, entre otras), envoltorios de portabilidad, bucle de eventos, subprocesos, carga dinámica de módulos y muchas más funciones.

GObject por su parte, simplifica los paradigmas de Programación orientada a objetos y la Arquitectura dirigida por eventos para C. La Arquitectura dirigida por eventos no solo es útil para interfaces gráficas de usuario sino también para demonios que respondan a cambios de hardware (insertar un USB, un monitor, etc), software de red u otros procesos, puede llegar a ser de bastante utilidad.

GIO depende de las anteriores bibliotecas del conjunto, este proporciona un API de alto nivel para entrada y salida, ya sea la lectura de un archivo local, una secuencia de red, procesos de comunicación con D-Bus y muchos usos más.

Este conjunto de bibliotecas permite escribir servicios del sistema operativo, bibliotecas, utilidades de linea de comando y mucho más. Por lo que puede llegar a ser más cómodo y recomendable escribir un programa en C haciendo uso de GLib, además, ofrece una API de alto nivel como el estándar POSIX.

Biblioteca que contiene un kit de herramientas de widgets basados en GLib, este kit realmente contiene los objetos y funciones para crear la Interfaz gráfica de usuario (GUI). Maneja "widgets" como ventanas, botones, menús, etiquetas, deslizadores, pestañas, etc.

Biblioteca que actúa como intermediario entre gráficos de bajo nivel y gráficos de alto nivel. De manera que libera a GTK de preocupaciones de bajo nivel como la recopilación de entrada, arrastrar y soltar y la conversión de formato de píxeles. GDK es una capa intermedia que separa GTK de los detalles del sistema de ventanas.

GDK es una parte importante de la portabilidad de GTK. Dado que GLib ya proporciona la funcionalidad multiplataforma de bajo nivel, todo lo que se necesita para hacer que GTK se ejecute en otras plataformas es portar GDK a la capa de gráficos del sistema operativo subyacente. Por lo tanto, los puertos GDK a la API de Windows y Quartz son los que permiten que las aplicaciones GTK se ejecuten en Windows y macOS, respectivamente.

Biblioteca para crear interfaces con características de una gran accesibilidad muy importante para personas discapacitadas o minusválidos. Pueden usarse utilerías como lupas de aumento, lectores de pantalla, o entradas de datos alternativas al clásico teclado o ratón.

Biblioteca para el diseño y renderizado de texto, hace hincapié especialmente en la internacionalización. Está diseñado de forma modular, esto significa que Pango puede ser usado por aplicaciones de diferentes arquitecturas o metodologías de programación. Además, es el núcleo para manejar las fuentes y el texto de la versión 2 de GTK.

Biblioteca de renderizado avanzado de controles de aplicación. usada para proporcionar imágenes basadas en gráficos vectoriales. Además, proporciona primitivas para el dibujo bidimensional a través de una serie de backends diferentes.

Algunos de los programas para diseño de interfaces gráficas con GTK son los siguientes:

Es una herramienta de desarrollo visual de interfaces gráficas mediante GTK/GNOME. Es independiente del lenguaje de programación y predeterminadamente no genera código fuente sino un archivo XML gracias a GTKBuilder, que puede ser usado en numerosos lenguajes de programación incluyendo C, C++, C#, Vala, Java, Perl, Python. La posibilidad de generar automáticamente código fuente fue descontinuada desde la versión 3 de Glade.

De manera más técnica la clase describe la interfaz en un archivo de Lenguaje de marcado extensible (XML) y luego carga la descripción XML en tiempo de ejecución y crea los objetos automáticamente. Glade Interface Designer permite la creación de la interfaz de usuario de manera WYSIWYG. Por lo que la descripción de la interfaz de usuario es independiente del lenguaje de programación utilizado.

Glade puede ser utilizado de manera directa, pero también está totalmente integrado en Anjuta 2. El proyecto se encuentra bajo la licencia GPL.

Parte de MonoDevelop, orientado al trabajo con GTK#, Stetic es muy similar a Glade Interface Designer, pero está integrado en MonoDevelop con funciones como arrastrar y soltar . Ha sido criticado por ser más difícil de trabajar que Qt Designer y Microsoft Visual Studio Windows Forms Editor cuando el programador aún no tiene un diseño concreto en mente.

MonoDevelop puede ejecutarse en las distintas distribuciones de Linux y en Mac. Desde la versión 2.2, MonoDevelop ya cuenta con soporte completo para GNU/Linux, Windows y Mac. Además, incluye un compilador que admite C # 1.0, C # 2.0, C # 3.0, C # 4.0, C # 5.0 y C # 6.0.

Algunas aplicaciones que usan GTK para desarrollar sus interfaces de usuario incluyen:

Los entornos de escritorio no son necesarios para ejecutar los programas GTK. Si las bibliotecas que requiere el programa están instaladas, un programa GTK puede ser ejecutado por encima de otros entornos basadas en X11 como KDE o cualquier otro entorno, lo que incluye Mac OS X, si X11.app está instalado. GTK también puede ejecutarse en Microsoft Windows, es utilizado por algunas aplicaciones populares multiplataforma como Pidgin y GIMP. wxWidgets, un toolkit gráfico multiplataforma usa GTK en sistemas tipo Unix. Algunos de los ports más inusuales incluyen directfb y ncurses.

Dado que GTK es una biblioteca C pura, se puede vincular fácilmente a muchos otros lenguajes de programación. Con los lenguajes de programación orientados a objetos, los GObjects no se usan ; los objetos GTK generalmente se pueden usar allí como objetos nativos del lenguaje de programación. El lenguaje de programación Vala utiliza GObject directamente como un sistema de objetos y, por lo tanto, no requiere una biblioteca en tiempo de ejecución para la conexión del lenguaje.

Existe una gran variedad de lenguajes de programación con los cuales se puede usar GTK, aunque no en todos está disponible en su última versión. Entre los más usados están los siguientes:

La forma clásica de programar con GTK es definir primero las propiedades de los elementos gráficos utilizados, luego agruparlos y vincularlos a rutinas específicas de manejo de eventos (un posible evento sería hacer clic en un botón , por ejemplo ). Sin embargo, también hay herramientas de diseño gráfico para superficies GTK como Glade o Stetic contenidas en MonoDevelop , que pueden ahorrarle los dos primeros pasos, expandir GTK con capacidades de creación de prototipos y hacer posible realizar cambios en la superficie de un programa sin tener que hacer cambios en el código fuente del software.

Un programa típico de Hello World que se muestra en la imagen podría verse así:
// Incluir gtk

static void on_activate (GtkApplication *app) {

int main (int argc, char *argv[]) {

Para compilar este código son necesarias algunas dependencias según el sistema que poseas, en caso de usar Debian o derivadas codice_2 o en caso de usar Fedora codice_3. Para compilar el código, es necesario ubicarse en la ruta del archivo y escribir lo siguiente en una terminal
gcc ejemplogtk.c -o ejemplogtk `pkg-config --cflags --libs gtk+-3.0`
Para ejecutar el archivo, se escribe
./ejemplogtk
import gi
gi.require_version("Gtk", "3.0")
from gi.repository import Gtk

window = Gtk.Window(title="¡Hola Wikipedia!")
window.show()
window.connect("destroy", Gtk.main_quit)
Gtk.main()
Para abrir este código son necesarias algunas dependencias según el sistema que poseas, en caso de usar Debian o derivadas codice_4 o en caso de usar Fedora codice_5.

Para ejecutar el archivo, es necesario ubicarse en la ruta del archivo, luego ejecutarlo escribiendo
python3 hello.py
La apariencia de GTK es en gran medida configurable por el usuario, existiendo una gran cantidad de temas para elegir. Estos temas pueden ser descargados de diferentes sitios como GNOME Look. Los siguiente son los más populares:

Es el nuevo tema estándar de GTK desde mediados de 2014, siendo el tema sobre el que se recomienda trabajar para realizar desarrollos en interfaces gráficas. Esto fue expresado en una carta abierta por los desarrolladores de las aplicaciones GNOME, donde invitaban a usar el tema predeterminado (es decir Adwaita), ya que aseguraban que hacer uso de otros temas podría traer diferencias en la percepción del usuario final, en un mismo programa.

Afirmando que al alterar las hojas de estilo GTK las aplicaciones podrían parecer dañadas o inutilizables, los iconos podrían representar conceptos diferentes al pensado por el programador. También aseguraban que las capturas encontradas dentro de tiendas de software podrían engañar al usuario de la apariencia de la aplicación, y ademas, que podría llegar a generar conflictos en la compresión de la documentación para el usuario final.

Esta propuesta fue aceptada por diferente distribuciones, las cuales adoptaron directamente Adwaita, o basaron sus temas sobre este, como es el caso de Ubuntu con su tema Yaru.

Es el tema oficial de Ubuntu, posee diferentes tonalidades las cuales puede elegir el usuario. El tema se basa en Adwaita.

La crítica más común de GTK es la falta de compatibilidad con versiones anteriores en las principales actualizaciones, sobre todo en la interfaz de programación de aplicaciones (API) y temas.

Benjamin Otte explicó que las interrupciones de compatibilidad entre lanzamientos menores durante el ciclo de desarrollo de GTK 3.x se debieron a fuertes presiones para innovar, como proporcionar las características que los usuarios modernos esperan y apoyar el protocolo de servidor de pantalla Wayland cada vez más influyente. Con el lanzamiento de GTK 4, se habrá liberado la presión de la necesidad de innovar y el equilibrio entre estabilidad e innovación se inclinará hacia la estabilidad. Del mismo modo, los cambios recientes en la temática están destinados específicamente a mejorar y estabilizar esa parte de la API, lo que significa que alguna inversión ahora debería recompensarse más adelante.



</doc>
<doc id="3534" url="https://es.wikipedia.org/wiki?curid=3534" title="GIMP">
GIMP

GIMP (siglas en inglés de GNU Image Manipulation Program) es un programa de edición de imágenes digitales en forma de mapa de bits, tanto dibujos como fotografías. Es un programa libre y gratuito. Forma parte del proyecto GNU y está disponible bajo la Licencia pública general de GNU y GNU Lesser General Public License

Es el programa de manipulación de gráficos disponible en más sistemas operativos (Unix, GNU/Linux, FreeBSD, Solaris, Microsoft Windows y macOS, entre otros).

La interfaz de GIMP está disponible en varios idiomas, entre ellos: español, inglés (el idioma original), catalán, gallego, euskera, alemán, francés, italiano, ruso, sueco, noruego, coreano y neerlandés.

GIMP tiene herramientas que se utilizan para el retoque y edición de imágenes, dibujo de formas libres, cambiar el tamaño, recortar, hacer fotomontajes, convertir a diferentes formatos de imagen, y otras tareas más especializadas. Se pueden también crear imágenes animadas en formato GIF e imágenes animadas en formato MPEG usando un plugin de animación.

Los desarrolladores y encargados de mantener GIMP se esfuerzan en mantener y desarrollar una aplicación gráfica de software libre, de alta calidad para la edición y creación de imágenes originales, de fotografías, de íconos, de elementos gráficos tanto de páginas web como de elementos artísticos de interfaz de usuario.

Los iniciadores del desarrollo de GIMP en 1995 fueron los en aquella época estudiantes Spencer Kimball y Peter Mattis como un ejercicio semestral en la Universidad de Berkeley en el club informático de estudiantes. A 2020 hay un numeroso equipo de voluntarios que se encarga del desarrollo del programa. La primera versión de GIMP se desarrolló inicialmente en sistemas Unix y fue pensada especialmente para GNU/Linux como una herramienta libre para trabajar con imágenes, y desde hace unos años se ha convertido en una alternativa libre y eficaz al Photoshop para gran número de usos.

Las siglas de GIMP significaban inicialmente «"General Image Manipulation Program"» («Programa general para manipulación de imágenes»), pero en 1997 se cambió al significado «"GNU Image Manipulation Program"» («Programa de manipulación de imágenes de GNU»). GIMP forma parte oficial del Proyecto GNU.

GIMP sirve para procesar gráficos y fotografías digitales. Los usos típicos incluyen la creación de gráficos y logos, el cambio de tamaño, recorte y modificación de fotografías digitales, la modificación de imágenes, la combinación y alteración de colores usando un paradigma de capas, la eliminación o alteración de elementos no deseados en imágenes o la conversión entre distintos formatos de imágenes. También se puede utilizar el GIMP para crear imágenes animadas sencillas, la manipulación de vectores, y en edición avanzada de vídeo.

GIMP es también conocido por ser quizás la primera gran aplicación libre para usuarios no profesionales o expertos. Productos libres originados anteriormente, como GCC, el núcleo Linux, etc., eran principalmente herramientas de programadores para programadores. GIMP es considerado por algunos como una demostración fehaciente de que el proceso de desarrollo de software libre puede crear aplicaciones que los usuarios comunes, no avanzados, pueden usar de manera productiva. De esta forma, Gimp abrió camino a otros proyectos como KDE, GNOME, Mozilla Firefox, OpenOffice.org y otras aplicaciones posteriores.

Wilber es la mascota oficial del proyecto GIMP. Fue creado el 25 de septiembre de 1997 por Tuomas Kuosmanen, más conocido como "tigert". Hay otros desarrolladores de GIMP que han contribuido con accesorios adicionales. Imágenes de esta mascota pueden encontrarse en el ""Wilber Construction Kit"", incluido en el código fuente de GIMP dentro del archivo Wilber. Wilber fue dibujado usando GIMP.

GIMP es un programa de manipulación de imágenes que ha ido evolucionando notablemente a lo largo del tiempo. Ha ido soportando formatos adicionales, sus herramientas son más potentes, y además es capaz de funcionar con extensiones o plugins y scripts. Desde hace mucho tiempo se puede usar también con tabletas digitalizadoras.

GIMP utiliza GTK como biblioteca de controles gráficos. En realidad, GTK era simplemente al principio una parte de GIMP, originada al reemplazar la biblioteca comercial Motif usada inicialmente en las primeras versiones de GIMP. GIMP y GTK fueron originalmente diseñados para el sistema gráfico X Window ejecutado sobre sistemas operativos tipo Unix. GTK ha sido portado posteriormente a Microsoft Windows, OS/2, macOS y SkyOS.

GIMP permite el tratado de imágenes en capas, para poder modificar cada objeto de la imagen en forma totalmente independiente a los demás elementos en otras capas de la imagen. También pueden subirse o bajarse de nivel las capas, en una pila, para facilitar el trabajo de la imagen. Cada capa tiene su propia visibilidad y grado de transparencia y, además, hay una larga serie de maneras de combinar las relaciones entre capas. La imagen final puede guardarse en el formato original de GIMP que soporta capas, o en muchos formatos planos sin capas, como puede ser png, bmp, jpg, GIF, PDF, etc.

Con GIMP es posible producir también imágenes de manera totalmente no interactiva automatizada (por ejemplo, generar al vuelo imágenes para una página web usando scripts o guiones CGI) y también realizar un procesamiento por lotes que cambie el color o convierta series de imágenes. Para tareas automatizables más simples, probablemente sea más rápido utilizar un paquete como ImageMagick.

El nombre de GIMP en español se forma con las iniciales de Programa de Manipulación de Imágenes de GNU, leídas de atrás para adelante.

GIMP lee y escribe la mayoría de los formatos de ficheros gráficos, entre ellos; JPG, GIF, PNG, PCX, TIFF, bmp, pix y también la mayoría de los psd (de Photoshop) además de poseer su propio formato abierto de almacenamiento de ficheros, el XCF. Es capaz también de importar y exportar ficheros en pdf y postcript (ps). También importa imágenes vectoriales en formato SVG creadas, por ejemplo, con Inkscape. Gimp también escribe y lee imágenes en formato ora (formato OpenRaster, formato binario abierto), lo que permite transferir gráficos fácilmente por ejemplo con Krita, otro programa de edición de gráficos de código abierto.

GIMP cuenta con muchas herramientas, entre las que se encuentran las siguientes;


Además de un uso interactivo, GIMP permite también la automatización de muchos procesos mediante macros o secuencias de comandos. Para ello incluye un lenguaje llamado Scheme para este propósito. También permite el uso para estas tareas de otros lenguajes como Perl, Python, Tcl y (experimentalmente) Ruby. De esta manera, es posible escribir secuencias de operaciones y "plugins" para GIMP que pueden ser después utilizados repetidamente.

Los "plugins" de GIMP pueden pedir al usuario que introduzca parámetros en las operaciones, ser interactivos, o no. Hay un extenso catálogo de plugins oficiales y también existen otros creados por usuarios que complementan en maneras específicas las funciones de GIMP. Estos plugins son comparables a las extensiones de otros programas, como las del navegador Mozilla Firefox o de Libreoffice.

Algunos de los plugins se van incorporando a las nuevas versiones de gimp formando parte del propio programa, una vez que pasan las pruebas necesarias de estabilidad y usabilidad.

GIMP fue desarrollado inicialmente para sistemas GNU/Linux, y desde muy al principio de su desarrollo fue portado a los sistemas operativos Windows y Mac OS. Sus versiones más recientes están disponibles para estos y para muchos otros sistemas operativos.

En enero de 1996 Spencer Kimball y Peter Mattis publicaron la versión 0.54, que ya soportaba la ampliación por plugins que permitían diseñar todo tipo de efectos, filtros y herramientas adicionales. En la versión 0.60 se mejoró la gestión de la memoria, mientras Peter Mattis desarrollaba el toolkit libre GDK/GTK.

El 26 de febrero de 1997 se publicó la versión 0.99, y en junio de 1997 la versión 0.99.10

La versión 1.0 de GIMP se publicó el 5 de junio de 1998.

La versión 1.2 se publicó en enero de 2001, incluye herramientas de medidas, un nuevo visor de imágenes, etc.

La versión 1.2.5 se publicó en agosto de 2003.

En marzo de 2004 se publicó GIMP 2.0.0, donde se puede apreciar el cambio al toolkit GTK 2.x.

La versión 2.4 se publicó en mayo de 2008 y los cambios más importantes con respecto a la versión 2.2 incluyen una interfaz retocada más pulida, una separación mayor entre la interfaz de usuario y el "back-end", mejoras en muchas de las herramientas como las de selección, y algunas nuevas como el clonado en perspectiva.

La versión de GIMP 2.6 se publicó en octubre de 2008.

La versión de GIMP 2.8 se publicó en mayo de 2012. La versión 2.8.0 incluye la posibilidad de tener una única ventana global que contiene todas las ventanas de imágenes, herramientas, pinceles, opciones de herramientas, etc...(a la manera de Photoshop), y también otras nuevas características como la posibilidad de agrupar capas en carpetas, el poder escribir directamente texto sobre la imagen en lugar de sobre una subventana intermedia, una nueva herramienta para clonado tridimensional ("jaula"), nuevos juegos de pinceles y parámetros predefinidos tomados de Gimp Paint Studio, selectores de parámetros de herramientas mediante deslizadores y también tecleando valores numéricos, y otras muchas mejoras, correcciones de errores y algunas otras modificaciones.

La versión de Gimp 2.10 se publicó en 2018 . Incorpora 6 años de mejoras. Por ejemplo la nueva herramienta de transformación universal, que combina en una sola varias de las existentes, una mejor herramienta de gestión del color, mejoras en la precisión del color (hasta 32 bits con coma flotante), mejoras en la vista previa de los resultados de los filtros, antes de ejecutarlos, sobre la propia área de trabajo o lienzo, nuevos pinceles adicionales (provenientes de My Paint), diez modos nuevos adicionales de combinación de las capas, mejoras en el control de la herramienta gradiente, nuevas herramientas (como por ejemplo Handle), gestión de la resolución de las imágenes y las capas, nuevos temas de la presentación del interfaz de usuario, nuevos iconos opcionales, entre otras muchas. La versión disponible en octubre de 2019 es la 2.10.12, en la cual se han incluido más mejoras (por ejemplo la gestión del modo CMYK) y se han corregido errores. Hay versión de la ayuda en español para complementar la instalación y también se puede visualizar la ayuda en línea. El manual de gimp 2.10 en español también está disponible en https://docs.gimp.org/2.10/es/.

El formato de ficheros xcf propio de gimp ha sido modificado en la versión 2.10 y, por tanto, ficheros grabados en 2.10 no pueden ser leídos por Gimp 2.8. El menú de grabado de ficheros de Gimp 2.10 proporciona la alternativa de grabar ficheros en formato legible por Gimp 2.8.
Con la evolución de la versión 2.8 a la 2.10 algunos de los muchos plugins no funcionan en esta última versión. Para los usuarios de plugins específicos esto puede representar una dificultad. Hay también plugins de la versión 2.8 que continúan funcionando en la 2.10. Funcionan por ejemplo plugins populares como G'mic, o darktable. Con el paso del tiempo, es probable que los plugins que no funcionen en 2.10 se adapten a esta versión e incorporen GEGL y BABL.

Desde octubre de 2019 los desarrolladores están trabajando en las versiones de desarrollo para la versión 3. La versión de desarrollo en este momento es la 2.99.1 . Para la versión 3.0 se planea acabar de pasar GIMP totalmente a la biblioteca gráfica GEGL.

Gimpshop fue una modificación de GIMP con una interfaz (ventanas, posición de los comandos en los menús, terminología, etc..) para hacerlo más parecido al Adobe Photoshop. La última versión es la 2.8.0 y está basada en la versión 2.8.0 de GIMP.

GimPhoto es otra modificación de GIMP cuya interfaz ha sido retocada para parecerse al Adobe Photoshop. La última versión es la 1.4.3 y está basada en la versión 2.4.3 de GIMP.

Seashore es un programa basado en GIMP diseñado para el sistema operativo Mac OS, que utiliza de forma nativa la interfaz Cocoa de OS X. Este programa en la actualidad (diciembre de 2008) se encuentra en la versión 0.1.9 e incluye por el momento solamente un conjunto limitado de los filtros disponibles en GIMP. 

Posee actualmente los elementos básicos del GIMP: Herramientas de selección rectangular, elíptica, lazo; herramientas de color con pincel, brocha, relleno, texto, goma de borrar, selector de color, gradientes, difumino, clonado, zoom. Utiliza capas como el gimp, con las mismas posibilidades de mezcla solapamiento y juegos de opacidad y transparencia entre ellas.

Posee una gestión del color (incluyendo la posibilidad de uso del CMYK) mucho más integrada en Mac OS que GIMP, ya que Seashore se comunica directamente con el Coloursync, mientras que Gimp utiliza la comunicación con el sistema operativo mediante el entorno gráfico intermedio de ventanas X11. 

Seashore escribe los ficheros en el formato del Gimp, el xcf, y también en jpeg, jpeg2000, gif, png y tiff. Es capaz de leer adicionalmente los ficheros pdf, ps. Es más rápido y ligero que el GIMP por su mayor integración en el sistema operativo y sus menores capacidades.

Tiene versiones en varios idiomas, incluido el español, su desarrollador principal considera que la fase temprana de desarrollo en la que se encuentra hace que la traducción requiera todavía muchos cambios.

Seashore puede ser utilizado para tareas sencillas que no requieran filtros avanzados, y es atractivo para los adeptos de la interfaz de usuario de Mac por su buena integración con él, cuando necesitan mezclar diferentes imágenes mediante el uso de capas.

No posee herramientas avanzadas del Gimp como el clonado en perspectiva, el escalado como herramienta (aunque puede escalar con su propio menú), rotación, y otras herramientas. Seashore no dispone de la posibilidad de utilizar rutas y máscaras.

CinePaint, anteriormente conocido como Film Gimp, es una modificación de GIMP que añade soporte para 16 bits de profundidad por canal de color, en total 48 bits por pixel, posee un gestor de fotogramas y otras mejoras, y que es utilizado en la industria cinematográfica.




</doc>
<doc id="3535" url="https://es.wikipedia.org/wiki?curid=3535" title="GNU Lesser General Public License">
GNU Lesser General Public License

La Licencia Pública General Reducida de GNU, o más conocida por su nombre en inglés GNU Lesser General Public License (antes "GNU Library General Public License" o Licencia Pública General para Bibliotecas de GNU), o simplemente por su acrónimo del inglés GNU LGPL, es una licencia de software creada por la Free Software Foundation que pretende garantizar la libertad de compartir y modificar el software cubierto por ella, asegurando que el software es libre para todos sus usuarios. 

Esta licencia permisiva se aplica a cualquier programa o trabajo que contenga una nota puesta por el propietario de los derechos del trabajo estableciendo que su trabajo puede ser distribuido bajo los términos de esta "LGPL Lesser General Public License". El "Programa", utilizado en lo subsecuente, se refiere a cualquier programa o trabajo original, y el "trabajo basado en el Programa" significa ya sea el programa o cualquier trabajo derivado del mismo bajo la ley de derechos de autor: es decir, un trabajo que contenga el Programa o alguna porción de él, ya sea íntegra o con modificaciones o traducciones a otros idiomas. 

Otras actividades que no sean copia, distribución o modificación no están cubiertas en esta licencia y están fuera de su alcance. El acto de ejecutar el programa no está restringido, y la salida de información del programa está cubierta solo si su contenido constituye un trabajo basado en el Programa (es independiente de si fue resultado de ejecutar el programa). Si esto es cierto o no depende de la función del programa. 

La principal diferencia entre la GPL y la LGPL es que la última puede enlazarse a (en el caso de una biblioteca, 'ser utilizada por') un programa no-GPL, que puede ser software libre o software no libre. A este respecto, la GNU LGPL versión 3 se presenta como un conjunto de permisos añadidos a la GNU GPL. 

Estos programas no-GPL o no-LGPL se pueden distribuir bajo cualquier condición elegida si no se tratan de trabajos derivados (derivative work). Si se trata de un trabajo derivado entonces los términos deben permitir modificación por parte del usuario para uso propio y la utilización de técnicas de Ingeniería inversa para desarrollar dichas modificaciones. 
Definir cuándo un trabajo que usa un programa LGPL es un trabajo derivado o no es un asunto legal (ver el texto de la LGPL). Un ejecutable independiente que enlaza a una biblioteca se acepta por lo general como un trabajo que no es derivado de la biblioteca. Sería considerado como un trabajo que utiliza la biblioteca y se aplicaría el párrafo 5 de la LGPL.

De la traducción no oficial al español:

Esencialmente debería ser posible enlazar el software con una nueva versión del programa cubierto por la LGPL. El método utilizado comúnmente para lograr esto es utilizar un mecanismo apropiado de bibliotecas dinámicas o compartidas. En forma alternativa, está permitido enlazar estáticamente una biblioteca LGPL (ver bibliotecas estáticas) si se proporciona el código fuente del programa o se brinda el código objeto para enlazar contra la biblioteca LGPL.

Una característica de la LGPL es que se puede convertir cualquier código LGPL en código GPL (sección 2 de la licencia).
Esta característica es útil para reutilización directa de código LGPL en código GPL de bibliotecas y aplicaciones, o si se quisiera crear una versión del código que no pueda utilizarse en software propietario.

El término "GNU Library General Public License" daba la impresión de que la FSF quería que todas las bibliotecas utilizaran la licencia LGPL y todos los programas utilizaran la licencia GPL. En febrero de 1999 Richard Stallman escribió el documento "Por qué en su próxima biblioteca no debería utilizar la Lesser GPL para Bibliotecas" explicando por qué este no era el caso, y que la LGPL no se debería utilizar necesariamente para bibliotecas:

Al contrario de lo que muchos creen, esto no significa que la FSF infravalore la LGPL, sino simplemente dice que no debería ser utilizada para "todas" las bibliotecas. En el mismo documento se lee:

De hecho, Stallman y la FSF abogan por el uso de licencias incluso menos restrictivas que la LGPL como estrategia (para maximizar la libertad de los usuarios). Un ejemplo destacado es la aprobación de Stallman para utilizar la licencia BSD en el proyecto Vorbis.




</doc>
<doc id="3540" url="https://es.wikipedia.org/wiki?curid=3540" title="Preguntas frecuentes">
Preguntas frecuentes

El término preguntas frecuentes (traducción al español de la expresión inglesa "Frequently Asked Questions", cuyo acrónimo es FAQ) se refiere a una lista de preguntas y respuestas que surgen frecuentemente dentro de un determinado contexto y para un tema en particular. 

En español, se pueden utilizar dos acrónimos: 

Los primeros sistemas FAQ fueron desarrollados en los años sesenta y eran unas interfaces construidos con lenguaje natural y su acceso era bastante restringido; entre los sistemas creados bajo este esquema estaba baseball (1961), el cual respondía a preguntas sobre la liga de béisbol de los estados unidos en el periodo de un año. También estaba el sistema lunar (1972), el cual respondía preguntas sobre el análisis geológico de las piedras lunares obtenidas en las misiones de apoyo en los viajes a la luna; con base en estos sistemas y tras el éxito obtenido de ellos, se fueron desarrollando más; los cuales eran basados en una base de datos de conocimiento escrita de forma manual por expertos.

Aunque el término PP. FF. es de uso reciente, el concepto es muy antiguo. Por ejemplo, Matthew Hopkins escribió "El Descubrimiento de las Brujas" (1647) en forma de preguntas y respuestas.

En el contexto de internet, las PP. FF. se originaron de la lista de correo de la NASA a comienzos de la década de los ochenta. Las primeras PP. FF. se desarrollaron en 1982, cuando el almacenamiento de información tenía todavía un coste muy elevado. En la lista de correos SPACE, se supuso que los usuarios almacenarían los mensajes anteriores, pero en la práctica esto no sucedió. Por este motivo, la dinámica de la lista de correo comenzó a transformarse en una repetición de preguntas que ya se habían respondido en mensajes anteriores.

La costumbre de publicar PP. FF. se extendió a otras listas de correo. La primera persona en publicar una lista de PP. FF. semanal fue Jef Poskanzer en USENET net.graphics/comp.graphics. Hoy en día, en USENET, preguntar por asuntos resueltos en las PP. FF. se considera una falta de "netiquette", puesto que esto indicaría que el usuario no se molestó en consultar la lista antes de preguntar.

En la actualidad, el término se utiliza para referirse a listas de preguntas frecuentes o cualquier listado de preguntas, independientemente de que se formulen con frecuencia o no.

El término PP. FF. —y el concepto de listas de preguntas— ha trascendido el ámbito de internet, y hoy en día es habitual verlo en folletos informativos sobre artículos de consumo.




</doc>
<doc id="3542" url="https://es.wikipedia.org/wiki?curid=3542" title="Número primo">
Número primo

En matemáticas, un número primo es un número natural mayor que 1 que tiene únicamente dos divisores positivos distintos: él mismo y el 1. Por el contrario, los números compuestos son los números naturales que tienen algún divisor natural aparte de sí mismos y del 1, y, por lo tanto, pueden factorizarse. El número 1, por convenio, no se considera ni primo ni compuesto.

Los 168 números primos menores que 1000 son: 

2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, , , 137, , , , , , , , , , , , , , 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991 y 997 . 

El primer número primo a partir del número mil es el 1009, luego de diez mil es el 10 007, a partir de cien mil es el 100 003, inmediatamente después de un millón es el 1 000 003.

La propiedad de ser número primo se denomina primalidad.

En la teoría algebraica de números, a los números primos se les conoce como números "racionales primos" para distinguirlos de los números gaussianos primos.. La primalidad no depende del sistema de numeración, pero sí del anillo donde se estudia la primalidad. Dos es primo racional; sin embargo tiene factores como entero gaussiano: 2 = (1+i)*(1-i).

El estudio de los números primos es una parte importante de la teoría de números, rama de las matemáticas que trata las propiedades, básicamente aritméticas, de los números enteros.

Los números primos están presentes en algunas conjeturas centenarias tales como la hipótesis de Riemann y la conjetura de Goldbach, resuelta por Harald Helfgott en su forma débil.

La distribución de los números primos es un asunto reiterativo de investigación en la teoría de números: si se consideran números aisladamente, los primos parecieran estar distribuidos de modo probabilístico, pero la distribución «global» de los números primos se ajusta a leyes bien definidas.

Las muescas presentes en el hueso de Ishango, que data de hace más de 20 000 años (anterior por tanto a la aparición de la escritura) y que fue hallado por el arqueólogo Jean de Heinzelin de Braucourt, parecen aislar cuatro números primos: 11, 13, 17 y 19. Algunos arqueólogos interpretan este hecho como la prueba del conocimiento de los números primos. Con todo, existen muy pocos hallazgos que permitan discernir los conocimientos que tenía realmente el hombre de aquella época.

Numerosas tablillas de arcilla seca atribuidas a las civilizaciones que se fueron sucediendo en Mesopotamia a lo largo del II milenio a.C. muestran la resolución de problemas aritméticos y atestiguan los conocimientos de la época. Los cálculos requerían conocer los inversos de los naturales, que también se han hallado en tablillas.
En el sistema sexagesimal que empleaban los babilonios para escribir los números, los inversos de los divisores de potencias de 60 ("números regulares") se calculan fácilmente; por ejemplo, dividir entre 24 equivale a multiplicar por 150 (2·60+30) y correr la coma sexagesimal dos lugares. El conocimiento matemático de los babilonios necesitaba una sólida comprensión de la multiplicación, la división y la factorización de los naturales.

En las matemáticas egipcias, el cálculo de fracciones requería conocimientos sobre las operaciones, la división de naturales y la factorización. Los egipcios solo operaban con las llamadas fracciones egipcias, suma de fracciones unitarias, es decir, aquellas cuyo numerador es 1, como formula_1, por lo que las fracciones de numerador distinto de 1 se escribían como suma de inversos de naturales, a ser posible sin repetición formula_2 en lugar de formula_3. Es por ello que, en cierta manera, tenían que conocer o intuir los números primos.

La primera prueba indiscutible del conocimiento de los números primos se remonta a alrededor del año 300 a. C. y se encuentra en los "Elementos" de Euclides (tomos VII a IX). Euclides define los números primos, demuestra que hay infinitos de ellos, define el máximo común divisor y el mínimo común múltiplo y proporciona un método para determinarlos que hoy en día se conoce como el algoritmo de Euclides. Los "Elementos" contienen asimismo el teorema fundamental de la aritmética y la manera de construir un número perfecto a partir de un número primo de Mersenne.

La criba de Eratóstenes, atribuida a Eratóstenes de Cirene, es un método sencillo que permite encontrar números primos. Hoy en día, empero, los mayores números primos que se encuentran con la ayuda de ordenadores emplean otros algoritmos más rápidos y complejos.

Después de las matemáticas griegas hubo pocos avances en el estudio de los números primos hasta el siglo XVII. En 1640 Pierre de Fermat estableció (aunque sin demostración) el pequeño teorema de Fermat, posteriormente demostrado por Leibniz y Euler. Es posible que mucho antes se conociera un caso especial de dicho teorema en China.

Fermat conjeturó que todos los números de la forma 2+1 eran primos (debido a lo cual se los conoce como números de Fermat) y verificó esta propiedad hasta "n" = 4 (es decir, 2 + 1). Sin embargo, el número de Fermat 2 + 1 es compuesto (uno de sus factores primos es 641), como demostró Euler. De hecho, hasta nuestros días no se conoce ningún número de Fermat que sea primo aparte de los que ya conocía el propio Fermat.

El monje francés Marin Mersenne investigó los números primos de la forma 2 − 1, con "p" primo. En su honor, se los conoce como números de Mersenne.

En el trabajo de Euler en teoría de números se encuentran muchos resultados que conciernen a los números primos. Demostró la divergencia de la serie formula_4, y en 1747 demostró que todos los números perfectos pares son de la forma 2(2 - 1), donde el segundo factor es un número primo de Mersenne. Se cree que no existen números perfectos impares, pero todavía es una cuestión abierta.

A comienzos del siglo XIX, Legendre y Gauss conjeturaron de forma independiente que, cuando "n" tiende a infinito, el número de primos menores o iguales que "n" es asintótico a formula_5, donde ln("n") es el logaritmo natural de "n". Las ideas que Bernhard Riemann plasmó en un trabajo de 1859 sobre la función zeta describieron el camino que conduciría a la demostración del teorema de los números primos. Hadamard y De la Vallée-Poussin, cada uno por separado, dieron forma a este esquema y consiguieron demostrar el teorema en 1896.

Actualmente no se comprueba la primalidad de un número por divisiones sucesivas, al menos no si el número es relativamente grande.

Durante el siglo XIX se desarrollaron algoritmos para saber si un número es primo o no factorizando completamente el número siguiente (p+1) o el anterior (p-1). Dentro del primer caso se encuentra el test de Lucas-Lehmer, desarrollado a partir de 1856. Dentro del segundo caso se encuentra el test de Pépin para los números de Fermat (1877). El caso general de test de primalidad cuando el número inmediatamente anterior se encuentra completamente factorizado se denomina test de Lucas.

Posteriormente se encontraron algoritmos de primalidad con solo obtener una factorización parcial de p+1 o p-1. Ejemplos de estos algoritmos son el test de Proth (desarrollado alrededor de 1878) y el test de Pocklington (1914). En estos algoritmos se requiere que el producto de los factores primos conocidos de p-1 sea mayor que la raíz cuadrada de "p". Más recientemente, en 1975, Brillhart, Lehmer y Selfridge desarrollaron el test BLS de primalidad que solo requiere que dicho producto sea mayor que la raíz cúbica de "p". El mejor método conocido de esta clase es el test de Konyagin y Pomerance del año 1997, que requiere que dicho producto sea mayor que "p".

A partir de la década de 1970 varios investigadores descubrieron algoritmos para determinar si cualquier número es primo o no con complejidad subexponencial, lo que permite realizar tests en números de miles de dígitos, aunque son mucho más lentos que los métodos anteriores. Ejemplos de estos algoritmos son el test APRT-CL (desarrollado en 1979 por Adleman, Pomerance y Rumely, con mejoras introducidas por Cohen y Lenstra en 1984), donde se usan los factores de p-1, donde el exponente m depende del tamaño del número cuya primalidad se desea verificar, el test de primalidad por curvas elípticas (desarrollado en 1986 por S. Goldwasser, J. Kilian y mejorado por A. O. L. Atkin), que entrega un certificado consistente en una serie de números que permite después confirmar rápidamente si el número es primo o no. El desarrollo más reciente es el test de primalidad AKS (2002), que si bien su complejidad es polinómica, para los números que puede manejar la tecnología actual es el más lento de los tres.

Durante mucho tiempo, se pensaba que la aplicación de los números primos era muy limitada fuera de la matemática pura. Esto cambió en los años 1970 con el desarrollo de la criptografía de clave pública, en la que los números primos formaban la base de los primeros algoritmos, tales como el algoritmo RSA.

Desde 1951, el mayor número primo conocido siempre ha sido descubierto con la ayuda de ordenadores. La búsqueda de números primos cada vez mayores ha suscitado interés incluso fuera de la comunidad matemática. En los últimos años han ganado popularidad proyectos de computación distribuida tales como el GIMPS, mientras los matemáticos siguen investigando las propiedades de los números primos.

La cuestión acerca de si el número 1 debe o no considerarse primo está basada en la convención. Ambas posturas tienen sus ventajas y sus inconvenientes. De hecho, hasta el siglo XIX, los matemáticos en su mayoría lo consideraban primo. Muchos trabajos matemáticos siguen siendo válidos a pesar de considerar el 1 como un número primo, como, por ejemplo, el de Stern y Zeisel. La lista de Derrick Norman Lehmer de números primos hasta el 10.006.721, reimpresa hasta el año 1956 empezaba con el 1 como primer número primo.

Actualmente, la comunidad matemática se inclina por no considerar al 1 en la lista de los números primos. Esta convención, por ejemplo, permite una formulación muy económica del teorema fundamental de la aritmética: «"todo número natural tiene una representación única como producto de factores primos, salvo el orden"». Además, los números primos tienen numerosas propiedades de las que carece el 1, tales como la relación del número con el valor correspondiente de la función φ de Euler o la función divisor. Cabe también la igualdad para todo formula_6 entero positivo, formula_7, lo que permitiría decir que tiene formula_6 factores.

El teorema fundamental de la aritmética establece que todo número natural tiene una representación única como producto de factores primos, salvo el orden. Un mismo factor primo puede aparecer varias veces. El 1 se representa entonces como un producto vacío.

Se puede considerar que los números primos son los «ladrillos» con los que se construye cualquier número natural. Por ejemplo, se puede escribir el número 23.244 como producto de 2·3·13·149, y cualquier otra factorización del 23.244 como producto de números primos será idéntica excepto por el orden de los factores.

La importancia de este teorema es una de las razones para excluir el 1 del conjunto de los números primos. Si se admitiera el 1 como número primo, el enunciado del teorema requeriría aclaraciones adicionales.

A partir de esta unicidad en la factorización en factores primos se desarrollan otros conceptos muy utilizados en matemáticas, tales como el mínimo común múltiplo, el máximo común divisor y la coprimalidad de dos o más números. Así,


Las funciones aritméticas, es decir, funciones reales o complejas, definidas sobre un conjunto de números naturales, desempeñan un papel crucial en la teoría de números. Las más importantes son las funciones multiplicativas, que son aquellas funciones "f" en las cuales, para cada par de números coprimos ("a","b") se tiene

Algunos ejemplos de funciones multiplicativas son la función φ de Euler, que a cada "n" asocia el número de enteros positivos menores y coprimos con "n", y las funciones τ y σ, que a cada "n" asocian respectivamente el número de divisores de "n" y la suma de todos ellos. El valor de estas funciones en las potencias de números primos es

Gracias a la propiedad que las define, las funciones aritméticas pueden calcularse fácilmente a partir del valor que toman en las potencias de números primos. De hecho, dado un número natural "n" de factorización

se tiene que

con lo que se ha reconducido el problema de calcular "f"("n") al de calcular "f" sobre las potencias de los números primos que dividen "n", valores que son generalmente más fáciles de obtener mediante una fórmula general. Por ejemplo, para conocer el valor de la función φ sobre "n"=450=2·3·5 basta con calcular

Existen infinitos números primos. Euclides realizó la primera demostración alrededor del año 300 a. C. en el libro IX de su obra "Elementos" Una adaptación común de esta demostración original sigue así: Se toma un conjunto arbitrario pero finito de números primos "p", "p", "p", ···, "p", y se considera el producto de todos ellos más uno, formula_30. Este número es obviamente mayor que 1 y distinto de todos los primos "p" de la lista. El número "q" puede ser primo o compuesto. Si es primo tendremos un número primo que no está en el conjunto original. Si, por el contrario, es compuesto, entonces existirá algún factor "p" que divida a "q". Suponiendo que "p" es alguno de los "p", se deduce entonces que "p" divide a la diferencia formula_31, pero ningún número primo divide a 1, es decir, se ha llegado a un absurdo por suponer que "p" está en el conjunto original. La consecuencia es que el conjunto que se escogió no es exhaustivo, ya que existen números primos que no pertenecen a él, y esto es independiente del conjunto finito que se tome.

Por tanto, el conjunto de los números primos es infinito.

Si se toma como conjunto el de los "n" primeros números primos, entonces formula_32, donde "p"# es lo que se llama primorial de "p". Un número primo de la forma "p"# +1 se denomina número primo de Euclides en honor al matemático griego. También se puede elaborar una demostración similar a la de Euclides tomando el producto de un número dado de números primos "menos" uno, el lugar del producto de esos números primos "más" uno. En ese sentido, se denomina número primo primorial a un número primo de la forma "p"# ± 1.

No todos los números de la forma "p"# +1 son primos. En este caso, como se sigue de la demostración anterior, todos los factores primos deberán ser mayores que "n". Por ejemplo: 2·3·5·7·11·13+1=30031=59·509

Otros matemáticos han demostrado la infinitud de los números primos con diversos métodos procedentes de áreas de las matemáticas tales como al álgebra conmutativa y la topología.
Algunas de estas demostraciones se basan en el uso de sucesiones infinitas con la propiedad de que cada uno de sus términos es coprimo con todos los demás, por lo que se crea una biyección entre los términos de la sucesión y un subconjunto (infinito) del conjunto de los primos.

Una sucesión que cumple dicha propiedad es la sucesión de Euclides-Mullin, que deriva de la demostración euclídea de la infinitud de los números primos, ya que cada uno de sus términos se define como el factor primo más pequeño de uno más el producto de todos los términos anteriores. La sucesión de Sylvester se define de forma similar, puesto que cada uno de sus términos es igual a uno más el producto de todos los anteriores. Aunque los términos de esta última sucesión no son necesariamente todos primos, cada uno de ellos es coprimo con todos los demás, por lo que se puede escoger cualquiera de sus factores primos, por ejemplo, el menor de ellos, y el conjunto resultante será un conjunto infinito cuyos términos son todos primos.

Un resultado aún más fuerte, y que implica directamente la infinitud de los números primos, fue descubierto por Euler en el siglo XVIII. Establece que la serie formula_4 es divergente. Uno de los teoremas de Mertens concreta más, estableciendo que
donde la expresión "O"(1) indica que ese término está acotado entre -"C" y "C" para "n" mayor que "n", donde los valores de "C" y "n" no están especificados.

Otro resultado es el teorema de Dirichlet, que dice así:
El postulado de Bertrand enuncia así:

Una manera más débil pero elegante de formularlo es que, si "n" es un número natural mayor que 1, entonces siempre existe un número primo "p" tal que "n" < "p" < 2"n". Esto supone que, en una progresión geométrica de primer término entero mayor que 3 y razón igual a 2, entre cada término de la progresión y el siguiente, se tiene al menos un número primo.

Una vez demostrado la infinitud de los números primos, cabe preguntarse cómo se distribuyen los primos entre los números naturales, es decir, cuán frecuentes son y dónde se espera encontrar el "n"-ésimo número primo. Este estudio lo iniciaron Gauss y Legendre de forma independiente a finales del siglo XVIII, para el cual introdujeron la función enumerativa de los números primos π("n"), y conjeturaron que su valor fuese aproximadamente

El empeño de demostrar esta conjetura abarcó todo el siglo XIX. Los primeros resultados fueron obtenidos entre 1848 y 1859 por Chebyshov, quien demostró utilizando métodos puramente aritméticos la existencia de dos constantes "A" y "B" tales que
para "n" suficientemente grande. Consiguió demostrar que, si existía el límite del cociente de aquellas expresiones, este debía ser 1.

Hadamard y De la Vallée-Poussin elaboraron una demostración en 1896, independientemente el uno del otro, usando métodos similares, basados en el uso de la función zeta de Riemann, que había sido introducida por Bernhard Riemann en 1859. Hubo que esperar hasta 1949 para encontrar una demostración que usara solo métodos elementales (es decir, sin usar el análisis complejo). Esta demostración fue ideada por Selberg y Erdős. Actualmente, se conoce el teorema como teorema de los números primos.

El mismo Gauss introdujo una estimación más precisa, utilizando la función logaritmo integral:

En 1899 De la Vallée-Poussin demostró que el error que se comete aproximando formula_38 de esta forma es
para una constante positiva "a" y para cada entero "m". Este resultado fue ligeramente mejorado a lo largo de los años. Por otra parte, en 1901 Von Koch mostró que si la hipótesis de Riemann era cierta, se tenía la siguiente estimación, más precisa:

Una forma equivalente al teorema de los números primos es que p, el "n"-ésimo número primo, queda bien aproximado por "n"ln("n"). En efecto, "p" es estrictamente mayor que este valor.

Ligado a la distribución de los números primos se encuentra el estudio de los intervalos entre dos primos consecutivos. Este intervalo, con la única salvedad del que hay entre el 2 y el 3, debe ser siempre igual o mayor que 2, ya que entre dos números primos consecutivos al menos hay un número par y por tanto compuesto. Si dos números primos tienen por diferencia 2, se dice que son "gemelos", y con la salvedad del «triplete» formado por los números 3, 5 y 7, los números gemelos se presentan siempre de dos en dos. Esto también es fácil de demostrar: entre tres números impares consecutivos mayores que 3 siempre hay uno que es múltiplo de 3, y por tanto compuesto. Los primeros pares de números primos gemelos son (3,5), (5,7), (11, 13), (17, 19) y (29, 31).

Por otra parte, la diferencia entre primos consecutivos puede ser tan grande como se quiera. La demostración es relativamente sencilla: 

Sea un número natural formula_41. Entonces, todos los números de la forma 

formula_42

son números compuestos si formula_43, pues formula_44. 

Se puede construir así una lista con formula_41 números compuestos, y dado que formula_41 es un número natural arbitrario, entonces el intervalo puede hacerse tan grande como se desee. 

Por ejemplo, si se requiere construir un intervalo de cinco números consecutivos donde ninguno sea un número primo, se hace formula_47. Estos valores corresponden a:

El siguiente valor, 6!+7=727, es primo. De todas formas, el menor número primo que dista del siguiente en "n" es generalmente mucho menor que el factorial, por ejemplo, el caso más pequeño de dos primos consecutivos separados de ocho unidades es (89, 97), mientras que 8! es igual a 40.320.

La sucesión de las diferencias entre primos consecutivos ha sido profusamente estudiada en matemáticas, y alrededor de este concepto se han establecido muchas conjeturas que permanecen sin resolver.

El modelado de la distribución de los números primos es un tema de investigación recurrente entre los teóricos de números. La primalidad de un número concreto es (hasta ahora) impredecible a pesar de que existen leyes, como el teorema de los números primos y el postulado de Bertrand, que gobiernan su distribución a gran escala. Leonhard Euler comentó:
En una conferencia de 1975, Don Zagier comentó:

La criba de Eratóstenes es una manera sencilla de hallar todos los números primos menores o iguales que un número dado. Se basa en confeccionar una lista de todos los números naturales desde el 2 hasta ese número y tachar repetidamente los múltiplos de los números primos ya descubiertos. La criba de Atkin, más moderna, tiene una mayor complejidad, pero si se optimiza apropiadamente también es más rápida. También existe una reciente criba de Sundaram que genera únicamente números compuestos, siendo los primos los números faltantes.

En la práctica, lo que se desea es determinar si un número dado es primo sin tener que confeccionar una lista de números primos. Un método para determinar la primalidad de un número es la división por tentativa, que consiste en dividir sucesivamente ese número entre los números primos menores o iguales a su raíz cuadrada. Si alguna de las divisiones es exacta, entonces el número no es primo; en caso contrario, es primo. Por ejemplo, dado "n" menor o igual que 120, para determinar su primalidad basta comprobar si es divisible entre 2, 3, 5 y 7, ya que el siguiente número primo, 11, ya es mayor que √120. Es el test de primalidad más sencillo, y rápidamente pierde su utilidad a la hora de comprobar la primalidad de números grandes, ya que el número de factores posibles crece demasiado rápido a medida que crece el número potencialmente primo.

En efecto, el número de números primos menores que "n" es aproximadamente
De esta forma, para determinar la primalidad de "n", el mayor factor primo que se necesita no es mayor que √"n", dejando el número de candidatos a factor primo en cerca de
Esta expresión crece cada vez más lentamente en función de "n", pero, como los "n" grandes son de interés, el número de candidatos también se hace grande: por ejemplo, para "n" = 10 se tienen 450 millones de candidatos.

Asimismo, existen otros muchos tests de primalidad deterministas que se basan en propiedades que caracterizan a los números primos, pero su utilidad computacional depende mucho del test usado. Por ejemplo, se podría emplear el teorema de Wilson para calcular la primalidad de un número, pero tiene el inconveniente de requerir el cálculo de un factorial, una operación computacionalmente prohibitiva cuando se manejan números grandes. Aquí entre en juego el tiempo de ejecución del algoritmo empleado, que se expresa en la notación de Landau. Para poder determinar la primalidad de números cada vez más grandes (de miles de cifras) se buscan aquellos algoritmos cuyo tiempo de ejecución crezca lo más lentamente posible, a ser posible, que se pueda expresar como un polinomio. Si bien el test de primalidad AKS cumple con esta condición, para el rango de números que se usa en la práctica este algoritmo es extremadamente lento.

Por otra parte, a menudo basta con tener una respuesta más rápida con una alta probabilidad (aunque no segura) de ser cierta. Se puede comprobar rápidamente la primalidad de un número relativamente grande mediante tests de primalidad probabilísticos. Estos tests suelen tomar un número aleatorio llamado "testigo" e introducirlo en una fórmula junto con el número potencialmente primo "n". Después de varias iteraciones, se resuelve que "n" es "definitivamente compuesto" o bien "probablemente primo". Estos últimos números pueden ser primos o bien pseudoprimos (números compuestos que pasan el test de primalidad). Algunos de estos tests no son perfectos: puede haber números compuestos que el test considere "probablemente primos" independientemente del testigo utilizado. Esos números reciben el nombre de pseudoprimos absolutos para ese test. Por ejemplo, los números de Carmichael son números compuestos, pero el test de Fermat los evalúa como probablemente primos. Sin embargo, los tests probabilísticos más utilizados, como el test de Miller-Rabin o el obsoleto test de Solovay-Strassen, superado por el anterior, no tienen este inconveniente, aun siendo igualmente tests probabilísticos.

Algunos tests probabilísticos podrían pasar a ser determinísticos y algunos tests pueden mejorar su tiempo de ejecución si se verifican algunas hipótesis matemáticas. Por ejemplo, si se verifica la hipótesis generalizada de Riemann, se puede emplear una versión determinística del test de Miller-Rabin, y el test de primalidad por curvas elípticas podría mejorar notablemente su tiempo de ejecución si se verificaran algunas hipótesis de teoría analítica de números.

Un algoritmo de factorización es un algoritmo que separa uno a uno los factores primos de un número. Los algoritmos de factorización pueden funcionar también a modo de tests de primalidad, pero en general tienen un tiempo de ejecución menos ventajoso. Por ejemplo, se puede modificar el algoritmo de división por tentativa de forma que no se detenga cuando se obtenga una división exacta, sino que siga realizando nuevas divisiones, y no sobre el número original, sino sobre el cociente obtenido. Después de la división por tentativa, los métodos más antiguos que se conocen son el método de Fermat, que se basa en las diferencias entre cuadrados y que es especialmente eficaz cuando "n" es el producto de dos números primos próximos entre sí, y el método de Euler, que se basa en la representación de "n" como suma de dos cuadrados de dos formas distintas.

Más recientemente, se han elaborado algoritmos basados en una gran variedad de técnicas, como las fracciones continuas o las curvas elípticas, aunque algunos son mejoras de métodos anteriores (la criba cuadrática, por ejemplo, se basa en una mejora del método de Fermat y posee complejidad computacional subexponencial sobre el número de cifras de "n"). Otros, como el método rho de Pollard, son probabilísticos, y no garantizan hallar los divisores de un número compuesto.

Hoy por hoy, el algoritmo determinístico más rápido de uso general es el "general number field sieve", que también posee complejidad computacional subexponencial sobre el número de cifras de "n". Se ha propuesto un algoritmo cuyo tiempo de ejecución es polinómico sobre el número de cifras de "n" (el algoritmo de Shor), pero requiere ser ejecutado en un ordenador cuántico, ya que su simulación en un ordenador normal requiere un tiempo exponencial. No se conocen algoritmos para factorizar en una computadora tradicional en tiempo polinómico y tampoco se demostró que esto sea imposible.

A lo largo de la historia, se han buscado numerosas fórmulas para generar los números primos. El nivel más alto de exigencia para una fórmula así sería que asociara a cada número natural "n" el "n"-ésimo número primo. De forma más indulgente, se puede pedir una función "f" inyectiva que asocie a cada número natural "n" un número primo de tal forma que cada uno de los valores tomados aparezca solo una vez.

Además, se exige que la función se pueda aplicar, efectiva y eficazmente, en la práctica. Por ejemplo, el teorema de Wilson asegura que "p" es un número primo si y solo si ("p"-1)!≡-1 (mod "p"). Otro ejemplo: la función f("n") = 2 + ( 2("n"!) mod ("n"+1)) genera todos los números primos, solo los números primos, y solo el valor 2 se toma más de una vez. Sin embargo, ambas fórmulas se basan en el cálculo de un factorial, lo que las hace computacionalmente inviables.

En la búsqueda de estas funciones, se han investigado, notablemente, las funciones polinómicas. Cabe subrayar que ningún polinomio, aun en varias variables, devuelve solo valores primos. Por ejemplo, el polinomio en una variable "f"("n") = "n"² + "n" + 41, estudiada por Leonardo Euler, devuelve valores primos para "n" = 0, …, 39, sin embargo para n= 40, resulta formula_55 un número compuesto. Si el término constante vale cero, entonces el polinomio es múltiplo de "n", por lo que el polinomio es compuesto para valores compuestos de "n". En caso contrario, si "c" es el término constante, entonces "f"("cn") es múltiplo de "c", por lo que si el polinomio no es constante, necesariamente deberá incluir valores compuestos.

Sin embargo, hay polinomios en varias variables cuyos valores positivos (cuando las variables recorren números naturales) son precisamente números primos. Un ejemplo, es este polinomio descubierto por Jones, Sato, Wada y Wiens en 1976:

Al igual que ocurre con las fórmulas con factoriales, este polinomio no es práctico de calcular, ya que, aunque los valores positivos que toma son todos primos, prácticamente no devuelve otra cosa que valores negativos cuando se hacen variar las variables "a" a "z" de 0 a infinito.

Otro enfoque al problema de encontrar una función que solo genere números primos viene dado a partir del teorema de Mills, que indica que existe una constante θ tal que

es siempre un número primo, donde formula_63 es la función piso. Todavía no se conoce ninguna fórmula para calcular la constante de Mills, y las aproximaciones que se emplean en la actualidad se basa en la sucesión de los así llamados números primos de Mills (los números primos generados mediante esta fórmula), que no pueden ser obtenidos rigurosamente, sino solo de manera probabilística, suponiendo cierta la hipótesis de Riemann.

De mayor interés son otras fórmulas que, aunque no solo generen números primos, son más rápidas de implementar, sobre todo si existe un algoritmo especializado que permita calcular rápidamente la primalidad de los valores que van tomando. A partir de estas fórmulas se obtienen subconjuntos relativamente pequeños del conjunto de los números primos, que suelen recibir un nombre colectivo.

Los números primos primoriales, directamente relacionados con la demostración euclidiana de la infinitud de los números primos, son los de la forma "p" = "n"# ± 1 para algún número natural "n", donde "n"# es igual al producto 2 · 3 · 5 · 7 · 11 · … de todos los primos ≤ n. Asimismo, un número primo se dice primo factorial si es de la forma "n"! ± 1. Los primeros primos factoriales son:

Los números de Fermat, ligados a la construcción de polígonos regulares con regla y compás, son los números de la forma formula_64, con "n" natural. Los únicos números primos de Fermat que se conocen hasta la fecha son los cinco que ya conocía el propio Fermat, correspondientes a "n" = 0, 1, 2, 3 y 4, mientras que para valores de "n" entre 5 y 32 estos números son compuestos.

Para determinar su primalidad, existe un test especializado cuyo tiempo de ejecución es polinómico: el test de Pépin. Sin embargo, los propios números de Fermat crecen tan rápidamente que solo se lo ha podido aplicar para valores de "n" pequeños. En 1999 se lo aplicó para "n" = 24. Para determinar el carácter de otros números de Fermat mayores se utiliza el método de divisiones sucesivas y de esa manera a fecha de junio de 2009 se conocen 241 números de Fermat compuestos, aunque en la mayoría de los casos se desconozca su factorización completa.

Los números de Mersenne son los de forma M = 2 – 1, donde "p" es primo. Los mayores números primos conocidos son generalmente de esta forma, ya que existe un test de primalidad muy eficaz, el test de Lucas-Lehmer, para determinar si un número de Mersenne es primo o no.

Actualmente, el mayor número primo que se conoce es "M" = 2 - 1, que tiene 24 862 048 cifras en el sistema decimal. Se trata cronológicamente del 51º número primo de Mersenne conocido y su descubrimiento se anunció el 7 de diciembre de 2018 gracias al proyecto de computación distribuida «Great Internet Mersenne Prime Search» (GIMPS).

Existen literalmente decenas de "apellidos" que se pueden añadir al concepto de "número primo" para referirse a un subconjunto que cumple alguna propiedad concreta. Por ejemplo, los números primos pitagóricos son los que se pueden expresar en la forma 4"n"+1. Dicho de otra forma, se trata de los números primos cuyo resto al dividirlos entre 4 es 1. Otro ejemplo es el de los números primos de Wieferich, que son aquellos números primos "p" tales que "p" divide a 2 - 1.

Algunas de estas propiedades se refieren a una relación concreta con otro número primo:

Muchas conjeturas tratan sobre si hay infinitos números primos de una determinada forma. Así, se conjetura que hay infinitos números primos de Fibonacci e infinitos primos de Mersenne, pero solo un número finito de primos de Fermat. No se sabe si hay infinitos números primos de Euclides.

También hay numerosas conjeturas que se ocupan de determinadas propiedades de la distribución de los números primos. Así, la conjetura de los números primos gemelos enuncia que hay infinitos números primos gemelos, que son pares de primos cuya diferencia es de 2. La conjetura de Polignac es una versión más general y más fuerte de la anterior, ya que enuncia que, para cada entero positivo "n", hay infinitos pares de primos consecutivos que difieren en 2"n". A su vez, una versión más débil de la conjetura de Polignac dice que todo número par es la diferencia de dos números primos.

Asimismo, se conjetura la infinidad de los primos de la forma "n" + 1. Según la conjetura de Brocard, entre los cuadrados de primos consecutivos mayores que 2 existen siempre al menos cuatro números primos. La conjetura de Legendre establece que, para cada "n" natural, existe un número primo entre "n" y ("n"+1). Finalmente, la conjetura de Cramér, cuya veracidad implicaría la de Legendre, dice que:
Otras conjeturas relacionan algunas propiedades aditivas de los números con los números primos. Así, la conjetura de Goldbach dice que todo número par mayor que 2 se puede escribir como suma de dos números primos, aunque también existe una versión más débil de la misma conjetura según la cual todo número impar mayor que 5 se puede escribir como suma de tres números primos. El matemático chino Chen Jingrun demostró, en 1966, que en efecto, todo número par suficientemente grande puede expresarse como suma de dos primos o como la suma de un primo y de un número que es el producto de dos primos. (""semi"-primo").

En 1912, Landau estableció en el Quinto Congreso Internacional de Matemáticos de Cambridge una lista de cuatro de los problemas ya mencionados sobre números primos, que se conocen como los problemas de Landau. Ninguno de ellos está resuelto hasta la fecha. Se trata de la conjetura de Goldbach, la de los números primos gemelos, la de Legendre y la de los primos de la forma "n" + 1.

El concepto de número primo es tan importante que se ha visto generalizado de varias maneras en diversas ramas de las matemáticas.

Se pueden definir los elementos primos y los elementos irreducibles en cualquier dominio de integridad.
En cualquier dominio de factorización única, como por ejemplo, el anillo formula_66 de los enteros, el conjunto de elementos primos equivale al conjunto de los elementos irreducibles, que en formula_66 es {…, −11, −7, −5, −3, −2, 2, 3, 5, 7, 11, …}.

Considérense por ejemplo los enteros gaussianos formula_68, es decir, los números complejos de la forma "a"+"bi" con "a", "b" ∈ formula_66. Este es un dominio de integridad, y sus elementos primos son los primos gaussianos. Cabe destacar que el 2 "no" es un primo gaussiano, porque admite factorización como producto de los primos gaussianos (1+"i") y (1-"i"). Sin embargo, el elemento 3 sí es primo en los enteros gaussianos, pero no lo es en otro dominio entero. En general, los primos racionales (es decir, los elementos primos del anillo formula_66) de la forma 4"k"+3 son primos gaussianos, pero no lo son aquellos de la forma 4"k"+1.

En teoría de anillos, un ideal "I" es un subconjunto de un anillo "A" tal que

Un ideal primo se define entonces como un ideal que cumple también que:

Los ideales primos son una herramienta relevante en álgebra conmutativa, teoría algebraica de números y geometría algebraica. Los ideales primos del anillo de enteros son los ideales (0), (2), (3), (5), (7), (11), …

Un problema central en teoría algebraica de números es la manera en que se factorizan los ideales primos cuando se ven sometidos a una extensión de cuerpos. En el ejemplo de los enteros gaussianos, (2) se "ramifica" en potencia de un primo (ya que formula_71 y formula_72 generan el mismo ideal primo), los ideales primos de la forma formula_73 son "inertes" (mantienen su primalidad) y los de la forma formula_74 pasan a ser producto de dos ideales primos distintos.

En teoría algebraica de números surge otra generalización más. Dado un cuerpo formula_75, reciben el nombre de valoraciones sobre formula_75 determinadas funciones de formula_75 en formula_78. Cada una de estas valoraciones genera una topología sobre formula_75, y se dice que dos valoraciones son "equivalentes" si generan la misma topología. Un "primo de formula_75" es una clase de equivalencia de valoraciones. Con esta definición, los primos del cuerpo formula_81 de los números racionales quedan representados por la función valor absoluto así como por las valoraciones "p"-ádicas sobre formula_81 para cada número primo "p".

En teoría de nudos, un nudo primo es un nudo no trivial que no se puede descomponer en dos nudos más pequeños. De forma más precisa, se trata de un nudo que no se puede escribir como suma conexa de dos nudos no triviales.

En 1949 Horst Schubert demostró un teorema de factorización análogo al teorema fundamental de la aritmética, que asegura que cada nudo se puede obtener de forma única como suma conexa de nudos primos. Por este motivo, los nudos primos desempeñan un papel central en la teoría de nudos: una clasificación de los nudos ha sido desde finales del siglo XIX el tema central de la teoría.





El algoritmo RSA se basa en la obtención de la clave pública mediante la multiplicación de dos números grandes (mayores que 10) que sean primos. La seguridad de este algoritmo radica en que no se conocen maneras rápidas de factorizar un número grande en sus factores primos utilizando computadoras tradicionales.

Los números primos han influido en numerosos artistas y escritores. El compositor francés Olivier Messiaen se valió de ellos para crear música no métrica. En obras tales como "La Nativité du Seigneur" (1935) o "Quatre études de rythme" (1949-50) emplea simultáneamente motivos cuya duración es un número primo para crear ritmos impredecibles. Según Messiaen, esta forma de componer fue «inspirada por los movimientos de la naturaleza, movimientos de duraciones libres y desiguales».

En la novela escrita en 1968 "", Arthur C. Clarke menciona que el monolito de origen extraterrestre tiene la proporción del cuadrado de los primeros tres números primos: 1,4,9.

En su novela de ciencia ficción "Contact", posteriormente adaptada al cine, Carl Sagan sugiere que los números primos podrían ser empleados para comunicarse con inteligencias extraterrestres, una idea que había desarrollado de manera informal con el astrónomo estadounidense Frank Drake en 1975.

"El curioso incidente del perro a medianoche", de Mark Haddon, que describe en primera persona la vida de un joven autista muy dotado en matemáticas y cálculo mental, utiliza únicamente los números primos para numerar los capítulos.

En la novela "PopCo" de Scarlett Thomas, la abuela de Alice Butler trabaja en la demostración de la hipótesis de Riemann. El libro ilustra una tabla de los mil primeros números primos.

"La soledad de los números primos", novela escrita por Paolo Giordano, ganó el premio Strega en 2008.

También son muchas las películas que reflejan la fascinación popular hacia los misterios de los números primos y la criptografía, por ejemplo, "Cube", "Sneakers", "El amor tiene dos caras" y "Una mente maravillosa". Esta última se basa en la biografía del matemático y premio Nobel John Forbes Nash, escrita por Sylvia Nasar.

El escritor griego Apostolos Doxiadis, escribió "El tío Petros y la conjetura de Goldbach", que narra cómo un ficticio matemático prodigio de principios del siglo XX se sumerge en el mundo de las matemáticas de una forma apasionante. Tratando de resolver uno de los problemas más difíciles y aún no resueltos de la matemática, la conjetura de Goldbach, la cual reza: «Todo número par puede expresarse como la suma de dos números primos».



</doc>
<doc id="3543" url="https://es.wikipedia.org/wiki?curid=3543" title="Necronomicón">
Necronomicón

El Necronomicón (en griego Nεκρονομικόv, en árabe: العزيف ) es un grimorio ficticio ideado por el escritor estadounidense H. P. Lovecraft (1890-1937), uno de los maestros de la literatura de terror y ciencia ficción. Es mencionado por primera vez en el cuento "La ciudad sin nombre" («The nameless city») de 1921 donde también se indica que su autor fue el «árabe loco» Abdul Alhazred. un seudónimo empleado por Lovecraft desde su infancia.

El libro es, asimismo, mencionado por otros autores del círculo lovecraftiano, como August Derleth o Clark Ashton Smith. Desde entonces, el libro ficticio ha inspirado la publicación de diversas obras de igual título.

La etimología de "Necronomicón" es más transparente de lo que suele creerse. Aunque la forma no está testimoniada en griego antiguo, se trata de una construcción análoga a adjetivos comunes como "ἀστρονομικός" (astronómico), o "οἰκονομικός" (económico). Estos adjetivos están formados por tres elementos: un lexema ("ἀστρο"-, "οἰκο"-, "νεκρο"-) + el lexema "νόμος" ('ley, administración') + el sufijo -"ικος", sin significado, que sirve para formar adjetivos. Así pues, "astronómico" significa etimológicamente «relativo a la ley u ordenación de los astros»; el neologismo "necronómico" sería «relativo a la ley (o las leyes) de los muertos».

Cuando estos adjetivos se ponen en neutro singular ("ἀστρονομικόν") o plural ("ἀστρονομικά"), adquieren un valor genérico: en el ejemplo, «lo relativo a los astros», «las cosas relativas a la ordenación de los astros». "Necronomicón", neutro singular, es por tanto «(el libro que contiene) lo relativo a la(s) ley(es) de los muertos», del mismo modo que el "Astronomicon" del poeta latino Marco Manilio (s. I d. C.) es un tratado sobre los astros.

En una carta de 1937 dirigida a Harry O. Fischer, Lovecraft revela que el título del libro se le ocurrió durante un sueño. Una vez despierto, hizo su propia interpretación de la etimología. A su juicio, significaba «Imagen de la Ley de los Muertos», pues en el último elemento (-"icon") quiso ver la palabra griega "εἰκών" (latín "icon"), «imagen».

El "Necronomicón" es descrito como un libro de saberes arcanos y magia ritual, cuya lectura provoca la locura y la muerte. En los cuentos de Lovecraft y sus continuadores aparece como un registro de fórmulas olvidadas que permiten contactar con unas entidades sobrenaturales de un inmenso poder, los Antiguos.

Quizás la cita más famosa del "Necronomicón" en la narrativa de Lovecraft sea esta:

That is not dead which can eternal lie, / And with strange aeons even death may die. («Que no está muerto lo que yace eternamente, y con eones extraños incluso la muerte puede morir»).

El "Necronomicón" aparece en gran parte de los escritos de Lovecraft, quien cita también otros libros de magia, como "De Vermis Mysteriis" (en latín, «Sobre los misterios del gusano») y "Le culte des goules" («El culto de los gules»), atribuido al Conde D'Erlette (un guiño literario a August Derleth, miembro del «Círculo de Lovecraft»). Otros de los libros que aparecen en los relatos de Lovecraft son los ficticios "Manuscritos Pnakóticos"; y los reales "L’Image du monde", de Gautier de Metz y "El gran dios Pan," de Arthur Machen.

En 1927, Lovecraft escribió una breve nota sobre la autoría del "Necronomicón" y la historia de sus traducciones, que fue publicada en 1938, tras su muerte, como "Una historia del Necronomicón".

Según esta obra, el libro fue escrito con el título de "Kitab Al-Azif" (en árabe: «El rumor de los insectos por la noche», rumor que en el folclore arábigo se atribuye a demonios como los "djins" y "gules") alrededor del año 730 d.C. por el poeta árabe Abdul Al-Hazred, de Saná (Yemen), de quien dice que murió a plena luz del día devorado por una bestia invisible.

Lovecraft abunda en datos para hacer verosímil la existencia del libro. Por ejemplo, cita como uno de sus compiladores a Ibn Khallikan, erudito iraní o árabe que existió realmente.

También cuenta que hacia el año 950 fue traducido al griego por Theodorus Philetas y adoptó el título actual griego, "Necronomicón". Tuvo una rápida difusión entre los filósofos y hombres de ciencia de la Baja Edad Media. Sin embargo, los horrendos sucesos que se producían en torno al libro hicieron que la Iglesia católica lo condenara en el año 1050. En el año 1228 Olaus Wormius (el verdadero Wormius vivió en el siglo XVII) tradujo el libro al latín, en la que es la versión más famosa, pues (siempre según la ficción lovecraftiana) aún quedan algunos ejemplares de ella, mientras que los originales árabe y griego se creen perdidos.

A pesar de la persecución, según Lovecraft se realizaron distintas impresiones en España y Alemania durante el siglo XVII. Supuestamente, se conservarían cuatro copias completas: una en la biblioteca Widener de la Universidad de Harvard, dentro de una caja fuerte; una copia del siglo XV, en la Biblioteca Nacional de París; otra en la (ficticia) Universidad de Miskatonic en la inexistente ciudad de Arkham (Massachusetts, Estados Unidos) y otra en la Universidad de Buenos Aires (Argentina).

Sobre el carácter ficticio del libro, Lovecraft escribió lo siguiente:

De hecho, el famoso árabe loco Abdul Alhazred no es más que un apodo que él mismo se puso en la infancia, inspirado en la reciente lectura de "Las mil y una noches" (Alhazred = "all has read", el que lo ha leído todo).

Lovecraft logró hacer un excelente engaño al aportar datos respecto al "Necronomicón". Por ejemplo, señalaba que quedaban muy pocos ejemplares de tal libro "prohibido" y "peligroso". En el cuento "El horror de Dunwich" se ubican ejemplares en la Universidad de Buenos Aires, en la Biblioteca de Widener de Harvard, la Biblioteca Nacional de París, en el Museo Británico y en la inexistente Universidad de Miskatonic en la ciudad imaginaria de Arkham (que aparece repetidamente en los cuentos de Lovecraft). Según August Derleth, esta supuesta precisión ha hecho que numerosas personas creyeran en la real existencia del libro, y solicitaran ejemplares o el acceso a las bibliotecas donde se lo guardaba. En el artículo «The Making of a Hoax», Derleth cuenta que durante 1962, en la publicación "Antiquarian Bookman" apareció un anuncio, que decía:

En el mismo artículo relata que un estudiante gastó la broma de incluir su ficha en el registro de la Biblioteca General de la Universidad de California, en la sección BL 430, dedicada a las religiones primitivas. Una leyenda urbana narraba que Jorge Luis Borges, quien no tenía una opinión positiva de Lovecraft, había creado una ficha sobre el mismo en la Biblioteca Nacional de Argentina.

Bromistas, fanes o timadores han publicado supuestas ediciones del libro; la más famosa de ellas es la conocida como "Necronomicón de Simon", también existen páginas en internet donde se ofrece el libro a la venta. Algunos de estos ejemplares son simples listados de los primigenios más conocidos, junto a símbolos y oraciones sin significado imitando el estilo de Lovecraft.

El extraordinario dibujante H. R. Giger publicó una recopilación de sus dibujos bajo el título "Giger's Necronomicon", en dos volúmenes, en una edición muy cuidada pensada para coleccionistas (encuadernados en piel negra, 666 ejemplares, con un holograma escondido). La editorial española La factoría de ideas ha publicado también con este título un libro de relatos escritos por seguidores de Lovecraft. Cabe destacar el "Necronomicón" de Donald Tyson, publicado en 2004 por Edaf, escrito como la biografía en primera persona de Abdul Alhazred, siguiendo el estilo literario de los escritores árabes, y que recoge y explica todos los mitos y ciudades que aparecen en los relatos de Lovecraft, incluyendo la explicación del origen del mundo con el estilo trágico de Lovecraft.



Según Lovecraft en "Historia del Necronomicón", las copias del Necronomicón original se encuentran solamente en unas pocas bibliotecas, entre las que menciona:




</doc>
<doc id="3544" url="https://es.wikipedia.org/wiki?curid=3544" title="Italia">
Italia

Italia, oficialmente la República Italiana (), es uno de los veintisiete estados soberanos que forman la Unión Europea. Su forma de gobierno es la república parlamentaria. Su territorio, con capital en Roma, se divide en veinte regiones formadas estas, a su vez, por 106 provincias.

Italia se ubica en el centro del mar Mediterráneo, en Europa Meridional. Ocupa la península itálica, así como la llanura Padana, la islas de Sicilia y Cerdeña y alrededor de entre las que se destacan las islas Tremiti en el mar Adriático, los archipiélagos Campano y Toscano en el mar Tirreno, o las islas Pelagias en África septentrional, entre otras. En el norte, está rodeada por los Alpes y tiene frontera con Francia, Suiza, Austria, y Eslovenia. Los Estados de San Marino y Ciudad del Vaticano son enclaves dentro del territorio italiano. A su vez, Campione d'Italia es un municipio italiano que forma un pequeño enclave en territorio suizo.

Ha sido el hogar de muchas culturas europeas como la civilización nurágica, los etruscos, los griegos, los romanos y también fue la cuna del Humanismo y del Renacimiento, que comenzó en la región de Toscana y pronto se extendió por toda Europa. La capital de Italia, Roma, ha sido durante siglos el centro político y cultural de la civilización occidental. Además, es la ciudad santa para la Iglesia católica, siendo el Papa el obispo de Roma y encontrándose dentro de la ciudad el microestado del Vaticano. El significado cultural del país se refleja en todos sus , ya que tiene 55, el país con mayor número del mundo.

Es el tercer país de la Unión Europea que más turistas recibe por año, siendo Roma la tercera ciudad más visitada.
Otras ciudades importantes son: Milán, centro de finanzas y de industria, y, según el Global Language Monitor, la capital de la Moda; Nápoles, importante puerto en el Mediterráneo, capital histórica y ciudad más poblada del Mezzogiorno; Turín, centro de industria automovilística y de diseño industrial. Italia es una república democrática, forma parte del G7 o grupo de las siete más grandes naciones avanzadas del mundo y es un país desarrollado con una calidad de vida muy alta, encontrándose en 2005 entre las siete primeras del mundo. 

Es el país número 28 (informe 2017) en materia de . Es una potencia regional y mundial miembro fundador de la Unión Europea, firmante del Tratado de Roma en 1957. También es miembro fundador de la Organización del Tratado del Atlántico Norte (OTAN) y miembro de la Organización para la Cooperación y el Desarrollo Económico, de la Organización Mundial del Comercio, del Consejo de Europa y de la Unión Europea Occidental. El país, y especialmente Roma, tiene una fuerte repercusión en temas de política y cultura, en organizaciones mundiales como la Organización para la Agricultura y la Alimentación (FAO), el Fondo Internacional de Desarrollo Agrícola (IFAD), el Glocal Forum, o el Programa Mundial de Alimentos (WFP).

Según el historiador griego Antíoco de Siracusa, la palabra "Italia" designaba en el siglo V a. C. a la parte meridional de la actual región italiana de Calabria —el antiguo Brucios—, habitada por los ítalos (actualmente esta zona comprende la provincia de Reggio y parte de las provincias de Vibo Valentia y de Catanzaro).

La explicación más aceptada es la que ofrecen dos escritores griegos algo más recientes, Helánico y Timeo, los cuales relacionaron el mismo nombre (Italia) con la palabra oscana "vitéliu" (vitulus en latín), que significa 'ternero'. La explicación para este nombre es el hecho de ser un país rico en ganado bovino. En el siglo I a. C., el toro, símbolo del pueblo samnita sublevado contra Roma, fue representado en las monedas emitidas por los insurrectos abatiendo a una loba, símbolo de Roma: la leyenda del "vitéliu" (de los ítalos) confirma que vinculaban el nombre de Italia con el ternero-toro. Por otra parte, también es posible que los ítalos tomaran su nombre de un animal-tótem, el ternero, que, en una lejana primavera sagrada, los había guiado hasta los lugares en los que se asentaron definitivamente.

Con el tiempo, el nombre se extendió por toda la Italia meridional para abarcar después toda la península. En el siglo II a. C., el historiógrafo griego Polibio llamó Italia al territorio comprendido entre el estrecho de Mesina y los Apeninos septentrionales, aunque su contemporáneo Catón el Viejo extendió el concepto territorial de Italia hasta el arco alpino. Sicilia, Cerdeña y Córcega no pasarán a formar parte de Italia hasta el siglo III d. C., como consecuencia de las reformas administrativas de Diocleciano, aunque sus estrechos lazos culturales con la península permiten considerarlas como parte integrante.

Entre y el siglo II existió en Cerdeña la cultura nurágica.
Durante la Edad del Hierro se sucedieron varias culturas que pueden ser diferenciadas en tres grandes núcleos geográficos, la del Lacio Antiguo, la de Magna Grecia y la de Etruria. Una de estas culturas, los ligures, fueron un enigmático pueblo que habitaba en el noroeste de Italia y el sureste de Francia (Niza).

Otro pueblo, los etruscos, poseían su núcleo histórico en la Toscana, y tuvieron un origen incierto. Desde la Toscana se extendieron por el sur hacia el Lacio y parte septentrional de la Campania, en donde chocaron con las colonias griegas; hacia el norte de la península itálica ocuparon la zona alrededor del valle del río Po, en la actual región de Lombardía. Hacia el comenzó a deteriorarse fuertemente su poderío, en gran medida, al tener que afrontar casi al mismo tiempo las invasiones de los celtas y los ataques de griegos y cartagineses. Hacia el , Etruria (nombre del país de los etruscos, entre la Toscana y la Lombardía) fue absorbida por los romanos y, antes o después, lo fueron el resto de pueblos itálicos.

Como Antigua Roma se designa a una sociedad agrícola surgida a mediados del siglo VIII a. C. en el "Latium Vetus" (actual Lacio), que se expandió desde la ciudad de Roma a toda la península itálica, unificándola bajo el nombre de "Italia", y que creció durante siglos hasta convertirse en un imperio, que en su época de apogeo, llegó a abarcar desde la península ibérica a Anatolia y desde las islas británicas hasta Egipto, provocando un importante florecimiento cultural en cada lugar en el que gobernó. En un principio, tras su fundación (según la tradición en ) Roma fue una monarquía etrusca. Más tarde () se convirtió en República Romana latina, y en se convirtió en un imperio.

Al período de mayor esplendor se le conoce como Paz romana, debido al relativo estado de armonía que prevaleció en las "provincias" (territorios conquistados por los romanos fuera de Italia, que no era una provincia, sino el territorio metropolitano de Roma y centro absoluto del Imperio Romano), que estaban bajo el dominio romano de Julio César y luego del emperador Augusto, que cerró las puertas del templo de Jano (que permanecían abiertas en periodos de guerra), cuando creyó haber vencido a cántabros y astures (entre otros pueblos) en el año . Se suele aceptar como fecha de inicio de la paz romana (o "Pax Augustea") el , cuando Augusto declara el fin de las guerras civiles, y su duración hasta la muerte de Marco Aurelio (año 180).

Con el emperador Diocleciano se reorganizó el Imperio, pero tras Constantino I el Grande no volvió a estar unificado puesto que Teodosio I el Grande lo dividió entre sus dos hijos, Arcadio y Flavio Honorio, adjudicándoles a uno el Imperio romano de Oriente —con sede en Constantinopla— y al otro el Imperio romano de Occidente. Las invasiones bárbaras pondrán fin al Imperio Occidental en 476, dando paso a la Edad Media. Italia en este periodo quedó como "Regnum Italiae" (Reino ostrogodo de Italia), bajo los ostrogodos.

Los ostrogodos eran un grupo de godos que habían sido sojuzgados por los hunos, pero tras su liberación de estos, Teodorico el Grande, con la bendición del emperador romano de Oriente, condujo a su pueblo a Roma en 488.
En la península gobernaba el hérulo Odoacro tras deponer al último emperador romano de Occidente en 476, pero tras una campaña en el norte de la península, Teodorico tomó la capital, Rávena, matando a Odoacro en 493. En 526 la muerte de Teodorico acabó con la paz, heredando Italia su nieto, Atalarico, que murió sin hijos, lo que produjo una crisis que llevó al reino a la desaparición.

Bajo Justiniano I, el Imperio romano de Oriente inició una serie de campañas con el objetivo de reconstruir la unidad mediterránea. La debilidad del reino ostrogodo, y los deseos del Imperio de recobrar la ciudad de Roma convirtieron a Italia en un objetivo. En 535 el general Belisario invadió Sicilia y marchó a través de la península, tomando Nápoles y llegando a Roma en 536. Prosiguió hacia el norte y tomó "Mediolanum" (Milán) y Rávena en 540, y para el 561 había pacificado la zona.
Entre los diferentes pueblos germánicos que habían abandonado su antigua morada para vivir en mejores tierras, se contaban los lombardos, a los que Justiniano I había dejado asentarse en Panonia, a condición de que defendieran la frontera.
La presión de los lombardos sobre el papa hizo que el rey de los francos, Pipino el Breve, realizara entre 756 y 758 repetidas campañas en el norte de Italia. La situación se recrudeció a la muerte de Pipino, pero la reunificación de los francos bajo Carlomagno llevó a una nueva intervención en Italia en 774. Tras una breve batalla, Carlomagno se hizo con el reino de Lombardía, que, manteniendo su autonomía, se integró en el Imperio carolingio.
Entre los siglos X y XIII, ciertas repúblicas marítimas gozaron de una prosperidad económica, gracias a su actividad comercial, en un marco de amplia autonomía política. Generalmente, la definición se refiere en especial a cuatro ciudades: Amalfi, Pisa, Génova y Venecia. También otras ciudades del área gozaban de independencia (gobierno autónomo con forma de república oligárquica, moneda, ejército, etc.), habían participado en las cruzadas, contaban con una flota naval, tenían fundagos, "cónsules de las "nationes"", que vigilaban los intereses comerciales de sus respectivas ciudades en los puertos mediterráneos, y pueden ser incluidas de pleno derecho entre las repúblicas marítimas. Entre estas, cabría destacar Gaeta, Ancona, y Noli.

Durante los siglos XIV y XV, la Italia norte-septentrional estaba compuesto por distintas ciudades estados, siendo el resto de la península ocupado en su mayoría por los Estados Papales y el Reino de Sicilia. La mayoría de las ciudades estados estaban subordinadas a soberanías extranjeras, como el Ducado de Milán, estado constituyente del Sacro Imperio Romano Germánico, mas la mayoría mantenían la independencia de facto de estas soberanías extranjeras, que habían ocupado la mayoría de la península desde la Caída del Imperio romano de Occidente. Las más fuertes entre estas ciudades-estados gradualmente absorbieron los territorios que los rodeaban, dando a lugar a las Signoria, estados regionales dirigidos por familias mercantes que fundaban dinastías locales. La guerra entre estas ciudades-estado era habitual y principalmente llevada a cabo por bandas de mercenarios conocidos como condotieros dirigidos por capitanes italianos. Décadas de enfrentamientos dejaron como potencias regionales a Florencia, Milán y Venecia, quienes firmaron el Tratado de Lodi en 1454, que llevó a la paz en la región por primera vez en siglos. La paz duraría por los siguientes cuarenta años.

El Renacimiento europeo, un periodo de reavivamiento de las artes y ciencias, originado en Italia gracias a varios factores, como la gran riqueza acumulada por las ciudades, el mecenazgo de las familias dominantes como los Medici en Florencia, la migración de los estudiosos griegos debido a la conquista de Constantinopla por parte del Imperio otomano. El renacimiento terminó a mediados del siglo XVI debido a las desastrosas guerras italianas. Las ideas e ideales del renacimiento se esparcieron por la Europa Nórdica, Francia, Inglaterra y el resto de Europa.

Durante Guerras Italianas (1494-1559) provocadas por la rivalidad entre Francia y España, las ciudades-estado gradualmente perdieron su independencia y estuvieron bajo la dominación extranjera, primero bajo España (1559-1713) y después Austria (1713-1796). Entre 1629-1631 una nueva plaga aniquiló el 14 % de la población. La decadencia del Imperio Español en el siglo XVII se llevó consigo a Nápoles, Sicilia, Cerdeña y Milán. En particular, el sur de Italia se vio excluido del escenario europeo.En el siglo XVIII, debido a la Guerra de Sucesión Española, Austria reemplazo a España como la principal potencia extranjera. Durante las Guerras Napoleónicas, el norte de Italia fue invadido y reorganizado como el Reino de Italia, un estado títere del Imperio Francés, mientras que el sur fue gobernado por Joaquín Murat, cuñado de Napoleón, coronado como rey de Nápoles. En 1814 el Congreso de Viena restauró la situación del siglo XVIII, mas los ideales de la Revolución Francesa no podrían ser erradicados. 

El nacimiento del Reino de Italia fue gracias a los esfuerzos unidos de los nacionalistas y monárquicos leales a la casa de Saboya, para establecer un estado unificado en la península itálica. En el contexto de las revoluciones liberales de 1848 a través de Europa, se produjo una infructuosa guerra contra Austria. El Reino de Cerdeña atacó nuevamente a Austria en la Segunda Guerra Italiana de Independencia en 1859, con la ayuda de Francia, resultando en la liberación de Lombardía.

En 1860-61, el general Giuseppe Garibaldi llevó a cabo la unificación en Nápoles y Sicilia, haciendo que el conde de Cavour declarara un reino unificado el 17 de marzo de 1861. En 1866, Víctor Manuel II se alió con Prusia durante la guerra austro-prusiana, en la Tercera Guerra Italiana de Independencia que permitió la anexión de Venecia. Finalmente, después de la Guerra Franco-Prusiana de 1870, Francia abandono sus intereses en Roma, lo cual permitió la captura de Roma y el fin de los Estados Pontificios.

El Estatuto Albertino de 1848 se extendió a todo el Reino de Italia en 1871, proveyéndole de libertades básicas, aunque las leyes electorales excluían a las personas sin propiedades y los no educados. El nuevo gobierno del reino era una monarquía parlamentaria constitucional, dominada por las fuerzas liberales. El sufragio universal masculino fue adoptado en 1913. Mientras el norte se industrializaba rápidamente, el sur y las zonas rurales del norte permanecieron subdesarrolladas y sobrepobladas, forzando a millones de personas a migrar. El Partido Socialista Italiano se fortalecía y desafiaba a los tradicionales partidos liberales y conservadores. Desde finales del siglo XIX, Italia se convirtió en una fuerza colonial, con colonias en Somalia, Eritrea, Libia y el Dodecaneso.

Italia, aliada de los imperios alemán y austrohúngaro en la Triple Alianza, en 1915 se unió a las fuerzas aliadas en la Primera Guerra Mundial, con la promesa de extender su territorio, con los terrenos de Carniola Interior, el litoral austriaco y Dalmacia. El ejército italiano quedó inicialmente estancado en una guerra de trincheras en los Alpes. En octubre de 1918, los italianos lanzaron una feroz ofensiva que culminó en victoria en la batalla de Vittorio Veneto. La victoria aseguró el final de la guerra en el frente italiano. Dos semanas después acababa el conflicto.

Durante la guerra, murieron 650 000 soldados y muchos civiles, llevando a la quiebra al reino. Los tratados de Saint Germain, Rapallo y Roma, concedieron la mayoría de los territorios reclamados, más no la costa dálmata, lo que hizo que varios grupos nacionalistas definieran la victoria como <nowiki>"mutilada"</nowiki>. Más adelante, tras la creación del Estado Libre de Fiume por el poeta Gabriele D'Annunzio, también Fiume fue anexionada.

Las agitaciones socialistas que siguieron a la Primera Guerra Mundial, inspiradas por la Revolución Rusa, llevaron a una contrarrevolución y represión. Debido al temor de una revolución, el pequeño Partido Nacional Fascista, liderado por Benito Mussolini, se convirtió en una importante fuerza política. En octubre de 1922, las camisas negras del PNF, llevaron a cabo un intento de golpe de estado (la Marcha sobre Roma), que fracasó en último instante, mas el rey Víctor Manuel III rehusó declarar el estado de sitio y convirtió a Mussolini en primer ministro. En los siguientes años, Mussolini eliminó todos los partidos políticos y libertades personales, estableciéndose una dictadura fascista. Estas acciones inspiraron el surgimiento de otras en Europa, como la Alemania nazi o la España franquista.

En 1935, Italia invadió Etiopía en la segunda guerra ítalo-etíope, llevando a la salida del país de la Sociedad de las Naciones. Italia se alió con la Alemania nazi, el Imperio del Japón y apoyó a Francisco Franco en la guerra civil española. En 1939 se anexionó Albania, protectorado de facto durante décadas. Italia entró en la Segunda Guerra Mundial el 10 de junio de 1940. Después de haber avanzado inicialmente en la Somalia Británica y Egipto, fueron derrotados en el Norte de África, en Grecia y en el Frente Oriental.

Después del ataque sobre Yugoslavia de la Alemania nazi e Italia, la fuerte presión sobre la resistencia partisana y los intentos de italianización de la población resultaron en los crímenes de guerra italianos y en la deportación de 25 000 personas a los campos de concentración. Cerca de 250 000 italianos y eslavos anticomunistas abandonaron el país en el éxodo istriano.

La invasión aliada de Sicilia comenzó en julio de 1943, lo cual llevó al colapso del régimen el 25 de julio. El 8 de septiembre se rindió en el Armisticio entre Italia y las fuerzas armadas aliadas. Rápidamente los alemanes tomaron el poder sobre el centro y sur del territorio. El país se mantuvo como un campo de batalla el resto de la guerra, mientras los aliados avanzaban lentamente fuera del sur.

Para contrarrestar el avance aliado se creó la República Social Italiana, un estado títere nazi, con Mussolini a su cabeza. Los paisanos organizaron un movimiento de resistencia contra el nazismo y el fascismo. Las hostilidades acabaron el 29 de abril de 1945, cuando la resistencia derrotó a los nazis, obligándolos a abandonar el país. Mussolini fue fusilado. Casi un millón de italianos (incluyendo civiles) murieron en la guerra y la economía nacional estaba totalmente destruida. 

Italia se convirtió en república después de un plebiscito realizado el 2 de junio de 1946. En esta oportunidad por primera vez las mujeres pudieron votar. Humberto II fue forzado a la abdicación y el exilio. La Constitución Republicana fue aprobada el 1 de enero de 1948. Se perdió la mayoría de la Venecia Julia con Yugoslavia y el Territorio libre de Trieste se dividió entre los dos estados. Se perdieron todas las posesiones coloniales, acabando con el Imperio Italiano.

El miedo al triunfo del comunismo en el país fue crucial en la primera elección del país, en abril de 1948, la cual dio la victoria a la Democracia Cristiana, bajo el liderazgo de Alcide De Gásperi. Consecuentemente, en 1949, Italia se unió a la OTAN. El Plan Marshall ayudó a revivir a la economía nacional, la cual hasta finales de la década de 1960, vivió una época de auge, conocida como el milagro económico. En 1957 fue un miembro fundador de la Comunidad Económica Europea (CEE), que en 1993, se convirtió en la Unión Europea (UE).

Desde finales de los años 1960 hasta los finales de los años 1980, se vivieron los años de plomo (anni di piombo), caracterizados por la crisis económica (especialmente en la crisis del petróleo de 1973), conflictos sociales y ataques terroristas por grupos de extrema oposición, debidos a la guerra fría y la intromisión de las inteligencias norteamericanas y soviéticas. La época culminó con el asesinato del líder democratacristiano Aldo Moro en 1978 y la masacre de la estación de tren Bologna en 1980, dejando 85 muertos.

En los años 1980, se rompió la hegemonía de la Democracia Cristiana, con un gobierno liberal (Giovanni Spadolini en 1981) y otro socialista (Bettino Craxi en 1983), pero la Democracia Cristiana siguió siendo el principal partido. Durante el gobierno de Craxi la economía se recuperó, llegando a ser la quinta nación más industrializada, siendo parte del G7. Pero, debido a los gastos del gobierno, la deuda se disparó, llegando al 100 % del PIB.

Las elecciones de 1992 se caracterizaron por el fracaso de los grandes partidos, producto de la parálisis política, la excesiva deuda y la corrupción del sistema electoral, desvelada por la investigación Manos Limpias, requiriéndose cambios radicales. Los escándalos envolvían a la mayoría de los partidos, pero especialmente en el partido gobernante: la Democracia Cristiana, que gobernaban desde hace más de 50 años, sufrieron una fuerte crisis y se desintegraron entre varias facciones. Los comunistas se reorganizaron como una fuerza socialdemócrata. En 1993 se sucedieron distintas dimisiones, entre ellas las del actual Primer Ministro y Bettino Craxi. Durante los años 1990 y 2000, la centro-derecha (liderada por el magnate de los medios Silvio Berlusconi) y las coaliciones de centro-izquierda (lideradas por el profesor universitario Romano Prodi) se alternaron el gobierno del país.

En el 2008 el país fue víctima de la recesión. Hasta el 2015, sufrió 42 meses de recesión económica. La crisis económica fue uno de los principales factores que hicieron que Berlusconi renunciara en 2011. En 2012 se produce el accidente del Costa Concordia, crucero semihundido con 32 muertes. En la elección general de 2013, el secretario general del Partido Democrático Enrico Letta formó un nuevo gobierno a la cabeza de la Gran Coalición. En 2014, desafiado por el nuevo secretario del PD, Matteo Renzi, renunció y fue reemplazado por el mismo. Este emprendió importantes reformas constitucionales como la abolición del senado y una nueva ley electoral. El 4 de diciembre el referéndum constitucional fue rechazado, provocando la renuncia de Renzi 12 días después. El ministro de relaciones exteriores Paolo Gentiloni fue nombrado nuevo primer ministro.

Italia fue afectada por la crisis migratoria europea en 2015 debido a que se convirtió en el punto de entrada y principal destino para la mayoría de los buscadores de asilo en la Unión Europea. El país recibió sobre medio millón de refugiados, causando gran repudio en la población y un surgimiento hacia el apoyo de los partidos de extrema derecha y euro-escépticos, basados en el Brexit, lo que condujo al primer gobierno antisistema de la Unión Europea en 2018.

La política se basa en un sistema republicano parlamentarista con democracia representativa desde el 2 de junio de 1946, cuando la monarquía fue abolida por referéndum popular. El poder ejecutivo está a cargo del Consejo de ministros que están liderados por el jefe de gobierno ("Presidente del Consiglio dei Ministri"), informalmente llamado primer ministro, uno de los cinco cargos más importantes del país junto a los de presidente de la República, presidente del Senado de la República, presidente de la Cámara de diputados y presidente de la Corte constitucional.
El poder legislativo está a cargo del Parlamento y del Consejo de ministros. El poder judicial es independiente del ejecutivo y el legislativo. Además, es un sistema multipartidista. En el sur de la península y en la isla de Sicilia, la mafia tiene tanto o más poder que el Estado, llegando a controlar periódicos, jueces y policías.
En 1992, el asesinato de Giovanni Falcone, un magistrado que investigaba el crimen organizado, y la subsecuente campaña de ""mani pulite"" que se desató conmocionaron a las instituciones italianas, pero tras años de intensas investigaciones, ha habido pocos resultados.
Silvio Berlusconi, ex primer ministro, siempre ha sido sospechoso de corrupción, y, sin embargo, fue elegido en tres ocasiones para su cargo.
Dimitió el 12 de noviembre de 2011 debido a la grave situación económica. En 2018 asumió el primer gobierno antisistema de la Unión Europea.

Italia es parte de la ONU, la UE, la OTAN, la OCDE, la OSCE, el CAD, la OMC, el G-6, G7, G8, G10, G20, la Unión Latina, el Consejo de Europa, la ASEM, el MEF, entre otros. Asimismo, Italia participa en grupos de toma de decisiones importantes como el G-4 (Europa), el "Quint" y el Grupo de contacto.. 

Fue miembro fundador de la Comunidad Europea, ahora Unión Europea. Fue admitida en la Organización de las Naciones Unidas en 1955, y es asimismo miembro de la OTAN, del GATT, de la Organización para la Cooperación y el Desarrollo Económico, de la Organización para la Seguridad y la Cooperación en Europa y del Consejo de Europa. Italia actualmente cumple un rol importante como Potencia Occidental en la lucha contra el terrorismo, al liderar varias fuerzas multinacionales y, al tener tropas desplegadas en el Medio Oriente, en países como Libia, Iraq y Afganistán. También, es importante destacar que desplegó tropas de apoyo en misiones de pacificación de las Naciones Unidas en Somalia, Mozambique y Timor Oriental, y apoyó a la OTAN y a las Naciones Unidas en Bosnia, Kosovo y Albania. Asimismo, el país también juega un papel importante en las antiguas colonias y territorios del Imperio italiano, y es considerado un actor clave en la región mediterránea.

Las Fuerzas Armadas de Italia están formadas por el Ejército, la Marina, la Aeronáutica y el Arma de Carabineros, todos bajo el Consejo Supremo de Defensa presidido por el Presidente de la República Italiana. Desde el año 2005, en el país el servicio militar es enteramente voluntario. En el año 2010, las fuerzas armadas italianas tenían un personal de 293 202 militares, de los cuales 114 778 eran carabineros. Ese mismo año, el presupuesto militar de Italia fue el décimo más alto del mundo, equivalente al 1,7% del PIB de la nación. Como miembro de la estrategia de reparto nuclear de la OTAN, el país transalpino custodia noventa armas nucleares estadounidenses, que están almacenadas en las bases aéreas de Ghedi y Aviano.

Italia es la tercera potencia militar de Europa, tras Francia y Reino Unido, y cuarta potencia europea en cuanto a gastos en presupuesto militar, tras Francia, Reino Unido y Alemania.

El Ejército italiano es la fuerza militar terrestre, compuesta en el año 2012 por 105 062 efectivos. Sus materiales de combate principales son el vehículo de combate de infantería Dardo, el cazacarros Centauro, el tanque Ariete o el helicóptero de ataque Mangusta, desplegado en misiones de la ONU. Además, el ejército italiano dispone de otros vehículos acorazados como el Leopard 1 y el M113.

La Marina Militare tenía 32 000 militares en el año 2013 y cuenta como naves destacadas con dos portaaviones, el "Giuseppe Garibaldi" y el nuevo "Cavour", once fragatas, entre ellas tres nuevas de la clase FREMM, y ocho submarinos. En los últimos tiempos la marina italiana, como miembro de la OTAN, ha participado en varias operaciones de la coalición en diversas partes del mundo, tales como la Intervención militar en Libia y la Guerra de Afganistán.
La Aeronautica Militare cuenta con más de 40 000 militares y en el año 2013 operaba 470 aeronaves y seis aviones no tripulados. Entre estos aparatos había 218 cazas de combate y 108 helicópteros. El equipamiento más destacado de la fuerza aérea transalpina son sus 87 cazas Eurofighter Typhoon, a los que sumarán en próximos años otros nueve que están encargados y que reemplazan a los más antiguos F-16.La aviación tiene 67 Panavia Tornado tipo IDS y ECR. Italia también está adquiriendo 90
Lockheed Martin F-35 Lightning II tipo A (60) y B (30) para la aviación y marina capaz de transportar todos los modelos B61, incluido el nuevo tipo 12. Las capacidades de transporte aéreo están cubiertas por doce aviones de carga Alenia C-27J Spartan, cuatro Boeing KC-767 y veintiuna aeronaves de transporte militar C-130J Super Hercules.

Además, Italia cuenta con un cuerpo autónomo de las fuerzas armadas, el Arma de Carabineros, que cumple funciones tanto civiles como militares, pues son la gendarmería y la policía militar italianas.

La Constitución de la República Italiana organiza el territorio desde 1948 en tres niveles de gobierno local, y declara a Roma como la capital de la República. Tradicionalmente se divide en cinco grandes áreas geopolíticas y en veinte regiones administrativas:

De las veinte regiones, cinco (Valle de Aosta, Friuli-Venecia Julia, Sicilia, Cerdeña y Trentino-Alto Adigio) gozan, por motivos históricos y geográficos, de autonomía y de un estatuto especial. De ellas, Sicilia adquirió su derecho a un estatuto especial autonómico en 1946 debido a su condición geográfica y política (cíclico sentido independentista); las otras adquirieron estatuto propio en los siguientes años: Cerdeña, Valle de Aosta y Trentino-Alto Adigio en 1948, por motivos lingüísticos, y en 1963 Friuli-Venecia Julia. La provincia es una división administrativa de nivel intermedio entre el municipio o comuna y la región .

El relieve presenta cuatro grandes unidades regionales: al norte, un sector continental dominado por los Alpes; al sur un sector peninsular articulado por los Apeninos; entre ambas está el valle del Po o Padana; y finalmente las islas volcánicas. El sistema alpino extiende por territorio italiano la casi totalidad de su vertiente meridional. En este gran conjunto montañoso destacan las formaciones calcáreas de los Dolomitas (Marmolada, 3342 m de altura) y en el sector cristalino, las principales cumbres de todo el sistema alpino como el Monte Bianco (4810 m), el Monte Rosa (4634 m) o el Cervino (4478 m). Algunos pasos de montaña (Mont Cenis, Simplon, Brennero) facilitan la comunicación con las regiones vecinas. La región prealpina presenta largos y profundos valles, con numerosos lagos: Garda (370 km²), Mayor, Como, Iseo. Al sur de los Alpes, entre estos y los Apeninos, se extiende el valle del Po (el río más largo del país, con 652 km de longitud), fosa tectónica rellenada por los depósitos sedimentarios aportados por los ríos que descienden de los Apeninos y, sobre todo, de los Alpes (Adigio, 410 km; Piave), y que se abre al mar Adriático por el litoral noreste de Italia.

El resto de Italia, aunque numerosos valles, son de escasa extensión, y se localizan preferentemente en el litoral tirrénico, y algunas formadas por importantes ríos como el Arno o el Tíber. La cadena de los Apeninos constituye la espina dorsal de la península italiana, y en ella se distinguen tres sectores: los Apeninos septentrionales, los de menor altura y de formas más suaves (monte Cimone, 2163 m); los Apeninos centrales, también denominados Abruzos, que constituyen el techo de la cadena (Gran Sasso d'Italia, 2914 m), y presentan modelados de tipo cárstico; y por último, los Apeninos meridionales, que tienen su punto culminante en el monte Pollino (2271 m). En ambas vertientes de la cadena se extienden formaciones de colinas, denominadas Subapeninos o Antiapeninos, destacando las del reborde Oeste, donde se elevan algunos volcanes (Vesubio, monte Amiata, Campos Flégreos).

En el extremo sur de la península itálica, la isla de Sicilia es considerada una prolongación de los Apeninos (montes Nebrodi, Peloritani, Madonia), destacando el monte Etna, que con sus 3345 m es el volcán activo más alto de Europa. La isla de Cerdeña es asimismo montañosa (Gennargentu), aunque cabe destacar la llanura de origen fluvial de Campidano, entre Oristán y Cagliari.

La climatología italiana, tiene en general un carácter continental (en el norte central), mediterráneo y subtropical, pero presenta notables variaciones regionales. En primer lugar, por efecto de su considerable extensión en latitud: medias anuales en Milán de 25,0 °C en julio y 1,4 °C en enero, mientras que en Palermo, dichas medias son de 29,3 y 13 °C, respectivamente. El lugar con más precipitaciones del país es la provincia de Udine, en el nordeste, con 1530 mm, y por el contrario, el lugar con menores precipitaciones está en el sur de la región de Apulia, en la provincia de Foggia y en la parte sur de Sicilia, las regiones áridas con aproximadamente 460 mm. Se puede diferenciar el país en tres regiones climáticas: el clima mediterráneo en el sur de Italia (bajo Roma), con veranos calurosos superando los 30 °C, los llanos del río Po, donde el invierno es muy frío como en los países del norte y los Alpes, y los Apeninos (Liguria), con clima suave en inviernos y calor en verano y precipitaciones fuertes.

La mayor parte de Italia corresponde al bioma de bosque mediterráneo, aunque también están presentes el bosque templado de frondosas, entre el valle del Po y los Apeninos, y el bosque templado de coníferas en los Alpes.

Según el Fondo Mundial para la Naturaleza, el territorio de Italia se divide en ocho ecorregiones diferentes:

La actividad industrial ha sido el motor del desarrollo italiano, y el actual eje de su economía. Frente a ello, las actividades agrícolas han experimentado un considerable retroceso, tanto en ocupación de la población activa (7,3%), como en su participación en el PIB (3,7%). La producción agrícola no abastece la demanda alimenticia de la población, y es especialmente escasa en la rama ganadera: bovino (Cerdeña) y porcino (Emilia-Romania).

La agricultura está más extendida con cultivos de cereales (trigo, arroz ―primera productora europea―, maíz), leguminosas, plantas industriales (remolacha azucarera), hortalizas (pimientos, berenjenas, cebollas) y flores. Mención especial merece la fruticultura (peras, melocotones y manzanas en Emilia, Véneto y Campania; agrios en Sicilia), el olivo (en Liguria y el "Mezzogiorno"), que genera la segunda producción mundial de aceite (435 300 t), y finalmente, la vid, cuyo cultivo sitúa a Italia a la cabeza de la producción mundial de vinos (68,6 millones de hl), reconocidos internacionalmente por su calidad.

El turismo es uno de los sectores con más crecimiento de la economía nacional con 60,5 millones de turistas por año y un total de 52 700 millones de dólares generados, siendo así el cuarto país con más turismo del mundo. Roma, la capital, es uno de los destinos más visitados del mundo, con una media de 12 a 15 millones de turistas al año. El Coliseo de Roma, con cuatro millones de turistas, es uno de los lugares más visitados de Italia. También se beneficia del turismo religioso y cultural que genera la Ciudad del Vaticano, con lugares tan visitados como los Museos Vaticanos, la Basílica de San Pedro o la Capilla Sixtina, así como de una infinidad de otros lugares de interés espiritual, artístico, arquitectónico, paisajístico, naturalístico, arqueológico, histórico y cultural esparcidos por toda la geografía italiana.

Algunos otros lugares de gran interés de la capital italiana incluyen: el Panteón de Agripa, la Fontana de Trevi, la Plaza Navona, la Plaza de España, la Plaza del Popolo, el Foro Romano, los Foros imperiales, el Monte Palatino, los Museos Capitolinos, el Museo Nacional Romano, la Galería Borghese, el Campidoglio, el Palacio del Quirinal, las Termas de Caracalla, las Termas de Diocleciano, el Castillo Sant'Angelo, el Vittoriano, el Ara Pacis, el Arco de Tito, el Arco de Constantino, la Domus Aurea, la Pirámide Cestia, el Mausoleo de Augusto, la Basílica de San Juan de Letrán, la Basílica de San Pablo Extramuros o la Basílica de Santa María la Mayor, entre otros.
El interés cultural del país también se refleja en todos los de la Unesco que posee, ya que es el país que contiene el mayor número de lugares en el mundo con 55 , además de sus casi 8000 km de costas con innumerables y diversificados paisajes marinos tan famosos como la Costa Amalfitana, la Costa Smeralda, la Riviera Italiana, la isla de Capri, las islas Eolias, etc., grandes instalaciones de esquí y amplias posibilidades de montañismo en los Alpes, los Dolomitas y los Apeninos, los famosos grandes lagos italianos como el lago de Garda, el lago de Como, el lago Mayor, etc., 
importantes sitios arqueológicos como Pompeya, Herculano, el Valle de los Templos de Agrigento, Ostia Antica, etc., o ciudades tan conocidas y visitadas como Florencia, Venecia, 
Nápoles, Milán, Verona, Turín, Bolonia, Génova, Pisa, Siena, Palermo, Matera, Siracusa, entre otras. Muy importantes son también el turismo gastronómico, el enoturismo y el turismo termal. 

Según el Fondo Monetario Internacional en 2018 fue la economía mundial y la cuarta de Europa por PIB. 
Como país avanzado tiene la sexta riqueza nacional en el mundo. Pertenece al G7, a la Unión Europea y a la Organización para la Cooperación y el Desarrollo Económico. El comercio exterior, en su mayor parte desarrollado dentro de la órbita de la UE, presenta habitualmente una balanza comercial en positivo, siendo el sexto país del mundo en volumen de exportación en 2008 con 546 900 millones de dólares. Su PIB per cápita (PPP) es de 30 200 dólares (estimaciones de 2009), 101% de la UE-27 en 2007.Milán y Roma son la 11ª y la 18ª ciudades más caras del mundo, además de ser Milán la 26ª con mayor producto interno bruto, con 115 mil millones de dólares.
Las mayores exportaciones del país son los vehículos automotores (Ferrari, Maserati, Fiat, Alfa Romeo, Lancia, Aprilia, Piaggio, Gilera), todas las empresas anteriores pertenecen al Grupo Fiat, que es desde 2009 el accionista mayoritario de la Chrysler Motor Corporation, y es considerada ahora como la 3ª empresa automotriz más grande del mundo. También las motocicletas (Ducati, Moto Guzzi, Cagiva y Bimota). En la alimentación (Ferrero, Agnesi, Barilla, Campari, Lavazza, Parmalat, Bertolli). Los astilleros para la fabricación de barcos de crucero y militares (Fincantieri) y los yates (Gruppo Ferretti, Cigarrette), la petroquímica (ENI, ERG), la energía (Enel), los electrodomésticos (Indesit, Candy, Berezza, Rancilio, Ariston, De Longhi), la ingeniería aeroespacial (Alenia, Piaggio Aero, Leonardo-Finmeccanica, Agusta), las armas de fuego (Beretta). También otro rubro muy usado fuera de sus fronteras por incontables empresas de manufactura: la consultoría de diseño industrial o gráfico Ital Design, Pininfarina, Bertone, Ghia, Domus, Cassina o Alessi) por nombrar las más destacadas. Una parte importante del PIB del país es producido por la moda, con marcas como Gucci, Armani, Versace, Dolce & Gabbana, Benetton Group, Prada, Miu Miu, Gianfranco Ferre, Fendi, Lotto, Salvatore Ferragamo, Bvlgari, Bruno Magli, Fratelli Rossetti, Sergio Rossi, Vic Matie, Etro, Luxottica, Moschino, Diesel, Cavalli, Valentino, Bottega Veneta, Diadora o Ellesse.

En el contexto socioeconómico, según datos del Banco Mundial, Italia destaca por tener una tasa de natalidad muy reducida: apenas un niño por mujer, con un descenso continuado en las últimas dos décadas, que la sitúan en el puesto 162 de los países con mayor índice de nacimientos. También es relevante la alta penetración de Internet (41%), entre la población global italiana, un ratio relativamente bajo para un país europeo, especialmente en ciertas áreas del país. Eurostat indica que Italia es el quinto país de Europa en número de muertes por VIH, algo que, sin embargo, no afecta (al menos de manera significativa) a la esperanza de vida, en la que Italia se mantiene en los primeros puestos. La siguiente tabla muestra el contexto socioeconómico de Italia partiendo de datos del Banco Mundial, Eurostat y el Foro Económico Mundial:

Ferrovie dello Stato nació en 1905, y es la más importante compañía ferroviaria pública de Italia. A partir del año 2000, siguiendo la normativa europea que obliga a la separación del sector transporte de pasajeros, del sector infraestructura, la sociedad fue reorganizada. Por ejemplo, Ferrovie dello Stato Spa es la sociedad principal, Trenitalia es la sociedad que se encarga del transporte de cargas y de pasajeros, la Rete Ferroviaria Italiana es la sociedad encargada de la infraestructura ferroviaria y la Treno Alta Velocità es la sociedad que tiene a su cargo la construcción de la red de alta velocidad, aunque existen otras. Actualmente, los trenes de alta velocidad italianos son los ETR 500 y ETR 1000, y las líneas que existen en este momento son: Roma-Florencia, Roma-Nápoles, Turín-Novara, Padua-Venecia, Milán-Treviglio y Milán-Bolonia.

En total, en 2003, había 16 287 kilómetros de vías de tren, 668 721 kilómetros de carreteras, de los cuales 6487 kilómetros eran de autopista, y 4379 kilómetros de transporte por tubería. Los aeropuertos con más tráfico aéreo en 2003 fueron Roma-Fiumicino, Milán-Malpensa, Milán-Linate, Venecia y Catania-Fontanarossa. Por su parte, los puertos con más carga fueron Génova, Trieste, Nápoles, Augusta y Gioia Tauro. En 2005, 590 de cada 1000 italianos poseían un coche y en la mayoría de las ciudades el 60% de los ciudadanos no estaban satisfechos con el transporte público, razones por las cuales el número de pasajeros en dichos transportes ha disminuido.

A finales de 2008 la población del país superó los 60 millones, siendo el cuarto país más poblado de Europa y con la quinta mayor densidad poblacional, con un promedio de 198 personas por kilómetro cuadrado. A partir de los años sesenta del siglo XX, la población italiana experimentó un cambio en su ritmo de crecimiento, que decreció hasta el 0% de media anual entre 1985 y 1990. El descenso de la tasa de mortalidad fue acompañado por un descenso considerable de la tasa de natalidad, siendo en 2008 uno de cada cinco italianos mayor de 65 años. El cambio en las tendencias demográficas afectó asimismo los tradicionales movimientos migratorios que hasta entonces habían hecho de Italia una de las mayores reservas de mano de obra de Europa (Francia, Reino Unido y Alemania, principalmente) y América (Estados Unidos, Brasil, Argentina, Venezuela y Uruguay cuentan con numerosas comunidades de origen italiano). Italia pasó a convertirse en punto de llegada de inmigrantes del tercer mundo, pero, sobre todo, se establecieron importantes corrientes migratorias internas. Con un movimiento masivo de población del sur hacia Roma y el norte industrializado (Turín, Milán, Génova, Venecia y Bolonia), pero no hacia el noreste, aún muy pobre, lo cual no ha hecho sino radicalizar las diferencias entre el norte y el sur, pero que a su vez ayudó a que la natalidad creciera. La tasa de fertilidad creció en pocos años desde 1,32 niños por mujer en 2005 hasta 1,41 en el año 2008. La concentración de la población italiana en los núcleos urbanos (69 % de población urbana) ha generado una red homogénea de grandes ciudades, que desempeñan el papel de centros regionales (Nápoles, 973 132 habitantes; Turín, 963 128; Palermo, 663 173; Génova, 610 887; Bolonia, 372 256, y Florencia, 364 710), con dos destacados núcleos a nivel nacional; Roma (2 718 768 hab.), la capital política, y Milán (1 299 633), la capital económica.

Ciudades metropolitanas y sus delimitaciones previstas por el ordenamiento jurídico nacional y regiones con estatuto especial. 

Los grupos minoritarios son pequeños, siendo el mayor de estos el de habla alemana en la provincia autónoma de Bolzano (según el censo de 1991, la población de la provincia de Bolzano se encuentra compuesta por 287 503 personas de habla alemana y 116 914 de habla italiana), seguido por los francoprovenzales en la región del Valle de Aosta y los eslovenos alrededor de Trieste. El idioma ladino es el más hablado de la región de los Dolomitas.

Otros grupos minoritarios con lenguajes parcialmente tutelados incluyen los friulianos y los sardos, incluyendo los hablantes de catalán en Alguer. Italia tiene 66 676 779 habitantes (Istat 04.2008), y está compuesta étnicamente (datos 2006) por 97,6% de europeos (italianos 95,9% + otros europeos 1,5%), 0,5% de africanos (mayoría de marroquíes), 1,3% de asiáticos (mayoría de chinos), 0,8% de americanos (mayoría de ecuatorianos).

La lengua oficial de Italia es el italiano que es un idioma romance que proviene de una variedad de toscano, el florentino arcaico, y pertenece al grupo itálico de la familia de lenguas indoeuropeas. Hasta el siglo XVI el italiano se identificó plenamente con el toscano y con los grandes escritores prerrenacentistas de aquella región (Dante Alighieri, Francesco Petrarca y Giovanni Boccaccio) cuyas obras gozaron de gran prestigio y tuvieron una notable difusión en toda Italia y Europa. A partir de aquel siglo, con la internacionalización del Renacimiento, la literatura y el idioma italiano se propagaron aún más rápidamente que en el período anterior en todo el mundo occidental. En aquella época la lengua italiana (denominación que había terminado por prevalecer, durante el siglo XVI, sobre cualquier otra) había dejado de identificarse con el vulgar florentino y, gracias al alto nivel de su literatura, se había ido imponiendo como uno de los grandes idiomas de cultura en la Europa del tiempo. Hacia 1550 se empezaron a escribir gramáticas y vocabularios italianos destinados a extranjeros y a menudo escritos por extranjeros. A finales del siglo XVI las publicaciones en lengua italiana superaron en número por primera vez en Italia a las escritas en latín, que, sin embargo, siguió manteniendo una notable importancia en el campo de la filosofía, del derecho y de las ciencias. No sin razón, un célebre lingüista italiano puso de relieve que en 1500–1600 se produjo la primera unificación lingüística de Italia gracias al italiano escrito, cuando todavía no existía una unidad política del país. Antes de que Italia se constituyera en estado unitario (1861), el italiano ya era el único idioma administrativo y de cultura con difusión nacional y monopolizaba la comunicación pública y literaria, pero, a pesar de eso, tenía un carácter fuertemente elitista y solo una pequeña minoría de italianos lo hablaba, o sea todos los que habían cursado estudios superiores. La gran mayoría prefería expresarse en los varios dialectos, hablas e idiomas locales que caracterizaban la comunicación oral en la Italia de entonces. Por lo que se refiere al italiano escrito, su difusión estaba condicionada negativamente en la edad preunitaria por el bajo nivel de alfabetización (algo menos de la cuarta parte de la población italiana sabía leer y escribir en el año 1861).

Con la proclamación del Reino de Italia el italiano fue proclamado lengua oficial del nuevo estado y tuvo inicio un largo proceso de escolarización de las masas que culminó en el siglo siguiente con la desaparición del analfabetismo, el nacimiento de un tipo de italiano estándar y luego neoestándard casi universalmente aceptado y la italianización irreversible de los dialectos. Durante aquel mismo siglo, gracias también a la difusión de los medios de comunicación de masa (radio, televisión, y, en nuestros días, la informática), se produjo una difusión generalizada del idioma italiano como medio de comunicación coloquial y familiar, convirtiéndose con el tiempo en la lengua materna o primera lengua de todos, o casi todos, los sesenta millones de italianos. Según un informe de la Comisión Europea del año 2006, el 95% de los italianos y de los extranjeros empadronados en Italia, habla como lengua materna o primera lengua el italiano (el porcentaje de los hablantes de las respectivas lenguas nacionales en los otros cuatro más importantes países europeos es el siguiente: Francia 95%, Reino Unido 92%, Alemania 90% y España 89%).

Existen dentro de Italia algunos grupos minoritarios hablantes lenguas no romances, siendo dentro de estos el de habla alemana en la zona del Alto Adigio y unos pocos hablantes de esloveno alrededor de Trieste, además de los arbëreshë (albanoparlantes o albaneses de Italia) y los hablantes de griko y grecanico (unas formas de griego antiguo aún vivas y existentes solo en algunos pueblos del sur de Italia). Otros grupos minoritarios de idiomas parcialmente oficiales incluyen hablantes de francés en las regiones de Valle de Aosta (co-oficial) y Piamonte, la minoría de habla francoprovenzal en la región del Valle de Aosta, el occitano en el Piamonte, el sardo de Cerdeña, el friulano de Friuli, el ladino en los picos dolomitas y, exclusivamente en el pueblo sardo de Alguer ("Alghero" en italiano) el catalán, siendo todos ellos idiomas romances.

Entre los idiomas y dialectos no reconocidos por el Estado italiano se encuentra el véneto que recibió un reconocimiento por la Asamblea legislativa regional veneta como lengua desde 2007 tras la aprobación de la ley 8/2007 para su «protección» y «fomento». La única lengua oficial del Veneto sigue siendo, no obstante eso, el italiano.

La educación en Italia es gratuita y obligatoria entre los 6 y los 16 años. Consta de cinco niveles: "scuola dell'infanzia", "scuola primaria", "scuola secondaria di primo grado", "scuola secondaria di secondo grado" y "università". Las "Scuola superiore universitaria" son instituciones independientes similares a las Grandes Escuelas francesas que ofrecen formación e investigación avanzadas a través de cursos de tipo universitario o se dedican a la enseñanza a nivel de graduado o doctorado.

En Italia existen una amplia variedad de universidades y academias. La universidad más antigua del país y de todo Occidente es la Universidad de Bolonia, fundada en 1088, una institución que además está considerada por el periódico "The Times" como la mejor de Italia y una de las 200 mejores del mundo. La Universidad Bocconi de Milán es una de las mejores escuelas de negocio del mundo gracias a su máster en Administración y Dirección de Empresas, cuyos estudiantes acaban trabajando en grandes compañías multinacionales. Entre las instituciones politécnicas italianas sobresalen el Politécnico de Turín y el de Milán, la Universidad de Roma "La Sapienza" y la Universidad de Milán, todas ellas con presencia habitual en los listados de los mejores centros de estudio en el campo científico.

Según los Indicadores Científicos Nacionales (1981–2002), una base de datos creada por el Grupo de Servicios de Investigación que contiene listados de estadísticas de citación de publicaciones de más de noventa países, Italia está por encima de la media mundial en la citación en revistas científicas sobre ciencia espacial, matemáticas, informática, neurociencia y física. También están por encima de la media, aunque menos destacadas, la citación de publicaciones italianas sobre ciencias sociales, psicología, psiquiatría, economía y negocios.

La Salud en Italia es conocida por tener en general un buen sistema de salud, siendo uno de los mejores de toda Europa y también a nivel mundial según la Organización Mundial de la Salud, con una muy alta esperanza de vida entre su población y unas tasas muy bajas de mortalidad infantil, mortalidad neonatal y mortalidad materna. Al igual que con cualquier otro país desarrollado, Italia también ha desarrollado una distribución adecuada y suficiente del agua y de los alimentos, y los niveles de nutrición y saneamiento son también altos, así como la cocina y dieta relativamente saludables.

Según un estudio publicado en el periódico "Corriere della Sera" en el año 2006, el 87,8% de los italianos se declaran católicos, uno de los porcentajes más altos de Europa. Los practicantes alcanzan el 36,8%, mientras que se reúne en misa todos los domingos el 30,8% de los entrevistados entre 18 y 24 años, frente al 22,4% y el 28,5% de los sujetos entrevistados pertenecientes, respectivamente, a la franja de edad entre 24 y 34 años y entre 34 y 44 años. La discrepancia que hay tras el que se declara católico y el de estricta observancia, aunque es menor respecto a los otros países de Europa occidental, es importante, como indican las opiniones relativas a la fecundación asistida y uniones civiles.

Los cristianos (católicos, protestantes, ortodoxos, etc.) junto con Testigos de Jehová y mormones representan la religión mayoritaria. Como en muchos países occidentales, el proceso de secularización es creciente, sobre todo entre los jóvenes, aunque no falta la presencia de movimientos católicos como Acción Católica, la Juventud Franciscana, la AGESCI, Comunión y Liberación y Camino Neocatecumenal que intentan revertir o paliar este proceso. La religión más antigua presente en el país es el judaísmo, el cual tiene una presencia ininterrumpida en Roma. Actualmente, la comunidad judía se compone de unas 45 000 personas.

Importante para la gastronomía italiana, así como para otras gastronomías europeas, fue el descubrimiento de América, debido a la adquisición de nuevos vegetales como la patata, el tomate, el morrón o el maíz, aunque no fueron utilizados a gran escala hasta el siglo XVIII. La gastronomía de Italia es muy variada: el país fue unificado en el año 1861, y sus cocinas reflejan la variedad cultural de sus regiones así como la diversidad de su historia. La cocina italiana está incluida dentro de la denominada gastronomía mediterránea y, como dieta mediterránea, ha sido declarada Patrimonio Cultural Inmaterial de la Humanidad por la Unesco y es imitada y practicada en todo el mundo. Es muy común que se conozca a la gastronomía de Italia por sus platos más famosos, como son la pizza, la pasta, el risotto y el gelato, pero lo cierto es que es una cocina donde existen los abundantes aromas y los sabores del mar Mediterráneo. Se trata de una gastronomía con la que se han sabido perpetuar las antiguas recetas como la polenta (alimento de la legión romana) que hoy en día puede degustarse en cualquier trattoria italiana.

Italia es el mayor productor de vinos a nivel mundial y posee la más grande variedad de quesos en el mundo.

Los orígenes de la pintura renacentista se hallan en el arte de la Antigua Roma y en el arte helenístico, que fueron retomados por mano de los artistas italianos del Quattrocento y del Cinquecento. Los procedimientos usados en esta pintura debieron ser el encausto, el temple y el fresco. Sus géneros son el decorativo de vajillas y muros, y el histórico y mitológico en los cuadros murales. Se cultivaron con dicho carácter decorativo mural, el paisaje, la caricatura, el retrato, los cuadros de costumbres, las imitaciones arquitectónicas y las combinaciones fantásticas de objetos naturales, constituyendo el género que los artistas del Renacimiento llamaron "grutesco", hallado en las antiguas Termas de Tito y que sirvió al célebre Rafael Sanzio como fuente de inspiración para decorar las Logias del Vaticano. Destacó también el arte pictórico de la civilización romana en el procedimiento del mosaico o la miniatura sobre pergamino.

La pintura renacentista llegó a su fase perfecta poco después que su precursora la escultura, es decir, durante el siglo XV en Florencia y ya entrado el siglo siguiente en los otros países. En general, el siglo XV es de iniciación y los siglos XVI y XVII lo son de apogeo para la pintura del renacimiento clásico. Algunos de sus pintores más conocidos son: Sandro Botticelli, Leonardo da Vinci, Miguel Ángel,Donatello, Marco Palmezzano, Andrea Mantegna, Cariani o Rafael Sanzio. En Italia, una fase de decadencia a finales del siglo XVI, lleva a los grandes maestros italianos de Renacimiento en otra fase y al desarrollo del arte Barroco. La decadencia total en los diferentes países europeos corresponde al siglo XVIII, siguiéndole la restauración a finales de dicho siglo.

La escultura de Roma, lo mismo que la arquitectura, es original, pero en ella pesan mucho los aportes formales etruscas y griegas (helenísticas), siendo de hecho buena parte de la producción escultórica romana copia de originales griegos. Se conservan muchas esculturas romanas, hechas preferentemente en mármol y en menor medida en bronce u otros materiales como el marfil, si bien parte de ella está dañada. Son frecuentes el retrato y el relieve histórico narrativo, en los que los romanos fueron grandes creadores. Hay también muchas esculturas de emperadores romanos.

La escultura del Renacimiento clásico se reconoce por dos principios fundamentales: el estudio e imitación de la Naturaleza y la adopción de las formas y maneras clásicas de Grecia y Roma para la interpretación de la misma Naturaleza en el terreno plástico. Así logró interpretar la Naturaleza y traducirla con libertad y soltura por medio del pincel y el escoplo en gran multitud de obras maestras. Lorenzo Ghiberti, Donatello y Luca della Robbia, con los discípulos del segundo Verrocchio y Antonio Pollaiuolo, constituyeron la llamada escuela florentina, al mismo tiempo que Jacopo della Quercia formaba en Siena la escuela sienense. También destaca Miguel Ángel, que resume en su persona casi todo el arte escultórico de su época en Italia (1475-1564). A esta misma época de apogeo en el estilo renacentista pertenecen: Benvenuto Cellini, Jacobo Tatti, Pedro Torrigiani, Leone Leoni y Pompeo Leoni. Gian Lorenzo Bernini es el más importante escultor del barroco. El periodo neoclásico o de restauración greco-romana comienza con el último cuarto del siglo XVIII, iniciándose por el escultor Antonio Canova (1757-1822).

Un personaje italiano entre los más importantes de la historia de la música es Guido d'Arezzo. Conocido también con el nombre de Guido Aretinus, fue un monje benedictino que reformó el sistema de notación musical.
Durante el siglo XI, Guido d'Arezzo perfeccionó la escritura musical con la implementación definitiva de líneas horizontales que fijaron alturas de sonido e inventó además de las notas musicales, el famoso tetragrama, que luego evolucionó al pentagrama. Actualmente se le considera el "padre de la música". En la Edad Media, las notas musicales se denominaban por medio de las primeras letras del alfabeto: A, B, C, D, E, F, G (comenzando por la actual nota la). En aquella época solía cantarse un himno a San Juan el Bautista, conocido como Ut queant laxis, atribuido a Pablo el Diácono, que tenía la particularidad de que cada frase musical empezaba con una nota superior a la que antecedía.

La primera obra considerada una ópera, data aproximadamente del año 1597. Esta fue "Dafne" (obra actualmente desaparecida) escrita por Jacopo Peri para un círculo de humanistas letrados florentinos conocidos como los Camerata Florentina y que fue un intento por revivir la tragedia griega propia del Renacimiento. Un siguiente trabajo de Peri, "Eurídice", que data del año 1600, es la primera ópera que haya sobrevivido hasta la actualidad. No obstante, el uso del término "ópera" se inicia cincuenta años después, a mediados del siglo XVII para definir las piezas de teatro musical, a las cuales se les refería como "dramma per musica" ('drama musical') o "f"a"vola in musica" ('fábula musical'). En el año 1637 en Venecia emergió la idea de una "temporada" de óperas de asistencia abierta a todo público, financiada por la venta de entradas.

Influyentes compositores del Renacimiento incluyen a Giovanni Pierluigi da Palestrina, Francesco Cavalli, Carlo Gesualdo y Claudio Monteverdi, cuyo "Orfeo" (1607) es la ópera más antigua que todavía se representa hoy en día. Los "libretti" italianos fueron la norma, incluso para compositores alemanes como Georg Friedrich Händel que escribía para audiencias londinenses, o Wolfgang Amadeus Mozart en Viena, cerca de finales del siglo XVIII. Los compositores más importantes del incluyen a Alessandro Scarlatti, Arcangelo Corelli, Antonio Vivaldi y Domenico Scarlatti, los clásicos a Giovanni Paisiello, Domenico Cimarosa, Niccolò Paganini y Gioachino Rossini, y los Vincenzo Bellini, Giuseppe Verdi y Giacomo Puccini. Además en el país son habituales los centros musicales, como los importantes Teatro de La Scala o Teatro de San Carlos.

Numerosos instrumentos musicales fueron inventados en Italia y por italianos, entre los más famosos el piano, el violín, la viola, el violonchelo, el contrabajo o la mandolina, entre otros.
En los años 70 el movimiento del rock progresivo creó bandas como Premiata Forneria Marconi, Goblin, Area. Otros cantantes famosos son Luciano Pavarotti, Domenico Modugno, Raffaella Carrà, Ricchi e Poveri, Al Bano & Romina Power, Mina, Fabrizio De André, Francesco Guccini, Paolo Conte, Lucio Dalla, Lucio Battisti, Nicola di Bari, Sandro Giacobbe, Umberto Tozzi, Laura Pausini, Eros Ramazzotti, Marco Mengoni, Mango, Andrea Bocelli, Tiziano Ferro y Il Volo, estos últimos, pertenecen a la nueva generación de artistas italianos de gran renombre. Además la música popular napolitana del siglo XIX y comienzos del XX ha hecho canciones como "''O sole mio", "Funiculì, funiculà" y "'O surdato 'nnammurato".
La arquitectura de la Antigua Roma se caracteriza por lo grandioso de las edificaciones, y su solidez que ha permitido que muchas de ellas perduren hasta nuestros días. La organización del Imperio romano normalizó las técnicas constructivas de forma que se pueden ver construcciones muy semejantes a miles de kilómetros unas de otras. Tiene su origen en la arquitectura etrusca, sumada a influjos de la arquitectura griega, sobre todo después de las guerras púnicas (146 a. C.). Hoy se hace datar la arquitectura romana en la fecha en que se construyeron la primera vía ("Vía Apia") y el primer acueducto ("Aqua Appia"), año 312 a. C. Los elementos más significativos de la arquitectura romana son la bóveda, el arco y por tanto la cúpula. Un ejemplo soberbio es la cúpula del panteón de Agripa. Los romanos, no solo construyeron bóvedas de cañón y cúpulas, sino rudimentarias bóvedas de arista y de crucería, como las termas de Caracalla y las de la basílica de Majencio.
La arquitectura gótica llegó de forma tardía y arraigó poco, fueron los cistercienses los introductores y fundaron en la región del Lazio la abadía de Fossanuova, primer monumento gótico italiano. En el siglo XIII las órdenes mendicantes de dominicos y franciscanos se adhieren al estilo cisterciense, y en este siglo se crean la catedral de Siena, los palacios comunales de Siena y el palazzo Vecchio de Florencia. Durante el siglo XIV, destacan la catedral de Orvieto, la iglesia de la Santa Cruz y el interior de la iglesia de Santa María Novella. En el siglo XV, el final del gótico empieza a confundirse con los inicios del Renacimiento. En Venecia se termina el palacio Ducal, destacando también el palacio Contarini del Bovolo y Ca' d'Oro. La obra magna del gótico italiano es la catedral de Milán, que destaca por el recargamiento de su decoración y su magnitud.

La arquitectura del Renacimiento es aquella producida durante el período artístico del Renacimiento europeo, que abarcó los siglos XIV, XV y XVI. Se caracteriza por ser un momento de ruptura en la historia de la arquitectura, en especial con respecto al estilo arquitectónico previo, el gótico. Produce innovaciones en los medios de producción, como en el lenguaje arquitectónico, que se plasmó en una adecuada y completa teorización, en la nueva actitud de los arquitectos, pasando de ser artesanos a verdaderos profesionales, marcando en cada obra su estilo personal. Las grandes catedrales góticas son en su mayoría anónimas, sin embargo, las grandes obras renacentistas están todas firmadas. Inspiraron su labor en su interpretación propia de la Antigüedad clásica, en particular en su vertiente arquitectónica, que consideraban modelo perfecto de las Bellas Artes. La arquitectura del Renacimiento estuvo bastante relacionada con una visión del mundo durante ese período sostenida en dos pilares esenciales, el clasicismo y el humanismo.
La palabra "Barroco" significa "irregular", y es un arte muy cercano al catolicismo en una época de división entre católicos y protestantes. La arquitectura del Barroco se inicia con figuras tan determinantes como Gian Lorenzo Bernini y Francesco Borromini. En este período se crean monumentos como la plaza de San Pedro, la iglesia de Sant'Andrea al Quirinale, la fontana de Trevi y la iglesia iglesia de San Carlo alle Quattro Fontane. Destaca a su vez el Barroco siciliano que creció durante la gran reconstrucción edilicia que siguió al terremoto de 1693. El estilo decorativo del barroco siciliano duró apenas cincuenta años, y reflejó perfectamente el orden social de la isla en una época en que —dominada nominalmente por España— fue gobernada de hecho por una aristocracia hedonista y extravagante. La arquitectura barroca ha dado a la isla un carácter arquitectónico que permanece en el siglo XXI.

La historia del cine italiano comenzó apenas algunos meses después de que los hermanos Lumière hubieran descubierto el medio, cuando el papa León XIII fue filmado durante algunos segundos en los jardines Vaticanos, terminando con la bendición de la cámara. La industria cinematográfica italiana nació entre 1903 y 1908 con la Società Italiana Cines, la Ambrosio Film y la Itala Film. Más tarde el cine fue utilizado por Benito Mussolini como propaganda para la Segunda Guerra Mundial.

Algunos de los más famosos directores italianos, sobre todo de la época del Neorrealismo italiano, han sido Luchino Visconti, Vittorio De Sica, Federico Fellini, Sergio Leone, Pier Paolo Pasolini, Roberto Rossellini o Michelangelo Antonioni u otros contemporáneos como Dario Argento, Sergio Rubini, Giuseppe Tornatore, Matteo Garrone o Paolo Sorrentino. Algunas de las películas más conocidas han sido "El gatopardo", "La dolce vita", "Il buono, il brutto, il cattivo", "Ladri di biciclette", "La vita é bella" o "Il Postino", estas últimas con los famosos actores Roberto Benigni y Massimo Troisi u otras como "Malèna", "La gran belleza" o "Tale of Tales".

Italia es el país que más premios ha recibido en la historia del cine.

A lo largo de los siglos en la península itálica han nacido grandes científicos. Famosos polímatas italianos como Leonardo da Vinci, Miguel Ángel o Leon Battista Alberti han hecho importantes contribuciones a diversos campos del saber como la biología, la arquitectura o la ingeniería. El físico, astrónomo y matemático Galileo Galilei jugó un papel esencial en la denominada revolución científica gracias a logros como la decisiva mejora del telescopio, que permitió aumentar las observaciones astronómicas y confirmar de manera irrefutable el triunfo de las teorías copernicanas sobre el sistema Ptolemaico. Los astrónomos Giovanni Domenico Cassini y Giovanni Schiaparelli realizaron importantes descubrimientos sobre el sistema solar. Joseph-Louis de Lagrange (nacido como Giuseppe Lodovico Lagrangia), Fibonacci y Gerolamo Cardano consiguieron decisivos avances en las matemáticas. El físico Enrico Fermi, galardonado con el Premio Nobel, lideró el equipo que construyó el primer reactor nuclear y codesarrolló la teoría cuántica. Algunos otros 
científicos italianos de renombre son Amedeo Avogadro (célebre por sus aportaciones a la teoría molecular, por la Ley y la Constante de Avogadro), Evangelista Torricelli (inventor del barómetro), Alessandro Volta (inventor de la batería eléctrica), Guillermo Marconi (inventor de la radio), Antonio Meucci (inventor del teléfono), Ettore Majorana (que descubrió el fermión de Majorana), Emilio G. Segrè (que descubrió dos elementos químicos, el tecnecio y el astato, así como el antiprotón), Federico Faggin (creador del primer microprocesador comercial), Leonardo Chiariglione (creador de los formatos MPEG y MP3) o Carlo Rubbia (premio Nobel de física en 1984 por su trabajo liderando el descubrimiento de los bosones W y Z en el CERN), entre otros.

En biología sobresalen Marcello Malpighi, que fundó en el siglo XVII la histología, Lazzaro Spallanzani, que desarrolló importantes investigaciones sobre las funciones corporales, la reproducción animal y la teoría celular, Camillo Golgi, que entre sus muchos logros tiene el descubrimiento del aparato de Golgi y allanó el camino para la aceptación de la doctrina de la neurona, o Rita Levi-Montalcini, cuyo descubrimiento del factor de crecimiento nervioso le valió el Nobel de fisiología de 1986. En química, Giulio Natta fue premio Nobel de química en 1963 por su trabajo sobre los polímeros. Giuseppe Occhialini recibió el Premio Wolf en Física por el descubrimiento del pion en 1947 y Ennio de Giorgi, que fue en 1990, resolvió el problema de Bernstein sobre superficies mínimas y el decimonoveno problema de Hilbert.

Conforme el Imperio romano de Occidente desaparecía, el latín tradicional se mantuvo vivo gracias a escritores como Casiodoro, Boecio y Símaco. Las artes liberales florecieron en Rávena bajo Teodorico el Grande y los reyes godos se rodearon con maestros de retórica y gramáticos. El año 1230 marca el comienzo de la escuela siciliana y el inicio de una literatura que muestra ya rasgos más uniformes.

El italiano moderno es un dialecto que ha conseguido imponerse como lengua propia de una región mucho más vasta que su región dialectal, en este caso se trata del dialecto toscano de Florencia, Pisa y Siena y que ha evolucionado a partir del latín. El toscano es en efecto la lengua en la que escribieron durante la Edad Media y el Renacimiento Dante Alighieri, Petrarca, Giacomo Leopardi, Alessandro Manzoni, Torquato Tasso, Ludovico Ariosto y Giovanni Boccaccio, considerados como los grandes escritores italianos que ejercieron una gran influencia sobre la literatura europea en general y española en particular; siendo adoptadas algunas formas estróficas como el soneto, la lira o la octava real, al popularizarse los versos endecasílabos y octosílabos.

Algunos filósofos importantes han sido Tomás de Aquino, Bernardino Telesio, Giordano Bruno, Marsilio Ficino, Giovanni Pico della Mirandola, Nicolás Maquiavelo y Giambattista Vico. Otras figuras importantes del país han sido los poetas Giosuè Carducci, Gabriele D'Annunzio, Salvatore Quasimodo, Eugenio Montale, Giuseppe Ungaretti, la escritora Grazia Deledda, y los autores teatrales Luigi Pirandello y Dario Fo, todos ellos ganadores del .

El deporte más popular en el país es el fútbol, denominado en italiano "calcio". Desde el siglo XVI se practica el llamado calcio florentino, que consiste en dos equipos de 27 jugadores y 5 porteros, donde el objetivo es sumar más puntos que el equipo rival. El "calcio" se ha intensificado a nivel local, llegando a fundarse en 1898 la Federación Italiana de Fútbol, que se encarga de los campeonatos de fútbol de clubes y de la selección nacional, que ha ganado entre otros trofeos, cuatro Copas del Mundo FIFA en 1934, 1938, 1982 y 2006, y una Eurocopa, en 1968. Además, ha logrado alcanzar el número 1 de la clasificación de la FIFA en 1993 y 2007. Los principales clubes del país son la Juventus FC, el AC Milan y el Inter de Milán.

La Juventus FC es el club que más títulos ha logrado en el fútbol italiano, es el octavo club con el mayor número de trofeos internacionales conquistados en el mundo (cuarto en Europa) y además, el en el planeta que ha conquistado todas las competiciones organizadas por alguna de las seis confederaciones continentales de fútbol y el . El AC Milan es el tercer club que más títulos internacionales ostenta (18), y el Inter de Milán además de poseer varios títulos nacionales e internacionales es el único que ha participado en todas las ediciones de la Serie A, desde su institución en 1929, además es el único equipo italiano que ha ganado cinco títulos en un mismo año solar. Otros clubes que han ganado torneos internacionales son: la ACF Fiorentina, la SS Lazio, el SSC Napoli, el Parma FC, la AS Roma, el UC Sampdoria y el Torino FC.

En voleibol tanto a nivel de selección como de equipos de clubes Italia forma parte, sobre todo en masculino, de la élite mundial del deporte. La selección masculina es una de las más exitosas de la historia tras ganar, entre otros, tres Mundiales, seis Eurocopas, ocho World Leagues y cinco medallas olímpicas (platas y tres bronces). También la selección femenina ha conseguido levantar un Mundial y dos Eurocopas.

Los equipos de clubes a nivel masculino son los más laureados de Europa, con en todas la competiciones, y del Mundo gracias a sus ocho Campeonatos Mundial de Clubes en nueve ediciones. Ganaron trofeos sobre todo conjuntos como el Pallavolo Modena, el Pallavolo Parma, el Pallavolo Torino y el Porto Ravenna Volley en los años 80 y en los primeros años 90 y el Sisley Treviso, el PV Cuneo, el Lube Macerata y el Trentino Volley en la segunda mitad de los 90 y en el siglo XXI. Los equipos femeninos han logrado 39 títulos europeos y 1 mundial de clubes.

En automovilismo, es sede de la famosa escudería Ferrari, la más reconocida en disciplinas como la Fórmula 1. Además aquí se celebra el Gran Premio de Italia, una de las competiciones más importantes a escala internacional. En motociclismo cabe destacar la celebración del Gran Premio de Italia y a los pilotos Giacomo Agostini (15 títulos mundiales), Carlo Ubbiali (9) y Valentino Rossi (9).

En ciclismo posee una de las tres grandes Vueltas a nivel mundial, el Giro de Italia. Además han destacado ciclistas como Alfredo Binda ganador en tres ocasiones del Campeonato Mundial y en cinco del Giro de Italia, Fausto Coppi también ganador en cinco ocasiones del Giro de Italia y en dos del Tour de Francia o Marco Pantani ganador de un Giro de Italia y un Tour de Francia. También destacan en el ciclismo en pista, modalidad en la que han conseguido varios campeonatos del mundo.

En rugby, los equipos disputan el Pro14 que da acceso a disputar la competición europea Heineken Cup. La selección de rugby participa en el Torneo de las Seis Naciones y habitualmente en la Copa del Mundo, siendo su único título el obtenido en 1997 en la European Nations Cup.

En béisbol han logrado grandes resultados a nivel europeo y participado en todas las competiciones internacionales gracias a la presencia en su equipo nacional de jugadores estadounidenses con ascendencia italiana, los cuales componen casi en su totalidad la selección.

Francesco Molinari ganó el Open Británico del año 2018, siendo el primer jugador italiano en conseguir ganar uno de los torneos majors de golf.

Otro deportista destacado ha sido Reinhold Messner, primera persona del mundo en escalar las 14 cumbres de más de 8000 metros, además de ser la primera persona en ascender el monte Everest en solitario y sin ayuda de oxígeno en 1980.

En los Juegos Olímpicos es el tercer país con más medallas de oro acumuladas, tras los Estados Unidos, la Unión Soviética y Alemania, siendo el cuarto con más participaciones (45) tras Francia, Reino Unido y Suiza con 46 ediciones.
Los deportistas que más medallas han obtenido han sido Edoardo Mangiarotti (6 medallas de oro, 5 de plata y 2 de bronce), Nedo Nadi (6 medallas de oro) y Valentina Vezzali (5 medallas de oro, 1 de plata y 1 de bronce) todos en esgrima. Además el país ha organizado cuatro ediciones de los Juegos Olímpicos, una de verano en Roma 1960 y tres de invierno, en Cortina d'Ampezzo en 1944 y 1956 y en Turín en 2006.






</doc>
<doc id="3545" url="https://es.wikipedia.org/wiki?curid=3545" title="Teoría del color">
Teoría del color

En el arte de la pintura, el diseño gráfico, el diseño visual, la fotografía, la imprenta y en la televisión, la teoría del color es un grupo de reglas básicas en la mezcla de colores para lograr el efecto deseado combinando colores de luz o pigmento. La luz blanca se puede producir combinando el rojo, el verde y el azul, mientras que combinando pigmentos cyan, magenta y amarillo se produce el color negro.

En su libro "Teoría de los colores", el poeta y científico alemán Johann Wolfgang von Goethe propuso un círculo de color simétrico, el cual comprende el establecido por el matemático y físico inglés Isaac Newton y los espectros complementarios. En contraste, el círculo de color de Newton, con siete ángulos de color desiguales y subtendidos, no exponía la simetría y la complementariedad que Goethe consideró como característica esencial del color. Para Newton, solo los colores espectrales podían considerarse como fundamentales. El enfoque más empírico de Goethe le permitió admitir el papel esencial del color magenta, que no es espectral, en un círculo de color. Posteriormente, los estudios de la percepción del color definieron el estándarCIE 1931, el cual es un modelo perceptual que permite representar colores primarios con precisión y convertirlos a cada modelo de color de forma apropiada.

La teoría del color es una propuesta por el químico y filósofo alemán Wilhelm Ostwald consta de cuatro sensaciones cromáticas elementales (amarillo, rojo, azul y verde) y dos sensaciones acromáticas intermedias.

La mezcla de los colores primarios de la luz, que son rojo, verde y azul (RGB, iniciales en inglés de los colores primarios), se realiza utilizando el sistema de color aditivo, también conocido como el modelo RGB o el espacio de color RGB. Todos los colores posibles que pueden ser creados por la mezcla de estas tres luces de color son aludidos como el espectro de color de estas luces en concreto. Cuando ningún color luz está presente, se percibe el negro. Los colores primarios de luz tienen aplicación en los monitores de un ordenador, televisores, proyectores de vídeo y todos aquellos sistemas que utilizan combinaciones de materiales que fosforecen en el rojo, verde y azul.

Se debe tener en cuenta que sólo con unos colores «primarios» ficticios se pueden llegar a conseguir todos los colores posibles. Estos colores primarios son conceptos idealizados utilizados en modelos de color matemáticos que no representan las sensaciones de color reales o incluso los impulsos nerviosos reales o procesos cerebrales. En otras palabras, todos los colores «primarios» perfectos son completamente imaginarios, lo que implica que todos los colores primarios que se utilizan en las mezclas son incompletos o imperfectos.

El Círculo cromático suele presentarse como una rueda dividida en doce partes. Los colores primarios se colocan de modo que uno de ellos esté en la porción superior central y los otros dos en la cuarta porción a partir de esta, de modo que si unimos los tres con unas líneas imaginarias formarían un triángulo equilátero con la base horizontal. Entre dos colores primarios se colocan tres tonos secundarios de modo que en la porción central entre ellos correspondería a una mezcla de cantidades iguales de ambos primarios y el color más cercano a cada primario sería la mezcla del secundario central más el primario adyacente.

Los círculos cromáticos actuales utilizados por los artistas se basan en el modelo CMYK, si bien los colores primarios utilizados en pintura difieren de las tintas de proceso en imprenta en su intensidad. Los pigmentos utilizados en pintura, tanto en óleo como acrílico y otras técnicas pictóricas suelen ser el azul de ftalocianina (PB15 en notación "Color Index") como cian, el magenta de quinacridona (PV19 en notación "Color Index") y algún amarillo arilida o bien de cadmio que presente un tono amarillo neutro (existen varios pigmentos válidos o mezclas de ellos utilizables como primarios amarillos). Varias casas poseen juegos de colores primarios recomendados que suelen venderse juntos y reciben nombres especiales en los catálogos, tales como «azul primario» o «rojo primario» junto al «amarillo primario», pese a que ni el azul ni el rojo propiamente dichos son en realidad colores primarios según el modelo CMYK utilizado en la actualidad.

No obstante, como los propios nombres dados por los fabricantes a sus colores primarios evidencian, existe una tradición todavía anclada en el modelo RYB y que ocasionalmente se encuentra todavía en libros y en cursos orientados a aficionados a la pintura. Pero la enseñanza reglada, tanto en escuelas de arte como en la universidad, y los textos de referencia importantes ya han abandonado tal modelo hace décadas. La prueba la tenemos en los colores orientados a la enseñanza artística de diferentes fabricantes, que sin excepción utilizan un modelo de color basado en CMYK, que además de los tres colores primarios CMYK incluyen negro y blanco como juego básico para el estudiante.

La mezcla de los colores primarios de la luz, que son rojo, verde y azul (RGB, iniciales en inglés de los colores primarios), se realiza utilizando el sistema de color aditivo, también conocido como el modelo RGB o el espacio de color RGB. Todos los colores posibles que pueden ser creados por la mezcla de estas tres luces de color son aludidos como el espectro de color de estas luces en concreto. Cuando ningún color luz está presente, se percibe el negro. Los colores primarios de luz tienen aplicación en los monitores de un ordenador, televisores, proyectores de vídeo y todos aquellos sistemas que utilizan combinaciones de materiales que fosforecen en el rojo, verde y azul.

Se debe tener en cuenta que sólo con unos colores «primarios» ficticios se pueden llegar a conseguir todos los colores posibles. Estos colores primarios son conceptos idealizados utilizados en modelos de color matemáticos que no representan las sensaciones de color reales o incluso los impulsos nerviosos reales o procesos cerebrales. En otras palabras, todos los colores «primarios» perfectos son completamente imaginarios, lo que implica que todos los colores primarios que se utilizan en las mezclas son incompletos o imperfectos.

Todos los matices o colores que percibimos poseen tres atributos básicos:


El grado en que uno o dos de los tres colores primarios RGB (esta clasificación es referente a los colores básicos en la composición luminosa de una pantalla informática R=Red, G=Green, B=Blue, con los que se componen por medio de adición lumínica, distinta a la clasificación de los colores básicos o primarios de la pintura, en la que se mezclan por adición de pigmentos matéricos o físicos) predominan en un color. A medida que las cantidades de RGB se igualan, el color va perdiendo saturación hasta convertirse en gris o blanco.

Las teorías modernas del uso del color determinan que sus propiedades son dos: matiz y luminosidad

El "matiz" tiene que ver con el tipo de color: tierra siena tostada, verde, negro titanio, blanco marfil, rosa, etc.

La "luminosidad" es la cantidad de luz que cada color tiene y es posible de ser diferenciada en oposición a otros colores, por ejemplo, un amarillo es más claro que un azul o un verde más claro que un marrón.

La "saturación" bien entendida tiene que ver con la cantidad de materia que se aplica sobre una superficie, por ende saturar significa colmar una superficie con pigmento. El agregado de gris a los colores como forma de saturar, no hace otra cosa que obtener un nuevo color producto de la mezcla. Puede probarse por experimentación. Por ende un color, inclusive al que se le agregara gris, puede saturar una superficie con mayor o menor efectividad dependiendo de la técnica utilizada y de la calidad de los materiales con los que se ha fabricado. Por ejemplo, la técnica de acuarela tiene menor capacidad para saturar que la del acrílico.

Los colores armónicos son aquellos que funcionan bien juntos, es decir, que producen un esquema de color sensible al mismo sentido (la armonía nace de la percepción de los sentidos y, a la vez, esta armonía retroalimenta al sentido, haciéndolo lograr el máximo equilibrio que es hacer sentir al sentido). El círculo cromático es una herramienta útil para determinar armonías de color. Los colores complementarios son aquellos que se contraponen en dicho círculo y que producen un fuerte contraste. Así, por ejemplo, en el modelo RGB el verde es complementario del rojo, mientras que en el modelo CMY el verde es el complementario del magenta.

Un espacio de color define un modelo de composición del color. Por lo general un espacio de color lo define una base de N vectores (por ejemplo, el espacio RGB lo forman 3 vectores: rojo, verde y azul), cuya combinación lineal genera todo el espacio de color. Los espacios de color más generales intentan englobar la mayor cantidad posible de los colores visibles por el ojo humano, aunque existen espacios de color que intentan aislar tan solo un subconjunto de ellos.

Existen espacios de color de:

De los cuales, los espacios de color de tres dimensiones son los más extendidos y los más utilizados. Entonces, un color se especifica usando tres coordenadas, o atributos, que representan su posición dentro de un espacio de color específico. Estas coordenadas no nos dicen cuál es el color, sino que muestran dónde se encuentra un color dentro de un espacio de color en particular.

RGB es conocido como un espacio de color aditivo (colores primarios) porque cuando la luz de dos diferentes frecuencias viaja junta, desde el punto de vista del observador, estos colores son sumados para crear nuevos tipos de colores. Los colores rojo, verde y azul fueron escogidos porque cada uno corresponde aproximadamente con uno de los tres tipos de conos sensitivos al color en el ojo humano (65 % sensibles al rojo, 33 % sensibles al verde y 2 % sensibles al azul). Con la combinación apropiada de rojo, verde y azul se pueden reproducir muchos de los colores que pueden percibir los humanos. Por ejemplo, rojo puro y verde claro producen amarillo, rojo y azul producen magenta, verde y azul combinados crean cian y los tres juntos mezclados a máxima intensidad, crean el blanco intenso.

Existe también el espacio derivado RGBA, que añade el canal alfa (de transparencia) al espacio RGB original.

CMY trabaja mediante la absorción de la luz (colores secundarios). 
Los colores que se ven son la parte de luz que no es absorbida. En CMY, magenta más amarillo producen rojo, magenta más cian producen azul, cian más amarillo generan verde y la combinación de cian, magenta y amarillo forman negro.
El negro generado por la mezcla de colores primarios sustractivos no es tan denso como el color negro puro (uno que absorbe todo el espectro visible). Es por esto que al CMY original se ha añadido un canal clave ("key"), que normalmente es el canal negro ("black"), para formar el espacio CMYK o CMYB. Actualmente las impresoras de cuatro colores utilizan un cartucho negro además de los colores primarios de este espacio, lo cual genera un mejor contraste. Sin embargo el color que una persona ve en una pantalla de computador difiere del mismo color en una impresora, debido a que los modelos RGB y CMY son distintos. El color en RGB está hecho por la reflexión o emisión de luz, mientras que el CMY, mediante la absorción de ésta.

Fue una recodificación de color realizada para la norma de televisión cromática estadounidense NTSC, que debía ser compatible con la televisión en blanco y negro. Los nombres de los componentes de este modelo son Y por luminancia ("luminance"), I fase ("in-phase") y Q cuadratura ("quadrature"). La primera es la señal monocromática de la televisión en blanco y negro y las dos últimas generan el tinte y saturación del color. Los parámetros I y Q son nombrados en relación con el método de modulación utilizado para codificar la señal portadora. Los valores de las señales RGB son sumados para producir una única señal Y’ que representa la iluminación o brillo general de un punto en particular. La señal I es creada al restar el Y' de la señal azul de los valores RGB originales y luego el Q se realiza restando la señal Y' del rojo.

Es un espacio cilíndrico, pero normalmente asociado a un cono o cono hexagonal, debido a que es un subconjunto visible del espacio original con valores válidos de RGB.

En el modelo de color RYB, el rojo, el amarillo y el azul se consideran colores primarios, y en teoría, el resto de colores puros (color materia) puede ser creados mezclando pintura roja, amarilla y azul. A pesar de su obsolescencia e imprecisión, mucha gente aprende algo sobre este modelo en los estudios de educación primaria, mezclando pintura o lápices de colores con estos colores primarios.

El modelo RYB es aún utilizado en general en conceptos de arte y pintura tradicionales, pero ha sido totalmente dejado de lado en la mezcla industrial de pigmentos de pintura. Aun siendo usado como guía para la mezcla de pigmentos, el modelo RYB no representa con precisión los colores que resultan de mezclar los tres colores RYB primarios, puesto que el azul y el rojo son tonalidades verdaderamente secundarias. A pesar de la imprecisión de este modelo –su corrección es el modelo CMYK–, se sigue utilizando en las artes visuales, el diseño gráfico y otras disciplinas afines, por tradición del modelo original de Goethe de 1810.

En la retina del ojo existen millones de células especializadas en detectar las longitudes de onda procedentes de nuestro entorno. Estas células fotorreceptoras, conos y los bastones, recogen parte del espectro de la luz y, gracias al efecto fotoeléctrico, lo transforman en impulsos eléctricos, que son enviados al cerebro a través de los nervios ópticos, para crear la sensación del color.

Existen grupos de conos especializados en detectar y procesar un color determinado, siendo diferente el total de ellos dedicados a un color y a otro. Por ejemplo, existen más células especializadas en trabajar con las longitudes de onda correspondientes al rojo que a ningún otro color, por lo que cuando el entorno en que nos encontramos nos envía demasiado rojo se produce una saturación de información en el cerebro de este color. 

Cuando el sistema de conos y bastones de una persona no es el correcto se pueden producir una serie de irregularidades en la apreciación del color, al igual que cuando las partes del cerebro encargadas de procesar estos datos están dañadas. Esta es la explicación de fenómenos como el daltonismo. Una persona daltónica no aprecia las gamas de colores en su justa medida, confundiendo los rojos con los verdes. 
Debido a que el proceso de identificación de colores depende del cerebro y del sistema ocular de cada persona en concreto, podemos medir con toda exactitud el espectro de un color determinado, pero el concepto del color producido es totalmente subjetivo, dependiendo de la persona en sí. Dos personas diferentes pueden interpretar un color dado de forma diferente, y puede haber tantas interpretaciones de un color como personas hay. 

El mecanismo de mezcla y producción de colores producido por la reflexión de la luz sobre un cuerpo no es el mismo al de la obtención de colores por mezcla directa de rayos de luz.




</doc>
<doc id="3551" url="https://es.wikipedia.org/wiki?curid=3551" title="Sistema nervioso central">
Sistema nervioso central

El sistema nervioso central es una de las porciones en que se divide el sistema nervioso. En los animales vertebrados está constituido por el encéfalo y la médula espinal, se encuentra revestido por tres membranas: duramadre (membrana externa), aracnoides (intermedia), piamadre (membrana interna), denominadas genéricamente meninges y protegido por envolturas óseas, que son el cráneo y la columna vertebral respectivamente.

Se trata de un sistema muy complejo, ya que se encarga de percibir estímulos procedentes del mundo exterior, procesar la información y transmitir impulsos a nervios y músculos. El sistema nervioso de los animales vertebrados, incluidos los mamíferos y el hombre, puede dividirse en dos partes bien diferenciadas, el sistema nervioso central, constituido por el encéfalo y la médula espinal y el sistema nervioso periférico que está formado por los nervios sensitivos y motores que enlazan el sistema nervioso central con el resto del organismo.

El sistema nervioso central está formado por el encéfalo y la médula espinal.

En el año 1878, Korbinian Brodmann realizó un estudio de la corteza cerebral y la dividió en 52 áreas diferentes según su localización. Se ha comprobado que muchas de estas áreas tienen una función específica, por ejemplo el área 17 situada en el lóbulo occipital corresponde a la corteza visual primaria y es donde se procesan los impulsos nerviosos procedentes del nervio óptico, las áreas 44 y 45 se llaman área de Broca y están relacionadas con el lenguaje.
Se encuentra en la parte anterior del cerebro, su tamaño corresponde aproximadamente un tercio de la corteza cerebral. Evolutivamente es una de las partes del cerebro más modernas y está muy desarrollado en la especie humana. La cisura de Rolando separa al lóbulo frontal del lóbulo parietal situado detrás, mientras que la cisura de Silvio sirve de límite con el lóbulo temporal ubicado debajo. 
Sus funciones son de gran importancia, dentro del lóbulo frontal se encuentra el área motora primaria que está encargada de emitir órdenes para realizar movimientos de todos los músculos voluntarios y el área de Broca relacionada con la producción del lenguaje. Sus circuitos neuronales están muy relacionados con la capacidad de razonamiento, la solución de problemas complejos y el pensamiento abstracto.

El lóbulo parietal forma parte de la corteza cerebral, está situado detrás del lóbulo frontal, separado de este por la cisura de Rolando. En su porción posterior entra en contacto con el lóbulo occipital, mientras que la Cisura de Silvio lo separa del lóbulo temporal situado debajo.

En el lóbulo parietal se encuentra el área somatosensitiva que capta y procesa las sensaciones de tacto, dolor y temperatura de todo el cuerpo. Cuando existen lesiones que afectan al lóbulo parietal puede producirse un síntoma que se llama asomatognosia y consiste en que el paciente no es capaz de reconocer partes de su cuerpo como una extremidad inferior o superior, lo cual puede ser causa de gran inquietud y preocupación.

En este lóbulo se localiza el área auditiva primaria que recibe y procesa la información procedente del oído. Por ello una lesión en el lóbulo temporal puede provocar sordera parcial aunque el oído y el nervio auditivo no estén dañados. Próxima a la anterior se encuentra el área auditiva secundaria y de asociación en la que está incluida el área de Wernicke muy importante en la función lingüística y la comprensión de las palabras.

El lóbulo occipital es más pequeño que los anteriores y está situado en la región posterior del cerebro, separado del cerebelo por la duramadre. Contiene la corteza visual primaria que recibe la información proveniente de la retina a través del nervio óptico. Las neuronas de la corteza visual primaria son las encargadas de procesar los estímulos visuales e interpretar las formas, el movimiento y otros aspectos de la visión. Por ello cuando existen lesiones que afectan al lóbulo occipital puede producirse ceguera cortical que se caracteriza porque la persona no puede ver aunque el ojo no presenta ningún daño aparente.

El cuerpo calloso es una importante estructura del cerebro que está formada por fibras que actúan como vía de comunicación entre el hemisferio cerebral derecho y el izquierdo, con la finalidad de que ambos funcionen de forma conjunta y complementaria.

La cápsula interna es un grueso conjunto de fibras nerviosas tanto ascendentes como descendentes que comunican la corteza con las regiones inferiores del sistema nervioso central, las fibras son de origen diverso, pero muchas de ellas transportan información motora o sensitiva. En su trayecto pasan cerca de la región del tálamo y los ganglios basales. La cápsula interna es una región muy sensible, cualquier lesión en esta zona daña numerosas fibras nerviosas y provoca en consecuencia déficits neurológicos graves.

El tálamo es una porción del cerebro situada por encima del tronco del encéfalo, casi en el centro del cerebro. Mide alrededor de 3 cm de largo y está formado por materia gris, es decir, el soma de células neuronales. Cumple la función de estación de relevo de las señales nerviosas y centro de integración donde se procesan los impulsos sensoriales antes de continuar su recorrido hasta la corteza cerebral. También recibe señales que siguen la dirección opuesta y llegan al tálamo procedente de la corteza cerebral.

El hipotálamo es una pequeña región del cerebro formada por sustancia gris. Esta situado inmediatamente debajo del tálamo. Tiene el tamaño aproximado de una almendra y desempeña importantes funciones, entre ellas enlazar el sistema nervioso con el sistema endocrino a través de la hipófisis.

Los ganglios basales en realidad deberían llamarse núcleos basales pues no son auténticos ganglios. Son unas estructuras cerebrales formadas por cuerpos neuronales (sustancia gris) situadas en la base del cerebro. Están constituidos por diferentes núcleos: núcleo caudado, putamen, globo pálido, núcleo accumbens, núcleo lenticular, cuerpo estriado, amígdala cerebral y sustancia negra. Durante muchos años se ha considerado que la función de los ganglios basales es únicamente el control de la motilidad corporal, sin embargo se ha comprobado que juegan un importante papel en otras funciones como el aprendizaje y la memoria. La alteración funcional de los ganglios basales causa la enfermedad de Parkinson.

Las células que forman el sistema nervioso central se disponen de tal manera que dan lugar a dos formaciones muy características: 
El sistema nervioso central dispone de unas cavidades que se llaman ventrículos cerebrales en el encéfalo y conducto ependimario en la médula espinal. Estos espacios están llenos de un líquido incoloro y transparente, que recibe el nombre de líquido cefalorraquídeo. Sus funciones son muy variadas: sirve como medio de intercambio de determinadas sustancias, como sistema de eliminación de productos residuales, para mantener el equilibrio iónico adecuado y como sistema amortiguador mecánico.

El sistema de ventrículos cerebrales está formado por dos ventrículos laterales que se sitúan de forma simétrica y están conectados con el tercer ventrículo, el cual a través del acueducto de Silvio se comunica con el cuarto ventrículo.

El sistema nervioso central de los vertebrados se desarrolla como un tubo hueco producto de la unión de dos pliegues ectodérmicos. La médula espinal conserva esa estructura de tubo, mientras que en el encéfalo el tubo se ensancha en tres vesículas primitivas que se denominan prosencéfalo (cerebro anterior), mesencéfalo (cerebro medio) y rombencéfalo (cerebro posterior). Posteriormente, estas tres vesículas se transforman en cinco al dividirse el procencéfalo en diencéfalo y telencéfalo y el rombencéfalo en metencéfalo y mielencéfalo. Estas 5 vesículas primitivas dan origen a todas las porciones del encéfalo adulto, según el siguiente esquema. 

El sistema nervioso central está cubierto en el embrión por una única meninge: la meninge primitiva. Esta es la única membrana que se desarrolla en peces. Luego se divide en la duramadre, más externa, y una meninge secundaria más interna, la piaracnoides, y está eventualmente se divine en la piamadre y en la aracnoides.
El sistema nervioso central puede ser blanco de infecciones, provenientes de cuatro vías de entrada principales, la diseminación por la sangre que es la vía más frecuente, la implantación directa del germen por traumatismos o causas iatrogénicas, la extensión local secundaria a una infección local y el propio sistema nervioso periférico, como ocurre en la rabia. 

La encefalitis en un proceso inflamatorio difuso agudo que produce muerte neuronal, generalmente de origen infeccioso. Aunque existen muchas causas posibles, una de las más frecuentes es el virus del herpes (encefalitis herpética).
La meningitis es una inflamación o infección de las meninges, bien sea leptomeningitis que es centrada en el espacio subaracnoideo, o paquimeningitis que es centrada en la duramadre. La meningitis piógena es causada por bacterias, sobre todo .
Haemophilus influenzae, Neisseria meningitidis y neumococo.


En general, la frecuencia de tumores intracraneales está entre 10 y 17 por cada 100 000 habitantes. Aproximadamente la mitad son tumores primarios y el resto son metastásicos. Los tumores del sistema nervioso central derivan de diversos tejidos, por lo que se dividen en neuroepiteliales y no neuroepiteliales

Los tumores neuroepiteliales son un grupo de tumores encefálicos primarios llamados gliomas. Derivan de los astrocitos, oligodendrocitos, epéndimo, plexos coroideos, neuronas y células embrionarias. Por lo general, infiltran difusamente el encéfalo adyacente, haciendo difícil su resección quirúrgica. Los más frecuentes son: astrocitoma, oligodendroglioma, ependimoma, neuroblastoma y meduloblastoma.

Los tumores no neuroepiteliales pueden ser de diversos tipos: meningioma, schwannoma, también llamados neurinomas, linfoma cerebral primario y tumor de células germinales.



</doc>
<doc id="3552" url="https://es.wikipedia.org/wiki?curid=3552" title="Meninges">
Meninges

Las meninges (del griego "μῆνιγξ" "mēninx", ‘membrana’) son las membranas de tejido conectivo que cubren todo el sistema nervioso central, añadiéndole una protección blanda que complementa a la dura de las estructuras óseas. En los mamíferos se distinguen tres capas con dos espacios intermedios, de dentro hacia fuera:
La región externa limita con el periostio en el encéfalo y con el espacio epidural en el tubo neural. A pesar de estar en estrecho contacto siempre se interpone una capa de procesos gliales.

Las meninges actúan como barrera selectiva:

Cuando las meninges o el líquido cefalomedular son atacados por células (bacterias, virus, etc) o sustancias químicas (normalmente por inoculaciones tras accidentes graves), se produce un daño, que puede ser de tipo inflamatorio o infeccioso. Esto puede provocar la meningitis, que precisa de un diagnóstico rápido y preciso para actuar en consecuencia, ya que si no, la vida del sujeto se puede ver seriamente comprometida.

 


</doc>
<doc id="3553" url="https://es.wikipedia.org/wiki?curid=3553" title="28 de septiembre">
28 de septiembre

El 28 de septiembre es el 271.º (ducentésimo septuagésimo primer) día del año en el calendario gregoriano y el 272.º (ducentésimo septuagésimo segundo) en los años bisiestos. Quedan 94 días para finalizar el año.









</doc>
<doc id="3554" url="https://es.wikipedia.org/wiki?curid=3554" title="27 de septiembre">
27 de septiembre

El 27 de septiembre es el 270.º (ducentésimo septuagésimo) día del año en el calendario gregoriano y el 271.º en los años bisiestos. Quedan 95 días para finalizar el año.





















</doc>
<doc id="3555" url="https://es.wikipedia.org/wiki?curid=3555" title="26 de septiembre">
26 de septiembre

El 26 de septiembre es el 269.º (ducentésimo sexagésimo noveno) día del año en el calendario gregoriano y el 270.º en los años bisiestos. Quedan 96 días para finalizar el año.








</doc>
<doc id="3556" url="https://es.wikipedia.org/wiki?curid=3556" title="25 de septiembre">
25 de septiembre

El 25 de septiembre es el 268.º (ducentésimo sexagésimo octavo) día del año en el calendario gregoriano y el 269.º en los años bisiestos. Quedan 97 días para finalizar el año.








</doc>
<doc id="3557" url="https://es.wikipedia.org/wiki?curid=3557" title="24 de septiembre">
24 de septiembre

El 24 de septiembre es el 267.º (ducentésimo sexagésimo séptimo) día del año en el calendario gregoriano y el 268.º en los años bisiestos. Quedan 98 días para finalizar el año.









</doc>
<doc id="3558" url="https://es.wikipedia.org/wiki?curid=3558" title="23 de septiembre">
23 de septiembre

El 23 de septiembre es el 266.º (ducentésimo sexagésimo sexto) día del año en el calendario gregoriano y el 267.º en los años bisiestos. Quedan 99 días para finalizar el año.

Además, junto con el 22 de septiembre, este es uno de los días en los que suele producirse el equinoccio de otoño en el hemisferio norte y el equinoccio de primavera en el hemisferio sur, y cambia el signo zodiacal de Virgo a Libra.





























</doc>
<doc id="3559" url="https://es.wikipedia.org/wiki?curid=3559" title="22 de septiembre">
22 de septiembre

El 22 de septiembre es el 265.º (ducentésimo sexagésimo quinto) día del año en el calendario gregoriano y el 266.º en los años bisiestos. Quedan 100 días para finalizar el año.

Además, junto con el 23 de septiembre, este es uno de los días en los que suele producirse el equinoccio de otoño en el hemisferio norte y el equinoccio de primavera en el hemisferio sur, y cambia el signo zodiacal de Virgo a Libra.










</doc>
<doc id="3560" url="https://es.wikipedia.org/wiki?curid=3560" title="21 de septiembre">
21 de septiembre

El 21 de septiembre es el 264.º (ducentésimo sexagésimo cuarto) día del año en el calendario gregoriano y el 265.º en los años bisiestos. Quedan 101 días para finalizar el año.













</doc>
<doc id="3561" url="https://es.wikipedia.org/wiki?curid=3561" title="20 de septiembre">
20 de septiembre

El 20 de septiembre es el 263.º (ducentésimo sexagésimo tercer) día del año en el calendario gregoriano y el 264.º en los años bisiestos. Quedan 102 días para finalizar el año.









</doc>
<doc id="3566" url="https://es.wikipedia.org/wiki?curid=3566" title="Notas de la Iglesia">
Notas de la Iglesia

Las notas de la Iglesia hacen referencia a cuatro caracteres o atributos señalados ya en el Símbolo niceno-constantinopolitano del año 381, que calificó a la Iglesia como «una, santa, católica y apostólica». Los católicos profesan su fe en esas cuatro notas de la Iglesia a través del credo de Nicea-Constantinopla, por lo que se las tiene como artículos o dogmas de fe. El Concilio Vaticano II también hizo referencia a la «única Iglesia de Cristo que en el símbolo confesamos una, santa, católica y apostólica» ("Lumen gentium" 8). Finalmente, esos cuatro atributos son señalados por el Catecismo de la Iglesia católica como inseparablemente unidos entre sí, y como indicativos de rasgos esenciales de la Iglesia y de su misión (CIC, 811).

Cada uno de los cuatro atributos de la Iglesia tiene un significado particular:



</doc>
<doc id="3567" url="https://es.wikipedia.org/wiki?curid=3567" title="Cinturón de Kuiper">
Cinturón de Kuiper

El cinturón de Kuiper (), ocasionalmente llamado cinturón de Edgeworth-Kuiper, es un disco circunestelar en el Sistema Solar exterior, que se extiende desde la órbita de Neptuno (a 30 UA) hasta aproximadamente 50 UA del Sol. Es similar al cinturón de asteroides, pero es mucho más grande: 20 veces más ancho y 20 a 200 veces más masivo. Al igual que el cinturón de asteroides, se compone principalmente de pequeños cuerpos o restos de cuando se formó el Sistema Solar. Si bien muchos asteroides están compuestos principalmente de roca y metal, la mayoría de los objetos del cinturón de Kuiper están compuestos principalmente de volátiles congelados (denominados "hielos"), como metano, amoníaco y agua. El cinturón de Kuiper alberga tres planetas enanos reconocidos oficialmente: Plutón, Haumea y Makemake. Algunas de las lunas del Sistema Solar, como el Tritón de Neptuno y el Febe de Saturno, pueden haberse originado en la región.

El cinturón de Kuiper lleva el nombre del astrónomo holandés-estadounidense Gerard Kuiper, aunque no predijo su existencia. En 1992, se descubrió el planeta menor (15760) Albion, el primer objeto del cinturón de Kuiper (KBO, por sus siglas en inglés) desde Plutón y Caronte. Desde su descubrimiento, el número de KBO conocidos ha aumentado a miles, y se cree que existen más de 100.000 KBO de más de 100 km de diámetro. Inicialmente se pensó que el cinturón de Kuiper era el principal depósito de cometas periódicos, aquellos con órbitas que duraban menos de 200 años. Los estudios realizados desde mediados de la década de 1990 han demostrado que el cinturón es dinámicamente estable y que el verdadero lugar de origen de los cometas es el disco disperso, una zona dinámicamente activa creada por el movimiento hacia afuera de Neptuno hace 4.500 millones de años; los objetos de disco dispersos como Eris tienen órbitas extremadamente excéntricas que los llevan hasta 100 UA del Sol.

El cinturón de Kuiper es distinto de la teórica nube de Oort, que está mil veces más distante y es en su mayoría esférica. Los objetos dentro del cinturón de Kuiper, junto con los miembros del disco disperso y cualquier posible nube de Hills u objeto de nube de Oort, se denominan colectivamente objetos transneptunianos (TNO, por sus siglas en inglés). Plutón es el miembro más grande y masivo del cinturón de Kuiper, y el TNO más grande y segundo más masivo conocido, solo superado por Eris en el disco disperso. Originalmente considerado un planeta, el estado de Plutón como parte del cinturón de Kuiper hizo que fuera reclasificado como planeta enano en 2006. Es compositivamente similar a muchos otros objetos del cinturón de Kuiper y su período orbital es característico de una clase de KBO, conocida como "plutinos".

El cinturón de Kuiper y Neptuno pueden tratarse como un marcador de la extensión del Sistema Solar, siendo las alternativas la heliopausa y la distancia a la que la influencia gravitacional del Sol se compara con la de otras estrellas (estimada entre 50.000 UA y aproximadamente 2 años luz).

Después del descubrimiento de Plutón en 1930, muchos especularon que podría no estar solo. La región que ahora se llama cinturón de Kuiper se planteó como hipótesis de diversas formas durante décadas. Recién en 1992 se encontró la primera prueba directa de su existencia. El número y la variedad de especulaciones anteriores sobre la naturaleza del cinturón de Kuiper han llevado a una incertidumbre continua sobre quién merece crédito por proponerlo primero.

El primer astrónomo que sugirió la existencia de una población transneptuniana fue Frederick C. Leonard. Poco después del descubrimiento de Plutón por Clyde Tombaugh en 1930, Leonard reflexionó sobre si "no era probable que en Plutón haya salido a la luz el primero de una serie de cuerpos ultraneptunianos, cuyos miembros restantes aún esperan ser descubiertos pero que finalmente están destinados ser detectado". Ese mismo año, el astrónomo Armin O. Leuschner sugirió que Plutón "puede ser uno de los muchos objetos planetarios de largo período aún por descubrir".

En 1943, en el "Journal of the British Astronomical Association", Kenneth Edgeworth planteó la hipótesis de que, en la región más allá de Neptuno, el material dentro de la nebulosa solar primordial estaba demasiado espaciado para condensarse en planetas y, por lo tanto, más bien condensado en una miríada de cuerpos más pequeños. De esto concluyó que "la región exterior del sistema solar, más allá de las órbitas de los planetas, está ocupada por una gran cantidad de cuerpos comparativamente pequeños" y que, de vez en cuando, uno de ellos "se aleja de su propia esfera y aparece como un visitante ocasional del sistema solar interior", convirtiéndose en cometa.

En 1951, en un artículo publicado en "Astrophysics: A Topical Symposium", Gerard Kuiper especuló que un disco similar se había formado temprano en la evolución del Sistema Solar, pero no creía que tal cinturón todavía existiera hoy. Kuiper operaba bajo la suposición, común en su época, de que Plutón era del tamaño de la Tierra y, por lo tanto, había dispersado estos cuerpos hacia la nube de Oort o fuera del Sistema Solar. Si la hipótesis de Kuiper fuera correcta, hoy no habría un cinturón de Kuiper.

La hipótesis adoptó muchas otras formas en las décadas siguientes. En 1962, el físico Al G.W. Cameron postuló la existencia de "una enorme masa de material pequeño en las afueras del sistema solar". En 1964, Fred Whipple, quien popularizó la famosa hipótesis de la ""bola de nieve sucia"" para la estructura del cometa, pensó que un "cinturón de cometas" podría ser lo suficientemente masivo como para causar las supuestas discrepancias en la órbita de Urano que habían provocado la búsqueda del Planeta X, o, al menos, lo suficientemente masivo como para afectar las órbitas de cometas conocidos. La observación descartó esta hipótesis.

En 1977, Charles Kowal descubrió 2060 Chiron, un planetoide helado con una órbita entre Saturno y Urano. Usó un microscopio de parpadeo, el mismo dispositivo que le había permitido a Clyde Tombaugh descubrir Plutón casi 50 años antes. In 1992, another object, 5145 Pholus, was discovered in a similar orbit. Hoy en día, se sabe que existe una población completa de cuerpos parecidos a cometas, llamados centauros, en la región entre Júpiter y Neptuno. Las órbitas de los centauros son inestables y tienen una vida dinámica de unos pocos millones de años. Desde el momento del descubrimiento de Quirón en 1977, los astrónomos han especulado que, por lo tanto, los centauros deben reponerse con frecuencia mediante algún depósito externo.

Más tarde, del estudio de los cometas surgieron más pruebas de la existencia del cinturón de Kuiper. Se sabe desde hace algún tiempo que los cometas tienen una vida útil finita. A medida que se acercan al Sol, su calor hace que sus superficies volátiles se sublimen en el espacio, dispersándolas gradualmente. Para que los cometas sigan siendo visibles durante la era del Sistema Solar, deben reponerse con frecuencia. Una de esas áreas de reabastecimiento es la nube de Oort, un enjambre esférico de cometas que se extiende más allá de las 50.000 UA desde el Sol, cuya primera hipótesis fue el astrónomo holandés Jan Oort en 1950. Se cree que la nube de Oort es el punto de origen de cometas de período largo, que son aquellos, como Hale-Bopp, con órbitas que duran miles de años.

Existe otra población de cometas, conocida como cometas de período corto o periódicos, que consiste en aquellos cometas que, como el cometa Halley, tienen períodos orbitales de menos de 200 años. En la década de 1970, la velocidad a la que se estaban descubriendo cometas de período corto se estaba volviendo cada vez más inconsistente con su surgimiento únicamente de la nube de Oort. Para que un objeto de la nube de Oort se convierta en un cometa de período corto, primero tendría que ser capturado por los planetas gigantes. En un artículo publicado en "Monthly Notices of the Royal Astronomical Society" en 1980, el astrónomo uruguayo Julio Fernández afirmó que por cada cometa de período corto que fuera enviado al interior del Sistema Solar desde la nube de Oort, 600 tendrían que ser eyectados al espacio interestelar. Especuló que se requeriría un cinturón de cometas de entre 35 y 50 UA para dar cuenta del número observado de cometas. Siguiendo el trabajo de Fernández, en 1988 el equipo canadiense de Martin Duncan, Tom Quinn y Scott Tremaine realizó una serie de simulaciones por computadora para determinar si todos los cometas observados podrían haber llegado desde la nube de Oort. Descubrieron que la nube de Oort no podía dar cuenta de todos los cometas de período corto, especialmente porque los cometas de período corto se agrupan cerca del plano del Sistema Solar, mientras que los cometas de la nube de Oort tienden a llegar desde cualquier punto del cielo. Con un "cinturón", como lo describió Fernández, agregado a las formulaciones, las simulaciones coincidieron con las observaciones. Según se informa, debido a que las palabras "Kuiper" y "cinturón de cometas" aparecieron en la frase inicial del artículo de Fernández, Tremaine llamó a esta hipotética región "cinturón de Kuiper".

En 1987, el astrónomo David Jewitt, entonces del MIT, se sintió cada vez más desconcertado por "el aparente vacío del Sistema Solar exterior". Alentó a la entonces estudiante de posgrado Jane Luu a que lo ayudara en su esfuerzo por localizar otro objeto más allá de la órbita de Plutón, porque, como él le dijo, "Si no lo hacemos, nadie lo hará". Utilizando telescopios en el Observatorio Nacional Kitt Peak en Arizona y el Observatorio Interamericano Cerro Tololo en Chile, Jewitt y Luu llevaron a cabo su búsqueda de la misma manera que lo hicieron Clyde Tombaugh y Charles Kowal, con un microscopio de parpadeo. Inicialmente, el examen de cada par de placas tomó aproximadamente ocho horas, pero el proceso se aceleró con la llegada de dispositivos electrónicos de carga acoplada o CCD, que, aunque su campo de visión era más estrecho, no solo eran más eficientes en la recolección de luz (retuvieron el 90% de la luz que los golpeó, en lugar de el 10% logrado por fotografías) pero permitió que el proceso de parpadeo se hiciera virtualmente, en una pantalla de computadora. Hoy en día, los CCD forman la base de la mayoría de los detectores astronómicos. En 1988, Jewitt se trasladó al Instituto de Astronomía de la Universidad de Hawaii. Más tarde, Luu se unió a él para trabajar en el telescopio de 2,24 m de la Universidad de Hawaii en Mauna Kea. Finalmente, el campo de visión de los CCD aumentó a 1024 por 1024 píxeles, lo que permitió que las búsquedas se realizaran mucho más rápidamente. Finalmente, luego de cinco años de búsqueda, Jewitt y Luu anunciaron el 30 de agosto de 1992 el "Descubrimiento del candidato cinturón de Kuiper objeto 1992 QB1". Seis meses después, descubrieron un segundo objeto en la región, (181708) 1993 FW. Para 2018, se habían descubierto más de 2000 objetos de cinturones de Kuiper.

Más de mil cuerpos se encontraron en un cinturón en los veinte años (1992-2012), después de encontrar 1992 QB1 (nombrado en 2018, 15760 Albion), mostrando un vasto cinturón de cuerpos más que Plutón y Albion. Para la década de 2010, se desconoce en gran medida el alcance y la naturaleza completos de los cuerpos del cinturón de Kuiper. Finalmente, a fines de la década de 2010, dos objetos del cinturón de Kuiper pasaron de cerca por una nave espacial no tripulada, lo que proporcionó observaciones mucho más cercanas del sistema plutoniano.

Los estudios realizados desde que se trazó por primera vez la región transneptuniana han demostrado que la región ahora llamada cinturón de Kuiper no es el punto de origen de los cometas de período corto, sino que derivan de una población vinculada llamada disco disperso. El disco disperso se creó cuando Neptuno migró hacia el exterior en el cinturón de proto-Kuiper, que en ese momento estaba mucho más cerca del Sol, y dejó a su paso una población de objetos dinámicamente estables que nunca podrían verse afectados por su órbita (el cinturón de Kuiper propiamente dicha), y una población cuyos perihelios están lo suficientemente cerca como para que Neptuno aún pueda perturbarlos mientras viaja alrededor del Sol (el disco disperso). Debido a que el disco disperso es dinámicamente activo y el cinturón de Kuiper relativamente estable dinámicamente, el disco disperso ahora se considera el punto de origen más probable para los cometas periódicos.

Los astrónomos a veces usan el nombre alternativo de "cinturón de Edgeworth-Kuiper" para acreditar a Edgeworth, y los objetos del cinturón de Kuiper se conocen ocasionalmente como "objetos Edgeworth-Kuiper". Brian G. Marsden afirma que ninguno de los dos merece un crédito verdadero: "Ni Edgeworth ni Kuiper escribieron sobre nada remotamente parecido a lo que estamos viendo ahora, pero Fred Whipple sí". David Jewitt comenta: "En todo caso, diría que J. Fernández casi merece el crédito por predecir el Cinturón de Kuiper basado en declaraciones claras y razonamiento físico. Su artículo de 1980 ("Monthly Notices of the Royal Astronomical Society", "192, 481-491") merece una lectura atenta".

Los objetos del cinturón de Kuiper a veces se denominan "kuiperoides", un nombre sugerido por Clyde Tombaugh. Varios grupos científicos recomiendan el término "objeto transneptuniano" (TNO) para los objetos en el cinturón porque el término es menos controvertido que todos los demás; sin embargo, no es un sinónimo exacto, ya que los TNO incluyen todos los objetos que orbitan alrededor del Sol más allá del órbita de Neptuno, no solo los del cinturón de Kuiper.

El cinturón de Kuiper en ocasiones es llamado el cinturón de Edgeworth o cinturón de Edgeworth-Kuiper. Hay astrónomos que utilizan nombres más largos todavía, como cinturón de Leonard-Edgeworth-Kuiper. Aunque la denominación de «objetos transneptunianos» es recomendada por ciertos grupos de astrónomos, ya que evitaría las controversias entre los nombres más personales. En estricto rigor, «objeto transneptuniano» no es sinónimo de «objetos del cinturón de Kuiper», ya que los primeros engloban también a otros objetos situados en el exterior del sistema solar.

Han sido observados más de 800 objetos del cinturón de Kuiper (KBO de las siglas anglosajonas "Kuiper Belt Objects"). Durante mucho tiempo los astrónomos habían considerado a Plutón y Caronte como los únicos objetos de gran tamaño de este grupo.

Sin embargo, el 4 de junio de 2002 se descubrió (50000) Quaoar, un objeto de tamaño inusual. Este cuerpo resultó tener un tamaño de la mitad que el de Plutón. Al ser también mayor que la luna Caronte, pasó a convertirse durante un tiempo en el segundo objeto más grande del cinturón de Kuiper. Otros objetos menores del cinturón de Kuiper se fueron descubriendo desde entonces.

Sin embargo, el 13 de noviembre de 2003 se anunció el descubrimiento de un cuerpo de grandes dimensiones mucho más alejado que Plutón, al que denominaron Sedna. El objeto 90337 Sedna destronó a Quaoar del puesto de segundo objeto transneptuniano más grande. Su pertenencia al cinturón de Kuiper está cuestionada por algunos astrónomos que lo consideran un cuerpo demasiado lejano, representante quizás del límite inferior de la nube de Oort. En tal caso, (148209) pertenecería también a esta clase.

La sorpresa llegó el 29 de julio de 2005 cuando se anunció el descubrimiento de tres nuevos objetos: Eris, Makemake y Haumea, ordenados de mayor a menor. En un principio, se creyó que Eris era mayor que el propio Plutón, por lo que se lo llegó a apodar como el décimo planeta y considerándoselo en su momento como el legendario Planeta X. Sin embargo, la sonda de la NASA New Horizons ha revelado en 2015 que el diámetro de Plutón es de 2370 kilómetros, o sea, alrededor de 80 kilómetros mayor que las estimaciones previas y, por tanto, ahora sabemos con seguridad que Eris (2326±12 km) es ligeramente más pequeño que Plutón. Estrictamente hablando, Eris no pertenece al cinturón de Kuiper. Es miembro del disco disperso pues su distancia media al Sol es de 67 ua.

La clasificación exacta de todos estos objetos no es clara dado que las observaciones ofrecen muy pocos datos sobre su composición o superficies. Incluso las estimaciones sobre su tamaño son dudosas dado que en muchos casos se basan, tan solo, en datos indirectos sobre su albedo comparada con la de otros cuerpos semejantes como Plutón.

Los KBO ("Kuiper Belt Objects") son objetos con órbitas situadas entre unas 30 y 50 ua del Sol. Orbitan sobre el plano de la eclíptica, aunque sus inclinaciones pueden ser bastante elevadas.

Algunos KBO están en resonancia orbital con Neptuno. Sus periodos orbitales son fracciones enteras del periodo orbital de Neptuno. Los objetos en resonancia 1:2 y 2:3 se denominan twotinos y plutinos respectivamente.

Los orígenes y estructura actual del cinturón de Kuiper todavía no han sido aclarados, mientras los astrónomos esperan al telescopio Pan-STARRS, con el que se deberían localizar muchos más KBOs y comprender muchos aspectos de la formación del sistema solar. 

Diferentes simulaciones por ordenador de las interacciones gravitatorias del periodo de formación del sistema solar indican que los objetos del cinturón de Kuiper pudieron crearse más hacia el interior del sistema solar y haber sido desplazados hasta sus posiciones actuales entre 30 y 50 UA por las interacciones con Neptuno y Urano ocasionado a su vez por la influencia gravitacional de Júpiter al entrar en resonancia 2:1 con Saturno, dispersando así los planetesimales que conformarían el cinturón de Kuiper y el disco disperso, otra región más externa del sistema solar.

Estas simulaciones indican que podría haber algunos objetos de masa significativa en el cinturón, posiblemente del tamaño de Marte.

En la actualidad se desarrollan numerosos programas de búsqueda de KBO. La sonda espacial New Horizons, la primera misión dedicada a la exploración del cinturón de Kuiper, fue lanzada el 19 de enero de 2006 y alcanzó la menor distancia con Plutón el 14 de julio de 2015. Una vez pasado Plutón está previsto que explore uno o varios KBO. Todavía no se ha determinado cuáles serán los KBO concretos a explorar, pero deberán tener entre 40 y 90 km de diámetro e, idealmente, ser blancos o grises para contrastar con el color rojizo de Plutón.

El acantilado de Kuiper es el nombre que le dan los científicos a la parte más alejada del cinturón de Kuiper. Es una incógnita que ha existido durante años. La densidad de objetos en el cinturón de Kuiper decrece drásticamente, de ahí el nombre de acantilado.

Para esta anomalía se manejan varías hipótesis, la más aceptada explica que en realidad sí hay una población de objetos en la parte más alejada del cinturón de Kuiper, solo que aún no se han agrupado en objetos más masivos del tamaño suficiente como para que puedan ser observados y detectados. La segunda hipótesis explica que los objetos en esta área fueron barridos por un cuerpo planetario qué tendría qué tener el tamaño de la Tierra o de Marte, lo qué sugiere qué se trataría de un hipótetico planeta transneptuniano.






</doc>
<doc id="3568" url="https://es.wikipedia.org/wiki?curid=3568" title="Cristo">
Cristo

Cristo (del latín Christus, y este del griego antiguo Χριστός, "Christós") es una traducción del término hebreo «Mesías» (מָשִׁיחַ, "Māšîaḥ"), que significa «ungido», y que se emplea como título o epíteto de Jesús de Nazaret en el Nuevo Testamento. En el cristianismo, Cristo se utiliza como sinónimo de Jesús.

Los seguidores de Jesús son conocidos como «cristianos» porque ellos creen y confiesan que Jesús es el Mesías profetizado en el Antiguo Testamento, por lo cual le llamaban «Jesús Cristo», que quiere decir «Jesús, el Mesías» (en hebreo: «Yeshua Ha'Mashiaj»), o bien, en su uso recíproco: «Cristo Jesús» («El Mesías Jesús»).

El título «Cristo» también se encuentra dentro del nombre personal «Jesucristo», y se menciona como un sinónimo de Jesús de Nazaret en la fe cristiana, que lo considera salvador y redentor de los hombres, el «Verbo» (o Palabra) de Dios encarnado y «el Hijo unigénito de Dios».

Las principales creencias cristianas acerca de Jesucristo incluyen su consideración como el Hijo de Dios, constituido como Señor; que fue concebido por el Espíritu Santo y que nació de la Virgen María; que fue crucificado, muerto y sepultado durante el mandato de Poncio Pilato; que descendió a los infiernos y posteriormente resucitó de la muerte y subió a los cielos, donde se encuentra junto a Dios Padre y desde donde volverá para el Juicio Final.

La cristología, un área de la teología, se ocupa principalmente de estudiar la naturaleza divina de la persona de Jesucristo, según los evangelios canónicos y los demás escritos del Nuevo Testamento.

El título «Mesías» fue utilizado en el Libro de Daniel, que habla de un «Mesías Príncipe» en la profecía acerca de «las setenta semanas».
También aparece en el Libro de los Salmos, donde se habla de los reyes y príncipes que conspiran contra Yahveh y contra su ungido. Pero fundamentalmente en el libro del profeta Isaías se expresa la llamada corriente mesiánica (Is 9, 1-7) atribuida a Cristo según los escritos del Nuevo Testamento.

Jesús es llamado «el Cristo» en los cuatro evangelios del Nuevo Testamento donde se le describe como ungido con el Espíritu Santo. Algunas referencias incluyen Mateo 1:16, Mateo 27:17, Mateo 27:22, Marcos 8:29, Lucas 2:11, Lucas 9:20 y Juan 1:41. En el evangelio de Mateo se trata el tema en el siguiente pasaje:

En el evangelio de Juan, el título de «Cristo» se usa como nombre de Jesús:

En el Libro de Daniel se afirma que el mesías príncipe sería cortado, y no tendría nada.
La antigua versión de Reina-Valera traduce ‘será muerto y nada tendrá’ y en el margen de la paráfrasis ‘será echado de la posesión’. Esto se cumplió cuando, en lugar de ser aceptado como Mesías por los judíos, fue rechazado, cortado, y no recibió ninguno de los honores mesiánicos que le pertenecían, aunque, con su muerte, echó los cimientos de su futura gloria en la Tierra, obrando la redención eterna para los salvos. En la Primera Carta a los Corintios san Pablo de Tarso escribió que así como el cuerpo es uno y tiene muchos miembros, así es el Cristo: la cabeza y los miembros en el poder y la unción del Espíritu forman un solo cuerpo.

En el Libro de Juan, este título es relacionado con el de Mesías, «llamado el Cristo».

Habiendo sido rechazado como mesías en la tierra, él ha sido hecho, ya resucitado de los muertos, Señor y Cristo, y así se cumplen los consejos de Dios con respecto a él y al hombre en él. Se revela que los santos habían sido escogidos en Cristo desde antes de la fundación del mundo. Todas las cosas en el cielo y en la tierra tienen que ser encabezadas en el Cristo, ya que el Cristo es la cabeza del cuerpo de la Iglesia.

La palabra «ungir» ―del latín "únguere"― significa ‘elegir a alguien para un puesto o un cargo muy notable’ (como sumo sacerdote o rey).

La concepción hebrea del ungido o entronizado proviene de la antigua creencia que establece que untar a una persona u olear un objeto con aceite otorga cualidades extraordinarias, incluso sobrenaturales, cuando estas provienen de una autoridad divina. En el Israel de la antigüedad, la costumbre de ungir a una persona otorgaba la potestad para ejercer algún cargo importante. El término Cristo no solo se utilizaba con los sacerdotes que eran mediadores entre Dios y la humanidad, sino también con los reyes teocráticos que eran representantes de Dios y adquirían de esa manera dignidad sacerdotal. Más tarde se aplicó a los profetas e incluso se vinculó con los patriarcas.
Sin embargo, en la transformación del concepto mesiánico, el uso del término se restringió al redentor y restaurador de la nación judía.

En el Nuevo Testamento, la palabra Cristo se utiliza como nombre común y como nombre propio. En ambas acepciones aparece con o sin artículo definido, en solitario o asociada a otros términos o nombres. Cuando se usa como nombre propio y, muchas veces, en los otros casos, designa a Jesús de Nazaret, el esperado Mesías de los judíos. De esta manera, para las confesiones cristianas, Jesucristo es el mesías, aquel que el Antiguo Testamento anunciaba que llegaría como plan de salvación de Dios para la humanidad. Otras religiones, sobre todo los musulmanes, judíos ortodoxos, conservadores, y reformistas, lo consideran solamente como un gran profeta o predicador de su pueblo ―el pueblo judío― y el fundador de la religión cristiana, a quien sus seguidores consideran el hijo encarnado de Dios.

La palabra salvador, a su vez, era el título calificativo que los judíos aplicaban a sus sacerdotes, reyes, y profetas, ya que estos debían ser ungidos con aceites como parte del rito que los consagraba a su labor. Los seguidores de Jesús de Nazaret, considerando que este era el Mesías prometido por las profecías mesiánicas de la Tanaj, le aplicaron este título a su líder, llamándole Cristo Jesús o el Salvador. A mediados del siglo II -unos cien años después de la muerte y resurrección de Jesús de Nazaret—se les comenzó a conocer por cristianos en Antioquía, ya que se decían seguidores del Cristo.

Según algunas confesiones cristianas, como la Iglesia católica, la Iglesia ortodoxa, la Iglesia anglicana o las principales iglesias protestantes, la Salvación es una venida de Dios. Sustentan este punto de vista en las palabras del Apóstol Pedro: «Por el contrario, creemos que tanto ellos como nosotros somos salvados por la gracia del Señor Jesús». Esta gracia se obtiene a través de la fe y el obrar cristiano, según católicos y ortodoxos, o exclusivamente por la fe, según los protestantes, es decir, en creer o confiar en que Jesucristo es el Hijo de Dios, el Salvador y el Único Perdonador de pecados.

En la carta de Pablo a los romanos se explica lo que es la salvación, pero con más precisión en la carta del apóstol Pablo a los Efesios: «Cristo, con su muerte y su Resurrección, es quien elimina la deuda del pecado humano y vehicula en su persona esa gracia redentora». Para el cristianismo la salvación está disponible para todos los que creen y actúan en consecuencia.

La creencia cristiana afirma que Dios se manifestó a los hombres en la persona de Jesús de Nazaret (en hebreo: "Yeshúa"), siendo el Hijo de Dios hecho hombre y, por tanto, el Mesías anunciado por los profetas en las escrituras, y ansiosamente esperado por Israel. Escrituras.
De hecho, Jesús mismo afirmó ser el Cristo. En el Evangelio de Juan, cuando Jesús habla con la mujer samaritana, se registra el siguiente evento:

A raíz de esto, se narra a los samaritanos diciendo: «nosotros mismos hemos oído, y sabemos que verdaderamente éste es el Salvador del mundo, el Cristo.» (Juan 4:42)

En el Evangelio de Marcos también se narra a Jesús afirmando ser el Mesías, cuando los sacerdotes del templo estaban interrogándolo:

Según el cristianismo, Jesús de Nazaret es el Cristo (el Mesías), Hijo de Dios hecho hombre (según el Evangelio de Mateo), concebido por el Espíritu Santo y nacido de la virgen María. Después de la crucifixión, al tercer día resucitó y posteriormente subió al Cielo; y se espera su regreso al final de los tiempos en lo que se llama la «segunda venida de Cristo», o Parusía. El cristianismo explica que el sufrimiento de Jesús era necesario. Frecuentemente se cree que el padecimiento de Jesús se desarrolló en la cruz, en realidad su padecimiento comenzó desde el huerto de Getsemaní. En este pasaje se describe como Jesús lleno de angustia oraba intensamente, su sudor era como grandes gotas de sangre que caían hasta la tierra.

La religión cristiana se inició en el seno del judaísmo como uno de tantos movimientos mesiánicos, centrado en la persona de Jesús de Nazaret. Sus seguidores extendieron su culto por todo el mundo basándose en la idea de que Jesús había resucitado.

Los seguidores de Cristo en el mundo actual no forman un conjunto único y uniforme, sino que se agrupan en distintas confesiones, como las iglesias católica, ortodoxa, anglicana, luterana, bautista, anabaptista, menonita, presbiteriana, metodista, mormona, etc. Y aún los hay que no reconocen un vínculo con algún grupo.

La fe en Cristo de la mayoría de estas comunidades puede sintetizarse en esta antiquísima profesión de fe:

Existe un movimiento llamado ecumenismo, el cual trata de buscar la unidad de todos los seguidores de Cristo. A este respecto, dentro de la Iglesia católica, el Concilio Vaticano II, en su decreto "Unitatis redintegratio", ha expresado, refiriéndose a la división de los cristianos, «abiertamente repugna a la voluntad de Cristo y es piedra de escándalo para el mundo y obstáculo para la causa de la difusión del Evangelio por todo el mundo».

Antes de su realización, el papa Juan XXIII creó el Pontificio Consejo para la Promoción de la Unidad de los Cristianos. Esta llamada ha sido continuada por los papas siguientes.

Para el catolicismo, Cristo es el Hijo de Dios hecho hombre para la salvación del género humano, y esa es la «Buena Nueva»: Dios ha enviado a su Hijo.
Hijo de Dios hecho hombre: para la Iglesia católica esto significa que la segunda Persona de la Santísima Trinidad, el Hijo, se hizo hombre en el seno de María. Cristo, siendo una sola Persona divina, es perfecto Dios y perfecto hombre. Esta doctrina encuentra sus antecedentes en distintos textos de la Sagrada Escritura, entre los que se puede citar:

Se han producido dentro de la Iglesia católica distintos debates referidos a cómo deben interpretarse estas afirmaciones. Su posición oficial ha quedado fijada en las decisiones de los distintos Concilios:

El Primer Concilio de Nicea, en el año 325, el primer concilio ecuménico que la Iglesia católica pudo realizar terminadas las persecuciones que padeció sus primeros 300 años, profundizó los textos bíblicos citados, afirmando que Jesucristo es consustancial al Padre (de la misma sustancia que el Padre), es decir, verdadero Dios.

El Primer Concilio de Constantinopla, en el año 381, continuó con la profundización de la doctrina, redactando el Credo Niceno-Constantinopolitano:
Los Concilios siguientes han continuado precisando la doctrina:





Estas precisiones han surgido como respuesta a distintas doctrinas que fueron apareciendo. Por ejemplo:







En todas ellas, la Iglesia ha visto en el fondo la negación de la redención, porque creían que era necesario que Cristo fuera Dios, para poder redimir; que fuera hombre, para poder padecer; y que fuera una sola persona, para poder referir la divinidad y la humanidad «en concurrencia inefable y misteriosa en la unidad».

Para la Iglesia católica, Cristo, en el mundo actual, es «Lumen Gentium», «Luz de los pueblos». Por ello san Juan Pablo II, en la homilía de comienzo de su pontificado, exclamaba: «¡No temáis! ¡Abrid, más todavía, abrid de par en par las puertas a Cristo!».

Más recientemente, el Papa Francisco ha expresado:

El Catecismo de la Iglesia católica destaca que «los Padres ven en la concepción virginal el signo de que es verdaderamente el Hijo de Dios el que ha venido en una humanidad como la nuestra».

La Iglesia católica resalta el papel de María en la concepción virginal de Cristo, en su relación de fe hacia Él y en la redención por él obrada.
Los Padres de la Iglesia abordaron la íntima unión de Cristo y María en la obra de la redención. Por ejemplo:

Por un lado, la Iglesia católica sostiene que Dios ha preparado a María para tal misión, «en atención a los méritos de Cristo Jesús», preservándola del pecado original, en lo que se denomina su Inmaculada Concepción y concediéndole multitud de gracias, las que ella misma reconoció diciendo: «Porque el Todopoderoso ha hecho en mí grandes cosas» y a las que ella correspondió con absoluta fidelidad y entrega.

Por otro, ha visto en el sí de María, al aceptar el ofrecimiento del ángel a ser madre de Jesús, el sí de la humanidad, que aceptaba a través de ella la salvación que traería Cristo.

Por el hecho de ser madre de Cristo, que según se ha visto la Iglesia católica enseña que es la segunda Persona de la Santísima Trinidad que se hizo hombre sin perder su condición divina, la Iglesia la llama Madre de Dios.

Los evangelios detallan los hechos de la vida de Cristo más sobresalientes, sin embargo, en los mismos no pasa desapercibida la discreta presencia de María: el Hijo de Dios se hace hombre luego de su consentimiento; los pastores y los magos encuentran al Niño Prometido junto a ella;
Cristo hace su primer milagro a su pedido; está firme al pie de la Cruz, junto a su Hijo.
La Iglesia ha visto en las palabras de Jesús: «Mujer, ahí tienes a tu hijo» y a Juan: «Ahí tienes a tu madre» la entrega de María como madre de todos los cristianos, representados en la persona de Juan, por lo que es llamada «Madre de la Iglesia».
Y ella, que «conservaba cuidadosamente todas las cosas en su corazón», perseveraba en la oración junto a la Iglesia naciente, según cuenta el libro de los Hechos de los Apóstoles.
El Apocalipsis habla de una mujer, vestida de sol, con la luna bajo sus pies y una corona de doce estrellas sobre su cabeza y que da a luz un hijo varón que derrotará al dragón infernal.

En la misma promesa del Redentor, contenida en el libro del Génesis, se habla de una mujer, de la que nacería el vencedor de la serpiente:

A este respecto comenta san Alfonso María de Ligorio: «ya desde el principio de la Humanidad, Dios predijo a la serpiente infernal la victoria y el dominio que había de ejercer sobre él nuestra reina al anunciar que vendría al mundo una mujer que lo vencería […] ¿Y quién fue esta mujer su enemiga sino María, que con su preciosa humildad y vida santísima siempre venció y abatió su poder? «En aquella mujer fue prometida la Madre de nuestro Señor Jesucristo», dice san Cipriano. Y por eso argumenta que Dios no dijo «pongo», sino «pondré», para que no se pensara que se refería a Eva».

San Agustín, comentando el pasaje donde una mujer le dice a Jesús: «dichoso el vientre que te llevó» y el Señor contestó: «mejor, dichosos los que escuchan la palabra de Dios y la cumplen», dice que esto significa que María, no solamente escuchó la palabra y la cumplió sino que es más feliz por haber concebido a Cristo en su mente mediante la fe, que por haberlo llevado en su seno. A través de ella, la misma «Palabra se hizo carne, y habitó entre nosotros».

Por esta elección de Dios y su correspondencia por parte de María, ha visto la Iglesia en ella un modelo de perfecta cristiana, y un camino para llegar a Cristo.

En el Evangelio de Mateo, Jesús habla de «su Iglesia». La palabra «iglesia» viene del griego "ecclesia", que significa ‘asamblea’. San Pablo de Tarso dice que la iglesia es el cuerpo de Cristo.

La Iglesia católica afirma ser ella la iglesia fundada por Cristo, exhibiendo entre otros argumentos, la sucesión apostólica: todos los obispos católicos han sido ordenados por otro obispo, y así, remontándose hacia atrás, se llegará a uno de los apóstoles elegidos por Cristo. Dice así san Ireneo de Lyon:

Según la Iglesia, solo en ella puede encontrarse la plenitud total de los medios de salvación dados por Cristo.
Sin embargo, ella misma enseña que fuera de sus límites visibles, hay muchos elementos de santificación y de verdad.

Según el catolicismo, dentro de la sucesión apostólica que concierne a todos los obispos, está la del Obispo de Roma, el papa, sucesor de san Pedro hasta nuestros días. (Véase ). La Iglesia católica afirma que Cristo constituyó jefe de su Iglesia a San Pedro y en él a sus sucesores:

La Iglesia enseña que el papa es el «principio y fundamento perpetuo y visible de unidad, tanto de los obispos como de la muchedumbre de los fieles». Por esto, san Ambrosio de Milán pudo decir: «allí donde está Pedro, allí está la Iglesia».

Con referencia a esto, continúa san Ireneo de Lyon en la cita que se transcribió en la sección referida a Cristo y la Iglesia:

Y san Cipriano de Cartago:

Para la Iglesia, las enseñanzas de Dios están contenidas en la Biblia y en la transmisión oral de la predicación de los apóstoles, llamada Tradición Apostólica. A su vez, estas enseñanzas han llegado a los hombres de todos los tiempos a través del Magisterio de la Iglesia, ejercido por los obispos, sucesores de los apóstoles, en comunión con el sucesor de San Pedro, el Papa.

La interpretación de la Palabra en la Iglesia católica no es libre. Tratándose de la Sagrada Escritura, por ejemplo, la Iglesia enseña que debe hacerse “estando atentos a los que los autores humanos quisieron verdaderamente afirmar y a lo que de Dios quiso manifestarnos mediante sus palabras”.

Esta interpretación es realizada por la Iglesia, “columna y fundamento de la verdad”, como dice San Pablo. Y fue ejercida desde el comienzo, por los mismos apóstoles: “El Espíritu Santo, y nosotros mismos, hemos decidido…”.

La Iglesia primitiva no tenía Nuevo Testamento. La misma inclusión de los libros sagrados en el canon bíblico, ha sido un acto del Magisterio eclesiástico. El resto de las confesiones cristianas han heredado la Biblia (el Nuevo Testamento al menos) tal como quedó fijado por la Iglesia católica.

Ya desde el comienzo del cristianismo, surgieron opiniones divididas respecto a las enseñanzas transmitidas por Jesucristo. Por ejemplo el apóstol san Juan dice, refiriéndose a los disidentes: «ellos salieron de entre nosotros, sin embargo, no eran de los nuestros».

La Iglesia entiende que Dios, al revelar su palabra a través de Cristo, constituyó al mismo tiempo una autoridad presente en todos los tiempos, encargada de interpretarla sin equivocarse, a fin de mantener “la pureza de la fe transmitida por los apóstoles”, de otra manera no habría modo de saber sin que quede lugar a dudas cuál es la interpretación correcta. Esta capacidad de la Iglesia de interpretar sin equivocarse la palabra de Cristo, la Iglesia la llama “infalibilidad”, y ella entiende que la ha recibido de Cristo, conjuntamente con la misión de difundir su palabra.

Algunos párrafos del Catecismo de la Iglesia católica donde se explica la doctrina acerca de los sacramentos:

Especial mención merece la eucaristía. La Iglesia católica cree que la eucaristía o Santa Misa fue instituida por Cristo cuando en la Última Cena dijo: «Tomad y comed: esto es mi cuerpo», «Tomad y bebed, esto es mi sangre», «haced esto en conmemoración mía». Ella cree que en cada eucaristía se hace presente (“se re-presenta”) el sacrificio que Cristo hizo en la cruz de una vez para siempre, se perpetúa su recuerdo a través de los siglos y se aplica su fruto. Y que el sacrificio de la cruz y el sacrificio de la eucaristía son un único sacrificio, ya que tanto en uno como en otro, Cristo es el sacerdote que ofrece el sacrificio y la víctima que es ofrecida. Se diferencian sólo en la forma en que se ofrece el sacrificio. En la cruz Cristo lo ofreció en forma cruenta, y por sí mismo, y en la Misa en forma incruenta y por ministerio de los sacerdotes. Por esto san Juan Pablo II pudo decir que en la eucaristía “está inscrito de forma indeleble el acontecimiento de la pasión y muerte del Señor. No sólo lo evoca sino que lo hace sacramentalmente presente. Es el sacrificio de la Cruz que se perpetúa por los siglos”.

La Iglesia cree que Cristo mismo está presente en la eucaristía. Esta presencia no la entiende como la que se da en una efigie, imagen, símbolo o recordatorio, sino que ella cree que está Él en persona, vivo y entero, con su cuerpo, sangre, alma y divinidad, de una forma “verdadera, real y sustancial”.

Por esto san Juan Crisóstomo pudo decir: «Cuánta gente dice hoy: ‘Querría ver a Cristo en persona, su cara, sus vestidos, sus zapatos’. ¡Pues bien, en la eucaristía es a él al que vés, al que tocas, al que recibes! Deseabas ver sus vestidos; y es él mismo el que se te da no sólo para verle, sino para tocarlo, comerlo, acogerlo en tu corazón».

Y san Juan Pablo II: «La Iglesia ha recibido la eucaristía de Cristo, su Señor, no sólo como un don entre otros muchos, aunque sean muy valiosos, sino como el don por excelencia, porque es don de sí mismo, de su persona en su santa humanidad y, además, de su obra de salvación».

La Iglesia entiende que la eucaristía se destaca del resto de los sacramentos ya que mientras ellos tienen la misión de santificar, en la eucaristía se halla el autor mismo de la santidad. Por ello es llamada "Santísimo Sacramento del Altar", "Santísimo Sacramento", o sencillamente "Santísimo".

Cristo ha prometido la vida eterna a quienes lo reciben en este Sacramento:




</doc>
<doc id="3569" url="https://es.wikipedia.org/wiki?curid=3569" title="Pecado original">
Pecado original

El pecado original, también llamado pecado ancestral, es una doctrina cristiana del estado de pecado en el cual se halla la humanidad cautiva como consecuencia de la caída del hombre, originado por la rebeldía de Adán y Eva en el Jardín del Edén, es decir, el pecado de la desobediencia al consumir del árbol del conocimiento del bien y del mal. Dicho estado de pecado sería transmitido a toda la humanidad y consistiría en la privación de la santidad y de la justicia originales, las cuales Adán y Eva poseían en un principio antes de comer del fruto prohibido.

El concepto del pecado original fue originalmente concebido en el siglo II por Ireneo, obispo de Lyon, en su controversia con algunos gnósticos dualistas. Otros padres eclesiásticos como Agustín de Hipona (354-430) también desarrollaron la doctrina, quienes la justificaron en las enseñanzas de Pablo de Tarso (Romanos 5:12–21 y 1 Corintios 15:21-22) y en el versículo Salmos 51:5. Tertuliano, Cipriano, Ambrosio y Ambrosiaster consideraron que la humanidad comparte el pecado de Adán, trasmitido de generación en generación. Interpretación particular hicieron Martín Lutero y Juan Calvino, quienes lo identificaron con la concupiscencia la cual, según su interpretación, destruiría el libre albedrío. Dentro del catolicismo, el movimiento jansenista, al que la Iglesia declaró herético, también mantenía que el pecado original destruía el libre albedrío. En su lugar la Iglesia católica declara que "El bautismo, dando la vida de la gracia de Cristo, borra el pecado original y devuelve el hombre a Dios, pero las consecuencias para la naturaleza, debilitada e inclinada al mal, persisten en el hombre y lo llaman al combate espiritual." "Aún debilitado y disminuido por la caída de Adán, el libre albedrío no es destruido en la carrera." En cuanto al protestantismo algunas denominaciones tienen diferentes interpretaciones del pecado original.

La doctrina cristiana católica con respecto al pecado original se fijó en el concilio de Cartago y se precisó posteriormente en el concilio de Orange y el concilio de Trento. Los detalles de su forma actual se encuentran ya en los escritos de san Agustín de Hipona, a través del cual la noción de una corrupción fundamental de la naturaleza humana hizo pie en la Iglesia. Los escasos fragmentos de doctrina sobre el pecado original contenidos en los escritos de los Apóstoles (especialmente Romanos 5:12) no efectúan mayores precisiones sobre el texto del Génesis.

La teología escolástica distingue entre el pecado original originante ("peccatum originale originans"), el acto concreto de desobediencia cometido por Adán y Eva, y el pecado original originado ("peccatum originale originatum"), las consecuencias que ese pecado provocaría sobre la constitución de la especie humana. En virtud del "peccatum originale originatum", no solo se perderían los dones preternaturales de la inmortalidad y la exención del sufrimiento, sino que las capacidades del espíritu humano —tanto las morales como las intelectuales— carecerían de su vigor natural, sometiendo la voluntad a las pasiones y el intelecto al error. De acuerdo con la doctrina fijada en el concilio de Trento, la condición de "naturaleza caída" ("natura lapsa") se transmite a cada uno de los nacidos tras la expulsión del Edén.

En los concilios se estableció que el bautismo borra el pecado original y devuelve el hombre a Dios, pero las consecuencias para la naturaleza, debilitada e inclinada al mal, persisten en el hombre y lo llaman al combate espiritual. La Iglesia católica y otras que practican el culto mariano excluyen, sin embargo, de las consecuencias del pecado original a la Virgen María, en virtud de una gracia especial de Dios para que Jesucristo no tuviera el pecado original.

De acuerdo con el Catecismo de la Iglesia católica 

Por su pecado, Adán, en cuanto primer hombre, perdió la santidad y la justicia originales que había recibido de Dios no solamente para él, sino para todos los humanos.

Adán y Eva transmitieron a su descendencia la naturaleza humana herida por su primer pecado, privada por tanto de la santidad y la justicia originales. Esta privación es llamada "pecado original".

Como consecuencia del pecado original, la naturaleza humana quedó debilitada en sus fuerzas, sometida a la ignorancia, al sufrimiento y al dominio de la muerte, e inclinada al pecado (inclinación llamada "concupiscencia").
San Anselmo decía que "el pecado de Adán fue una cosa pero el pecado de los niños al nacer es algo distinto; el primero fue la causa, el segundo es el efecto" El pecado original en un niño es distinto de la falta de Adán; es uno de sus efectos. Los efectos del pecado de Adán de acuerdo con la Enciclopedia Católica son:

1. Muerte y sufrimiento.

2. Concupiscencia (o inclinación al pecado). El bautismo borra el pecado original, pero la inclinación al pecado permanece.

3. La ausencia de la gracia santificante en los niños recién nacidos es también efecto del primer pecado, ya que Adán, habiendo recibido de Dios la santidad (o gracia santificante) y la justicia, no solo la perdió para él, sino para nosotros. El bautismo confiere la gracia santificante original, perdida por el pecado de Adán, eliminando así el pecado original y cualquier pecado personal.

El catecismo presenta la doctrina del pecado original como el “anverso de la redención” (cf. CEC 389). Recuerda que aunque el relato está hecho de imágenes o se encuentre redactado usando figuras literarias, se trata de un acontecimiento real de los inicios de la historia y que la marca (cf. CEC 390). 

La Iglesia católica siempre ha sostenido que "por el Bautismo, todos los pecados son perdonados, el pecado original y todos los pecados personales" ya que el bautismo confiere la gracia santificante original, perdida por el pecado de Adán, eliminando así el pecado original y los pecados personales. Aunque propio de cada uno el pecado original no tiene, en ningún descendiente de Adán, un carácter de falta personal. El pecado original es la privación de la santidad y de la justicia originales.

Luego, entre los números 396 y 409 analiza los diversos elementos relacionados con este pecado: La prueba que implicaba el no poder comer del árbol del conocimiento del bien y del mal como una muestra del límite que la libertad humana tiene por el hecho de ser una criatura; el pecado presentado como un acto de desconfianza primero y de desobediencia después; las consecuencias: pérdida de la santidad original, se destruye la armonía del mundo y del interior del hombre, la muerte entra en la historia. La universalidad del castigo a partir del pecado original se sostiene con textos tomados de san Pablo:

En el número 404 se dice que el pecado de Adán es el pecado de todos los hombres que vienen después de él, ya que, según una expresión de Tomás de Aquino, la humanidad es en Adán como el cuerpo de un único hombre. Ahora bien, el catecismo afirma que la transmisión de ese pecado es un misterio, y que, por tanto, la expresión “pecado” se usa de manera análoga, puesto que no se trata de una falta “cometida”, sino de un pecado “contraído”.

El pelagianismo, que rechaza la condición caída de la naturaleza humana como una corrupción maniquea de la doctrina cristiana, fue declarado herético en el concilio de Cartago; el primer partidario de esta doctrina del que se tienen noticias fue Teodoro de Mopsuestia, aunque su influencia fue mayor en la iglesia occidental a través de Pelagio y Celestio. Según los pelagianos, la introducción del pecado por Adán se limita a proporcionar un mal ejemplo a sus descendientes, pero no hiere sus facultades.

Los líderes de la Reforma admitían el dogma del pecado original, pero el día de hoy hay algunas denominaciones protestantes que ya no creen en el pecado original influenciados por la doctrina sociniana del reformador Fausto Socino.

El libro apócrifo 4 Esdras escrito por un judío en el siglo I presenta a Adán como el autor de la caída de la raza humana:

y a Adán como quien transmitió a toda su progenie la enfermedad permanente, la malignidad y la mala semilla del pecado: 

De acuerdo con el texto bíblico del Génesis 1-3, tras haber sido creados Adán y Eva residían en el jardín del Edén en perfecta armonía con Dios; el único mandato al que debían acogerse era la abstención de comer del árbol de la ciencia del bien y del mal, cuyo consumo ocasionaría la muerte (Génesis 2:17). Sin embargo, Eva y —por su intermediación— Adán cedieron a la tentación de la serpiente (identificada con Satán o "Shaitan", "el tentador") y descubrieron, comiendo del árbol, su desnudez. La consecuencia de la violación de su mandato llevó a la muerte —""[volverás] a la tierra, porque de ella fuiste tomado; pues polvo eres, y al polvo volverás"" (Génesis 3:19) y la expulsión del jardín del Edén.

La tradición talmúdica identifica este acto como החטא הקדמון (en hebreo "hajet hakadmon", "la falta primordial") de la desobediencia al mandato divino. Sin embargo, los efectos de este pecado se reducen a castigos personales, como la expulsión del paraíso, dolores de parto en el caso de Eva —y de toda su descendencia—, la multiplicación excesiva de la dificultad del trabajo (no el trabajo en sí mismo, que es descrito antes como un don divino y precedía al pecado mismo), la enfermedad, la vejez y la muerte.

Las corrientes renovadoras dentro del judaísmo interpretan la caída como el primer acto de libre albedrío del hombre, y la consideran como parte del plan divino, puesto que la falta representaría la admisión de la responsabilidad; en otras palabras, el mito de la caída sería una elaborada alegoría del pasaje a la adultez y la autonomía.

Existe controversia entre los teólogos judíos respecto a la causa de lo que es llamado "pecado original". Algunos enseñan que fue debido a la claudicación de Adán en la tentación de comer el fruto prohibido y fue heredado a sus descendientes; la mayoría, sin embargo, no considera culpable a Adán de los pecados de la humanidad, sino que de acuerdo a Génesis 8:21 y Génesis 6:5-8 Dios reconoció que los pecados de Adán son solo suyos. No obstante, algunos consideran que esto trajo la muerte al mundo; debido a su pecado, sus descendientes viven una vida mortal que termina con la muerte de sus cuerpos. La doctrina del "pecado heredado" no se encuentra en la mayoría del judaísmo tradicional. Aunque algunos judíos ortodoxos culpan a Adán por la corrupción general del mundo, y a pesar de que existieron algunos maestros judíos de los tiempos talmúdicos que creían que la muerte era un castigo llevado a la humanidad debido al pecado de Adán, esa no es la postura dominante en el judaísmo actual. Los judíos modernos generalmente enseñan que los humanos nacen libres de pecado y puros y luego eligen pecar llevando el sufrimiento a sus vidas. El concepto del pecado heredado no existe bajo ninguna forma en el islam.

En el Islam no existe la noción de pecado original, más bien se rechaza rotundamente.

De acuerdo con el Corán, la transgresión cometida por Adán y Eva —y que fue responsabilidad de ambos, y no de Eva en mayor grado— quedó zanjada con el castigo recibido, es decir, con la expulsión del Paraíso. El islam no condena a la naturaleza humana como tal y además rechaza explícitamente que otro pague por los errores de los demás: "Nadie cargará con la culpa ajena" (Sura 17, versículo 15).

La ausencia del pecado original acentúa la idea de responsabilidad individual, que es central en el islam. Esa libertad es la base sobre la cual puede Dios decidir castigar o premiar.


</doc>
<doc id="3570" url="https://es.wikipedia.org/wiki?curid=3570" title="Credo">
Credo

Un credo es una profesión, declaración o confesión de fe que es compartida por una comunidad religiosa, y en particular es una fórmula fija que se recita en la liturgia.

La fórmula más conocida es el símbolo niceno-constantinopolitano (llamado también símbolo niceno). En la liturgia de la Iglesia católica y de varias denominaciones protestantes se usa también el símbolo de los apóstoles.

El Credo niceno-constantinopolitano, que a finales del siglo V se recitaba en la liturgia en Antioquía y desde 511 en Constantinopla, fue introducido en la liturgia de la cristiandad occidental por decisión del III Concilio de Toledo en el año 589.

La práctica se extendió en España, las Islas Británicas y el reino franco, pero por mucho tiempo no fue aceptada en Roma. Cuando Carlomagno convocó en 809 un concilio en Aquisgrán y quiso obtener la aprobación papal de la decisión del concilio de incluir en el Credo la cláusula "Filioque", el papa León III se opuso a la añadidura (a pesar de declarar ortodoxa la doctrina expresada) y sugirió seguir el ejemplo de Roma al no incluir el Credo en la celebración de la misa.

En 1014 con motivo de su coronación como emperador del Sacro Imperio, Enrique II solicitó al papa Benedicto VIII la recitación del Credo en la misa. El papa accedió a su petición, con lo que por primera vez en la historia el Credo se usó en la misa en Roma.

Son dos las fórmulas utilizadas: el Símbolo Niceno-Constantinopolitano (a menudo llamado el Símbolo Niceno) y el Símbolo de los Apóstoles.

Lo que se llama comúnmente el "credo niceno" y más correctamente el "credo niceno-constantinopolitano" no es exactamente el texto formulado durante el Segundo Concilio Ecuménico en la Ciudad de Constantinopla (año 381), revisión radical del texto del Primer Concilio Ecuménico en Nicea (en el año 325). Los textos que, por ser de las liturgias bizantina y romana, son más conocidos difieren del de ese Concilio del año 381 al utilizar el número singular de los verbos "Creo", "Confieso", "Espero": lo que el texto original dice es "Creemos" (πιστεύομεν), "Confesamos" (ὁμολογοῦμεν), "Esperamos" (προσδοκοῦμεν). El texto de la liturgia mozárabe conserva la forma plural. Los textos en latín tienen dos frases ausentes en el texto original del Concilio de Constantinopla (381). Una, "Deum de Deo", se encontraba en el símbolo del Concilio de Nicea (325) mas no en el texto del año 381. Sobre la otra, "'Filioque" en el rito romano, "et Filio" en el rito mozárabe, ha habido una importante controversia entre las Iglesias católica y ortodoxa. Además el texto mozárabe añade, después de "Per quem omnia facta sunt", "quae in cælo, et quae in terra", frase también presente en el símbolo 325, mas no en el símbolo 381, y omite, al hablar de la crucifixión y la resurrección de Jesús, las dos frases "pro nobis" y "secundum Scripturas".

El llamado Símbolo de los Apóstoles o Credo de los Apóstoles es el símbolo bautismal de la Iglesia romana. Su gran autoridad le viene de este hecho. San Ambrosio dijo: "Es el símbolo que guarda la Iglesia romana, la que fue sede de Pedro, el primero de los Apóstoles, y a la cual él llevó la doctrina común." Y su nombre, "Símbolo de los Apóstoles", se debe al hecho de ser considerado el resumen fiel de la fe de los apóstoles.

Su contenido dogmático es el siguiente:

En la misa del rito romano revisada en 1969, el Credo se recita a conclusión de la Liturgia de la Palabra, después de la homilía y antes de la Oración de los Fieles. "El Símbolo o profesión de fe tiende a que todo el pueblo congregado responda a la Palabra de Dios anunciada en las lecturas de la Sagrada Escritura y expuesta en la homilía, y a que, al proclamar la norma de su fe, con la fórmula aprobada para el uso litúrgico, recuerde y confiese los grandes misterios de la fe, antes de comenzar su celebración en la Eucaristía."

Se recita sólo en domingos y solemnidades, mas puede también decirse en peculiares celebraciones más solemnes. En ocasiones, se omite después de la profesión de fe expresada en una renovación de promesas bautismales.

Si se canta, lo inicia el sacerdote o, según la oportunidad, un cantor, o el coro, pero lo cantan todos juntos. Si no se canta, lo recitan todos juntos, o a dos coros alternando entre sí.

A la mención de la Encarnación de Jesucristo, debe hacerse una profunda reverencia. En la Navidad y el día de la Anunciación, todos se arrodillan en esta parte.

El Misal indica en primer lugar el Símbolo niceno-constantinopolitano, pero permite de reemplazarlo, particularmente en la Cuaresma y el Tiempo Pascual, por el Símbolo bautismal de la Iglesia romana conocido como el Símbolo de los Apóstoles.

En la Misa tridentina el Credo se recita los domingos (incluso si, como era común antes de la reforma de Pío X, la misa celebrada no es la del domingo sino la de una fiesta en la que normalmente no se recita el Credo), en las fiestas de los apóstoles y de los doctores de la Iglesia, y en varias otras misas listadas en las ediciones del Misal romano anteriores a la de 1962. La edición de 1962 publicada por Juan XXIII redujo el número de tales misas, excluyendo, por ejemplo, la recitación del Credo por el solo motivo de una conmemoración de otra fiesta. A las palabras "et incarnatus est de Spiritu Sancto ex Maria Virgine: et homo factus est", se hace siempre una genuflexión.

En la actualidad, debido al motu proprio "Summorum Pontificum" de Benedicto XVI está permitido a todos los sacerdotes de la Iglesia latina emplear la edición 1962 del Misal romano en lugar de la más reciente, sin pedir el permiso a nadie, cuando celebran la misa privadamente, y también, bajo ciertas condiciones, con el permiso del solo rector de la iglesia cuando celebran públicamente.

En las Iglesias ortodoxas el texto del Credo es siempre la forma litúrgica griega del Símbolo Niceno-Constantinopolitano y se recita en cada celebración de la Divina Liturgia, no solo en los domingos. Durante la recitación se ventila el pan y el vino con un velo, acción que representa el descenso del Espíritu Santo.

El Credo se recita en el rito mozárabe después de la consagración y antes del Padre nuestro. Antes de su aparición en cualquier otra liturgia occidental, lo insertó en la misa mozárabe o hispana el Tercer Concilio de Toledo en 589, a imitación de lo que se hacía en el oriente y "con la función precisa de preparar los fieles a la comunión … para que la comunidad cristiana se uniese a Cristo, en la oración y en la comunión sacramental, habiendo confirmado su fe en la plena divinidad del mismo, Dios igual al Padre, según la doctrina de la Iglesia católica. " Como en el rito bizantino, el Credo se dice en todas las misas y no está reservado, como en el rito romano, a los domingos y fiestas más solemnes.

Al celebrar la "Santa Comunión", la Iglesia de Inglaterra ahora autoriza el uso no solo del Símbolo Niceno-Constantinopolitano, el Símbolo de los Apóstoles y el Símbolo Quicumque (estos tres mencionados en los "Treinta y Nueve Artículos"), sino también de muchas otras profesiones de fe.

John Wesley, fundador del Metodismo, al revisar la liturgia anglicana, omitió la mención de los tres símbolos entonces reconocidos por la Iglesia de Inglaterra, mas conservó en las Oraciones de la Mañana y de la Tarde (Maitines y Vísperas). A cierto punto se dejó de celebrar estas, pero el Símbolo de los Apóstoles se insertó en 1896 en el servicio principal de culto.

En Alemania, los luteranos usan el Símbolo de los Apóstoles en su liturgia. En los Estados Unidos usan también el Símbolo Niceno-Constantinopolitano, sugiriendo este último para las ocasiones más solemnes.

En una misa cantada, el Credo (en la forma tradicional del Símbolo Niceno-Constantinopolitano) es el texto más largo de todo el Ordinario de la Misa del domingo o de la solemnidad. Por eso las composiciones polifónicas del Ordinario de la Misa, que no incluyen el Credo, son denominadas "Missae breves". Este término no se aplica normalmente a un réquiem polifónico: una misa de réquiem no tiene Credo, mas el arreglo musical generalmente incluye el "Dies irae", que es más largo que el Credo.

Entre los compositores de misas musicales están Josquin des Prés (quien perfeccionó la técnica de la misa parodia), Giovanni Pierluigi da Palestrina, Alessandro Scarlatti, Joseph Haydn, Wolfgang Amadeus Mozart (por ejemplo, la Misa Credo), Ludwig van Beethoven, Franz Liszt, Charles Gounod, Anton Bruckner e Ígor Stravinski.





</doc>
<doc id="3573" url="https://es.wikipedia.org/wiki?curid=3573" title="Italiano">
Italiano

Italiano puede hacer referencia a:

Además, puede referirse a los siguientes clubes de fútbol:


</doc>
<doc id="3586" url="https://es.wikipedia.org/wiki?curid=3586" title="Louis Pasteur">
Louis Pasteur

Louis Pasteur (; Dole, Francia el 27 de diciembre de 1822-Marnes-la-Coquette, Francia el 28 de septiembre de 1895) fue un químico, físico, matemático y bacteriólogo francés, cuyos descubrimientos tuvieron una enorme importancia en diversos campos de las ciencias naturales, sobre todo en la química y microbiología. A él se debe la técnica conocida como pasteurización (eliminar parte o todos los gérmenes de un producto elevando su temperatura durante un corto tiempo) que permitió desarrollar la esterilización por autoclave. A través de experimentos, refutó definitivamente la teoría de la generación espontánea y desarrolló la teoría germinal de las enfermedades infecciosas. Por sus trabajos, se le considera el pionero de la microbiología moderna, con lo que inició la llamada «Edad de Oro de la Microbiología».

Aunque la teoría microbiana fue muy controvertida en sus inicios, hoy en día es fundamental en la medicina moderna y la microbiología clínica, que condujo a innovaciones tan importantes como el desarrollo de vacunas, de los antibióticos, la esterilización y la higiene como métodos efectivos de cura y prevención contra la propagación de las enfermedades infecciosas. Esta idea representa el inicio de la medicina científica, al demostrar que la enfermedad es el efecto visible (signos y síntomas) de una causa que puede buscarse y eliminarse mediante un tratamiento específico. En el caso de las enfermedades infecciosas, se debe buscar el germen causante de cada enfermedad para hallar un modo de combatirlo.

Su primera contribución importante a la ciencia fue en físico-química, con el descubrimiento del dimorfismo del ácido tartárico, al observar al microscopio que el ácido racémico presentaba dos tipos de cristal, con simetría especular, contradiciendo los descubrimientos de Eilhard Mitscherlich, químico de prestigio de la época. Este descubrimiento lo realizó cuando contaba con poco más de 20 años de edad. Fue por tanto el descubridor de las formas dextrógiras y levógiras que desviaban el plano de polarización de la luz con el mismo ángulo, pero en sentido contrario.

Hijo de Jean-Joseph Pasteur y de Jeanne-Étiennette Roqui, Louis Pasteur nació el 27 de diciembre de 1822 en Dole, localidad del Franco Condado donde transcurrió su infancia. Era hijo de un curtidor, y de joven no fue un estudiante prometedor en ciencias naturales; de hecho, si demostraba alguna actitud especial, era en el área artística de la pintura. Su primera ambición fue la de ser profesor de arte. En 1842, tras ser maestro en la Escuela Real de Besanzón, obtuvo su título universitario de Bachelier ès Sciences Mathématiques (el equivalente en inglés a Bachelor of Science in Mathematics) en Dijon, con calificación «mediocre» en química. Su padre lo mandó a la Escuela Normal Superior de París, pero allí no duró mucho tiempo, ya que regresó a su tierra natal. Pero al año siguiente retornó a París. En agosto de 1847 obtuvo su Doctorado en Ciencias (Docteur ès Sciences) en la Facultad de Ciencias de Paris, con una tesis de Físico-Química titulada (a) Tesis de Química: "Recherches sur la capacité de saturation de l'acide arsénieux. Etudes des arsénites de potasse, de soude et d'ammoniaque." (b) Tesis de Física: "1. Études des phénomènes relatifs à la polarisation rotatoire des liquides. 2. Application de la polarisation rotatoire des liquides à la solution de diverses questions de chimie." Tras pasar por la École Normale Supérieure, se convirtió en profesor de física en el Liceo de Dijon, aunque su verdadero interés era ya la química. Entre 1847 y 1853, fue profesor de química en Dijon y luego en Estrasburgo, donde conoció a Marie Laurent, la hija del rector de la Universidad, con quien contrajo matrimonio en 1849. El matrimonio tuvo cinco hijos, pero solo sobrevivieron hasta la vida adulta dos de ellos: Jean-Baptiste y Marie-Luise. Los otros tres fallecieron tempranamente, afectados por el tifus.

En 1854, fue nombrado decano de la Facultad de Ciencias en la Universidad de Lille. En 1857, desempeñó el cargo de director de estudios científicos de la Escuela Normal de París, cuyo laboratorio dirigió a partir de 1867. Desde su creación en 1888 y hasta su muerte, fue director del instituto que lleva su nombre.

El académico francés Henri Mondor manifestó: ""Louis Pasteur no era médico ni cirujano pero nadie ha hecho tanto como él en favor de la medicina y de la cirugía.""

En 1848, Pasteur resolvió el misterio del ácido tartárico (C4H6O6). Esta sustancia parecía existir en dos formas de idéntica composición química pero con propiedades diferentes, dependiendo de su origen: el ácido tartárico proveniente de seres vivos (por ejemplo, el que existe en el vino) era capaz de polarizar la luz, mientras que el producido sintéticamente no lo hacía a pesar de contar con la misma fórmula química.

Pasteur examinó al microscopio cristales diminutos de sales formadas a partir de ácido tartárico sintetizado en el laboratorio, y observó algo muy curioso: había cristales de dos tipos distintos, ambos casi exactamente iguales pero con simetría especular, como nuestras manos. La composición era la misma, pero la forma en la que los átomos se asociaban podía tomar dos formas diferentes y simétricas: mientras una forma polarizaba la luz a la derecha, la otra la polarizaba a la izquierda, una era dextrógira y otra levógira.

Más curioso aún fue que, cuando examinó cristales formados a partir de ácido tartárico natural, solo eran de uno de los dos tipos —los seres vivos producían el ácido de una manera en la que solo se creaba uno de ellos: el que polarizaba la luz a la derecha—. Este hallazgo le valió al joven químico la concesión de la "Legión de Honor", a los 30 años de edad.

Algunos de sus contemporáneos, incluido el eminente químico alemán Justus von Liebig, insistían en que la fermentación era un proceso químico y que no requería la intervención de ningún organismo. Con la ayuda de un microscopio, Pasteur descubrió que, en realidad, intervenían dos organismos —dos variedades de levaduras— que eran la clave del proceso. Uno producía alcohol y el otro, ácido láctico, que agriaba el vino.

Utilizó un nuevo método para eliminar los microorganismos que pueden degradar el vino, la cerveza o la leche, después de encerrar el líquido en cubas bien selladas y elevando su temperatura hasta los 44 grados centígrados durante un tiempo corto. A pesar del rechazo inicial de la industria ante la idea de calentar vino, un experimento controlado con lotes de vino calentado y sin calentar demostró la efectividad del procedimiento. Había nacido así la pasteurización, el proceso que actualmente garantiza la seguridad de numerosos productos alimenticios del mundo.

Demostró que todo proceso de fermentación y descomposición orgánica se debe a la acción de organismos vivos y que el crecimiento de los microorganismos en caldos nutritivos no era debido a la generación espontánea. Para demostrarlo, expuso caldos hervidos en matraces provistos de un filtro que evitaba el paso de partículas de polvo hasta el caldo de cultivo, simultáneamente expuso otros matraces que carecían de ese filtro, pero que poseían un cuello muy alargado y curvado que dificultaba el paso del aire, y por ello de las partículas de polvo, hasta el caldo de cultivo. Al cabo de un tiempo observó que nada crecía en los caldos demostrando así que los organismos vivos que aparecían en los matraces sin filtro o sin cuellos largos provenían del exterior, probablemente del polvo o en forma de esporas. De esta manera Louis Pasteur mostró que los microorganismos no se formaban espontáneamente en el interior del caldo, refutando así la teoría de la generación espontánea y demostrando que todo ser vivo procede de otro ser vivo anterior ("Omne vivum ex vivo"). Este principio científico que fue la base de la teoría germinal de las enfermedades y la teoría celular y significó un cambio conceptual sobre los seres vivos y el inicio de la microbiología moderna. Anunció sus resultados en una gala de la Sorbona en 1864 y obtuvo todo un triunfo.

Luego de resolver el problema de la industria vinícola, Pasteur fue contactado en 1865 por el gobierno francés para que ayudara a resolver la causa de una enfermedad de los gusanos de seda del sur de Francia, la cual estaba arruinando la producción. Pasteur, como él mismo reconoció, no sabía nada de gusanos de seda, sin embargo creía que su ignorancia le significaba una ventaja, pues le permitiría afrontar el problema sin prejuicios. Tras los éxitos obtenidos, confiaba que el método científico sería la herramienta que esclarecería el problema ayudándole a encontrar una solución.

Emprendió una investigación de ensayo y error durante 4 años y tras estudiar meticulosamente las enfermedades del gusano de seda pudo comprender los mecanismos de contagio. Mediante el microscopio descubrió que realmente no tenían una enfermedad, sino dos, provenientes de sendos parásitos que infectaban a los gusanos en su etapa inicial y a las hojas de morera de que se alimentaban: el hongo de la pebrina (nosema bombycis) y diversas bacterias intestinales de la flacidez. Su diagnóstico fue drástico: los huevos y hojas infectadas tenían que ser destruidos y reemplazados por otros nuevos. Mediante una rigurosa selección pudo aislar un grupo sano y cuidó que no se contagiara. Sin embargo, no todo resultaba bien para Pasteur: sufrió una hemorragia cerebral que lo dejó casi hemipléjico del lado izquierdo. En cuanto convaleció publicó un libro en el que detallaba sus ensayos y descubrimientos, conocimiento que otros países no tardaron en aplicar. Ya entonces la industria local de la seda o sericicultura recogía los frutos de su aporte y obtenía ganancias por primera vez en una década, y países como Australia e Italia imitaban ampliamente su técnica de selección.

El descubrimiento de la cura de la enfermedad de los gusanos de seda aumentó su fama y atrajo su atención hacia el resto de enfermedades contagiosas. La idea de que las enfermedades pueden ser trasmitidas entre criaturas vivientes era evidente en las epidemias, como el brote de cólera de 1854 en la calle Broad, Londres, que cobró la vida de 500 personas en un escaso radio de 200 metros. Mediante la interrogación de los infectados y el seguimiento epidemiológico del contagio, John Snow logró identificar el origen del brote una fuente de agua pública. Snow convenció a las autoridades de que clausuraran el pozo y la epidemia cesó. No obstante, la idea de una enfermedad contagiosa no resultaba obvia para la población, pues chocaba con el pensamiento de la época. La pieza que faltaba para dar coherencia a esta línea de pensamiento y resolver sus puntos débiles e inexplicables era descubrir qué era exactamente el transmisor de la enfermedad.

En estas circunstancias demostró experimentalmente y desarrolló la teoría germinal de las enfermedades infecciosas, según la cual toda enfermedad infecciosa tiene su causa (etiología) en un ente vivo microscópico con capacidad para propagarse entre las personas, además de ser el causante de procesos químicos como la descomposición y la fermentación, y su causa no provenía de adentro del cuerpo debido a un desequilibrio de humores como se creía tradicionalmente. Su teoría fue controvertida e impopular: resultaba ridículo pensar que algo insignificantemente pequeño hasta lo invisible pudiese ocasionar la muerte de seres mucho más «fuertes».

Uno de los más famosos cirujanos que siguió sus consejos fue el británico Joseph Lister, quien desarrolló las ideas de Pasteur y las sistematizó en 1865. Lister es considerado hoy el padre de la antisepsia moderna, y realizó cambios radicales en el modo en el que se realizaban las operaciones: los doctores debían lavarse las manos y utilizar guantes, el instrumental quirúrgico debía esterilizarse justo antes de ser usado, había que limpiar las heridas con disoluciones de ácido carbólico (que mataba los microorganismos). Antes de Lister y Pasteur, pasar por el quirófano era, en muchos casos, una sentencia de gangrena y muerte. El propio Pasteur, en 1871 sugirió a los médicos de los hospitales militares que hirvieran el instrumental y los vendajes. Describió un horno, llamado «horno Pasteur», antecesor del autoclave, útil para esterilizar instrumental quirúrgico y material de laboratorio y en él tuvieron entero apoyo.

En 1880, Pasteur se encontraba realizando experimentos con pollos para determinar los mecanismos de transmisión de la bacteria responsable del cólera aviar que acababa con muchos de ellos. Junto con su ayudante, Charles Chamberland, inoculaba la bacteria ("Pasteurella multocida") a pollos y evaluaba el proceso de la enfermedad.

La historia cuenta que Pasteur iba a tomarse unas vacaciones, y encargó a Chamberland que inoculase a un grupo de pollos un cultivo de la bacteria. Pero Chamberland olvidó hacerlo, y se fue de vacaciones. Cuando ambos volvieron al cabo de un mes, los pollos estaban sin infectar y el cultivo de bacterias continuaba donde lo dejaron, pero muy debilitado. Chamberland inoculó a los pollos de todos modos y los animales no murieron. Desarrollaron algunos síntomas, y una versión leve de la enfermedad, pero sobrevivieron.

El ayudante, abochornado, iba a matar a los animales y empezar de nuevo, cuando Pasteur lo detuvo: la idea de una versión débil de la enfermedad causante de la inmunidad a su símil virulenta era conocida desde 1796 gracias a Edward Jenner y Pasteur estaba al tanto. Expuso a los pollos una vez más al cólera y nuevamente sobrevivieron pues habían desarrollado respuesta inmune. Llamó a esta técnica vacunación en honor a Edward Jenner. La diferencia entre la vacuna de Jenner y la de ántrax y cólera aviar, es que estas fueron las primeras vacunas de patógenos artificialmente debilitados. A partir de ese momento no hacía falta encontrar bacterias adecuadas para las vacunas, las propias bacterias de la enfermedad podían ser debilitadas y vacunadas.

Pasteur puso este descubrimiento en práctica casi inmediatamente en el caso de otras enfermedades causadas por agentes bacterianos. En 1881, hizo una demostración dramática de la eficacia de su vacuna contra el carbunco, inoculando la mitad de un rebaño de ovejas mientras inyectaba la enfermedad ("Bacillus anthracis") a la otra mitad. Las inoculadas con la vacuna sobrevivieron, el resto, murió.

En sus estudios contra la rabia, utilizaba conejos infectados con la enfermedad, y cuando estos morían secaba su tejido nervioso para debilitar el agente patógeno que la produce, que hoy sabemos que es un virus. En 1885 un niño, Joseph Meister, fue mordido por un perro rabioso cuando la vacuna de Pasteur solo se había probado con unos cuantos perros. El niño iba a morir sin ninguna duda cuando desarrollase la enfermedad, pero Pasteur no era médico, de modo que si lo trataba con una vacuna sin probar suficientemente podía acarrear un problema legal. Sin embargo, tras consultar con sus colegas, el químico se decidió a inocular la vacuna al muchacho. El tratamiento tuvo un éxito absoluto, el niño se recuperó de las heridas y nunca desarrolló la rabia, Pasteur nuevamente fue alabado como héroe.



En respuesta a experiencias desagradables para Pasteur, le rogó a su familia, en 1878, nunca mostrar sus cuadernos de laboratorio a nadie. Después de la muerte del fisiólogo Claude Bernard, tenía uno publicado con notas de estudiantes, y Bernard había dudado de la teoría de la fermentación de Pasteur. Eso obligó a Pasteur a tomar públicamente una posición contra las posiciones de Bernard. Con el fin de no provocar una situación similar a sí mismo, impuso la prohibición de la publicación de sus cuadernos de laboratorio.

En 1964, el último sobreviviente descendiente varón directo Pasteur Vallery, entregó los cuadernos de laboratorio a la Biblioteca Nacional de Francia. Y quedaron disponibles con la muerte de Pasteur Vallery-Radot, en 1971, y prácticamente utilizables solo con el catálogo 1985. En general, son 144 cuadernos, 42 de ellos recortes de periódicos, apuntes de clase, etc. Los 102 cuadernos restantes, son notas de laboratorio reales, y 40 documentos de investigación anual.







</doc>
<doc id="3587" url="https://es.wikipedia.org/wiki?curid=3587" title="Historia de la medicina">
Historia de la medicina

La historia de la medicina es la rama de la historia dedicada al estudio de los conocimientos y prácticas médicas a lo largo del tiempo también es una parte de cultura "es en realidad la historia de los problemas médicos".

Desde sus antiguos orígenes, el ser humano ha tratado de explicarse la realidad y los acontecimientos trascendentales que en ella tienen lugar como la vida, la muerte o la enfermedad. La medicina tuvo sus comienzos en la prehistoria, la cual también tiene su propio campo de estudio conocido como antropología médica. Se utilizaban plantas, minerales y partes de animales, en la mayoría de las veces estas sustancias eran utilizadas en rituales mágicos por chamanes, sacerdotes, magos, brujos, animistas, espiritualistas o adivinos. Las primeras civilizaciones y culturas humanas basaron su práctica médica en dos pilares aparentemente opuestos: un empirismo primitivo y de carácter pragmático (aplicado fundamentalmente al uso de hierbas o remedios obtenidos de la naturaleza) y una medicina mágico-religiosa, que recurrió a los dioses para intentar comprender lo inexplicable.

Los datos de la Edad Antigua encontrados muestran la medicina en diferentes culturas como la medicina Āyurveda de la India, el antiguo Egipto, la antigua China y Grecia. Uno de los primeros reconocidos personajes históricos es Hipócrates quien es también conocido como el padre de la medicina; supuestamente descendiente de Asclepio, por su familia: los Asclepíades de Bitinia; y Galeno. Posteriormente a la caída de Roma en la Europa Occidental la tradición médica griega disminuyó. En el siglo V a. C. Alcmeón de Crotona dio inicio a una etapa basada en la técnica ("tekhné"), definida por la convicción de que la enfermedad se originaba por una serie de fenómenos naturales susceptibles de ser modificados o revertidos. Ese fue el germen de la medicina moderna, aunque a lo largo de los siguientes dos milenios surgirán otras muchas corrientes (mecanicismo, vitalismo...) y se incorporarán modelos médicos procedentes de otras culturas con una larga tradición médica, como la china.

En la segunda mitad del siglo VIII, los musulmanes tradujeron los trabajos de Galeno y Aristóteles al arábigo por lo cual los doctores islámicos se indujeron en la investigación médica. Algunas figuras islámicas importantes fueron Avicena, que junto con Hipócrates se le ha sido mencionado también como el padre de la medicina, Abulcasis el padre de la cirugía, Avenzoar el padre de la cirugía experimental, Ibn al-Nafis padre de la fisiología circulatoria, Averroes y Rhazes, padre de la pediatría. Ya para finales de la Edad Media posterior a la peste negra, importantes figuras médicas emergieron de Europa como William Harvey y Grabiele Fallopio.

En el pasado la mayor parte del pensamiento médico se debía a lo que habían dicho anteriormente otras autoridades y se veía del modo tal que si fue dicho permanecía como la verdad. Esta forma de pensar fue sobre todo sustituida entre los siglos XIV y XV, tiempo de la pandemia de la peste negra. Asimismo, durante los siglos XV y XVI, la anatomía atravesó un gran avance gracias a la aportación de Leonardo Da Vinci, quien proyectó junto con Marcantonio della Torre, un médico anatomista de Pavía, uno de los primeros y fundamentales tratados de anatomía, denominado "Il libro dell'Anatomia". Aunque la mayor parte de las más de 200 ilustraciones sobre el cuerpo humano que realizó Da Vinci para este tratado desaparecieron, se pueden observar algunas de las que sobrevivieron en su "Tratado sobre la pintura".

A partir del siglo XIX se vieron grandes cantidades de descubrimientos. Investigaciones biomédicas premodernas desacreditaron diversos métodos antiguos como el de los cuatro humores de origen griego, pero es en el siglo XIX, con los avances de Leeuwenhoek con el microscopio y descubrimientos de Robert Koch de las transmisiones bacterianas, cuando realmente se vio el comienzo de la medicina moderna. El descubrimiento de los antibióticos que fue un gran paso para la medicina. Las primeras formas de antibióticos fueron las drogas sulfas. Actualmente los antibióticos se han vuelto muy sofisticados. Los antibióticos modernos puede atacar localizaciones fisiológicas específicas, algunas incluso diseñadas con compatibilidad con el cuerpo para reducir efectos secundarios. El Dr. Edward Jenner descubrió el principio de la vacunación al ver que las ordeñadoras de vacas que contraían el virus de vaccinia al tener contacto con las pústulas eran inmunes a la viruela. Años después Louis Pasteur le otorgó el nombre de vacuna en honor al trabajo de Jenner con las vacas. A finales del siglo XIX, los médicos franceses Auguste Bérard y Adolphe Marie Gubler resumían el papel de la medicina hasta ese momento: «Curar pocas veces, aliviar a menudo, consolar siempre».

La medicina del siglo XX, impulsada por el desarrollo científico y técnico, se fue consolidando como una disciplina más resolutiva, aunque sin dejar de ser el fruto sinérgico de las prácticas médicas experimentadas hasta ese momento. La medicina basada en la evidencia se apoya en un paradigma fundamentalmente biologicista, pero admite y propone un modelo de salud-enfermedad determinado por factores biológicos, psicológicos y socioculturales. La herbolaria dio lugar a la farmacología: de los diversos fármacos derivados de plantas como la atropina, warfarina, aspirina, digoxina, taxol etc.; el primero fue la arsfenamina descubierta por Paul Ehrlich en 1908 después de observar que las bacterias morían mientras las células humanas no lo hacían.

En el siglo XXI, el conocimiento sobre el genoma humano ha empezado a tener una gran influencia, razón por la que se han identificado varios padecimientos ligados a un gen en específico en el cual la biología celular y la genética se enfocan para la administración en la práctica médica, aun así, estos métodos aún están en su infancia.

Para hablar de los orígenes de la medicina, es preciso hacerlo antes de los rastros dejados por la enfermedad en los restos humanos más antiguos conocidos y, en la medida en que eso es posible, de las huellas que la actividad médica haya podido dejar en ellos.

Marc Armand Ruffer (1859-1917), médico y arqueólogo británico, definió la paleopatología como la ciencia de las enfermedades que pueden ser demostradas en restos humanos de gran antigüedad.

Dentro de las patologías diagnosticadas en restos de seres humanos datados en el Neolítico se incluyen anomalías congénitas como la acondroplasia, enfermedades endocrinas (gigantismo, enanismo, acromegalia, gota), enfermedades degenerativas (artritis, espondilosis) e incluso algunos tumores (osteosarcomas), principalmente identificados sobre restos óseos. Entre los vestigios arqueológicos de los primeros "Homo sapiens" es raro encontrar individuos por encima de los cincuenta años por lo que son escasas las evidencias de enfermedades degenerativas o relacionadas con la edad. Abundan, en cambio, los hallazgos relacionados con enfermedades o procesos traumáticos, fruto de una vida al aire libre y en un entorno poco domesticado. 

Una de las hipótesis más aceptadas sobre el surgimiento del "Mycobacterium" (el germen causante de esta enfermedad) propone que el antepasado común denominado "Marchaicum, ""bacteria libre", habría dado origen a los modernos "Mycobacterium", incluido el "M. tuberculosis". La mutación se habría producido durante el Neolítico, en relación con la domesticación de bóvidos salvajes en África. Las primeras evidencias de tuberculosis en humanos se han encontrado en restos óseos del Neolítico, en un cementerio próximo a Heidelberg, supuestamente pertenecientes a un adulto joven, y datados en torno a 5000 años antes de nuestra era. También se han encontrado datos sugestivos de tuberculosis en momias egipcias datadas entre los años 3000 y 2400 a. C.

En cuanto a los primeros tratamientos médicos de los que se tiene constancia hay que hacer mención a la práctica de la trepanación (perforación de los huesos de la cabeza para acceder al encéfalo). Existen hallazgos arqueológicos de cráneos con signos evidentes de trepanación datados del período Neolítico, hace entre 4000 y 2400 años, por razones que se supone pueden ser diversas. Restos óseos trepanados con un excelente nivel de conservación, obtenidos por excavaciones arqueológicas realizadas en Ensisheim (Alsacia), permiten suponer que ya se practicaban intervenciones quirúrgicas craneales más de 7000 años atrás. Existen además otras evidencias de cirugías craneales antiguas obtenidas de excavaciones en la cuenca del Danubio, Dinamarca, Polonia, Francia, Reino Unido, Suecia, España o Perú.

La etnología, por otra parte, extrapola los descubrimientos realizados en culturas y civilizaciones preindustriales que han conseguido sobrevivir hasta nuestros días para comprender o deducir los modelos culturales y conductuales de las primeras sociedades humanas.

En las sociedades sedentarias neolíticas, había un personaje que tenía la función de un líder espiritual, es decir, curaba a los heridos de caza apoyado por la influencia divina y ayudaba a la comunidad a manipular el ánima para la caza. Estos sanadores suelen ocupar una posición social privilegiada y en muchos casos se subespecializan para tratar diferentes enfermedades, como se evidenció entre los mexicas, entre los que podía encontrarse el médico chamán "(ticitl)" más versado en procedimientos mágicos, el "teomiquetzan", experto sobre todo en heridas y traumatismos producidos en combate, o la "tlamatlquiticitl", comadrona encargada del seguimiento de los embarazos. Por el contrario, las sociedades nómadas, recolectoras y cazadoras, no poseen la figura especializada del sanador y cualquier miembro del grupo puede ejercer esta función, de manera principalmente empírica. Solían considerar al enfermo como un «impuro», especialmente ante procesos patológicos incomprensibles, acudiendo a la explicación divina, como causa de los mismos.

El enfermo lo es porque ha transgredido algún tabú que ha irritado a alguna deidad, sufriendo por ello el «castigo» correspondiente, en forma de enfermedad.

La evolución de la medicina en estas sociedades arcaicas encuentra su máxima expresión en las primeras civilizaciones humanas: Mesopotamia, Egipto, América precolombina, India y China. En ellas se expresaba esa doble vertiente, empírica y mágica, característica de la medicina primitiva.

La «tierra entre ríos» albergó desde el Neolítico a algunas de las primeras y más importantes civilizaciones humanas (sumeria, acadia, asiria y babilónica).

En torno al 4000 a. C. se establecieron en este territorio las primeras ciudades sumerias y durante más de tres mil años florecieron estas cuatro culturas, caracterizadas por el empleo de un lenguaje escrito (cuneiforme) que se ha conservado hasta nuestros días en numerosas tablillas y grabados.

Es precisamente esa capacidad de transmisión de la información, científica, social y administrativa, a través de un sistema perdurable lo que determinó el desarrollo cultural de los primeros asentamientos sumerios, y lo que permitió a los historiadores posteriores reconstruir su legado.

El principal testimonio de la forma de vida de las civilizaciones mesopotámicas se encuentra en el código de Hammurabi, una recopilación de leyes y normas administrativas recogidas por el rey babilónico Hammurabi, tallado en un bloque de diorita de unos 2,50 m de altura por 1,90 m de base y colocado en el templo de Sippar.
En él se determinan a lo largo de trece artículos, las responsabilidades en que incurren los médicos en el ejercicio de su profesión, así como los castigos dispuestos en caso de mala praxis.

Gracias a este texto y a un conjunto de unas 30  000 tablillas recopiladas por Asurbanipal (669-626 a. C.), procedentes de la biblioteca descubierta en Nínive por Henry Layarde en 1841 ha podido intuirse la concepción de la salud y la enfermedad en este período, así como las técnicas médicas empleadas por sus profesionales sanadores.

De todas esas tablillas unas 800 están específicamente dedicadas a la medicina, y entre ellas se cuenta la descripción de la primera receta conocida. Lo más llamativo es la intrincada organización social en torno a tabúes y obligaciones religiosas y morales, que determinaban el destino del individuo. Primaba una concepción sobrenatural de la enfermedad: esta era un castigo divino impuesto por diferentes demonios tras la ruptura de algún tabú.

De este modo, lo primero que debía hacer el médico era identificar cuál de los aproximadamente 6000 posibles demonios era el causante del problema.

Para ello empleaban técnicas adivinatorias basadas en el estudio del vuelo de las aves, de la posición de los astros o del hígado de algunos animales. A la enfermedad se la denominaba "shêrtu". Pero esta palabra asiria significaba, también, pecado, impureza moral, ira divina y castigo.

Cualquier dios podía provocar la enfermedad mediante la intervención directa, el abandono del hombre a su suerte, o a través de encantamientos realizados por hechiceros.

Durante la curación todos estos dioses podían ser invocados y requeridos a través de oraciones y sacrificios para que retirasen su nociva influencia y permitiesen la curación del hombre enfermo.
De entre todo el panteón de dioses Ninazu era conocido como «el señor de la medicina» por su especial relación con la salud.

El diagnóstico incluía, entonces, una serie de preguntas rituales para determinar el origen del mal:

Y los tratamientos no escapaban a este patrón cultural: exorcismos, plegarias y ofrendas son rituales de curación frecuentes que buscan congraciar al paciente con la divinidad o librarlo del demonio que le acecha.

No obstante, también es de destacar un importante arsenal herborístico recogido en varias tablillas: unas doscientas cincuenta plantas curativas se recogen en ellas, así como el uso de algunos minerales y de varias sustancias de origen animal.

El nombre genérico para el médico era "asû", pero pueden encontrarse algunas variantes como el "bârû", o adivinador encargado del interrogatorio ritual; el "âshipu", especializado en exorcismos; o el "gallubu", cirujano-barbero de casta inferior que anticipa la figura del barbero medieval europeo, y que encuentra homólogo en otras culturas (como el "Tepatl" azteca). Este sajador se encargaba de sencillas operaciones quirúrgicas (extracción de dientes, drenaje de abscesos, flebotomías...).

En el Museo del Louvre puede contemplarse un sello babilónico de alabastro de más de cuatro mil años de antigüedad con una leyenda en la que se menciona el primer nombre conocido de un médico: "¡Oh, Edinmungi, servidor del dios Girra, protector de las parturientas, Ur-Lugal-edin-na, el médico, es tu servidor!" Este sello, empleado para firmar documentos y recetas, representa dos cuchillos rodeados de plantas medicinales.

La invasión persa del año 539 a. C. marcó el final del imperio babilónico, pero hay que retroceder de nuevo unos tres mil años para hacer mención a la otra gran civilización del Próximo Oriente antiguo poseedora de un lenguaje escrito y de una cultura médica notablemente avanzada: la egipcia.

Durante los tres mil años largos de historia del Antiguo Egipto se desarrolló una larga, variada y fructífera tradición médica.

Heródoto llegó a llamar a los egipcios el pueblo de los "sanísimos", debido al notable sistema sanitario público que poseía, y a la existencia de «un médico para cada enfermedad» (primera referencia a la especialización en campos médicos.

En la "Odisea" de Homero se dice de Egipto que es un país «cuya fértil tierra produce muchísimos fármacos» y donde «cada hombre es un médico». La medicina egipcia mantiene en buena medida una concepción mágica de la enfermedad, pero comienza a desarrollar un interés práctico por campos como la anatomía, la salud pública o el diagnóstico clínico que suponen un avance importante en la forma de comprender el modo de enfermar.

El clima de Egipto ha favorecido la conservación de numerosos papiros con referencias médicas redactados con escritura jeroglífica (del griego "hierós:" ‘sagrado’, y "glypho:" ‘grabar’) o hierática:


Este papiro incluye la primera referencia escrita acerca de los tumores.

La información médica contenida en el papiro Edwin Smith incluye el examen, el diagnóstico, el tratamiento y el pronóstico de numerosas patologías, con especial dedicación a diversas técnicas quirúrgicas y descripciones anatómicas, obtenidas en el curso de los procesos de embalsamamiento y momificación de los cadáveres.

En este papiro se establecen por primera vez tres grados de pronóstico, de modo similar al de la medicina moderna: "favorable", "dudoso" y "desfavorable".


Dentro de las numerosas descripciones anatómicas ofrecidas por los textos egipcios hay que destacar las relativas al corazón y al aparato circulatorio, recogidas en el tratado «El secreto del médico: conocimiento del corazón», incorporado en el papiro Edwin Smith:

Las primeras referencias pertenecen a la temprana época monárquica (2700 a. C.). Según Manetón, sacerdote e historiador egipcio, "Atotis" o Aha, faraón de la primera dinastía, practicó el arte de la medicina, escribiendo tratados sobre la técnica de abrir los cuerpos.

De esa época datan también los escritos de Imhotep, visir del faraón Necherjet Dyeser, sacerdote, astrónomo, médico y primer arquitecto del que se tiene noticia. Tal fue su fama como sanador que acabó deificado, considerándose el dios egipcio de la medicina.

Otros médicos notorios del Imperio Antiguo (del 2500 al 2100 a. C.) fueron "Sachmet" (médico del faraón Sahura) o "Nesmenau", director de una de las "casas de la vida", templos dedicados a la protección espiritual del faraón, pero también protohospitales en los que se enseñaba a los alumnos de medicina mientras se prestaba atención a los enfermos.

Varios dioses velan por el ejercicio de la medicina: Thot, dios de la sabiduría, Sejmet, diosa de la misericordia y la salud, Duau y Horus, protectores de los especialistas en medicina ocular, Tueris, Heget y Neit, protectores de las embarazadas en el momento del parto, o el mismo Imhotep tras ser divinizado.

El papiro Ebers describe a tres tipos de médicos en la sociedad egipcia: los sacerdotes de Sejmet, mediadores con la divinidad y conocedores de un amplio surtido de drogas, los médicos civiles "(sun-nu)", y los magos, capaces de realizar curaciones mágicas.

Una clase de ayudantes, denominados "ut", que no se consideran sanadores, asistían en gran número a la casta médica, adelantando el cuerpo de enfermería.

Existe constancia de instituciones médicas en el antiguo Egipto como mínimo a partir de la primera dinastía.

En estas instituciones, ya en la decimonovena dinastía, sus empleados disponían de ciertas ventajas (seguro médico, pensiones y licencia por enfermedad), siendo su horario laboral de ocho horas.

También fue egipcia la primera médica conocida, Peseshet, quien ejerció su actividad durante la cuarta dinastía; además de su rol de supervisión, Peseshet evaluaba a parteras en una escuela médica en Sais.

La mayor parte del conocimiento que se tiene de la medicina hebrea durante el I milenio a. C. proviene del Antiguo Testamento de la Biblia. En él se citan varias leyes y rituales relacionados con la salud, tales como el aislamiento de personas infectadas ("Levítico" 13:45-46), lavarse tras manipular cuerpos difuntos ("Números" 19:11-19) y el entierro de los excrementos lejos de las viviendas ("Deuteronomio" 23:12-13).

Los mandatos incluyen profilaxis y supresión de epidemias, supresión de enfermedades venéreas y prostitución, cuidado de la piel, baños, alimentación, vivienda y ropas, regulación del trabajo, sexualidad, disciplina, etc.

Muchos de estos mandatos tienen una base más o menos racional, tales como la circuncisión, la supuesta impureza de las parturientas, impureza de la mujer durante la menstruación, las leyes relativas a la alimentación (prohibición de la sangre y del cerdo), el descanso del Sabbat, el aislamiento de los enfermos de gonorrea y de lepra, y la higiene del hogar.

El monoteísmo hebreo hizo que la medicina fuera teúrgica: Yahvé era el responsable tanto de la salud como de la enfermedad. 

La enfermedad puede ser también una prueba divina como en el caso de Job: «Entonces salió Satanás de la presencia de Jehová, e hirió a Job con una sarna maligna desde la planta del pie hasta la coronilla de la cabeza» (Job 2:7). Los hebreos adoptaron preceptos médicos de los pueblos con los cuales tuvieron contacto: Mesopotamia, Egipto y Grecia. En el Talmud se habla del número total de los huesos del hombre. Los hebreos notaron que en el hombre faltaba el báculo (el hueso interno del pene) típico en todos los animales machos. El médico era llamado "rophe", y el circuncidador era el "uman".

Hacia el año 2000 a. C. en la ciudad de Mohenjo-Daro (en la actual Pakistán), todas las casas disponían de cuarto de baño y muchas de ellas también poseían letrinas. Esta ciudad es considerada la más avanzada de la Antigüedad en lo que a higiene se refiere. Esa cultura del valle del Indo (Pakistán) desapareció sin dejar herencia en las culturas posteriores de la India.

El periodo védico (entre el siglo XVI y el VIII a. C.) fue una era de migraciones y guerras, que dejó textos como el "Rig-veda" (el texto más antiguo de la India, de mediados del II milenio a. C.), pero demuestra la ausencia completa de conocimiento médico.

En el período brahmánico (siglo VI a. C. a X d. C.) se formularon las bases de un sistema médico. Las enfermedades eran entendidas por los hinduistas como karma, un castigo de los dioses por las actividades de la persona. Pero, a pesar de su componente mágico-religioso, la medicina hinduista aiurveda realizó algunos aportes a la medicina en general, como por ejemplo, el descubrimiento de que la orina de los pacientes diabéticos es más dulce que la de los pacientes que no padecen esta patología.

Para poder diagnosticar una enfermedad, los médicos aiurvedas realizaban una exploración minuciosa a los pacientes, en la que se incluía la palpación y la auscultación. Una vez emitido el diagnóstico, el médico daba una serie de indicaciones dietéticas.

Los dos textos más famosos de la medicina tradicional india (aiurveda) son el "Cháraka-samjita" (siglo II a. C.) y el "Súsruta-samjita" (siglo III d. C.).

La primera escuela, Charaka, se basa en la mitología, pues dice que una divinidad bajó a la tierra y al encontrarse con tantas enfermedades dejó un escrito sobre como prevenirlas y tratarlas. Más adelante esta escuela se basaría en la creencia de que ni la salud ni la enfermedad son parte de lo que las personas deben vivir y que con esfuerzo la vida se puede alargar. Esta escuela es parecida a la medicina moderna en el ámbito de tratar las enfermedades crónicas. Uno de los mayores esfuerzos de esta escuela era mantener la salud del cuerpo y la mente, ya que, según sus creencias, se encontraban en constantes comunicación.

Según Cháraka, ni la salud ni la enfermedad están predeterminadas (lo cual contradecía la doctrina del karma predominante en el hinduismo de la época), y la vida puede ser alargada con algo de esfuerzo.

La segunda escuela, Súshruta, basó sus conocimientos en especialidades, técnicas conformadas para curar, mejorar y alargar la vida de las personas.

La medicina tradicional china surge como una forma fundamentalmente taoísta de entender la medicina y el cuerpo humano.

El tao es el origen del universo, que se sostiene en un equilibrio inestable fruto de dos fuerzas primordiales: el yin (la tierra, el frío, lo femenino) y el yang (el cielo, el calor, lo masculino), capaces de modificar a los cinco elementos de que está hecho el universo: agua, tierra, fuego, madera y metal.

Esta concepción cosmológica determina un modelo de enfermedad basado en la ruptura del equilibrio, y del tratamiento de la misma en una recuperación de ese equilibrio fundamental.

Uno de los primeros vestigios de esta medicina lo constituye el Nei jing, que es un compendio de escritos médicos datados alrededor del año 2600 a. C. y que representará uno de los pilares de la medicina tradicional china en los cuatro milenios siguientes.

Una de las primeras y más importantes revisiones se atribuyen al emperador amarillo, Huang Di.
En este compendio se encuentran algunos conceptos médicos interesantes para la época, especialmente de índole quirúrgica, aunque la reticencia en estudiar cadáveres humanos parece haber restado eficacia a sus métodos.

La medicina china desarrolló una disciplina a caballo entre la medicina y la cirugía denominada acupuntura: Según esta disciplina la aplicación de agujas sobre alguno de los 365 puntos de inserción (o hasta 600 según las escuelas) restauraría el equilibrio perdido entre el yin y el yang.

Varios historiadores de la medicina se han cuestionado el motivo por el que la medicina china quedó anclada en esta visión cosmológica sin alcanzar el nivel de ciencia técnica a pesar de su larga tradición y su amplio cuerpo de conocimientos, frente al modelo grecorromano clásico.

El motivo, según estos autores, se encontraría en el desarrollo del concepto de "logos" por parte de la cultura griega, como una explicación natural desligada de todo modelo cosmológico "(mythos)".

Con la llegada de la dinastía Han (220-206 d. C.), y con el apogeo del taoísmo (siglo II a VII d. C.), se empieza a enfatizar los remedios vegetales y minerales, los venenos, la dietética, así como las técnicas respiratorias y el ejercicio físico.

De esta dinastía, y hasta la dinastía Sui (siglo VI) destacaron los siguientes sabios:


Durante las dinastías Sui (581-618) y Tang (618-907) la medicina tradicional china vive grandes momentos.

En el año 624 fue creado el Gran Servicio Médico, desde donde se organizaban los estudios y las investigaciones médicas.

De esta época nos han llegado descripciones muy precisas de multitud de enfermedades, tanto infecciosas como carenciales, tanto agudas como crónicas.

Y determinadas referencias dejan entrever un gran desarrollo en especialidades como la cirugía, la ortopedia o la odontología.

El médico más destacable de este periodo fue Sun Simiao (581-682).

Durante la dinastía Song (960-1270) aparecen sabios multidisciplinares como Chen Kua, pediatras como Qian Yi, especialistas en medicina legal como Song Ci, o acupuntores como Wang Wei Yi.

Poco después, antes de la llegada de la dinastía Ming, cabe destacar a Hu Zheng Qi Huei (especialista en dietética), y a Hua Shuou (o Bowen, autor de una relevante revisión del clásico "Nan Jing").

Durante la Ming (1368-1644) aumentaron las influencias de otras latitudes, médicos chinos exploraron nuevos territorios, y médicos occidentales llevaron sus conocimientos a la China.

Una de las grandes obras médicas de la época fue el "Gran Tratado de Materia Médica" de Li Shizhen.

También cabe citar al acupuntor Yang Jizou.

A partir del siglo XVII y XVIII, las influencias recíprocas con Occidente y sus avances técnicos, y con las diferentes filosofías imperantes (por ejemplo el comunismo), acaban de conformar la actual medicina china.

El vasto territorio del continente americano acogió durante todo el período histórico previo a su descubrimiento por Europa a todo tipo de sociedades, culturas y civilizaciones, por lo que pueden encontrarse ejemplos de la medicina neolítica más primitiva, de chamanismo, y de una medicina casi técnica alcanzada por los mayas, los incas y los aztecas durante sus épocas de máximo esplendor.

Existen, sin embargo, algunas similitudes, como una concepción mágico-teúrgica de la enfermedad como castigo divino, y la existencia de individuos especialmente vinculados a los dioses, capaces de ejercer las funciones de sanador.

Entre los incas se encontraban médicos del Inca "(hampi camayoc)" y médicos del pueblo "(ccamasmas)", con ciertas habilidades quirúrgicas fruto del ejercicio de sacrificios rituales, así como con un vasto conocimiento herborístico.

Entre las plantas medicinales más usadas se encontraban la coca "(Erytroxilon coca)", el yagé "(Banisteriopsis caapi)", el yopo "(Piptadenia peregrina)", el pericá "(Virola colophila)", el tabaco "(Nicotiana tabacum)", el yoco "(Paulinia yoco)" o el curare y algunas daturas como agentes anestésicos.

El médico maya "(ah-men)" era propiamente un sacerdote especializado que heredaba el cargo por linaje familiar, aunque también cabe destacar el desarrollo farmacológico, reflejado en las más de cuatrocientas recetas compiladas por R. L. Roys.

La civilización azteca desarrolló un cuerpo de conocimientos médicos extenso y complejo, del que quedan noticias en dos códices: el "Códice Sahagún" y el "Códice Badiano".

Este último, de Juan Badiano, compila buena parte de las técnicas conocidas por el indígena Martín de la Cruz (1552), que incluye un curioso listado de síntomas que presentan los individuos que van a morir.

Cabe destacar el hallazgo de la primera escuela de medicina en Monte Albán, próximo a Oaxaca, datada en torno al año 250 de nuestra era, donde se han encontrado unos grabados anatómicos entre los que parece encontrase una intervención de cesárea, así como la descripción de diferentes intervenciones menores, como la extracción de piezas dentarias, la reducción de fracturas o el drenaje de abscesos.

Entre los aztecas se establecía una diferencia entre el médico empírico (de nuevo el equivalente del «barbero» tardomedieval europeo) o "tepatl" y el médico chamán "(ticitl)", más versado en procedimientos mágicos.

Incluso algunos sanadores se podían especializar en áreas concretas encontrándose ejemplos en el códice Magliabecchi de fisioterapeutas, comadronas o cirujanos.

El traumatólogo o »componedor de huesos» era conocido como "teomiquetzan", experto sobre todo en heridas y traumatismos producidos en combate.

La "tlamatlquiticitl" o comadrona hacía seguimientos del embarazo, pero podía realizar embriotomías en caso de aborto.

Es de destacar el uso de oxitócicos (estimulantes de la contracción uterina) presentes en una planta, el cihuapatl.

Francisco López de Gómara, en su "Historia de Indias", relata también las diferentes prácticas médicas con las que se encontraron los conquistadores españoles.

<div style="float:center; margin: 3mm; padding: 1mm; width: 700px; border: 0px solid;">

</center

De nuevo 3000 años antes de nuestra era, en la isla de Creta surge una civilización que supera el Neolítico, empleando los metales, construyendo palacios y desarrollando una cultura que culminará con el desarrollo de las civilizaciones minoica y micénica.

Estas dos culturas son la base de la Grecia Clásica, de influencia capital en el desarrollo de la ciencia moderna en general y de la medicina en particular.

El desarrollo de los conceptos de la "physis" (naturaleza) y del "logos" (razonamiento, ciencia) suponen el punto de partida de una concepción de la enfermedad como una alteración de mecanismos naturales, susceptible, por tanto, de ser investigada, diagnosticada y tratada, a diferencia del modelo mágico-teológico determinista predominante hasta ese momento.

Surge el germen del método científico, a través de la "autopsia" (‘visión por uno mismo’) y de la hermenéutica (interpretación).

El término clásico acuñado por los griegos para definir la medicina, "tekhne iatriké" (la técnica o el arte de curar), o los empleados para nombrar al «médico de las enfermedades» "(ietèr kakôn)" y al cirujano ("kheirourgein", ‘trabajador de las manos’) sintetizan ese concepto de la medicina como ciencia.

El ser humano comienza a dominar la naturaleza y se permite (incluso a través de sus propios mitos) retar a los dioses (Anquises, Peleo, Licaón u Odiseo).

La obra griega escrita más antigua que incluye conocimientos sobre medicina son los poemas homéricos: la "Ilíada" y la "Odisea".

En la primera se describe, por ejemplo, cómo Fereclo es lanceado por Meriones en la nalga, «cerca de la vejiga y bajo el hueso del pubis», o el tratamiento que recibe el rey Menelao tras ser alcanzado por una flecha en la muñeca durante el asedio a Troya: el cirujano resulta ser el médico Macaón, hijo de Asclepio, dios de la medicina griega, educado en la ciencia médica por el centauro Quirón.

De su nombre deriva "esculapio", un antiguo sinónimo de médico, y el nombre de Hygea, su hija, sirvió de inspiración para la actual rama de la medicina preventiva denominada higiene.

A Asclepio se atribuye también el origen de la Vara de Esculapio, símbolo médico universal en la actualidad.

En el siglo VI a. C. Alcmeón de Crotona, filósofo pitagórico dedicado a la medicina, desarrolló una teoría de la salud que comenzaba a dejar atrás los rituales sanadores pretécnicos que hasta ese momento cimentaban la medicina griega: la plegaria "(eukhé)" a los dioses de la salud (Asclepio, Artemisa, Apolo, Palas Atenea, Hygea...), las danzas o ritos sanadores (Dionisos) y el conocimiento empírico de remedios básicos.

En Crotona, Cos o Cnido comenzaron a florecer escuelas médicas seguidoras del concepto de Alcmeón, basado en la ciencia natural, o fisiología.

Pero la figura médica por excelencia de la cultura griega clásica es Hipócrates. De este médico se conoce, gracias a la biografía escrita por Sorano de Éfeso unos 500 años después de su muerte, que nació en Cos en torno al año 460 a. C. y su vida coincide con la edad de oro de la civilización helena y su novedosa cosmovisión de la razón frente al mito. Galeno y posteriormente la escuela alejandrina lo consideraron «el médico perfecto», por lo que ha sido aclamado clásicamente como el "Padre de la Medicina Moderna".

En realidad la obra atribuida a Hipócrates es una compilación de unos cincuenta tratados "(Corpus Hippocraticum)", elaborados a lo largo de varios siglos (la mayor parte entre los siglos V y IV a. C.), por lo que es más adecuado hablar de una «escuela hipocrática», fundada sobre los principios del denominado juramento hipocrático. Los campos médicos abarcados por Hipócrates en sus tratados incluyen la anatomía, la medicina interna, la higiene, la ética médica o la dietética.

En su teoría de los cuatro humores, Hipócrates despliega un concepto, próximo a la medicina oriental, de salud como equilibrio entre los cuatro humores del cuerpo, y de enfermedad "(nosas)" como alteración (exceso o defecto) de alguno de ellos. Sobre esta base teórica desarrolla entonces un cuerpo teórico de fisiopatología (cómo se enferma) y terapéutica (cómo se cura) basado en el ambiente, el aire, o la alimentación (la "dietética").

Los siguientes dos siglos (IV y III) supusieron el despegue de los movimientos filosóficos griegos.
Aristóteles aprendió medicina de su padre, pero no consta un ejercicio asiduo de esta disciplina. En cambio, su escuela peripatética fue la cuna de varios médicos importantes de la época: Diocles de Caristo, Praxágoras de Cos o Teofrasto de Ereso, entre otros.

En torno al año 300 a. C. Alejandro Magno funda Alejandría, la ciudad que en poco tiempo se convertiría en el referente cultural del Mediterráneo y Oriente Próximo. La escuela alejandrina compiló y desarrolló todos los conocimientos sobre medicina (como de muchas otras disciplinas) conocidos de la época, contribuyendo a formar algunos destacados médicos. Algunas fuentes apuntan la posibilidad de que los Ptolomeos pusieran a su disposición reos condenados a muerte para practicar vivisecciones.

Uno de los médicos más notables de la escuela alejandrina fue Erasístrato de Ceos, descubridor del colédoco (conducto de desembocadura de la bilis en el intestino delgado), y del sistema de circulación portal (un sistema venoso que atraviesa el hígado con sangre procedente del tracto digestivo).

Herófilo de Calcedonia fue otro de los grandes médicos de esta escuela: describió con acierto las estructuras denominadas meninges, los plexos coroideos y el cuarto ventrículo cerebral.

Paralelamente se desarrolla la escuela empirista, cuyo principal exponente médico fue Glauco de Tarentio (siglo I a. C.).

Podría considerarse a Glauco el precursor de la medicina basada en la evidencia, ya que para él sólo existía una base fiable: los resultados fundados en la experiencia propia, en la de otros médicos o en la analogía lógica, cuando no existían datos previos para comparar.

A partir de la incorporación de Egipto como provincia romana (30 a. C.), finaliza el periodo alejandrino y da inicio la época de esplendor de la medicina de Roma.

La medicina en la Antigua Roma fue una prolongación del saber médico griego.

La civilización etrusca, antes de importar los conocimientos de la cultura griega, apenas había desarrollado un "corpus" médico de interés, si se exceptúa una destacable habilidad en el campo de la odontología.

Pero la importancia creciente de la metrópoli durante las primeras épocas de expansión va atrayendo a importantes figuras médicas griegas y alejandrinas que acaban por conformar en Roma el principal centro de saber médico, clínico y docente, del área mediterránea.

Las figuras médicas más importantes de la Antigua Roma fueron Asclepíades de Bitinia (124 o 129 a. C.–40 a. C.), Celso y Galeno. El primero, abiertamente opuesto a la teoría hipocrática de los humores, desarrolló una nueva escuela de pensamiento médico, la Escuela metódica, basada en los trabajos de Demócrito, y que explica la enfermedad a través de la influencia de los átomos que atraviesan los poros del cuerpo, en un anticipo de la teoría microbiana.

Algunos médicos adscritos a esta escuela fueron Temisón de Laodicea, Tésalo de Trales o Sorano de Éfeso, el redactor de la primera biografía conocida de Hipócrates.

Entre los años 25 a. C. y 50 de nuestra era vivió otra figura médica de importancia: Aulo Cornelio Celso. En realidad no hay constancia de que ejerciera la medicina, pero se conserva un tratado de medicina "(De re medica libri octo)" incluido en una obra mayor, de carácter enciclopédico, llamada "De artibus" "(Sobre las artes)". En este tratado de medicina se incluye la definición clínica de la inflamación que ha perdurado hasta nuestros días: «Calor, dolor, tumor y rubor» (a veces también expresada como: «Tumor, rubor, ardor, dolor»).

Con el comienzo de la era cristiana se desarrolló otra escuela médica en Roma: la Escuela Pneumática. Si los hipocráticos se referían a los humores líquidos como la causa de la enfermedad y los atomistas acentuaban la influencia de las partículas sólidas denominadas átomos, los pneumáticos verían en el "pneuma" (gas) que penetra en el organismo a través de los pulmones, la causa de los trastornos patológicos padecidos por el ser humano. Fueron seguidores de esta corriente de pensamiento Ateneo de Atalia o Areteo de Capadocia.

En Roma la casta médica se organizaba ya (de un modo que recuerda a la actual división por especialidades) en médicos generales "(medici)", cirujanos "(medici vulnerum", "chirurgi)", oculistas "(medici ab oculis)", dentistas y los especialistas en enfermedades del oído. No existía una regulación oficial para ser considerado médico, pero a partir de los privilegios concedidos a los médicos por Julio César se estableció un cupo máximo por ciudad.

Por otra parte, las legiones romanas disponían de un cirujano de campaña y un equipo capaz de instalar un hospital "(valetudinaria)" en pleno campo de batalla para atender a los heridos durante el combate.

Uno de estos médicos legionarios, alistado en los ejércitos de Nerón, fue Pedanio Dioscórides de Anazarbus (Cilicia), el autor del manual farmacológico más empleado y conocido hasta el siglo XV. Sus viajes con el ejército romano le permitieron recopilar un gran muestrario de hierbas (unas seiscientas) y sustancias medicinales para redactar su magna obra: "De materia medica" ("Hylikà", conocido popularmente como «el Dioscórides»).

Pero la figura médica romana por excelencia fue Claudio Galeno, cuya influencia (y errores anatómicos y fisiológicos) perduraron hasta el siglo XVI (el primero en corregirlo fue Vesalio). Galeno de Pérgamo nació en el año 130 de nuestra era, bajo influencia griega y al amparo de uno de los mayores templos dedicados a Esculapio (Asclepios). Estudió medicina con dos seguidores de Hipócrates: Estraconio y Sátiro, y aún después visitó las escuelas de medicina de Esmirna, Corinto y Alejandría. Finalmente viajó a Roma donde su fama como médico de gladiadores le llevó a ser elegido médico del emperador (Marco Aurelio). Sin embargo, en Roma las autopsias estaban prohibidas, por lo que sus conocimientos de anatomía se fundaban en disecciones de animales lo que le llevó a cometer algunos errores. Pero también realizó aportaciones notables: corrigió el error de Erasístrato, quien creía que las arterias llevaban aire, y es considerado uno de los primeros experimentalistas de la medicina:

Fue el principal exponente de la escuela hipocrática, pero su obra es una síntesis de todo el saber médico de la época. Sus tratados se copiaron, tradujeron y estudiaron durante los siguientes trece siglos, por lo que es considerado uno de los médicos más importantes e influyentes en la medicina occidental.

Areteo de Capadocia no obtuvo la fama y el reconocimiento público de Galeno, pero el escaso material escrito que se ha conservado de él demuestra un gran conocimiento y un aún mayor sentido común. No se conocen muchos datos de este modesto médico romano, salvo su procedencia de la actual provincia turca de Capadocia y que vivió durante el primer siglo después de Cristo. Debió formarse en Alejandría (donde se permitían las autopsias), ya que sus conocimientos de anatomía visceral son muy completos. Es el primer médico en describir el cuadro clínico del tétanos, y a él se deben los nombres actuales de la epilepsia o la diabetes.

Hay que destacar una aportación capital de la medicina pública romana: Entre los principales arquitectos romanos (Columella, Marco Vitruvio o Marco Vipsanio Agripa) existía la convicción de que la malaria se propagaba a través de insectos o aguas pantanosas. Bajo este principio acometieron obras públicas como acueductos, alcantarillas y baños públicos encaminadas a asegurar un suministro de agua potable de calidad y un adecuado sistema de evacuación de excretas. La medicina moderna les dará la razón casi veinte siglos después, cuando se demuestre que el suministro de agua potable y el sistema de eliminación de aguas residuales son dos de los principales indicadores del nivel de salud de una población.

Según Henry Chadwick, "emeritus regius professor" en la Universidad de Cambridge e historiador del cristianismo primitivo, la práctica de la caridad expresada de forma eminente a través del cuidado de los enfermos fue probablemente una de las causas más poderosas de la expansión del cristianismo. Ya en el año 251, la Iglesia de Roma apoyaba a más de 1 500 personas en situación de necesidad. A pesar de la existencia de los protohospitales de campaña romanos, el Imperio careció de conciencia hospitalaria social hasta la fundación de los primeros grandes hospitales cristianos. En Oriente se fundó el hospital "Basiliade" cerca de Capadocia (inspirado por Basilio de Cesarea), y otro hospital en Edesa por parte de Efrén el Sirio, con trescientas camas para apestados.

En Occidente, el "nosocomium" fundado por Fabiola de Roma constituye el primer antecedente documentado de la «medicina social» e hizo de ella una de las mujeres más famosas en la historia de la medicina organizada.

En ese hospital, los pobres eran atendidos gratuitamente. Las excavaciones arqueológicas revelaron el plano y el arreglo de ese edificio único en su tipo en el cual las habitaciones y los pasillos para los enfermos y los pobres se agrupaban ordenadamente en torno al cuerpo edilicio principal, organizado en repartos, según las diferentes clases de enfermos. Según el historiador Camille Jullian, la fundación de este hospital constituye uno de los acontecimientos soberanos de la historia de la civilización occidental.

El Imperio Romano Oriental heredó, tras la división por la muerte de Teodosio, la cultura y la medicina griegas. En su afán por recuperar, o no perder los conocimientos clásicos la cultura bizantina ejerció una función fundamental recopilando y catalogando lo mejor de las tradiciones griega y romana, realizando, en cambio, pocas aportaciones novedosas.

El médico personal de Juliano el Apóstata, Oribasio de Pérgamo (325-403 d. C.) recogió en 70 volúmenes "(Las Sinagogas médicas)" todo el saber médico hasta esa fecha. Con el consejo de Oribasio, Juliano estableció la obligatoriedad de obtener a través de un examen una licencia "(symbolon)" oficial para ejercer la medicina.

Siguiendo con ese espíritu compilador, pero poco innovador, encontramos a Alejandro de Trales (hermano del arquitecto de la basílica de Santa Sofía), o a Aetius de Amida, en el siglo VII.

El médico más notable de este período fue Pablo de Egina, autor de "Epítome", "Hypomnema" o "Memorandum", siete volúmenes que recogen los conocimientos de medicina, cirugía y obstetricia. Entre sus aportaciones, destacan la descripción de los pólipos nasales o del líquido sinovial de las articulaciones, y describió algunas técnicas quirúrgicas novedosas, como una técnica para extirpar costillas.

Se fundaron varias escuelas médicas, como la "Stoa Basilike" (Escuela de Artes Liberales, en Constantinopla), o la escuela de Níbisis, en Siria, cuna de médicos como Zenón de Chipre, Asclepiodoto o Jacobo Psicresto, y en el siglo V Teodosio II funda un centro de formación intelectual y destina varios edificios públicos a la curación de enfermos.

Se conserva constancia de la existencia de algunos otros médicos y cirujanos de cierto relieve: Meletio, del siglo VII, autor de "Sobre la constitución del hombre"; Teófanes Nonno (siglo X); Miguel Psellos y Simeón Seth en el siglo XI; o, entre los siglos XII y XIII, Sinesio, Teodoro Pródromo o Nicolás Myrepso.

La razón del estancamiento de nuevos avances en medicina a partir de este período y durante la Edad Media responde a la importancia creciente del cristianismo en la vida política y social, reacio al concepto helénico de ciencia natural y más proclive a una visión determinista (teocentrista) de la enfermedad.

Véase 

A medida que las sociedades se desarrollaban en Europa y Asia, los sistemas de creencias iban siendo desplazados por un sistema natural diferente.

Todas las ideas desarrolladas desde la antigua Grecia hasta el Renacimiento, pasando por las de Galeno, se basaron en el mantenimiento de la salud a través del control de la dieta y de la higiene.

Los conocimientos anatómicos estaban limitados y había pocos tratamientos curativos o quirúrgicos.

Los médicos fundamentaban su trabajo en una buena relación con los pacientes, combatiendo las pequeñas dolencias y calmando las crónicas, y poco podían hacer contra las enfermedades epidémicas que acabaron expandiéndose por medio mundo.

La medicina medieval fue una mezcla dinámica de ciencia y misticismo. En la temprana Edad Media, justo tras la caída del Imperio Romano, el conocimiento médico se basaba básicamente en los textos griegos y romanos supervivientes que quedaron preservados en monasterios y otros lugares.

Las ideas sobre el origen y sobre la cura de las enfermedades no eran puramente seculares, sino que también tenían una importante base espiritual. Factores tales como el destino, el pecado, y las influencias astrales tenían tanto peso como los factores más físicos. Esto se explica porque desde los últimos años del imperio romano, la iglesia católica va adquiriendo un papel cada vez más protagonista en la cultura y la sociedad europeas. Su estructura jerárquica ejecuta un papel de funcionariado global, capaz de ejercer como depositario y administrador de la cultura y de amparar y adoctrinar a una población a la que ya no llegan las leyes del imperio.

Simultáneamente, el movimiento monacal, procedente de Oriente, comenzó en el siglo V a extenderse por Europa.

En los monasterios se acogía a peregrinos, enfermos y desahuciados, comenzando a formarse el germen de los hospicios u hospitales, aunque la medicina practicada por monjes y sacerdotes carecía, en general, de base racional, siendo más de índole caritativa que técnica.

En el Concilio de Clermont, en 1095, llegó a prohibirse a todo clérigo el estudio de cualquier forma de medicina. 

Existen antecedentes de estructuras hospitalarias en Egipto, la India o en Roma, pero su extensión y concepción actual se debe al modelo monástico iniciado por San Benito en Montecasino, y a sus variantes posteriores denominadas leproserías o lazaretos, en honor a su santo patrón san Lázaro.

Pero el mayor hospital conocido de la época se encontraba en El Cairo; Al-Mansur, recinto hospitalario fundado en 1283 se encontraba ya dividido en salas de especialidades médicas, al modo actual, contaba con una sección de dietética coordinada con la cocina del hospital, una sala para pacientes externos, sala de conferencias y biblioteca.

Tras la muerte de Mahoma en el año 632 comienza el período de expansión musulmana.
En apenas cien años los árabes ocupan Siria, Egipto, Palestina, Persia, la península ibérica y parte de la India. Durante esa expansión se van incorporando, por mandato del profeta («Buscad el saber aunque tengáis que ir a China»), los elementos culturales más relevantes de cada territorio, pasando en poco tiempo de practicar una medicina primitiva (empírico-mágica) a dominar la medicina técnica helénica de clara influencia hipocrática.

La primera generación de médicos persas de excelente reputación surgió de la "Academia Hippocratica" de Gundishapur, donde los nestorianos, cristianos herejes exiliados, se empleaban en la tarea de traducir las principales obras clásicas del griego al árabe.
Allí se formó la primera hornada de médicos árabes, bajo las enseñanzas de Hunayn ibn Ishaq (808-873), quien llegaría a ser médico personal del califa Al-Qasim al-Mamun.
Desde ese puesto fundó la primera escuela médica del Islam.
También fue allí donde el persa Al-Razi (Abu Bakr Muhammed ibn Zakkariya al-Rhazí, también conocido como Rhazes) (865-932) empezó a utilizar el alcohol (árabe al-khwl الكحول, o al-ghawl الغول) de forma sistemática en su práctica médica. De este médico, director fundador del hospital de Bagdad, se cuenta que para decidir su ubicación colgó cadáveres de animales en los cuatro puntos cardinales de la ciudad, optando por la localización en la que tardó más en producirse la descomposición.

Las tres obras principales de Al-Razi son Kitab-el-Mansuri ("Liber de Medicina ad Almansorem", síntesis de los conocimientos teóricos sobre anatomía, fisiología, patología); "Al-Hawi" (compendio clínico traducido al latín como "Continens", "La Continencia"). En ella registró los casos clínicos que trató, lo cual hizo del libro una fuente muy valiosa de información médica; y la obra monográfica titulada "Kitab fi al-jadari wa-al-hasbah", que contiene una introducción al sarampión y a la viruela de gran influencia sobre la Europa contemporánea.

Otra de las figuras representativas de la medicina islámica medieval fue Avicena (Ali ibn Sina). La obra de este filósofo persa, titulada "Canon de medicina", se considera la obra médica medieval más importante en la tradición islámica hasta su renovación con conceptos de medicina científica. Tuvo también gran influencia en toda Europa hasta la llegada de la Ilustración. Si Rhazes era el clínico interesado en diagnosticar al paciente, Avicena fue el teórico aristotélico dedicado a comprender las generalidades de la medicina.

Hay que destacar varias figuras médicas de interés originarias de Al-Ándalus, como Avempace (h. 1080-1138) y su discípulo Abentofail, Averroes (1126-1198) o Maimonides, que aunque judío, contribuyó de forma importante a la Medicina Árabe durante el siglo XII.
A finales del siglo XIII y principios del XIV, también en Al-Andalus, Al-Safra, médico personal del séquito de Muhammad ibn Nasr (sultán de Granada), en su libro "Kitāb al-Istiqsā", aporta diversos avances acerca de los tumores y medicamentos.
También es de destacar la influencia de Mesué Hunayn ibn Ishaq conocido abreviadamente con su nombre latino como "Johannitius" o Mesué el Viejo, que fue un destacado traductor de obras de medicina en Persia debido a su gran capacidad o 'don de idiomas', y que escribió varios estudios de oftalmología.

Ibn Nafis (Ala-al-din abu Al-Hassan Ali ibn Abi-Hazm al-Qarshi al-Dimashqi), médico sirio del siglo XII, contribuyó a la descripción del sistema cardiovascular. Su descubrimiento sería retomado en 1628 por William Harvey, a quien suele atribuirse dicho hallazgo. De la misma forma, muchas otras aportaciones médicas y astronómicas atribuidas a europeos tomaron como punto de partida los descubrimientos originales de autores árabes o persas.

Abulcasis (Abul Qasim Al Zaharawi) es el primer «especialista» cirujano conocido del mundo islámico. Nació en Medina Azahara en el año 936 y vivió en la corte de Abderramán III. Su principal obra compilatoria es "Kitàb al-Tasrìf" ("la práctica", "el método" o "la disposición"). En realidad se trata de una traducción ampliada de la de Pablo de Egina, a la que añadió una prolija descripción del instrumental quirúrgico de la época, y fue posteriormente traducida al latín) por Gerardo de Cremona.
En esta obra describe cómo quitar piedras del páncreas, operaciones oculares, del tracto digestivo, etc. así como el material quirúrgico necesario.

Otra cita atribuida al profeta Mahoma dice que sólo hay dos ciencias: la teología, para salvar el alma, y la medicina, para salvar el cuerpo. Entre los musulmanes "Al Hakim" (El Médico) era sinónimo de "sabio maestro". Los médicos árabes tenían la obligación de especializarse en algún campo de la medicina, y existían clases dentro de la profesión: De mayor a menor categoría encontramos al Hakim (el médico del "maristán", hospital), el Tahib, el Mutabbib (médico en prácticas) y el Mudawi (médico cuyo conocimiento es meramente empírico). Muchas de las figuras médicas y obras del islam influyeron de manera importante en la Europa medieval, especialmente gracias a las traducciones, de vuelta al latín, de la Escuela de Traductores de Toledo, o las de Constantino el Africano, que están en el origen de la primera escuela médica medieval europea de importancia: la Escuela de Salerno.

Entre los siglos XI y XIII se desarrolló al sur de Nápoles una escuela médica de especial interés: la Escuela Médica Salernitana.
La situación geográfica privilegiada de la Campania, en el sur de Italia, nunca del todo abandonada por la cultura tras la caída del imperio, ya que fue refugio de bizantinos y árabes, permitió el surgimiento de esta protouniversidad, fundada según una leyenda, por un griego (Ponto), un hebreo (Helino), un musulmán (Adela) y un cristiano (Magister Salernus), dándose originalmente el nombre de "Collegium Hippocraticum".

En ella, para la obtención del título de médico y, por tanto, el derecho de ejercicio de esta práctica, Roger II de Sicilia estableció un examen de graduación.

Algunos años después (en 1224) Federico II reformó el examen para que este fuese realizado de forma pública por el equipo de maestros de Salerno, y regulando para la práctica de la medicina un periodo de formación teórico (que incluía cinco años de medicina y cirugía) y un periodo práctico de un año.

Una figura de relevancia de esta escuela fue el monje Constantino el Africano (1010-1087), médico cartaginés que recogió numerosas obras médicas a lo largo de sus viajes y contribuyó a la medicina europea con la traducción del árabe de varios textos clásicos.
Esta labor le valió el título de "Magister orientis et occidentis".

Algunas de las obras traducidas por Constantino son el "Liber Regius", de Alí Abas; el "Viáticum", o ‘medicina de los viajes’, de Ibn Al-Gazzar; los "Libri universalium et particularium diaetarum" o el "Liber de urinis", de gran influencia en la escuela salernitana, hasta el punto de que el vaso de orina se convirtió en el signo distintivo del médico.

La orientación de la Escuela de Salerno es fundamentalmente experimental y descriptiva, y su obra más importante es el "Regimen Sanitatis Salernitanum" (1480), un compendio de normas higiénicas, de nutrición, de hierbas y de otras indicaciones terapéuticas, que llegó a alcanzar la cifra de 1500 ediciones.

En la "Escuela", aparte de las enseñanzas médicas (donde las mujeres eran admitidas como profesoras y como alumnas), había además cursos de filosofía, teología y derecho.

Su declive comienza a principios del siglo XIII, debido a la proliferación de Universidades por todo el continente (Bolonia, París, Oxford, Salamanca...).

Una de las secuelas más fructíferas de Salerno se encuentra en la Escuela Capitular de Chartres, de donde surgieron médicos como Guillermo de Conches, precursor de la escolástica, junto con Juan de Salisbury.

Entre las más destacadas figuras de la medicina europea medieval se encuentra el español Arnau de Vilanova (1238-1311). Formado en Montpellier y posiblemente también en Salerno, su fama lo llevó a ser médico de la corte de los reyes de Aragón, Pedro el Grande, Alfonso III y Jaime II. Además de algunas traducciones de Galeno y Avicena, desarrolla un cuerpo propio de investigación médica en torno a la tisis (una forma de presentación de la tuberculosis). A él se atribuye una recopilación de aforismos en versos leoninos del siglo XIII conocido como "Flos medicinae" (o "Flos sanitatis").

Dentro de la concepción teocentrista propia de este periodo se van introduciendo terapias alternativas de carácter sobrenatural. A partir de los siglos VII y VIII, con la extensión del cristianismo se incorporan a las ceremonias de coronación los ritos de unción real, que otorgan un carácter sagrado a la monarquía.

A estos reyes ungidos se les atribuyen propiedades mágico-curativas. La más popular es el "toque del rey": Felipe el Hermoso, Roberto II el Piadoso, San Luis de Francia o Enrique IV de Francia tocaban las úlceras (escrófulas, o lesiones tuberculosas cutáneas) de los enfermos pronunciando las palabras rituales "El rey te toca, Dios te cura" "(Le Roy te touche, et Dieu te guérit)".
Los reyes franceses solían peregrinar a Soissons para celebrar la ceremonia y se cuenta que Felipe de Valois (1328-1350) llegó a tocar a 1500 personas en un día.

La popularización de este tipo de ritos sanadores acabó por rebautizar a la escrófula-tuberculosis como «"mal du roi"» en Francia, o «"King's Evil"» en Inglaterra.
Tal fue la profusión de este tipo de ritos que llegaron a establecerse «especialidades» por monarquías; la «especialidad» del rey de Hungría era la ictericia, la del rey de España la locura, la de Olaf de Noruega el bocio y las de Inglaterra y Francia la escrófula y la epilepsia.

En el siglo XIII Roger Bacon (1214-1294) anticipó en Inglaterra las bases de la experimentación empírica frente a la especulación. Su máxima fue algo así como «duda de todo lo que no puedas demostrar», lo que incluía a las principales fuentes médicas clásicas de información.
En el "Tractatus de erroribus medicorum" describe hasta 36 errores fundamentales de las fuentes médicas clásicas. Pero tendrían que pasar doscientos años, hasta la llegada del Renacimiento, para que sus ideas se pusieran en práctica.

Dos hechos históricos marcaron el modo de ejercer la medicina, e incluso de enfermar, a partir del Renacimiento.

Por un lado, las grandes plagas que asolaron y protagonizaron el final de la Edad Media. Durante el siglo XIV hace su aparición en Europa la Peste Negra, causa de la muerte, por sí sola, de unos 20 o 25 millones de europeos.

Por otro, los siglos XV "(il Quattrocento)" y XVI "(il Cinquecento)" tuvieron en Italia el origen de unas filosofías de la ciencia y de la sociedad basadas en la tradición romana del humanismo. El florecimiento de Universidades en Italia al amparo de las nuevas clases mercantiles supuso el motor intelectual del que se derivó el progreso científico que caracterizó a este periodo. Esta "nueva era" recaló con especial intensidad en las ciencias naturales y la medicina, bajo el principio general del "revisionismo crítico". El universo comenzaba a contemplarse bajo una óptica mecanicista.

Es la época de los grandes anatomistas: la evidencia experimental acaba con los errores anatómicos y fisiológicos de Galeno y las propuestas adelantadas de Roger Bacon alcanzan a todas las disciplinas científicas: Copérnico publica su teoría heliocéntrica el mismo año en el que Andrés Vesalio, el principal anatomista de este período, publica "De humani corporis fabrica", su obra más relevante y manual imprescindible para los estudiantes de medicina de los siguientes cuatro siglos.

Vesalio se doctora en la universidad de Padua, tras formarse en París, y es nombrado ""explicator chirurgiae"" (profesor de cirugía) de esta universidad italiana. Durante sus años como profesor redactará su gran obra, acabando su carrera profesional como médico personal de Carlos I y, posteriormente, de Felipe II. Peregrinó a Jerusalén, según se revela en una carta de 1563, tras serle conmutada por el rey la pena de muerte por la penitencia de la peregrinación. El motivo de la condena es la disección que realizó a un joven noble español tras su muerte y el descubrimiento, al abrirle el pecho, de que el corazón aún latía.
Pero Vesalio es el resultado de un proceso que se desarrolló lentamente desde bien entrado el siglo XIV. En 1316 Mondino de Luzzi, medieval por nacimiento, pero renacentista por derecho, publicó en la Escuela de Bolonia su "Anathomia", el primero en hacer una descripción anatómica sobre una disección pública, dando paso a una sucesión de tratados anatómicos y quirúrgicos en los que la medicina debe reinventarse como disciplina empírica y protocientífica. El mismo Leonardo da Vinci publicó un innumerable catálogo de ilustraciones, a caballo entre la anatomía y el arte, basados en disecciones de, al menos, veinte cadáveres, y se publica la primera clasificación de las enfermedades mentales

La obra de Vesalio vio dos ediciones en vida del autor, y supuso una concepción de la anatomía radicalmente diferente a las anteriores: se trata de una anatomía funcional, más que topográfica, vislumbrando, en la descripción de las cavidades del corazón, lo que será el gran descubrimiento anatómico y fisiológico de la época: la circulación pulmonar o menor, que formularán de modo más completo dos grandes médicos renacentistas: Miguel Servet (en "Christianismi restitutio" de 1553) y Mateo Realdo Colombo (en "De re anatomica", 1559), y cuya paternidad se ha atribuido clásicamente al médico inglés del siglo XVII William Harvey.

Debido a su enorme influencia han quedado con el nombre de Vesalio algunos epónimos en estructuras anatómicas del cuerpo humano, como el "agujero de Vesalio" (orificio del hueso esfenoides), la "vena de Vesalio" (emisaria que pasa por el agujero de Vesalio), o el "ligamento de Vesalio" o de Poupart (en el borde inferior de la aponeurosis del músculo oblicuo mayor). También se convirtieron en epónimos anatómicos los nombres de algunos de sus discípulos o contemporáneos, como Gabriel Falopio (1523-1562) o Bartolomeo Eustachio (1524-1574).

Además de anatomistas en el Renacimiento, también surgieron algunas figuras médicas de interés, como Ambroise Paré, padre de la cirugía moderna, Girolamo Fracastoro y Paracelso.

Paré representa a la perfección el modelo renacentista de médico hecho a sí mismo y reinventor del papel de la medicina. Era de familia humilde, pero alcanzó tal fama que acabó siendo el médico de corte de cinco reyes. Su formación se inició en el gremio de los barberos y sacamuelas, pero compaginó su trabajo con la asistencia al Hôtel-Dieu de París. Sufrió un cierto rechazo de la comunidad médica, ya que su extracción humilde y su desconocimiento del latín y el griego le llevaron a escribir toda su obra en francés. Desde sus inicios fue considerado un "renovador", lo que no siempre le benefició, aunque su reputación fue hasta el final su principal aval. Buena parte de su obra es un compendio de análisis y refutación de costumbres, tradiciones o supersticiones médicas, sin fundamento científico ni utilidad real.

Del segundo habría poco que destacar, de no ser por una obra menor escrita en 1546 que no alcanzaría repercusión hasta varios siglos más tarde: "De contagione et contagiosis morbis". En ella Fracastoro introdujo el concepto de "Seminaria morbis" (semilla de enfermedad), un anticipo rudimentario de la teoría microbiana.

Y, en cuanto a Paracelso (Theophrastus Philippus Aureolus Bombastus von Hohenheim), su controvertida personalidad (el sobrenombre autoproclamado de Paracelso lo tomó por considerarse "superior a Celso", el médico romano) lo ha colocado en un lugar tal vez inmerecido de la historia: más próximo a la alquimia y a la magia que a la medicina. Hay que destacar, sin embargo, su estudio crítico de la teoría hipocrática de los humores, sus estudios sobre el líquido sinovial, o su oposición a la influencia de la escolástica y su predilección por la experimentación frente a la especulación. En 1527 proclama en Basilea:

Esta posición abiertamente enfrentada con la medicina más ortodoxa, así como sus estudios herborísticos, le valieron el rechazo de los médicos alemanes y, en general, de la historiografía médica oficial.

También destacaron algunos clínicos, como el francés Jean François Fernel, autor de "Universa Medicina", 1554, al que se debe el término venéreo: A finales del siglo XV se produjo en Europa una pandemia de sífilis. La máxima extensión de esta epidemia (en 1495) se dio durante el sitio de Nápoles, defendido por italianos y españoles y asediado por el ejército francés al servicio de Carlos VIII. Durante el asedio las prostitutas francesas propagaron la enfermedad entre los ejércitos mercenarios y los soldados españoles, bautizándose a la misteriosa plaga con el nombre de "morbo gallico" (enfermedad de los franceses), y más tarde como "enfermedad del amor".

El Renacimiento también es la época de despegue de la psicología, con Juan Luis Vives, de la bioquímica con Jan Baptist van Helmont, o de la anatomía patológica: Antonio Benivieni recopiló en su obra "De abditis morborum causis" (De las causas ocultas de las enfermedades, 1507) los resultados de las autopsias de muchos de sus pacientes, cotejándolos con los síntomas previos al fallecimiento, al modo del empirismo científico moderno. La gran figura de la anatomía patológica, sin embargo, pertenece al siguiente siglo: Giovanni Battista Morgagni.

En los comienzos del siglo XVII la profesión médica no gozaba todavía de excesivo prestigio entre la población. Francisco de Quevedo se explaya contra su incompetencia y su avaricia en numerosos versos:

Pero Isaac Newton, Leibniz y Galileo darán paso en este siglo al método científico. Mientras aún se catalogan enfermedades como la diabetes en función del sabor más o menos dulce de la orina, o mientras la viruela se convierte en la nueva plaga de Europa, los avances técnicos y científicos están a punto de inaugurar una época más eficaz y resolutiva. Edward Jenner, médico británico, observa que los ganaderos que han padecido una enfermedad leve procedente de sus vacas, en forma de pequeñas ampollas rellenas de líquido, no contraen la temible viruela, y decide realizar un experimento para contrastar su hipótesis: Con una lanceta inocula parte del líquido de una ampolla de una joven infectada por la viruela vacuna "(variolae vaccine)" a un niño llamado James Phipps, voluntario para el experimento. Tras unos días presenta los síntomas habituales: febrícula y algunas ampollas. A las seis semanas inocula al niño una muestra procedente de un enfermo de viruela humana y espera. James Phipps no contraerá la enfermedad y, desde entonces, a este tipo de inmunización se la conoce como "vacuna".

William Harvey, médico inglés, es el gran fisiólogo de este siglo, descubridor oficial de la circulación sanguínea, prolijamente descrita en su "Exercitatio anatomica de motu cordis et sanguinis in animalibus" (1628). En los últimos años de su vida también escribió algunos tratados embriológicos de interés. La teoría más extendida sobre la sangre antes de la publicación de la obra de Harvey es que esta se fabrica en el hígado constantemente a partir del alimento. Pero sus observaciones le demuestran que esto no es posible:

Harvey adopta una visión más vitalista frente al mecanicismo renacentista: los seres vivos están animados por una serie de fuerzas determinantes, que están en el origen de su actividad fisiológica, susceptibles de su estudio bajo una óptica científica, pero todas ellas supeditadas a una "vis" (fuerza) superior, origen de la vida, aunque no necesariamente de naturaleza divina.

Durante este siglo la experimentación avanzaba a un ritmo tal que la clínica era incapaz de absorber. Comienzan a fundarse las Academias de expertos para la transmisión de la información obtenida de los continuos hallazgos: la Academia dei Lincei en Roma, la Royal Society en Londres, o la Académie des Sciences en París. A consecuencia de las múltiples e innovadoras propuestas terapéuticas surge la iatroquímica como una disciplina con entidad propia, cuyo principal exponente es Franciscus Sylvius, heredero de la perspectiva química de la medicina anticipada por Helmont.

Importantes médicos adscritos a esta escuela iatroquímica fueron Santorio Sanctorius o Thomas Willis. Santorio fue el autor de un estudio que le colocó al inicio de una larga lista de endocrinólogos, al ser el primero en definir los procesos metabólicos: El primer experimento controlado sobre el metabolismo humano fue publicado en 1614 en su libro "Ars de statica medicina".
Santorio describía como se pesó a sí mismo antes y después de dormir, comer, trabajar, tener relaciones sexuales, beber y excretar. Encontró que la mayor parte de la comida que ingería se perdía en lo que él llamaba "transpiración insensible". Igual que Harvey, Santorio achacaba estos procesos a una "fuerza vital" que animaba al tejido vivo.

El vitalismo se desarrollaba como planteamiento filosófico y encontraba adeptos entre los médicos y naturalistas, alcanzando su máximo apogeo en pleno siglo XVIII, de la mano de Xavier Bichat (1771-1802), John Hunter (1728-1799), François Magendie (1783-1855) o Hans Driesch (1867-1941).

Thomas Willis, en su obra "Cerebri anatomi" (1664), describió varias estructuras anatómicas cerebrales, entre ellas el polígono vascular de Willis, así llamado en su honor; pero las mejoras técnicas, como el microscopio, iban ampliando el nivel de detalle de las descripciones anatómicas y pronto proliferan las estructuras epónimas bautizadas por sus descubridores o por los historiadores posteriores: Johann Georg Wirsung (que da nombre al conducto excretor del páncreas), Thomas Wharton (el conducto de Wharton es el de excreción de la glándula salival submandibular), Nicolás Stenon (conducto de Stenon: excretor de la glándula parótida), Caspar Bartholin, De Graaf y un largo etcétera.

Otro médico destacable de este período es Thomas Sydenham, apodado como el "Hipócrates inglés". Un clínico nato más interesado en la semiología (la descripción de los síntomas como método diagnóstico) que en la experimentación, y que también dejó su nombre asociado al de enfermedades como la Corea de Sydenham. En sus tratados se plantea el concepto de "entidad morbosa", un concepto muy actual de enfermedad, entendida como un proceso originado por las mismas causas, con un cuadro clínico y evolutivo similar y con un tratamiento específico. Este concepto de enfermedad lo completará, gracias a sus descripciones anatómicas microscópicas Giovanni Battista Morgagni. Morgagni, discípulo de Antonio María Valsalva destacó desde joven por sus inquietudes médicas. Su obra más importante es ""De sedibus et causis morborum per anatomen indicatis"" publicada en 1761 y en ella describe más de 700 historias clínicas con sus protocolos de autopsias. En su haber se cuenta la novedosa (y acertada) propuesta de que la tuberculosis era una enfermedad infecciosa, susceptible por tanto de ser contraída al contacto con enfermos. Esa teoría tardará en ser demostrada por Robert Koch, pero origina los primeros movimientos sociales de "cuarentena" en instituciones específicas para enfermos de este mal.

Marcello Malpighi también supo aprovechar las mejoras desarrolladas por Anton van Leeuwenhoek en el microscopio. Sus descripciones de tejidos observados bajo aumento le han valido el título de padre de la histología. En su honor han quedado bautizadas unas estructuras renales denominadas pirámides de Malpighi.

El despotismo ilustrado inspiró un humanismo vertical que está en el origen de la "medicina social" (antecedente de la salud pública), cuyo primer gran éxito es la implantación de la vacuna de la viruela tras el descubrimiento de Jenner. Ese mismo humanismo será el inspirador de los primeros trabajos en ética médica (Thomas Percival) y de los primeros estudios sobre historia de la medicina. Entre los cirujanos notables de esta época están Pierre Dessault o Dominique-Jean Larrey (cirujano de Napoleón) en Francia y John Hunter en Inglaterra.

Con la revolución industrial se dieron una serie de circunstancias sociales y económicas que impulsaron de nuevo a las ciencias médicas: por un lado se inauguran los fenómenos migratorios de grandes masas poblacionales que se hacinan en las ciudades, con las consecuencias insalubres correspondientes: mala alimentación y desarrollo de enfermedades relacionadas con la misma (pelagra, raquitismo, escorbuto...) y proliferación de enfermedades infecciosas (especialmente la tuberculosis). Pero también se dan las condiciones técnicas para que los descubrimientos apuntados durante la ilustración vean cumplido y mejorado su desarrollo técnico: El siglo XIX va a ser el siglo de la salud pública, de la asepsia, de la anestesia y de la victoria definitiva de la cirugía.

La medicina del siglo XIX todavía contiene muchos elementos de arte "(ars medica)", especialmente en el campo de la cirugía, pero empieza a vislumbrarse, merced a la imparable consecución de conocimientos y técnicas, un modo de ejercerla más científico y, por tanto, más independiente de la "habilidad" o la experiencia de quienes la practican. Este siglo verá nacer la teoría de la evolución, expresión antropológica del positivismo científico que le es propio. La realidad puede medirse, comprenderse y predecirse mediante leyes, que a su vez van siendo corroboradas por los sucesivos experimentos. Por ese camino avanzan la astronomía (Laplace, Foucault), la física (Poincaré, Lorentz), la química (Dalton, Gay-Lussac, Mendeleiev) y la propia medicina.

La figura médica por excelencia de este período fue Rudolf Virchow. Desarrolló las disciplinas de higiene y medicina social, en los orígenes de la medicina preventiva actual. Es el mismo Virchow el que postuló la teoría de ""Omnia cellula a cellula"" (toda célula proviene de otra célula) y explicó a los organismos vivos como estructuras formadas por células. Poco antes de su muerte, en 1902, será candidato al Premio Nobel de Medicina y Fisiología, junto al español Santiago Ramón y Cajal, quien obtendrá finalmente el galardón en 1906.

Las últimas décadas del siglo XIX fueron de gran trascendencia para el desarrollo de la medicina contemporánea. Joseph Skoda y Carl von Rokitansky fundaron la Escuela Moderna de Medicina de Viena (Neue Wiener Schule), cuna de la nueva hornada de figuras médicas de este siglo. Skoda es considerado el principal exponente del “nihilismo terapéutico”, corriente médica que propugnaba abstenerse de cualquier intervención terapéutica, dejando al cuerpo recuperarse sólo o a través de dietas apropiadas, como tratamiento de elección frente a muchas enfermedades. Fue un notable dermatólogo y clínico, alcanzando fama por sus diagnósticos brillantes, certeros e inmediatos. A él se debe la recuperación y expansión de las técnicas diagnósticas a través de la percusión (adelantadas por Leopold Auenbrugger un siglo antes), y crea en 1841 el primer departamento dermatológico junto a Ferdinand von Hebra, el maestro de la dermatología del siglo XIX.

Rokitansky es considerado por Rudolf Virchow «el Linneo de la anatomía patológica» debido a su meticulosidad descriptiva, lo que acabó dando nombre a varias enfermedades descritas por él (tumor de Rokitansky, úlcera de Rokitansky, síndrome de Rokitansky...).

En 1848 Claude Bernard, el gran fisiólogo de este siglo y "fundador" oficial de la medicina experimental, descubre la primera enzima (lipasa pancreática). En ese año comienza a emplearse el éter para sedar a los pacientes antes de la cirugía y a finales de este siglo Luis Pasteur, Robert Koch y Joseph Lister demostrarán inequívocamente la naturaleza etiológica de los procesos infecciosos mediante la teoría microbiana. En Francia y Alemania se desarrolla la bioquímica, rama de la biología y de la medicina que estudia las reacciones químicas implicadas en los procesos vitales. De aquí surgirán los estudios sobre vitaminas y se pondrán los cimientos de la nutrición y dietética modernas.

Ignaz Semmelweis (1818-1865) fue un médico húngaro que representa el paradigma de la ruptura definitiva de la medicina contemporánea, de índole empírica y sometida al método científico, con la medicina "artesanal" ejercida hasta ese momento: De origen humilde, se formó en Pest y posteriormente en el Hospital General de Viena, donde entró en contacto con Skoda, Virchow, Hebra y Rokitansky, estudiando junto a este último los procesos infecciosos en relación con las intervenciones quirúrgicas. De ahí nacerá la obsesión que le acompañará toda su vida, y que le llevará, durante su trabajo en una de las Maternidades del Hospicio General de Viena, a establecer la fuerte sospecha de que la mortalidad materna por una infección contraída durante el parto se debía a que los estudiantes no se lavaban las manos antes de asistir a las parturientas.

Obtuvo sus evidencias mediante un rudimentario, pero correcto estudio epidemiológico: comparando las salas donde las mujeres eran asistidas solo por matronas, con las salas en las que los estudiantes ayudaban al parto, y en las que la mortalidad era muy superior (hasta un 40 % de las mujeres que daban a luz en ellas morían por dicha infección).

En realidad, y así lo postulo Semmelweis, el origen de la infección se encontraba en que los estudiantes acudían a los partos después de asistir a las sesiones de disección de cadáveres, portando en sus manos un agente infeccioso procedente del material putrefacto de los mismos. Y la solución, propuesta y corroborada con un nuevo estudio por él mismo, se basaba en el lavado de manos previo al parto con una solución de cloruro cálcico. Sin embargo, y salvo contadas excepciones, el estamento médico oficial rechazó sus evidencias, tildándolo de farsante. 

Los avances en el conocimiento de los diferentes órganos y tejidos se multiplican durante todo el siglo. Theodor Schwann, Purkinje, la ley de Frank-Starling, François Magendie, el conducto de Volkmann, la angina de Ludwig, la enfermedad de Graves Basedow, la enfermedad de Addison, Santiago Ramón y Cajal... la lista de médicos insignes se hace interminable, cada uno especializado en un órgano o territorio específico. Fuera de este grupo, aun sin ser médico, pero de gran trascendencia para la ciencia médica, hay que destacar a Gregor Mendel, padre de la genética.

Louis Pasteur tampoco estudió medicina, pero puede considerarse uno de los investigadores más influyentes en la historia de la medicina del siglo XIX. Su formación como químico le llevó a diseñar un método de observación de sustancias químicas mediante luz polarizada, lo que le abrió las puertas para el estudio de los microorganismos (inicialmente levaduras), demostrando que en los procesos de fermentación no se producían fenómenos de "generación espontánea" sino de proliferación de microorganismos previamente presentes. Joseph Lister aplicaría posteriormente este conocimiento desarrollando mediante calor la práctica quirúrgica de la asepsia y la antisepsia, y consiguiendo así disminuir drásticamente las tasas de mortalidad tras las operaciones, principal obstáculo para el definitivo despegue de la cirugía. El golpe definitivo a las enfermedades infecciosas (tras las vacunas y la asepsia) lo dará Alexander Fleming a comienzos del siglo XX con el descubrimiento de la penicilina, el primer antibiótico.

El 8 de noviembre de 1895 Wilhelm Röntgen, un físico alemán, consiguió producir un nuevo tipo de radiación electromagnética en las longitudes de onda correspondientes a los actualmente llamados Rayos X. Por ese descubrimiento recibiría el en 1901. Es la primera de las técnicas de diagnóstico por imagen que permitirán observar el interior del cuerpo humano en vivo. En 1896 los físicos Henri Becquerel, Pierre Curie y Marie Curie descubrieron la radioactividad, que originaría la medicina nuclear.

Entre los siglos XIX y XX se desarrollan tres concepciones o paradigmas médicos: el anatomoclínico (el origen de la enfermedad está en la "lesión"), el fisiopatológico (se busca el origen en los "procesos" alterados) y el etiológico (o de las causas externas), todos ellos herederos del modelo científico, principalmente biologicista y fundamentación filosóficas en el positivismo. Cada vez despuntan menos genios individuales con repercusión general y la investigación se basa en equipos interdisciplinarios o dedicados a búsquedas muy específicas. En este siglo se articula la relación entre investigación e industria farmacéutica y se asienta la estadística como procedimiento principal para dotar a la medicina de base científica. De hecho hacia finales del siglo se acuña el término de medicina basada en la evidencia: los protocolos estandarizados de actuación, avalados por los estudios científicos, van sustituyendo a las opiniones y experiencias personales de cada facultativo, y consiguen otorgar al cuerpo de conocimientos teóricos médicos una validez global en un mundo cada vez más interconectado.

Entre los más destacados médicos de este siglo cabe destacar a Sigmund Freud, el gran revolucionario de la psiquiatría, Robert Koch, descubridor del bacilo causante de la tuberculosis, Paul Ehrlich, padre de la inmunología, Harvey Williams Cushing, padre de la neurocirugía, o Alexander Fleming, descubridor de la penicilina, con la que da comienzo la «era antibiótica» de la medicina.

En términos sociales, el conocimiento médico se consolida como un saber "experto" que permite definir lo normal y lo patológico y no sólo en un sentido corporal sino, también, en un sentido social y cultural y resolver así sin aparentes ambivalencias realidades culturales y sociales más complejas. Así se define la normalidad de las mujeres a las que la medicina atribuye, hasta bien entrado el siglo, un exclusivo papel como esposas y madres, en franca (y científicamente productiva) connivencia con las ideas sociales imperantes. Pero, además, la medicina contribuye a medicalizar comportamientos que habían sido manejados con destrezas culturales muy diversas. Desde la homosexualidad a la hiperactividad (comportamiento infantil travieso) van ocupándose territorios de la vida y generándose etiquetas médicas y tratamientos farmacéuticos que proclaman resolver complejas problemáticas sociales con la sistemática administración de ciertas píldoras. Pero la medicalización también ha contribuido a generar respuestas sociales muy diversas de carácter individual o colectivo y a tomar conciencia sobre la importancia de otros saberes culturales en la vida cotidiana que hoy en día se encuentran amenazados por el monopolio médico.

Y en ese denso entramado de equipos investigadores y superespecializaciones va desarrollándose también una nueva forma de entender la enfermedad, o más bien, al enfermo, al hilo de una sociedad que despierta al ecologismo (entendido como movimiento social que pretende integrar de nuevo al individuo en el ambiente). Los siglos XVII al XIX, profundamente racionalistas, se esforzaron en clasificar los órganos, tejidos y enfermedades y en establecer las leyes de funcionamiento de los procesos fisiológicos y patológicos. Pero la evidencia de la complejidad de los seres humanos lleva a la conclusión de que no hay enfermedades, sino personas enfermas. En este contexto se desarrollan los modelos de salud y enfermedad propuestos por la Organización Mundial de la Salud, y que incorporan las esferas psicológica y social a la biológica, como determinantes de la salud de las personas. En 1978 se celebra la Conferencia Internacional sobre Atención Primaria de Salud de Alma-Ata, donde se pone de manifiesto esa declaración de principios, así como la importancia crucial de las medidas sociales (suministro adecuado de agua potable y alimentos, vacunaciones...) y de la atención primaria de salud para la mejora del nivel sanitario de las poblaciones. El lema (finalmente no cumplido) de esta conferencia fue "Salud para todos en el año 2000".

Sin embargo, paralelamente a esa evidencia, el desarrollo de la farmacología a nivel industrial y económico ha convertido a la medicina del siglo XX en tributaria del medicamento como icono de salud. La aspirina, sintetizada por Felix Hoffmann en 1897 se ha convertido en uno de los símbolos de la cultura de ese siglo. Estos rasgos contradictorios (una medicina deshumanizada y mercantilizada, pero que ha conseguido erradicar enfermedades como la viruela o la poliomielitis y que ha conseguido aumentar la esperanza de vida media por encima de los 70 años en la mayoría de los países desarrollados) son la síntesis de la medicina moderna.

A partir de Emil Kraepelin y Eugen Bleuler, y posteriormente de Sigmund Freud, despega una de las ramas más tardías de la medicina moderna: la psiquiatría. El primero es el pionero en proponer que las enfermedades psiquiátricas son causadas principalmente por trastornos biológicos o genéticos. Bleuler realiza algunos aportes fundamentales en psiquiatría clínica (a él se deben los términos de esquizofrenia y autismo), y de Freud cabe decir que es el fundador del movimiento psicoanalítico. La escuela psicoanalítica, renovada por sus discípulos, ha seguido en mayor o menor grado vigente tras la muerte de su fundador y las ideas centrales han trascendido a la psiquiatría alcanzando disciplinas tan dispares como el arte, la religión, o la antropología pasando a formar parte de la cultura general. Posteriormente la psiquiatría recogerá, a través de Karl Jaspers, las influencias de la fenomenología y el existencialismo y a través de John Broadus Watson, del conductismo.

En las últimas décadas del siglo XX la psiquiatría desarrolló una escuela psicofarmacológica basada en la premisa de que el mecanismo de acción de los psicofármacos revelaba a su vez el mecanismo fisiopatológico secundario al trastorno psíquico acercándose de este modo a la neurofisiología.

Más logros técnicos que deben destacarse son la transfusión sanguínea, llevada a cabo por primera vez con éxito en este siglo gracias a los trabajos sobre grupos sanguíneos desarrollados por Karl Landsteiner, o el trasplante de órganos, abanderado, no por el primero, pero sí por el más mediático y exitoso de sus desarrolladores: Christiaan Barnard, primer cirujano en realizar con éxito un trasplante de corazón.

Nace la genética molecular, y se desarrollan las aplicaciones de la física en diferentes áreas de la medicina: el empleo de radioisótopos, la electroforesis, la cromatografía, la espectrofotometría, el uso del láser, el microscopio electrónico, las técnicas de ultrasonidos en ecografía, la tomografía axial computarizada o la resonancia magnética.

La automatización del cálculo mediante sistemas informatizados ha transformado la sociedad del siglo XX. Esa herramienta ha supuesto un gran impulso para muchas ciencias aplicadas como la medicina. Posiblemente el mayor logro médico del siglo XX sea la secuenciación del genoma humano y aunque todavía se tardarán algunas décadas en comprender y aprovechar ese enorme caudal de información, no cabe duda de que supondrá una nueva revolución en el modo de abordar muchas enfermedades e, incluso, en el modo de comprender y definir al ser humano.

En el siglo XXI las enfermedades aparecieron más recurrentemente tales como el SARS, el MERS y el actual COVID-19.

<div style="float:center; margin: 3mm; padding: 1mm; width: 700px; border: 0px solid;">

</center





</doc>
<doc id="3589" url="https://es.wikipedia.org/wiki?curid=3589" title="Juramento hipocrático">
Juramento hipocrático

El juramento hipocrático es un compromiso, que solo pueden hacer las personas que se gradúan en las carreras universitarias de Medicina. Tiene un contenido de carácter solo ético porque orienta al médico en la práctica de su profesión. En su forma original regula las obligaciones hacia el maestro y su familia, hacia los discípulos, hacia los colegas y hacia los pacientes. A partir del siglo XIX empezó a ser frecuente, se ha popularizado de forma universal, la realización de un juramento basado en un texto modernizado, inspirado por el antiguo, distinto según la escala de valores específica de cada tiempo y lugar.

Durante casi 2 mil años la medicina occidental y la medicina árabe estuvo dominada teóricamente por una tradición que, remontándose al médico griego Hipócrates adoptó su forma definitiva de la mano de Galeno, un griego que ejerció la medicina en la Roma imperial en el siglo II. Según la tradición, fue redactado por Hipócrates o un discípulo suyo. Lo cierto es que forma parte del "corpus hipocráticum", y se piensa que pudo ser obra de los pitagóricos. Según Galeno, Hipócrates creó el juramento cuando empezó a instruir, apartándose de la tradición de los médicos de oficio, a aprendices que no eran de su propia familia. Los escritos de Galeno han sido el fundamento de la instrucción médica y de la práctica del oficio hasta casi el siglo XX.

A partir del Renacimiento, época caracterizada por la veneración de la cultura grecolatina, el juramento empezó a usarse en algunas escuelas médicas, y esa costumbre se ha ido ampliando, desde el siglo XIX, en algunos países, y desde la Segunda Guerra Mundial en otros, aunque es completamente ignorada en muchos. Aun cuando sólo tenga en la actualidad un valor histórico y tradicional, allí donde se pronuncia, el tomarlo es considerado como un rito de pasaje o iniciación después de la graduación, y previo al ingreso a la práctica profesional de la medicina.

En el período clásico de la gran civilización griega sobresalió el arte de curar. Aunque seguía contemplando principios religiosos, la curación ya no estaba orientada por la magia, sino por lo clínico. En esa época se escribió el primer escrito ético relacionado con el compromiso que asumía la persona que decidía curar al prójimo; el compromiso del médico era actuar siempre en beneficio del ser humano, y no perjudicarlo.

El contenido del juramento se ha adaptado a menudo a las circunstancias y conceptos éticos dominantes de cada sociedad. El Juramento hipocrático ha sido actualizado por la Declaración de Ginebra de 1948. También existe una versión, muy utilizada actualmente en facultades de Medicina de países anglosajones, redactada en 1964 por el doctor Louis Lasagna

Texto original en español:

Texto original griego:

Ha habido varios intentos de adaptación del juramento hipocrático a lo largo de la historia.
En 1948, se redactó un juramento hipocrático en la convención de Ginebra, con el texto siguiente:

Una versión del juramento muy utilizada actualmente, sobre todo en países anglosajones, es la versión redactada en 1964 por el Doctor Louis Lasagna, Decano de la Facultad de Medicina de la Universidad de Tufts. El texto, en su traducción al castellano, dice así:

Conocida como "Declaración de Ginebra" fue adoptada por la Asociación Médica Mundial (AMM) en 1948 y ha sido revisada y enmendada en diferentes ocasiones (1968, 1983, 1994, 2005, 2006 y 2017). Éste es el texto aprobado en octubre de 2017, en Chicago.




</doc>
<doc id="3590" url="https://es.wikipedia.org/wiki?curid=3590" title="Acelerador de partículas">
Acelerador de partículas

Un acelerador de partículas es un dispositivo que utiliza campos electromagnéticos para acelerar partículas cargadas a altas velocidades, y así, hacerlas colisionar con otras partículas. De esta manera, se generan multitud de nuevas partículas que -generalmente- son muy inestables y duran menos de un segundo, esto permite estudiar más a fondo las partículas que fueron desintegradas por medio de las que fueron generadas. Hay dos tipos básicos de aceleradores de partículas: los lineales y los circulares. El tubo de rayos catódicos de un televisor es una forma simple de acelerador de partículas.

Los aceleradores de partículas se asemejan, en cierta forma, la acción de los rayos cósmicos sobre la atmósfera terrestre, lo cual produce al azar una lluvia de partículas exóticas e inestables. Sin embargo, los aceleradores prestan un entorno mucho más controlado para estudiar estas partículas generadas, y su proceso de desintegración.

Ese estudio de partículas, tanto inestables como estables, puede ser en un futuro útil para el desarrollo de la medicina, la exploración espacial, tecnología electrónica, etcétera.

Los aceleradores lineales (muchas veces se usa el acrónimo en inglés "linac") de altas energías utilizan un conjunto de placas o tubos situados en línea a los que se les aplica un campo eléctrico alterno. Cuando las partículas se aproximan a una placa, se aceleran hacia ella al aplicar una polaridad opuesta a la suya. Justo cuando la traspasan, a través de un agujero practicado en la placa, la polaridad se invierte, de forma que en ese momento la placa repele la partícula, acelerándola por tanto hacia la siguiente placa. Generalmente no se acelera una sola partícula, sino un continuo de haces de partículas, de forma que se aplica a cada placa un potencial alterno cuidadosamente controlado de forma que se repita de forma continua el proceso para cada haz.
A medida que las partículas se acercan a la velocidad de la luz, la velocidad de inversión de los campos eléctricos se hace tan alta que deben operar a frecuencias de microondas, y por eso, en muy altas energías, se utilizan cavidades resonantes de frecuencias de radio en lugar de placas.

Los tipos de aceleradores de corriente continua capaces de acelerar a las partículas hasta velocidades suficientemente altas como para causar reacciones nucleares son los generadores Cockcroft-Walton o los multiplicadores de potencial, que convierten una corriente alterna a continua de alto voltaje, o bien generadores Van de Graaf que utilizan electricidad estática transportada mediante cintas.

Estos aceleradores se usan en muchas ocasiones como primera etapa antes de introducir las partículas en los aceleradores circulares. El acelerador lineal más largo del mundo es el colisionador electrón-positrón Stanford Linear Accelerator (SLAC), de 3 km de longitud.

Estos aceleradores son los que se usan en radioterapia y radiocirugía. Utilizan válvulas klistrón y una determinada configuración de campos magnéticos, produciendo haces de electrones de una energía de 6 a 30 millones de electronvoltios (MeV). En ciertas técnicas se utilizan directamente esos electrones, mientras que en otras se les hace colisionar contra un blanco de número atómico alto para producir haces de rayos X. La seguridad y fiabilidad de estos aparatos está haciendo retroceder a las antiguas unidades de cobaltoterapia.

Dos aplicaciones tecnológicas de importancia en las que se usan este tipo de aceleradores son la Espalación para la generación de neutrones aplicables a los amplificadores de potencia para la transmutación de los isótopos radiactivos más peligrosos generados en la fisión.

Estos tipos de aceleradores poseen una ventaja añadida a los aceleradores lineales al usar campos magnéticos en combinación con los eléctricos, pudiendo conseguir aceleraciones mayores en espacios más reducidos. Además las partículas pueden permanecer confinadas en determinadas configuraciones teóricamente de forma indefinida.

Sin embargo poseen un límite a la energía que puede alcanzarse debido a la radiación sincrotrón que emiten las partículas cargadas al ser aceleradas. La emisión de esta radiación supone una pérdida de energía, que es mayor cuanto más grande es la aceleración impartida a la partícula. Al obligar a la partícula a describir una trayectoria circular realmente lo que se hace es "acelerar" la partícula, ya que la velocidad cambia su sentido, y de este modo es inevitable que pierda energía hasta igualar la que se le suministra, alcanzando una velocidad máxima.

Algunos aceleradores poseen instalaciones especiales que aprovechan esa radiación, a veces llamada luz sincrotrón. Esta radiación se utiliza como fuentes de Rayos X de alta energía, principalmente en estudios de materiales o de proteínas por espectroscopia de rayos X o por absorción de rayos X por la estructura fina (o espectrometría XAS).

Esta radiación es mayor cuando las partículas son más ligeras, por lo que se utilizan partículas muy ligeras (principalmente electrones) cuando se pretenden generar grandes cantidades de esta radiación, pero generalmente se aceleran partículas pesadas, protones o núcleos ionizados más pesados, que hacen que estos aceleradores puedan alcanzar mayores energías. Este es el caso del gran acelerador circular del CERN donde el LEP, colisionador de electrones y positrones, se ha sustituido por el LHC, colisionador de hadrones.

Los aceleradores de partículas más grandes y potentes, como el RHIC, el LHC o el Tevatrón se utilizan en experimentos de física de partículas.

El primer ciclotrón fue desarrollado por Ernest Orlando Lawrence en 1929 en la Universidad de California. En ellos las partículas se inyectan en el centro de dos pares de imanes en forma de "D". Cada par forma un dipolo magnético y además se les carga de forma que exista una diferencia de potencial alterna entre cada par de imanes. Esta combinación provoca la aceleración.

Estos aceleradores tienen un límite de velocidad bajo en comparación con los sincrotrones debido a los efectos. Aun así las velocidades que se alcanzan son bastante altas, llamadas relativistas por ser cercanas a la velocidad de la luz. Por este motivo se suelen utilizar unidades de energía (electronvoltios y sus submúltiplos habitualmente) en lugar de unidades de velocidad. Por ejemplo, para protones, el límite se encuentra en unos 10 MeV. Por este motivo los ciclotrones solo se pueden usar en aplicaciones de "bajas energías". Existen algunas mejoras técnicas como el sincrociclotrón o el ciclotrón síncrono, pero el problema no desaparece. Algunas máquinas utilizan varias fases acopladas para utilizar mayores frecuencias (por ejemplo el rodotrón).

Estos aceleradores se utilizan por ejemplo para la producción de radioisótopos de uso médico (como por ejemplo la producción de F para su uso en los PET), para la esterilización de instrumental médico o de algunos alimentos, para algunos tratamientos oncológicos y en la investigación. También se usan para análisis químicos, formando parte de los llamados espectrómetros de masas.

Para alcanzar energías superiores, del orden de los GeV y superiores, es necesario utilizar sincrotrones.
Uno de los primeros sincrotrones, que aceleraba protones, fue el Bevatron construido en el Laboratorio nacional Brookhaven (Nueva York), que comenzó a operar en 1952, alcanzando una energía de 3 GeV.

El sincrotrón presenta algunas ventajas con respecto a los aceleradores lineales y los ciclotrones. Principalmente que son capaces de conseguir mayores energías en las partículas aceleradas. Sin embargo necesitan configuraciones de campos electromagnéticos mucho más complejos, pasando de los simples dipolos eléctricos y magnéticos que usan el resto de aceleradores a configuraciones de cuadrupolos, sextupolos, octupolos y mayores.

Estos aceleradores llevan asociado el uso de mayores capacidades tecnológicas e industriales, tales como y entre otras muchas:

Al igual que en otras áreas de la tecnología de punta, existen múltiples desarrollos que se realizaron para su aplicación en estos aceleradores que forman parte de la vida cotidiana de las personas. Quizá el más conocido fue el desarrollo de la "World Wide Web" (comúnmente llamada web), desarrollado para su aplicación en el LEP.

La única forma de elevar la energía de las partículas con estos aceleradores es incrementar su tamaño. Generalmente se toma como referencia la longitud del perímetro de la circunferencia (realmente no forman una circunferencia perfecta, sino un polígono lo más aproximado posible a esta). Por ejemplo tendríamos el LEP con 26,6 km, capaz de alcanzar los 45 GeV (91 GeV para una colisión de dos haces en sentidos opuestos), actualmente reconvertido en el LHC del que se prevén energías superiores a los 7 TeV.

Existen varios proyectos para superar las energías que alcanzan los nuevos aceleradores. Estos aceleradores se espera que sirvan para confirmar teorías como la Teoría de la gran unificación e incluso para la creación de agujeros negros que confirmarían la teoría de supercuerdas.

Para 2015-2020 se espera que se construya el Colisionador lineal internacional, un enorme linac de 31 km de longitud, inicialmente de 500 GeV que se ampliarían hasta 1 TeV. Este acelerador utilizará un láser enfocado en un fotocátodo para la generación de electrones. En 2007 no se había decidido aún qué nación lo albergaría.

El Supercolisionador superconductor (SSC en su acrónimo inglés) fue un proyecto para la construcción de un sincrotrón de 87 km de longitud en Texas que alcanzaría los 20 TeV. En 1993 el proyecto se canceló después de haber construido 23,5 km del túnel debido a su altísimo coste motivado por la gran desviación sobre el presupuesto previsto. En 2006 las propiedades e instalaciones fueron vendidas a un grupo de inversión, estando el sitio en la actualidad en estado de abandono.

Se cree que la aceleración de plasmas mediante láseres conseguirán un incremento espectacular en las eficiencias que se alcancen. Estas técnicas han alcanzado ya aceleraciones de 200 GeV por metro, si bien en distancias de algunos centímetros, en comparación con los 0,1 GeV por metro que se consiguen con las radiofrecuencias.

Las partículas cargadas (las únicas que pueden acelerar los campos electromagnéticos presentes en los aceleradores) se generan de diversas formas. La forma más sencilla es utilizar el propio movimiento que se genera al calentar un material. Esto se hace habitualmente calentando un filamento hasta su incandescencia haciendo pasar por el una corriente eléctrica, aunque también se puede hacer enfocando un láser en él. Al aumentar la temperatura también aumenta la probabilidad de que un electrón de la corteza atómica la abandone momentáneamente. Si no existe un campo electromagnético cerca que lo acelere en dirección contraria este electrón (cargado negativamente) regresaría al poco tiempo al átomo ionizado (positivamente) al atraerse las cargas opuestas. Sin embargo, si colocamos cerca del filamento una segunda placa, creando una diferencia de potencial entre el filamento y ella, conseguiremos acelerar el electrón.

Si en esa placa efectuamos un pequeño agujero, y tras él un conducto al que se le haya extraído el aire, conseguiremos extraer electrones. Sin embargo, si no existe ese agujero el electrón impactará contra la placa generando rayos X.

Cuando se pretenden generar protones, sin embargo, es necesario ionizar átomos de hidrógeno (compuestos únicamente por 1 protón y 1 electrón). Para ello puede utilizarse como primera fase el sencillo acelerador de electrones descrito haciendo incidir el haz de electrones o de rayos X sobre una válvula rellena de gas hidrógeno. Si en esa válvula situamos de nuevo un par de placas sobre las que aplicamos un potencial se obtendrán por un lado electrones acelerados y por el opuesto, protones acelerados. Un ejemplo de este tipo de aceleradores es el LANSCE o si en el Laboratorio Nacional Los Álamos (Estados Unidos).

Los positrones se generan de forma similar, solo que necesitaremos hacer incidir fotones de energías superiores a los 1,1 MeV sobre un blanco (de oro, Wolframio o cualquier otro material pesado). Esa energía es la mínima necesaria para crear un par electrón-positrón. La eficiencia de esta generación es muy pequeña, con lo que en los colisionadores electrón-positrón se gasta gran parte de la energía consumida en este proceso.

Actualmente existe también interés en generar neutrones para utilizarlos en máquinas transmutadoras. Para ello se utilizan protones generados como se ha descrito, que impactan sobre blancos cuya sección eficaz o probabilidad de generación de neutrones sea alta. Al no poder acelerar más los neutrones (como se dijo, solo las partículas cargadas pueden acelerarse), su velocidad (o energía) final dependerá exclusivamente de la energía inicial del protón.

Prácticamente todas las partículas descritas se utilizan para tratamientos médicos, ya sea en diagnóstico (rayos X, TAC, PET), como en el tratamiento de tumores sólidos (el uso de protones y neutrones se está generalizando cada vez más para el tratamiento de tumores de difícil tratamiento).

Todos los aceleradores se rigen por las ecuaciones básicas del electromagnetismo desarrolladas por Maxwell. Sin embargo, existe una ecuación muy sencilla que sirve para definir las fuerzas que actúan en cada tipo de acelerador. Esta es la ecuación o ecuaciones (cuando se usan de forma separada) de Lorentz. La ecuación puede escribirse de forma básica como:

donde formula_2 es la fuerza que sufre la partícula cargada dentro del campo electromagnético, "q" es la carga de la partícula cargada (-1 para el electrón, +1 para el positrón o el protón, y mayores para núcleos pesados), formula_3 es el valor del campo eléctrico, formula_4 el campo magnético y formula_5 la velocidad de la partícula.

La ecuación se traduce en que la partícula recibe una aceleración que es proporcional a su carga e inversamente proporcional a su masa. Además, los campos eléctricos "empujan" a la partícula en la dirección del movimiento (el sentido dependerá del signo de la carga y del sentido del propio campo eléctrico), mientras que los campos magnéticos "curvan" la trayectoria de la partícula (solo cuando el campo magnético es perpendicular a la trayectoria), empujándola hacia el centro de una circunferencia cuyo radio dependerá de la magnitud del campo magnético, de la velocidad que posea la partícula en ese momento y de su carga y masa.

En resumen, los campos eléctricos aportan cambios en el módulo de la velocidad de la partícula, acelerándola o desacelerándola, mientras que los campos magnéticos la hacen describir trayectorias curvas sin modificar su módulo (esto no es exactamente así, ya que las partículas perderán energía por la radiación sincrotrón, pero sirve como primera aproximación).

Los aceleradores poseen unos cuantos componentes básicos que son:


Para "crear" las partículas generadas en los grandes aceleradores se necesitan blancos, donde las partículas impactan, generando una enorme cantidad de partículas secundarias.

Los blancos se pueden distinguir entre fijos o móviles. En los fijos se engloban todos aquellos que hacen impactar las partículas aceleradas contra un blanco inmóvil, como los aparatos de rayos X o los utilizados en la espalación. En los móviles se encuentran aquellos que hacen impactar las propias partículas entre ellas, por ejemplo en los colisionadores, duplicando de este modo de forma sencilla la energía que pueden alcanzar los aceleradores.

Para "ver" las partículas generadas en el impacto contra el blanco son necesarios los detectores, que actuarían como los ojos de los científicos.

Dos de los detectores más conocidos construidos para detectar las partículas creadas en las colisiones son: CMS y ATLAS, instalados en el LHC.

Una versión sencilla del conjunto "acelerador"-"blanco"-"detector" sería el aparato de televisión. En este caso el tubo de rayos catódicos es el acelerador, que impulsa los electrones hacia la pantalla revestida de fósforo interiormente que actuaría de blanco, transformando los electrones en fotones (con energía en el rango del visible) que, si estuviéramos mirando la televisión, impactarían en los conos y bastoncillos de nuestras retinas (detectores), enviando señales eléctricas a nuestro cerebro (el supercomputador) que interpreta los resultados.




</doc>
<doc id="3591" url="https://es.wikipedia.org/wiki?curid=3591" title="Large Electron-Positron collider">
Large Electron-Positron collider

LEP ("Large Electron-Positron collider") fue un acelerador-colisionador ee circular de unos 27 km de longitud, creado en 1989 y en funcionamiento hasta el 2000. Situado a 100 m bajo tierra en los terrenos de la Organización Europea para la Investigación Nuclear, en la frontera entre Francia y Suiza, fue reemplazado por el Gran colisionador de hadrones. Era el último paso del complejo de aceleradores del CERN, y en él los electrones y positrones eran inyectados y acelerados hasta la energía final de colisión mediante el uso de cavidades de radiofrecuencia. Un sistema de imanes dipolares curvaba los haces de electrones y positrones obligándoles a seguir una trayectoria circular.

En el LEP, los electrones y los positrones circulaban en sentidos opuestos a velocidades relativistas (cercanas a c, agrupados en paquetes ("bunches") de aproximadamente 1,6 cm de longitud y una sección de 0,3 × 0,01 mm².

Existían ocho puntos de colisión, en cuatro de los cuales había instalados varios experimentos: ALEPH, DELPHI, L3 y OPAL.

El LEP empezó a operar en agosto de 1989 y aunque originalmente fue diseñado para la producción de bosones Z (cuya masa es de 91,2 GeV/c), con energías por haz previstas para su primera fase en torno a los 45 GeV y luminosidades de 10 cm·s, las distintas mejoras que en los últimos años se introdujeron en él (incluyendo la instalación de cavidades superconductoras) permitieron alcanzar energías por haz de hasta 104,5 GeV. 

Se denominó LEP 2 (también LEP200 o LEP-II) a la segunda fase del acelerador de partículas LEP, en la cual se ha incrementó la energía de colisión en el centro de masas por encima de los 130 GeV. Este incremento permitió la producción de pares de bosones W y Z. Se esperaba que los sucesivos incrementos supusieran, incluso, el alcance del umbral de producción de nuevas partículas, como, por ejemplo, el bosón de Higgs. 

Las energías de colisión alcanzadas en el sistema centro de masas en cada año de funcionamiento, y la luminosidad integrada correspondiente recogida en el detector DELPHI, pueden verse en la siguiente tabla.

Parte de la infraestructura del LEP (en particular su túnel toroidal de 27 km) ha sido utilizada para construir el LHC ("Large Hadrons Collider").


</doc>
<doc id="3593" url="https://es.wikipedia.org/wiki?curid=3593" title="Jet">
Jet

El término jet puede referirse:


Azabache

</doc>
<doc id="3595" url="https://es.wikipedia.org/wiki?curid=3595" title="Ecuador terrestre">
Ecuador terrestre

La línea ecuatorial (del latín "æquātōris": igualador), también llamado ecuador terrestre, el ecuador o paralelo 0°, es el círculo máximo perpendicular al eje de rotación del planeta Tierra. Como todo círculo máximo, define un plano que pasa por el centro. Divide al planeta en los hemisferios norte y sur y se encuentra a la misma distancia de los polos geográficos. Por definición, la latitud del ecuador es 0°. El radio ecuatorial es de 6378,1 km y la circunferencia correspondiente es de 40 075 km.

El Sol, en su movimiento aparente, pasa por el plano ecuatorial dos veces al año (en los equinoccios de marzo y de septiembre) momentos en los que los rayos solares son perpendiculares a la superficie de la Tierra en la línea ecuatorial, es decir, el Sol se ubica en el cénit el observador.

En las regiones ubicadas sobre la línea ecuatorial terrestre la duración de la salida y de la puesta del Sol es más corta que en el resto del planeta, debido a que, en el transcurso de todo el año, el Sol «aparece» y «se oculta» casi verticalmente. La duración del día en la línea ecuatorial es prácticamente constante a lo largo de todo el año: aproximadamente 14 minutos más que la noche, propiciado por la refracción atmosférica y porque la salida y la puesta del Sol no están determinadas por el paso del centro del Sol sobre el horizonte, sino por el paso del borde del disco solar. Por lo tanto el momento del amanecer antecede al paso del centro del sol por el horizonte, y la puesta del Sol es posterior al paso del centro del Sol por la línea del horizonte. 

En la línea ecuatorial, la Tierra se ensancha ligeramente. El diámetro promedio del planeta es de 12 750 kilómetros. El radio ecuatorial es 43 kilómetros mayor que el resultante de medirlo pasando por los polos.

Los lugares cercanos a la línea son más adecuados para la ubicación de puertos espaciales, como el caso del Centro Espacial de Guayana, ubicado en Kourou (Guayana Francesa), porque su movimiento debido a la rotación de la Tierra es más rápido en comparación con el de otras latitudes, ya que esta adición de velocidad requiere menos combustible para lanzar vehículos espaciales. Para tomar ventaja de este hecho los lanzamientos deben dirigirse al este, al sureste o al noreste.

La latitud de la línea ecuatorial es por definición 0° (cero grados). Es el único de los cinco círculos notables en la latitud de la Tierra que es estrictamente un círculo, al igual que lo es el trazo imaginario que resulta de su proyección sobre la esfera celeste. Los otros cuatro «círculos» notables son los dos círculos polares y los dos círculos tropicales (trópico de Cáncer en el hemisferio norte y trópico de Capricornio en el hemisferio sur).

El núcleo externo de la Tierra produce el campo magnético terrestre que se extiende hasta la magnetósfera. En la superficie terrestre, ese campo orienta las agujas de las brújulas. La intensidad de campo en superficie no es constante. Es máxima cerca de los polos y mínima cerca del ecuador, definiendo así un ecuador magnético que sigue aproximadamente el ecuador geográfico. El punto donde más se aleja el ecuador magnético del geográfico es a los 15°S en Sudamérica en relación con la Anomalía del Atlántico Sur.

Las estaciones del año en los trópicos y en la línea ecuatorial difieren significativamente de las estaciones en las zonas templadas y de las polares. En muchas regiones tropicales se identifican únicamente dos estaciones, una de lluvia y otra de sequía, pero la mayoría de lugares cercanos a la línea ecuatorial son lluviosos durante todo el año. Sin embargo, las estaciones pueden variar dependiendo de una variedad de factores, que incluyen la elevación, vientos y la proximidad al océano.

Los meteorólogos definen el clima de un lugar como «ecuatorial», en vez de «tropical», si la diferencia entre las temperaturas normales de los meses más cálidos y más fríos es inferior a 2 °C y durante todo el año ocurren lluvias abundantes y constantes.

La línea ecuatorial posee características climáticas que implican alta temperatura, abundante lluvia, viento apacible y baja presión, los cuales son indicadores del clima ecuatorial lluvioso. Sin embargo, las medias climáticas máximas anuales longitudinales de estos valores tienen una trayectoria propia debido a diferencias climáticas en ambos hemisferios de la Tierra. El hemisferio norte posee la mayor superficie de los continentes y tierras emergidas, por lo que tiende a calentarse en mayor proporción que el hemisferio sur, por lo que estos indicadores climáticos ecuatoriales se sitúan con mayor frecuencia en el hemisferio norte; aunque durante el año migran de hecho de un hemisferio al otro. 

Se ha definido los siguientes ecuadores climáticos:

La superficie de la Tierra cruzada por la línea ecuatorial es mayoritariamente oceánica. La línea ecuatorial pasa por los siguientes países:



</doc>
<doc id="3597" url="https://es.wikipedia.org/wiki?curid=3597" title="Litología">
Litología

La litología (del griego λίθος, litos, piedra; y λόγος, logos, estudio) es la parte de la geología que estudia las características de las rocas que aparecen constituyendo una determinada formación geológica, es decir una unidad litostratigráfica, en la superficie del territorio, o también la caracterización de las rocas de una muestra concreta. Se distingue de la petrología, que estudia y describe (petrografía) en todos sus aspectos lo que caracteriza a los diversos tipos de rocas que existen, aunque en castellano y en francés litología se usó antiguamente como sinónimo de petrología. Por ejemplo, el estudio de las características de los granitos, o del tipo específico de granitos que se encuentran en cierta región, es hacer petrología; el estudio de las diversas rocas (que pueden incluir granitos) que debe atravesar una carretera en construcción, como parte de un estudio geotécnico, es hacer litología.

Ayuda a comprender el concepto de litología la existencia de mapas específicamente litológicos, que son mapas que representan la distribución de las rocas superficiales, las que afloran (al aire) o están cubiertas solo por regolito, suelo y vegetación, o productos de la actividad humana, como edificios o carreteras.

Cuando un tipo de roca, o la alternancia sistemática de varias, cubren una superficie extensa, la evolución del relieve queda condicionada. La geomorfología litológica es el estudio del relieve desde el punto de vista de la influencia del tipo de roca aflorante. Ejemplos significativos de tipos de relieve por causas litológicas son el relieve kárstico o los badlands; en ambos casos se requieren además condiciones climáticas o estructurales. También son muy característicos los relieves volcánicos, pero en este caso es más por las estructuras, como los conos volcánicos, que se explican por su proceso de formación, que por un comportamiento especial de las rocas volcánicas frente a los factores que modelan el relieve.

Aunque etimológicamente la palabra litología es el nombre de una ciencia, equivalente a la palabra petrología, en su uso moderno es una metonimia, por la que se utiliza para designar lo que la ciencia ha descrito, es decir, las rocas aflorantes de un lugar. Por ejemplo, podemos ver escrito «la litología de la depresión de Vera y de Sorbas oriental…» para describir el tipo de rocas que afloran en el relieve de esa comarca.




</doc>
<doc id="3598" url="https://es.wikipedia.org/wiki?curid=3598" title="Ideograma">
Ideograma

Un ideograma es un signo esquemático no lingüístico que representa globalmente conceptos o mensajes simples. Por ejemplo, las señales de tráfico o los símbolos matemáticos. Se caracterizan por su universalidad, su economía y la rapidez con que se verifica su percepción: de ahí su amplísimo uso.

El concepto de ideograma representa un ser o una idea directamente sin necesidad de transcribir palabras o frases que lo expliquen. En ciertas lenguas, además, el ideograma simboliza una palabra o lexema, pero no describe cada una de sus sílabas o fonemas, porque no son logogramas. Resulta así que, por ejemplo, el pueblo chino puede leer textos ideográmicos de su lengua de hace miles de años sin saber cómo se pronunciaban entonces las palabras correspondientes.
Se distingue de un pictograma en que ha perdido en parte o completamente su carácter icónico o figurativo: se trata de signos más elaborados y esquemáticos que los pictogramas, en camino de transformarse propiamente en símbolos; podría decirse que son pictogramas resumidos. 

Se presenta aislado o constituyendo conjuntos en escrituras ideográmicas homogéneas, o de forma mixta, mezclados con otros tipos de signos auxiliares (logogramas, fonogramas, pictogramas) en determinados sistemas de escritura o en diagramas o infogramas (por ejemplo, el mapa del metro).

Como ejemplos de escrituras donde hay una fuerte presencia de ideogramas pueden citarse el sistema nsibidi (una alternativa de escritura muy popular en el sur de Nigeria), el japonés y el chino. Además, a fin de aportar datos interesantes sobre los ideogramas, puede decirse que estos le han servido a los egipcios y a los mayas para desarrollar la escritura jeroglífica.
En la actualidad, los ideogramas adquieren especial relevancia en países como Japón, donde desde 1995 se suele organizar un concurso nacional de carácter anual para identificar al kanji (ideograma japonés) más popular del año. En esta oportunidad, según los datos que se dieron a conocer, el ideograma más votado de 2011 resultó ser el de kizuna (entendido como ‘vínculo’). Detrás quedaron los ideogramas de sai (‘desgracia’), shin (‘terremoto’), nami (‘ola’) y jo (‘ayuda’). En 2010, como consecuencia de la ola de calor que afectó al país asiático, el kanji ganador resultó ser el ideograma que representa a sho (‘calor’).
En el resto del mundo, los ideogramas son utilizados con frecuencia por motivos artísticos, ya que se los aplica a objetos decorativos para embellecerlos (jarrones, marcos, alfombras, etc.) y/o como diseños originales para tatuajes. 

Las escrituras ideográmicas son raras, siendo lo más común que los ideogramas se combinen con otro tipo de logogramas que no representan directamente ideas o conceptos. Las escrituras que usan algunos ideogramas como la jeroglífica egipcia, la sumeria o la china, rápidamente empezaron a usar el mismo signo para grupos de ideas semánticamente relacionadas o para palabras con un sonido similar pero para las cuales era más difícil crear un pictograma realista del concepto. 

Esos hechos hacen que muchas de estas escrituras evolucionaran hacia principios de representación mixtos que dejaban de ser estrictamente ideográmicos. Los ideogramas suelen formarse por la combinación de pictogramas, caracteres que indican una idea mediante su representación gráfica. Ambas están muy ligadas históricamente, aunque los ideogramas son posteriores. 

En ciertas escrituras, como la china, la japonesa, en su momento la náhuatl o la nsibidi, determinados símbolos representan palabras o ideas completas.

Por ejemplo, en la escritura china el pictograma 人 (pronunciado rén) significa persona y es una representación deformada del perfil de un hombre. Basándose en esto, el ideograma 囚 (qiú) representa a una persona dentro de un recuadro, y significa "prisionero". Otros ejemplos parecidos son 木 (mù), que significa árbol, ya que representa la forma de uno, y 林 (lín) donde se dibujan dos árboles, lo cual se interpreta en castellano como "bosque".

Las combinaciones de ideogramas para transmitir mensajes más elaborados y complejos suelen denominarse diagramas. Pero si un diagrama se combina con logogramas o, gracias a los medios electrónicos, se vuelve dinámico e interactivo, se puede hablar ya no de diagrama, sino de infografía.


</doc>
<doc id="3599" url="https://es.wikipedia.org/wiki?curid=3599" title="Literatura española">
Literatura española

La literatura española es aquella desarrollada en castellano en España. También podría incluirse en esta categoría la literatura hispanolatina clásica y tardía, la literatura judeoespañola y la literatura arábigoespañola, escritas respectivamente en latín, hebreo y árabe. Abarca desde las primeras expresiones poéticas conservadas en lengua vernácula (las jarchas) hasta la actualidad, más de mil años de historia. Es una rama de la literatura románica y ha dado lugar a otra importante rama, la literatura hispanoamericana.

La literatura española se engloba dentro de la literatura en español, en la que se incluyen las literaturas en castellano y español de todos los países hispanohablantes. Por otro lado, también está englobada en la literatura de España, junto con las de las demás lenguas habladas en el país.

Sólo a partir del siglo XIII y en un sentido exclusivamente geográfico es posible hablar de literatura española escrita. Hasta este período, se supone la coexistencia de una poesía de transmisión oral en lengua romance, tanto lírica como épica, junto a unos usos escriturales cultos cuya lengua de expresión y transmisión era el latín.

Hasta la década de 1950 fue habitual considerar que el comienzo de la literatura española se daba con una obra épica: el "Cantar de Mio Cid" (siglo XII), obra que era transmitida generalmente de forma oral por los juglares. La historiografía literaria no tuvo en cuenta datos proporcionados por crónicas anteriores a la definitiva fijación textual de dicho cantar de gesta. Estos datos se refieren a la tradición oral tanto en su versión lírica más antigua como a los romances, ambas formas de expresión que formaban parte del patrimonio popular. En el año 1948, Samuel Miklos Stern, un investigador húngaro, descubrió en antiguos manuscritos conservados en El Cairo, unas estrofas líricas en lengua romance aljamiada, denominadas jarchas. Actualmente, se asume que estas no reflejan un romance castellano, sino el romance mozárabe.


Se ha señalado que este texto podría interpretarse por sus características más bien como la variedad riojana del romance navarroaragonés.

Cronológicamente el primero en surgir es el Mester de Juglaría, formado por cantares de gesta que imitan las "chansons" francesas al principio y luego reaccionan con una temática nacional bien diferenciada agrupándose en varios ciclos, de los cuales los más importantes son los relativos a El Cid, a los Siete infantes de Lara y el relativo a Bernardo del Carpio. Frente a la épica francesa, la épica española posee unos rasgos diferenciales muy acusados:


En este mester podríamos agrupar también la literatura oral tradicional de las jarchas en lengua mozárabe, de las cantigas de amigo en gallego portugués y la literatura trovadoresca que, en lengua provenzal, empiezan a escribir algunos trovadores catalanes. En cuanto a lírica castellana en este siglo apenas nada se ha conservado, salvo algunos restos de villancicos.

Según Ramón Menéndez Pidal el "Cantar del Mío Cid" fue compuesto alrededor del año 1145, cuarenta y seis años después de la muerte del Cid; Antonio Ubieto Arteta, sin embargo, ha corregido esa hipótesis inicial y ha fechado la composición de la obra alrededor del año 1207. Se ignora el autor, aunque debía poseer algunos conocimientos jurídicos y quizá se hallaba relacionado con el culto sepulcral establecido en torno al sepulcro del Cid en el monasterio de San Pedro de Cardeña; Menéndez Pidal piensa, a causa de la distribución de los topónimos que se encuentran en el Cantar, que pudieron ser dos autores relacionados con San Esteban de Gormaz y Medinaceli; el manuscrito fue copiado por un tal Per Abbat, Pedro Abad.







Durante el siglo XV surge el llamado Prerrenacimiento, la producción literaria aumentó exponencialmente y los poetas más destacados de este siglo son Juan de Mena, Íñigo López de Mendoza (marqués de Santillana) y Jorge Manrique, quien con su obra "Coplas a la muerte de su padre" reflejó perfectamente la aceptación cristiana de la muerte.

El período histórico que sucede a la Edad Media en Europa es conocido como el Renacimiento, comprende todo el siglo XVI aunque sus precedentes se encuentran en los siglos XIV y XV y sus influencias se dejan notar en el XVII. 

Se inició en Italia y se extendió por toda Europa favorecido por el invento de la imprenta.

Los escritores del renacimiento adoptaron como modelos que debían ser imitados a los escritores de la antigüedad clásica, y a los grandes italianos del siglo XIV Dante, Petrarca, y Boccaccio. Este movimiento fue influido por los humanistas que estudiaron la cultura de Grecia y Roma, entre los que destacan Erasmo de Róterdam, Antonio de Nebrija y Juan Luis Vives.

Durante la Edad Media el arte es un medio para honrar a Dios. En el Renacimiento el centro del mundo es el hombre, los poetas cantan al amor humano, la naturaleza, los hechos guerreros, y también tratan temas filosóficos y políticos. 
Juan Boscán influido por los artistas italianos e instado por Navagero, introduce las nuevas formas, escribiendo muchos poemas de gran calidad. 

Su amigo Garcilaso de la Vega es el definitivo adaptador de las formas italianas, utilizando el verso endecasílabo y los recursos típicos de la poesía italiana: soneto, terceto, la canción, la lira, la rima interna, los versos sueltos.

Una serie de poetas siguieron los pasos formando la Escuela Petrarquista cuyos representantes más importantes son: 

Existen dos tendencias: 

La aparición de este género en España parece influenciada por místicos extranjeros anteriores como Kempis, Tauler, Ruysbrock, etc. 
Entre los primeros escritores ascéticos está el Beato Juan de Ávila (1500-1569). 
Los más importantes escritores ascéticos son: 






La Literatura española en el siglo XIX puede dividirse en varias etapas:
En 1898, con el desastre del 98, comienza el siglo XX respecto al ámbito literario.

Cansados del escrupuloso rigor de los escritores ilustrados, surge, en la década de 1830 y bajo la influencia de los escritores prerrománticos europeos, como Goethe o Rousseau, el Romanticismo en España. Los autores románticos se rebelan contra todo lo establecido por el Neoclasicismo, son atraídos por lo misterioso y tratan de evadirse del mundo que les rodea, disgustados por la sociedad burguesa y apática en la que les tocó vivir.

En esta época, los conservadores trataban de preservar sus privilegios, mientras los liberales luchaban por suprimirlos. En Europa se desarrolla fuertemente la industria y crece culturalmente, mientras España parecía aislarse cada vez más, dando la imagen de un país atrasado.

Las primeras manifestaciones del Romanticismo en España fueron en Andalucía, siendo uno de sus máximos exponentes la escritora Cecilia Böhl de Faber y Larrea, más conocida por su pseudónimo, Fernán Caballero. Fue precisamente su padre Juan Nicolás Böhl de Faber quien publicó en el "Diario Mercantil" de Cádiz una serie de artículos defendiendo el teatro del Siglo de Oro, y en Cataluña, a través del diario "El Europeo", siguiendo el modelo de Böhl y defendiendo un Romanticismo moderado y tradicionalista. Uno de los principales introductores del prerromanticismo fue Manuel José Quintana.

En la poesía, los poetas plasman con euforia y pasión todo cuanto sienten. Los principales temas son el amor pasional, las reivindicaciones sociales, el Yo del poeta y la naturaleza, ambientada en lugares oscuros y misteriosos.

El representante más destacado de la poesía del Romanticismo es José de Espronceda (1808-1842), aunque también cabe destacar a otros poetas como Carolina Coronado (1823-1911), Juan Arolas (1805-1873), el gallego Nicomedes Pastor Díaz (1811-1863), Gertrudis Gómez de Avellaneda (1814-1873) y Pablo Piferrer (1818-1848).

Canción del pirata es un poema escrito por José de Espronceda y publicado por primera vez en la revista El Artista en 1835.

El autor está tomando el sentido del poema con un claro tema de denuncia social y da énfasis a la libertad, la característica del romanticismo en este poema son que el poeta se rebela contra todo lo que se opone a su yo personal y a todas las limitaciones políticas, es simbólico porque no habla directamente del autor sino hace un paralelismo con un pirata. Defiende la libertad que constituye la base del pensamiento romántico. Todo el poema está relacionado con el mar que es el paisaje amplio y representa la libertad.

El teatro neoclásico no logró calar en los gustos de los españoles. A comienzos del siglo XIX aún se aplaudían las obras del Siglo de Oro. Estas obras eran despreciadas por los neoclásicos por no sujetarse a la regla de las tres unidades (acción, lugar y tiempo) y mezclar lo cómico con lo dramático. Sin embargo aquellas obras atraían fuera de España, precisamente por no sujetarse al ideal que defendían los neoclásicos.

El Romanticismo triunfa en el teatro español con "La conjuración de Venecia", de Francisco Martínez de la Rosa; "El Trovador", de Antonio García Gutiérrez; "Los amantes de Teruel", de Juan Eugenio Hartzenbusch; pero el año clave es 1835, cuando se estrena "Don Álvaro o la fuerza del sino", del Duque de Rivas (1791–1865). Cabe mencionar también la importante obra "Don Juan Tenorio" (1844) de José Zorrilla y "Muérete y verás" de Bretón de los Herreros. Lo más cultivado es el drama. Todas las obras contienen elementos líricos, dramáticos y novelescos. La libertad domina en el teatro en todos los aspectos.

Ya en la segunda mitad del siglo XIX, los gustos por lo histórico y lo legendario pasaron a un segundo plano, y la poesía se tornó sentimental e intimista. Los poetas están influenciados por la poesía alemana, en especial la de Heinrich Heine.

La poesía, al contrario de la novela y el teatro, continúa siendo romántica (la novela y el teatro seguirá la tendencia realista). Centra su atención a lo emotivo que puede poseer el poema. Se reduce la retórica y se aumenta el lirismo, con el amor y la pasión por el mundo por lo bello como temas principales. Se buscan nuevas formas métricas y nuevos ritmos. La homogeneidad de la que gozaba el Romanticismo se transforma en pluralidad en las ideas poéticas.

Los poetas más representativos de este período son Gustavo Adolfo Bécquer, Augusto Ferrán y Rosalía de Castro, aunque ya no triunfan en aquella sociedad de la Restauración, utilitaria y poco idealista. Se admiró más a los escritores que trataban temas de la sociedad contemporánea, como Ramón de Campoamor y Gaspar Núñez de Arce, pese a que hoy en día no tengan demasiada relevancia crítica.

En España el Realismo caló con suma facilidad, ya que existía un precedente en las novelas picarescas y en "El Quijote". Alcanzó su máximo esplendor en la segunda mitad del siglo XIX (Juan Valera, Pereda y Galdós), aunque sin llegar al punto de rigurosidad de los cánones establecidos por la escuela de Balzac.
También hay que destacar el auge del folletín, con autores como Manuel Fernández y González.
El naturalismo en España, al igual que en Francia, también tuvo sus detractores y se crearon grandes polémicas. Entre los opositores es encuentran Pedro Antonio de Alarcón y José María de Pereda, los cuales llegaron a calificarlo de «inmoral». Sus defensores más encarnizados fueron Benito Pérez Galdós y Emilia Pardo Bazán. La controversia más dura tuvo lugar a partir de 1883, a raíz de la publicación de "La cuestión palpitante" de Pardo Bazán.

Esta generación está formada por una serie de escritores considerada nueva clase nacional. El período de máxima coincidencia como generación tuvo lugar en la década de los ochenta. Dicha generación la integran: Pedro Antonio de Alarcón, José María de Pereda, Benito Pérez Galdós, Juan Valera, Leopoldo Alas "Clarín", Emilia Pardo Bazán y Armando Palacio Valdés.

Las características que definen a este grupo son una conciencia de clase y optimismo (que más tarde tornará al pesimismo, por la revolución de 1868). A nivel individual cada uno presenta un estilo propio. De todos los autores de este grupo, Alarcón es el único que presenta algunos rasgos heredados del romanticismo, sobre todo el costumbrismo más romántico. Esta influencia se aprecia claramente en "Cuentos amatorios" (1881), "Historias nacionales" (1881) y "Narraciones inverosímiles" (1881).

Cierto es que hacia la segunda mitad del siglo XIX la novela evolucionó rápidamente hacia el Realismo, pero esto no ocurrió con la lírica y en el teatro, cuya transformación fue menos violenta y aun continuaron impregnados de romanticismo hasta final de siglo.

Este romanticismo postrero es más aparente que real; en ocasiones carece de fondo y sin la exaltación lírica a la que se entregaba el romanticista de pro. Esto es debido a la sociedad, pues era el momento de la burguesía que consolidaría la Restauración de 1875. Dicha sociedad, que estaba sentando las bases del capitalismo y dando los primeros pasos de industrialización del país, no dejó cabida para las personas que admiraban el arte de forma desinteresada.

Los escritores más representativos son Gaspar Núñez de Arce y Ramón de Campoamor, en ocasiones adscritos al Romanticismo como opositores al movimiento, pues en este romanticismo tardío aun quedaban pequeños vestigios con Gustavo Adolfo Bécquer y Rosalía de Castro.

El teatro realista español describe un arco desde las posturas más conservadoras y acríticas a las más progresistas y ácidas: desde la alta comedia de Adelardo López de Ayala y Ventura de la Vega, al teatro éticamente inquieto de Benito Pérez Galdós y la acerada crítica de Enrique Gaspar y Rimbau, dramaturgo de minorías. Junto a estos autores, se reanudó el interés por el costumbrismo que reflejó el público burgués más conservador a través de géneros como la zarzuela o género chico, el sainete o el teatro por horas. Se trataba de un teatro fundamentalmente de evasión, que procuraba no plantear problemas de conciencia al burgués. Junto a ello, se intentaba revitalizar los antiguos valores conservadores de la honra con las iniciativas para hacer revivir el drama histórico romántico por parte de Manuel Tamayo y Baus o por parte del neorromanticismo del matemático José Echegaray.







</doc>
<doc id="3600" url="https://es.wikipedia.org/wiki?curid=3600" title="Asociación de Academias de la Lengua Española">
Asociación de Academias de la Lengua Española

La Asociación de Academias de la Lengua Española (ASALE) se conformó en México en 1951, originariamente compuesta por veintiuna academias, actualmente está integrada por las veintitrés academias de la lengua española existentes en el mundo.

Su comisión permanente se encuentra en Madrid (España), ciudad en la que también se encuentran la sede de la Real Academia Española (RAE) y la sede central del Instituto Cervantes. El lema de la ASALE es «Una estirpe, una lengua y un destino».

Por iniciativa de Miguel Alemán Valdés, entonces presidente de México, se convoca el ICongreso de Academias con el propósito de trabajar en unión por la integridad y crecimiento del idioma español.

Celebrado el Congreso de Academias entre el 23 de abril y el 6 de mayo de 1951, se crea la Asociación y su Comisión Permanente. En esta primera reunión no estuvo presente la Real Academia Española, pero sí participó en la Comisión Permanente. Desde el IICongreso, celebrado en 1956 en Madrid, la RAE participa regularmente.

Dicha colaboración entre la RAE y las academias de la lengua se expresa en la coautoría, a partir de la XXIIedición (2001), del "Diccionario de la lengua española", la "Ortografía" en sus ediciones de 1999 y 2010, considerada una obra panhispánica, y el "Diccionario panhispánico de dudas" (2005).

Obras conjuntas son la redacción por parte de la Asociación de la "Gramática" y la elaboración de un "Diccionario de americanismos". Desde 2000 organiza la Escuela de Lexicografía Hispánica, que cuenta con becas otorgadas por un convenio entre la RAE y la Fundación Carolina para la formación de expertos en lexicografía del español.

La Asociación, junto a la Real Academia Española, fue galardonada con el Premio Príncipe de Asturias de la Concordia 2000 con motivo de sus esfuerzos de colaboración y consenso.

La Asociación realiza un Congreso cada seis años. La dirección de la Asociación corresponde a la Comisión Permanente, integrada por un presidente (cargo ocupado por el Director de la RAE), un secretario general (que recae en un académico de cualquiera de las academias asociadas, excepto la RAE, elegido por el Congreso), el tesorero designado por la RAE y cuatro vocales de las academias asociadas que se turnan anualmente. Desde 1970, en el IIICongreso de Academias celebrado en Bogotá, Colombia, se aprueba un convenio multilateral por el cual los gobiernos de los países que cuentan con una academia de la lengua se comprometen a apoyarla y dotarla de los medios físicos y financieros para la realización de sus actividades. Estas medidas también se aplican a la Asociación de Academias de la Lengua Española.

Las siguientes Academias de la Lengua Española integran la Asociación (ordenadas por año de creación):

A partir de los estatutos de 1859 se regula el que la Real Academia Española pueda designar como académicos correspondientes a personalidades extranjeras de países vinculados en una u otra manera con la lengua española. Estos académicos, al llegar a cierta masa crítica, se coordinarán para ir dando lugar a partir de 1871 a la aparición de nuevas Academias, primero en países donde era lengua oficial pero ya en el s.XX en países con fuerte presencia hispana, como en 1973 con la aparición de la Academia Norteamericana de la Lengua Española establecida en Estados Unidos

En 2009, la Real Academia Española incluyó en el plantel de académicos a cinco miembros de Guinea Ecuatorial, república africana que formaba parte del grupo de países que, pese a presentar una intensa vinculación histórica con España (como Andorra y el Sahara Occidental) o estar insertos en un entorno cultural hispanohablante (como Belice), no poseían todavía una institución académica de la lengua española.

Y es a finales de 2013 que el presidente de Guinea Ecuatorial, Teodoro Obiang, inaugura la Academia Ecuatoguineana de la Lengua Española, según lo anunció el entonces director de la Real Academia Española, José Manuel Blecua Perdices. Desde el 19 de marzo de 2016 pertenece a la Asociación de Academias de la Lengua Española, después de haberse iniciado los trámites para su ingreso. Fue el 24 de noviembre de 2015 cuando la Academia Ecuatoguineana de la Lengua Española solicitó formalmente su ingreso a la ASALE.

La última academia en formalizarse sucedió tras el interés por la comunidad hispanojudía de establecer una academia de ladino. El 20 de febrero de 2018, se creó la Academia Nasionala del Ladino en Israel, no sin grandes dificultades y retos.






</doc>
<doc id="3601" url="https://es.wikipedia.org/wiki?curid=3601" title="3 de octubre">
3 de octubre

El 3 de octubre es el 276.º (ducentésimo septuagésimo sexto) día del año en el calendario gregoriano y el 277.º en los años bisiestos. Quedan 89 días para finalizar el año.


















</doc>
<doc id="3602" url="https://es.wikipedia.org/wiki?curid=3602" title="2 de octubre">
2 de octubre

El 2 de octubre es el 275.º (ducentésimo septuagésimo quinto) día del año en el calendario gregoriano y el 276.º en los años bisiestos. Quedan 90 días para finalizar el año.




Día Internacional de la No Violencia (Organización de las Naciones Unidas, 2007)






</doc>
<doc id="3604" url="https://es.wikipedia.org/wiki?curid=3604" title="Comunidad de Madrid">
Comunidad de Madrid

La Comunidad de Madrid es una comunidad autónoma del Reino de España que históricamente perteneció a la región de Castilla la Nueva. Está situada en el interior de la península ibérica, en la Submeseta Sur de la Meseta Central. Limita con las provincias de Guadalajara, Cuenca y Toledo (Castilla-La Mancha), así como con las de Ávila y Segovia (Castilla y León). La Comunidad de Madrid es uniprovincial, por lo que no existe diputación. Su capital, Madrid, es también la capital de España. La población asciende a y se concentra mayoritariamente en el área metropolitana de Madrid.

Es la tercera comunidad autónoma en población y la más densamente poblada. Posee una posición central en la red de medios de transportes de España. En 2018 el PIB de Madrid representa el 19,2 % del PIB nacional. Asimismo, cuenta con un rico patrimonio artístico y natural, con tres : el Monasterio y Sitio de El Escorial, la Universidad y casco histórico de Alcalá de Henares, y finalmente, el Paisaje cultural de Aranjuez.

La conformación de la actual comunidad autónoma vino precedida de un intenso debate político, en el contexto preautonómico de finales de los años 1970. La provincia estaba convencionalmente incluida en la región de Castilla la Nueva desde el siglo , junto a las provincias de Cuenca, Guadalajara, Ciudad Real y Toledo. En un principio se planteó la posibilidad de que la provincia formara parte de una autonomía junto a estas provincias, hoy parte de Castilla-La Mancha, si bien con un estatuto especial, dadas sus especiales condiciones al albergar la capitalidad del Estado. En 1981 se resolvió que la provincia de Madrid no se incluiría en una comunidad multiprovincial y se acordó la creación de una comunidad autónoma uniprovincial, aprobándose en 1983 su Estatuto de Autonomía.

La historia de la Comunidad de Madrid es muy reciente. La provincia se constituye administrativamente en el siglo y, a finales del siglo , se configura como una comunidad autónoma uniprovincial. No obstante, existen algunos hitos históricos anteriores, decisivos para la definición del actual perfil de la comunidad autónoma:


Entre todos estos hitos, la capitalidad se destaca como el de mayor determinación histórica, ya que se encuentra en el origen de la provincia madrileña, constituida en el marco de la división provincial de España en el siglo . A este hecho se le añade, en el siglo , la condición metropolitana de Madrid, aspecto clave para su segregación de la antigua región de Castilla la Nueva, en la que Madrid estaba integrada, dados los fuertes desequilibrios sociales, económicos y demográficos que la zona metropolitana de Madrid introducía, y su configuración como comunidad uniprovincial.

El territorio actual de la Comunidad de Madrid estuvo poblado desde el Paleolítico Inferior, principalmente en lo que respecta a los valles interfluviales de los ríos Manzanares, Jarama y Henares, donde se han hallado abundantes y ricos yacimientos arqueológicos. Entre los vestigios más importantes que se han encontrado, destaca especialmente el vaso campaniforme de Ciempozuelos, que ha dado nombre a un tipo especial de cerámica (data del Bronce Inicial, entre 1979 a. C. y 1970 a. C.). También se han descubierto pinturas y grabados rupestres en La Pedriza, en el término de Manzanares el Real, y en la cueva del Reguerillo, en Patones.

Durante el Imperio romano, la región quedó integrada en la provincia Citerior Tarraconense, excepto la parte suroccidental, en el Alberche, que pertenecía a la Lusitania. Estaba surcada por dos importantes calzadas romanas, la vía XXIV-XXIX (de Astorga a Laminium) y la XXV (de Augusta Emerita a Caesaraugusta), y contaba con algunas urbes de importancia. La ciudad de Complutum (Alcalá de Henares) alcanzó cierta relevancia hasta el Bajo Imperio, mientras que Titulcia y Miaccum, al pie de la sierra, destacaron como cruces de caminos.

En la época visigótica, la región perdió toda importancia. Su población se dispersó en pequeñas aldeas e, incluso, Complutum entró en decadencia. Alcalá de Henares fue designada sede episcopal en el siglo , por orden de Asturio Serrano, arzobispo de Toledo, pero este hecho no fue suficiente para devolverle el esplendor perdido.

El centro peninsular fue una de las regiones más despobladas de al-Ándalus hasta el siglo , cuando empezó a despuntar como un enclave militar de gran importancia estratégica. Los musulmanes pusieron en pie un sistema defensivo de fortalezas y atalayas, con el que intentaron detener el avance de los reinos cristianos, a lo largo y ancho del territorio actual de la comunidad autónoma.

El territorio constituía un área sin personalidad propia dentro de la Marca Media con centro en Toledo, y en el cual se erigieron algunos castillos o fortificaciones andalusíes ("ḥuṣūn"), alrededor de alguno de los cuales, como Madrid, se acabó conformando una ciudad ("madīna"):

La fortaleza de Maŷrit (Madrid) se erigió en una fecha indeterminada entre los años 860 y 880, como un "ribat", un recinto amurallado donde convivía una comunidad a la vez religiosa y militar, en lo que constituye el núcleo fundacional de la ciudad. Pronto se destacó como la fortificación de mayor valor estratégico en la defensa de Toledo, por encima de Talamanca de Jarama y de Qal‘at ‘Abd al-Salam (Alcalá de Henares), los otros dos enclaves militares más importantes de ese sistema defensivo.

Alrededor de las cabeceras principales, encargadas de defender los caminos fluviales del Manzanares, del Jarama y del Henares, respectivamente; se construyeron fortificaciones de carácter complementario como el caso de Qal‘at Jalifa (Villaviciosa de Odón, controlando la ruta del río Guadarrama).

También se erigió, probablemente en el siglo bajo iniciativa omeya, una red de atalayas más septentrional que permitía la vigilancia de los pasos; de las cuales se conoce la existencia de las de Venturada, El Vellón o El Berrueco en la zona del Jarama o las de Torrelodones y Collado de la Torrecilla en la zona de la sierra de Hoyo de Manzanares. Estas torres-vigía se comunicaban entre sí mediante señales de humo, cuando se producían situaciones de alerta.

En 1083, el rey Alfonso VI tomó la ciudad de Madrid y dos años después entró en Toledo. Por su parte, Alcalá de Henares sucumbió en 1118, en una nueva anexión del Reino de Castilla.

Las nuevas tierras conquistadas por los cristianos se disgregaron alrededor de varios dominios, como consecuencia de un largo proceso de repoblación (siglos a ), en el que entraron en conflicto los señores feudales o eclesiásticos y los diferentes concejos con potestad real para repoblar.

En primer lugar, en 1118 será reconquistada Alcalá de Henares y toda su Comunidad de Villa y Tierra. En 1135 la Tierra Complutense recibirá un compendio de leyes o fueros, denominado Fuero Viejo. Esto compensará en parte la integración de la Diócesis Complutense en el Arzobispado de Toledo en 1099. En 1223, el arzobispo Rodrigo Ximénez de Rada hará una modificación de estos fueros, en lo que se ha pasado a denominar Fuero Extenso. Será en 1509, cuando el cardenal Cisneros creará un nuevo y actualizado fuero, el Fuero Nuevo, que estará vigente hasta el final del Antiguo Régimen. Estas leyes daban una autonomía legal completa a la Tierra de Alcalá.

En el siglo , Madrid conservará, al igual que Alcalá, una personalidad jurídica propia, en primer término con el Fuero viejo y posteriormente con el Fuero Real, concedido por Alfonso X en 1262 y ratificado por Alfonso XI en 1339. Buitrago del Lozoya, Alcalá de Henares y Talamanca de Jarama destacarán por su importante capacidad repobladora hasta ese siglo.

La Tierra de Alcalá, área administrativa donde rigieron los fueros anteriormente citados, estaba conformada en su última fase (Fuero Nuevo) por los siguientes municipios: Ajalvir, Camarma de Esteruelas, Daganzo de Abajo (o Daganzuelo, hoy despoblado), Torrejón de Ardoz, Valdemora, Arganda, Ambite, Anchuelo, Bilches o Vilches, Campo Real, Carabaña, Corpa, Los Hueros, Loeches, La Olmeda, Orusco, Perales de Tajuña, Pezuela de las Torres, Querencia, Santorcaz, Los Santos de la Humosa, Tielmes, Torres de la Alameda, Valtierra, Valmores, Valverde de Alcalá, Villar del Olmo, Valdilecha y Villalbilla.

Alrededor de la actual capital de la comunidad, se constituyó un territorio administrativo denominado Tierra de Madrid, el primer germen de la provincia, que se extendía, en sus extremos, hasta los actuales términos municipales de San Sebastián de los Reyes, Cobeña, Las Rozas de Madrid, Rivas-Vaciamadrid, Torrejón de Velasco, Alcorcón, San Fernando de Henares y Griñón.

Este concejo mantuvo numerosos litigios con Segovia, por entonces una de las ciudades más influyentes de Castilla, por el control del Real de Manzanares, una vasta comarca, que, finalmente, fue cedida a la Casa de Mendoza. La Comunidad de ciudad y tierra de Segovia había convertido en su Sexmo de Casarrubios, en los valles del los ríos Guadarrama y Perales, el alfoz de la antigua medina islámica de Calatalifa (en el actual término de Villaviciosa de Odón).

La monarquía castellana empezó a mostrar una especial predilección por el centro peninsular, atraída por sus abundantes bosques y cotos de caza. El Pardo era un lugar muy frecuentado por los reyes, desde tiempos de Enrique III (siglo ). Asimismo, los Reyes Católicos impulsaron la construcción del Palacio Real de Aranjuez. En el siglo , San Lorenzo de El Escorial se sumó a la lista de Reales Sitios de la actual provincia.

La propia villa de Madrid, que formaba parte del grupo de dieciocho ciudades con derecho a voto en las Cortes de Castilla, acogió en numerosas ocasiones las Cortes del Reino. Al mismo tiempo, sirvió de residencia a varios monarcas, entre ellos el emperador Carlos I, que reformó y amplió su alcázar. A la creciente influencia sociopolítica de la región, se le añadió, en el siglo , el foco cultural de la Universidad de Alcalá de Henares, que abrió sus puertas en 1508, a instancias del cardenal Cisneros.

En 1561, el rey Felipe II situó la capital de su imperio en Madrid, en lo que puede considerarse el segundo embrión —y tal vez más decisivo— para la configuración posterior de la provincia madrileña.

Con la capitalidad, se impuso un marco de subordinación económica a las tierras colindantes con la villa de Madrid, que incluso iba más allá de los actuales límites de la Comunidad de Madrid. También se promovió una extensión competencial de la Sala de Alcaldes de Casa y Corte (de cinco a diez leguas en su torno), en un intento por articular una región alrededor de la capital.

Pero aún se estaba muy lejos de una auténtica realidad administrativa, sobre todo teniendo en cuenta que el Estado del Antiguo Régimen convivía con la existencia de numerosas jurisdicciones señoriales, tanto laicas como eclesiásticas. Entre las primeras, se encontraban señoríos de gran extensión, como el Real de Manzanares —en manos de los Mendoza— y otros de pequeñas dimensiones, como el señorío de Valverde de Alcalá. Entre las segundas, había jurisdicciones monásticas (como la Cartuja de El Paular), del clero secular (como las extensas posesiones del Arzobispado de Toledo) y de órdenes militares (caso de la Encomienda Mayor de Castilla de la Orden de Santiago, que ocupaba Valdaracete, Villarejo de Salvanés y Fuentidueña de Tajo).

En el siglo tampoco se corrigió la desarticulación administrativa de las tierras madrileñas, a pesar de algunos intentos. En la época de Felipe V, se creó, a escala nacional, la figura de las Intendencias, con poder político-administrativo. Sin embargo, la Intendencia de Madrid no resolvió el problema de raíz y la actual provincia continuó fragmentada en varios dominios, si bien se racionalizaron los procesos a la hora de ejecutar proyectos centralizados.

A Guadalajara le correspondían los partidos de Colmenar Viejo y Buitrago del Lozoya, así como el señorío del Real de Manzanares, coincidente en gran parte con la actual comarca de la Sierra de Guadarrama. Segovia extendía sus dominios al Norte y Oeste de la actual provincia madrileña, mientras que Toledo ocupaba el este, con Alcalá de Henares y Chinchón como núcleos destacados. De Madrid dependían Casarrubios, en la actual provincia de Toledo, y Zorita de los Canes, en la de Guadalajara.

Esta dispersión territorial afectaba a procesos tan básicos como el abastecimiento de Madrid, que había disparado su población hasta convertirse en la ciudad más habitada de la monarquía. El efecto fue drástico: mientras que la Villa de Madrid absorbía un mayor volumen de renta procedente de todo el país, su territorio colindante —en manos de casas nobiliarias y del poder eclesiástico o bajo el influjo real— tendía a empobrecerse, sin posibilidad alguna de desarrollarse un tejido socioeconómico acorde con las necesidades de la capitalidad.

Otro de los problemas que la capitalidad puso en evidencia fue la ausencia de infraestructuras. El entramado de caminos de la Submeseta Sur tenía su centro en Toledo y hubo que articular una red para garantizar el abastecimiento de la ciudad. Del siglo data la estructura radial de las comunicaciones españolas, que tiene su punto neurálgico en la ciudad de Madrid.

A lo largo del siglo , la villa de Madrid se transformó con grandes obras urbanísticas, al compás de las corrientes ilustradas. Destaca la labor de Carlos III, que dotó a la ciudad de algunos de sus más bellos edificios y monumentos, al tiempo que promovió la creación de instituciones sociales, económicas y culturales, que aún perviven.

La villa de Madrid cerró el siglo con 156 672 habitantes (antes de la capitalidad, se estimaba una población en torno a los 15 000 vecinos), según el censo realizado en 1787, el primero, con carácter oficial, que se realizó en la ciudad.

El territorio de la Comunidad de Madrid alcanzó a grandes rasgos sus límites territoriales actuales en 1833 con la división de España en provincias, una de las cuales fue la de Madrid. En esta división, la provincia fue adscrita a la región de Castilla la Nueva, la cual, como el resto de regiones, constituía apenas una clasificación, al carecer de cualquier órgano o institución administrativa. Junto con la de Madrid, fueron incluidas en Castilla la Nueva las provincias de Ciudad Real, Cuenca, Guadalajara y Toledo. Un cambio en los límites de la provincia posterior a la división territorial de 1833 afectaría al pequeño municipio de Valdeavero, de 19 km², hasta entonces perteneciente a la provincia de Guadalajara, que pasó a pertenecer a la provincia de Madrid en 1850.

En el siglo , durante el proceso preautonómico de finales de la década de 1970, en la antigua región de Castilla la Nueva reapareció el temor a que las especiales condiciones económicas y demográficas de Madrid fueran un factor de desequilibrio, por lo que finalmente, la provincia de Madrid se configuró como comunidad autónoma uniprovincial. Fue la última comunidad en constituirse.

Por su parte, las provincias de Ciudad Real, Cuenca, Guadalajara y Toledo (que pertenecían a Castilla la Nueva), junto con la de Albacete (que estaba integrada en la Región de Murcia), constituyeron la comunidad autónoma de Castilla-La Mancha.

El Estatuto de Autonomía de la Comunidad de Madrid fue aprobado el 1 de marzo de 1983. La provincia de Madrid se conformó como comunidad autónoma bajo la Ley Orgánica 3/1983, del 25 de febrero (BOE 1-3-83). Desde junio de 1995 la comunidad Autónoma de Madrid pasa a ser llamada "Comunidad de Madrid" porque las siglas CAM son las mismas que las de la Caja de Ahorros del Mediterráneo (de Alicante), que ganó el pleito interpuesto aduciendo que en la comunidad valenciana había confusión entre ambas siglas al leer los periódicos nacionales. Los madrileños no fueron llamados a las urnas para aprobar o desestimar la propuesta de comunidad autónoma.

Madrid fue elegida capital de la comunidad, si bien han surgido diferentes iniciativas para que otras ciudades alberguen la capitalidad. Es el caso de Alcalá de Henares, que presentó oficialmente su candidatura en los primeros años 1980 y, más recientemente, de Getafe, que en 2006 anunció su aspiración de arrebatarle el título de capital a la villa de Madrid.

Desde su nacimiento han sido elegidos ocho presidentes autonómicos: Joaquín Leguina (1983-1995), del PSOE, y el resto pertenecientes al PP, que son: Alberto Ruiz-Gallardón (1995-2003), Esperanza Aguirre (2003-2012), Ignacio González (2012-2015), Cristina Cifuentes (2015-2018), Ángel Garrido (2018-2019), Pedro Rollán (2019) e Isabel Díaz Ayuso (2019-?).

La Ley 2/1983, de 23 de diciembre, de la bandera, escudo e himno de la Comunidad de Madrid define y regula los símbolos de la Comunidad de Madrid:
La bandera madrileña toma el fondo rojo carmesí del pendón de Castilla. Las siete estrellas, que simbolizan la constelación de la Osa Menor, proceden del escudo de la Tierra de Madrid, concejo formado en tiempos de la Reconquista. Las cinco puntas de las estrellas representan a las cinco provincias limítrofes a Madrid (Ávila, Cuenca, Guadalajara, Segovia y Toledo). Las siete estrellas se alinean en dos filas: en la superior se sitúan cuatro y en la inferior las tres restantes. Diferentes municipios que formaron parte del antiguo concejo de la Tierra de Madrid las incorporan en sus escudos heráldicos. Es el caso de la Villa de Madrid y de Las Rozas de Madrid, que las integraba hasta 1995, cuando el consistorio roceño diseñó un nuevo escudo. Poblaciones situadas bajo la influencia de este concejo también incluyen en sus escudos las siete estrellas (Guadarrama, Valdemorillo o Fresno de Torote). El escudo de Tres Cantos igualmente las incorpora, aunque, en este caso, no como reflejo de su pertenencia al concejo de la Tierra de Madrid, sino en clara referencia a la simbología de la comunidad autónoma. Se trata del municipio más joven de la provincia, constituido en 1991 —cuando se segregó de Colmenar Viejo—, ocho años después de ser aprobada la bandera y el escudo de la Comunidad de Madrid. Popularmente, las siete estrellas se conocen como las siete puertas de entrada a Madrid.

En la Ley 2/1983, de 23 de diciembre, de la Bandera, Escudo e Himno de la Comunidad de Madrid del Estatuto de autonomía madrileño, se especifica sobre el motivo del rojo carmesí:

El escudo de la Comunidad de Madrid es descrito heráldicamente en el Anexo 2 del Decreto 2/1984, en los siguientes términos: 

Así mismo se especifica también sobre el motivo de los castillos pareados: 

La letra es obra del filólogo y poeta Agustín García Calvo y la música del compositor Pablo Sorozábal Serrano los cuales lo hicieron por encargo personal del Presidente de la Comunidad de Madrid, Joaquín Leguina.La composición es el himno oficial de la Comunidad de Madrid desde el 24 de diciembre de 1983, fecha de su publicación en el BOCM.

El texto consta de tres estrofas:
1

2

3

La Comunidad de Madrid se organiza territorialmente en 179 municipios y en 784 entidades singulares de población. Posee el 2,2 % de los municipios totales que integran el territorio español (8125). Es la vigésimo tercera provincia española en número de ayuntamientos y se sitúa ligeramente por encima de la media, cifrada en 162 municipios por provincia (Burgos cuenta con el mayor número de términos municipales con 371 ayuntamientos, y Las Palmas es la provincia que tiene menos con 34).

La superficie media de los municipios madrileños es de 44,8 km², un promedio bajo el que se esconden grandes oscilaciones. El más extenso de todos ellos es Madrid, ayuntamiento que anexionó, entre 1948 y 1954, los municipios limítrofes de Chamartín de la Rosa, Fuencarral, Barajas, El Pardo, Hortaleza, Canillas, Canillejas, Vicálvaro, Vallecas, Villaverde, Carabanchel Alto, Carabanchel Bajo y Aravaca, convertidos hoy en distritos o barrios.

Los cinco términos municipales más grandes son Madrid, con 605,8 km²; Aranjuez, con 189,1 km²; Colmenar Viejo, con 182,6 km²; Rascafría, con 150,3 km²; y Manzanares el Real, con 128,4 km². Los de menor superficie son Casarrubuelos, con 5,3 km²; La Serna del Monte, con 5,4 km²; Pelayos de la Presa, con 7,6 km²; Madarcos, con 8,5 km²; y Torrejón de la Calzada, con 9,0 km².

Existen veinte partidos judiciales, cuyas cabezas corresponden a los siguientes municipios (el histórico partido judicial de San Martín de Valdeiglesias perdió esta consideración en el año 1985):

A diferencia de otras comunidades autónomas, la Comunidad de Madrid carece de una comarcalización que tenga relevancia administrativa. No obstante, algunas instituciones autonómicas han delimitado diferentes áreas a partir de criterios de homogeneidad geográfica y sociodemográfica, que toman su nombre de los puntos cardinales y de los principales ríos de la región. Su validez se limita a promociones turísticas o a divisiones agrícolas.

Ni siquiera se encuentra definida legalmente el área metropolitana de Madrid, a pesar de su importancia social, demográfica y económica y las necesidades que, en términos de infraestructuras, urbanismo o transportes, comparten los municipios situados bajo la zona de influencia de la capital.

La clasificación que ha conseguido un mayor nivel de implantación, elaborada por la Dirección General de Turismo, establece ocho grandes comarcas y deja al margen al área metropolitana de Madrid: Sierra Norte, Sierra Oeste, Comarca de Las Vegas, Cuenca del Guadarrama, Cuenca Alta del Manzanares, Cuenca del Medio Jarama, Cuenca del Henares y Comarca Sur

Popularmente, los madrileños clasifican su región a partir de las áreas de influencia de las seis autovías radiales que surcan la provincia. La que se articula alrededor de la A-2 recibe el nombre oficioso del Corredor del Henares.

La ausencia de una comarcalización administrativa es consecuencia de la conformación del área metropolitana de Madrid a lo largo del siglo y, especialmente, en su segunda mitad. La progresiva implantación de esta nueva realidad desdibujó el perfil de las comarcas históricas madrileñas, que se relacionan a continuación: Valle del Lozoya,
Guadarrama, Somosierra, Valle del Alberche (compartido con Toledo), El Real de Manzanares, La Sagra, (compartida con Toledo), Lomo de Casarrubios, Alcarria de Alcalá, La Campiña (compartidas con Guadalajara), Alcarria de Chinchón y Cuesta de las Encomiendas.

La comunidad autónoma tiene una superficie de 8021,80 km². Sus límites describen un triángulo equilátero aproximado, en el que su base está en la linde con la provincia de Toledo, al sur, y su vértice superior en el puerto de Somosierra, al norte. El término municipal de Aranjuez rompe esta forma triangular, a modo de apéndice que se adentra en la provincia de Toledo. Fuera de ese triángulo, rodeada por las provincias de Ávila y Segovia, se encuentra la Dehesa de la Cepeda, enclave que pertenece al municipio madrileño de Santa María de la Alameda. La región está situada en el centro de la Meseta Central, en la parte septentrional de la Submeseta Sur, entre el Sistema Central (al norte y noroeste) y el río Tajo (al sur y sureste). Limita al norte y al oeste con Castilla y León (provincias de Segovia y Ávila) y al este y al sur con Castilla-La Mancha (provincias de Toledo, Guadalajara y Cuenca). Existe un diminuto enclave conocido como "Los Barrancos" y "La Canaleja", perteneciente al municipio de Torrejón del Rey (provincia de Guadalajara) que aunque se encuentra geográficamente dentro de la Comunidad de Madrid no pertenece a la región.

A pesar de la existencia de la gran presión urbanística que supone la existencia de más de 6 millones de personas en tan reducido espacio, la Comunidad de Madrid aún conserva algunos hábitats y paisajes notablemente intactos y diversos. Madrid alberga cimas de montañas que superan los 2000 metros de altitud, dehesas de encina y llanuras bajas. Existen tres tipos de paisajes en la Comunidad de Madrid: praderas alpinas y bosques de pinos en Guadarrama, bosques mediterráneos y dehesas en la zona norte más llana y estepa de matorral en el extremo sureste de la región.

Las laderas de la sierra de Guadarrama están cubiertas de densos bosques de pino silvestre y melojo. El valle del Lozoya sostiene una gran colonia de buitre negro, y uno de los últimos bastiones del águila imperial ibérica en el mundo se encuentra en el parque regional del Curso medio del río Guadarrama y su entorno, en las colinas de dehesa, entre las sierras de Gredos y Guadarrama. La posible detección reciente de la existencia de lince ibérico en el área entre los ríos Cofio y Alberche es un testimonio de la biodiversidad del área. También es destacable la presencia del lobo ibérico en la Sierra Norte de Madrid, cerca de Somosierra.

Aprovechando la orografía, existen varios embalses y presas en la región, siendo los de Santillana y El Atazar los más grandes.

La comunidad de Madrid tiene una altitud que varía entre los 430 msnm en el último tramo del río Alberche en término municipal de Villa del Prado y los 2428 metros en el pico de Peñalara. El relieve de la Comunidad de Madrid está definido por dos grandes unidades: la sierra y la llanura del río Tajo, separadas entre sí por el piedemonte.


Las sierras de Guadarrama (en su totalidad), Ayllón (la parte más occidental de esta, conocida como Sierra de Somosierra) y Gredos (la parte más oriental de esta) conforman un paisaje típico de montaña, con altitudes máximas —en cada una de las tres sierras— de 2428 m (Peñalara, el pico más alto de la región), 2129 m (Peña Cebollera o Pico de las Tres Provincias) y 1770 m (alto del Mirlo), respectivamente. Otros picos importantes son La Maliciosa (2227 m) y Siete Picos (2138 m), ambos en la sierra de Guadarrama. En lo que respecta a su litología, el granito y el gneis son las rocas dominantes en las dos primeras sierras, mientras que la pizarra y las cuarcitas lo son de Ayllón (este macizo presenta los materiales rocosos más antiguos de la Comunidad de Madrid, formados hace 450 millones de años). La sierra madrileña está estructurada en falla, aspecto que puede apreciarse a simple vista en la denominada falla de Torrelodones, en el municipio del mismo nombre.


Las campiñas, páramos y vegas configuran geomorfológicamente la segunda unidad de relieve, articulada alrededor de la cuenca del río Tajo. Aquí se encuentran las mínimas altitudes de la comunidad autónoma: 430 m en el cauce del río Alberche —a su paso por Villa del Prado— y 467 m en Fuentidueña de Tajo.

Esta unidad presenta una composición del terreno menos uniforme que la de la sierra. Las calizas, arcillas, yesos y margas son abundantes en los páramos, mientras que las arenas, margas arenosas, margas yesíferas y arcillas dan forma a las campiñas. Las vegas, por último, quedan perfiladas por las arenas, gravas y limos.

A modo de transición entre la sierra y las llanuras arenosas del río Tajo, aparece la llamada Rampa de la Sierra o piedemonte, que se extiende desde la confluencia de los ríos Jarama y río Lozoya, al norte del provincia, hasta el suroeste de la comunidad, formando una franja paralela a la sierra. No se trata exactamente de una unidad de relieve, aunque sí cabe definirla así desde un punto de vista geomorfológico. Se compone fundamentalmente de arenas, arcillas, margas y otros materiales detríticos.

Entre la máxima y mínima altitud de la región (Peñalara y Villa del Prado), se origina un desnivel de unos 2000 m, que se salvan a lo largo de poco más de 100 km (la altura media de la provincia es de 650 m, aproximadamente). Este complejo relieve convierte a Madrid en una comunidad autónoma de contrastes medioambientales. En ella se puede encontrar la mayor parte de los pisos bioclimáticos de la península ibérica (crioromediterráneo, oromediterráneo, supramediterráneo y mesomediterráneo), además de una rica variedad de ecosistemas.

Las montañas de la Comunidad de Madrid que tienen más de 2200 metros de altitud pertenecen a la sierra de Guadarrama. Estos picos, ordenados según altura, son los siguientes: Peñalara (2428 m), Cabezas de Hierro (2383 m), Cerro de Valdemartín (2280 m), Bola del Mundo (2265 m), Asómate de Hoyos (2242 m), La Maliciosa (2227 m) y El Nevero (2209).

La Comunidad de Madrid forma parte de la cuenca hidrográfica del Tajo, río que surca la zona meridional de la región, en la Comarca de Las Vegas, a la altura de Belmonte de Tajo, Brea de Tajo, Fuentidueña de Tajo y Aranjuez. Existen otras cuatro cuencas hidrográficas menores, todas ellas subsidiarias del Tajo: la del Jarama, la del Guadarrama, la del Alberche y la del Tiétar. Todos estos ríos recorren una distancia media de aproximadamente 167 km desde su nacimiento en el Sistema Central hasta su desembocadura en el Tajo. Sin salir de la región, es posible contemplar el curso alto de algunos de ellos, con paisajes típicos de ríos de montaña, así como su curso medio y bajo, como ríos de llanura.


Con sus 190 km, el Jarama es el río más largo e importante de la región —al margen del Tajo—. Su cuenca, la de mayor superficie de toda la provincia, queda integrada por los ríos Lozoya, Guadalix y Manzanares, que vierten sus aguas al Jarama por la derecha, y Henares y Tajuña, que lo hacen por la izquierda. Los embalses de El Atazar, Puentes Viejas, Riosequillo, Santillana y Pedrezuela (antes conocido como El Vellón) son los más relevantes de esta cuenca, responsable en su mayor parte del suministro de agua potable a toda la provincia. Hay que destacar, en este sentido, la importancia del río Lozoya, que, a pesar de su corto recorrido (apenas 91 km), es embalsado hasta en cinco ocasiones.


El Guadarrama surca la comunidad en su curso alto y medio. Sus afluentes se limitan a un único río, el Aulencia (por la derecha), y a dos arroyos mayores, el de La Vega y el de Los Combos (por la izquierda). El principal embalse de esta cuenca es el de Valmayor.

Se sitúa en el extremo suroccidental de la provincia. El Alberche, que pasa por los términos de San Martín de Valdeiglesias y Pelayos de la Presa, recibe por la derecha las aguas de los arroyos de Valdezate y de Tórtoles y por la izquierda las de los ríos Cofio y Perales. Es embalsado en los pantanos de San Juan y Picadas. La calidad de su agua es muy inferior a la de la cuenca del Jarama, razón por la cual se permite el baño, la navegación y la pesca en los citados embalses.


Sólo el vértice suroccidental de la provincia, más o menos coincidiendo con el término de Rozas de Puerto Real, se encuentra incluido dentro de la cuenca del Tiétar. El río nace en las proximidades del citado municipio y discurre por las provincias de Ávila y Cáceres.

El agua de la Comunidad de Madrid es una de las mejores de Europa gracias a las condiciones geológicas de la sierra de Guadarrama, unas montañas con abundancia de granito. Esta roca tan dura deja muy pocos residuos en los ríos de la sierra y hace que la pureza del agua sea muy elevada.

A pesar de su reducida superficie (8021,80 km²), la Comunidad de Madrid presenta dos climas diferenciados, consecuencia de su ubicación entre el Sistema Central y el valle del Tajo:


Las zonas más altas de las sierras de Guadarrama y Ayllón —aproximadamente por encima de los 1200 m— tienen clima de montaña, con temperaturas frías o muy frías en invierno y suaves en verano. Aquí las precipitaciones son abundantes: pueden superar los 1500 mm al año y son en forma de nieve durante el invierno y parte de la primavera y otoño.

El resto del territorio madrileño posee un clima mediterráneo continentalizado de veranos cálidos, de carácter atenuado en el piedemonte y extremado en la llanura mesetaria, en la que se sitúa la capital. En estas zonas los inviernos son frescos, con temperaturas inferiores a los 8 °C, heladas nocturnas muy frecuentes y nevadas ocasionales (tres o cuatro al año). Por el contrario, los veranos son calurosos, con temperaturas medias superiores a los 24 °C en julio y agosto y con máximas que muchas veces superan los 35 °C. La oscilación diaria es de aproximadamente de 10 °C. Las precipitaciones no suelen superar los 700 mm al año y se concentran especialmente en la primavera, seguida del otoño.

En cuanto a récords meteorológicos, la temperatura máxima absoluta alcanzada en la Comunidad de Madrid se dio el 31 de julio de 1878 en el Observatorio Astronómico de Madrid, cuando se llegó a los 44,3 °C. La temperatura mínima absoluta de la región (registrada en una estación meteorológica homologada) se dio el 25 de diciembre de 1962 en el puerto de Navacerrada (1858 m) cuando se alcanzaron los –20,3 °C. La precipitación máxima en 24 horas se produjo en este mismo puerto de montaña el 21 de enero de 1996, cuando cayeron 150,0 mm.

Madrid es una de las comunidades autónomas con mayor densidad de vías pecuarias. Dispone de un total de 4200 kilómetros que ocupan una superficie aproximada de 13 000 hectáreas y que representan el 1,6 % del territorio de la región.

La Ley 8/1998, de 15 de junio, de Vías Pecuarias de la Comunidad de Madrid creó el Patronato de la Red de las Vías Pecuarias, órgano consultivo en dicha materia. Este organismo está constituido por las Consejerías directamente implicadas en su gestión, la Federación Madrileña de Municipios, la Cámara Agraria, las Organizaciones Profesionales Agrarias y los colectivos que tengan por objeto la defensa de la naturaleza.

Para fomentar su uso complementario se han puesto en marcha iniciativas como Descubre tus Cañadas y TrashuMad.

La Comunidad de Madrid tiene una playa galardonada con el distintivo internacional de bandera azul desde 2018, y conservado en 2019, en la Playa Virgen de la Nueva, situada en el municipio de San Martín de Valdeiglesias. Esta playa forma parte de los 14 kilómetros de costa de la región, repartidos en tres playas: Playa Virgen de la Nueva, Playa Veracruz, y Playa El Muro, además de pequeñas calas y un espacio reservado para el nudismo. Allí también se encuentra el Real Club Náutico de Madrid.


Madrid es la provincia más poblada de España, con 6 661 949 habitantes, a 1 de enero de 2019 (INE). Por autonomías, esta comunidad uniprovincial es la tercera de mayor población del país, por detrás de Andalucía (en sus ocho provincias residen más 8,4 millones de personas) y de Cataluña (con 7,5 millones en cuatro provincias).

La densidad de población de la región es de 820 hab/km² (INE 2018), muy superior a la del conjunto español (93,51 hab/km²). Sin embargo, este indicador esconde enormes oscilaciones, conforme se considere la zona central de la provincia o los límites de la misma. Mientras que el municipio de Madrid arroja una densidad de 5321,05 hab/km², en la comarca de la Sierra Norte se reduce a menos de 9,9 hab/km².

La gran mayoría de la población de la comunidad autónoma se concentra en la capital y en sus alrededores, que conforman el área metropolitana más importante de España, donde reside aproximadamente el 90 % de los habitantes de la Comunidad de Madrid. A medida que aumenta la distancia de la capital, más se reducen las cifras demográficas, principalmente en lo que respecta al norte y al suroeste de la región.

La población madrileña presenta un perfil de edad preferentemente joven-adulto: el 44,4 % de los habitantes de la región tiene entre 16 y 44 años (INE 2006). A cierta distancia aparece el grupo de edad de 45 a 64 años, que supone el 24,3 %. Muy alejados se sitúan los niños y adolescentes (hasta 15 años), con un 15,2 %, y los mayores de 65 años, con un 16,7 %. Comparativamente con los datos nacionales, la región de Madrid muestra un componente joven-adulto más elevado. El 61,9 % de los madrileños tiene menos de 45 años, cifra superior a la del total español, con un 59,6 %. También hay diferencias por sexo. En la Comunidad de Madrid habitan más mujeres que en el conjunto de España, en términos relativos. Su perfil femenino se cifra en 2007 en un 51,6 %, un punto más que en el total español (un 50,6 %). El 48,4 % restante corresponde a la población masculina, frente al 49,4 % de todo el país.


La tasa de natalidad de la región madrileña (nacidos por cada 1000 habitantes) es de 11,80 puntos (INE 2005), cifra tímidamente superior a la del conjunto español (10,75 puntos).

En lo que respecta a la mortalidad, las diferencias con los datos nacionales son algo más acusadas. La tasa de mortalidad correspondiente a la comunidad autónoma (6,95) es inferior en dos puntos a la de toda España (8,93).


Según datos del Instituto de Estadística de la Comunidad de Madrid, referidos al año 2005, la esperanza de vida en la Comunidad de Madrid se sitúa en 81,87 años. Para las mujeres es de 84,98 años y para los hombres de 78,43. Este indicador no ha dejado de crecer año tras año, desde su control estadístico, iniciado en 1986 por el citado organismo.


La evolución demográfica de la Comunidad de Madrid, marcada por un crecimiento casi continuo, queda definida a partir de los siguientes hitos históricos:


En el periodo 1981-2005, el crecimiento demográfico de la región fue del 26,17 %, frente al 16,87 % de la media nacional. No todas las comarcas madrileñas participaron en los mismos términos de este aumento de población. Algunas, incluso, se vieron afectadas por un proceso de despoblamiento, caso de la llamada Sierra Norte (en el vértice septentrional de la provincia), conocida popularmente como la "Sierra pobre", con pueblos de pocas decenas de habitantes. En la primera década del siglo , el turismo rural pareció haber favorecido cierto repunte demográfico de esta comarca, pero posteriormente, en la segunda década, se ha vuelto a la pérdida de población en la mayoría de municipios.

Madrid se ha convertido, desde los años cincuenta y sesenta, en un polo industrial de primera magnitud que ha atraído a multitud de personas procedentes de las regiones menos favorecidas del país, así como también, desde principios de los años noventa, a multitud de inmigrantes provenientes de otros países. Según el censo del INE del año 2005, la comunidad autónoma cuenta con un 13,09 % de extranjeros, cinco puntos por encima de la media española (8,47 %).

Un 53,00 % de los no nacionales son iberoamericanos, un 18,36 % de la Europa no comunitaria, un 9,27 % de África del Norte, un 9,21 % de la Unión Europea, un 3,59 % del África subsahariana, un 3,36 % de Asia del Este y un 1,03 % de Filipinas. Por nacionalidades, las que más presencia tienen son la ecuatoriana (un 22,23 % sobre el total de extranjeros), la rumana (12,35 %), la colombiana (9,30 %), la marroquí (8,91 %) y la peruana (5,03 %).

La capital concentra el 58,5 % de la población inmigrante que reside en la región. Le siguen Alcalá de Henares (con un 3,7 %), Móstoles (un 2,5 %), Fuenlabrada (un 2,3 %), Leganés (un 2,2 %), Getafe (un 2,1 %), Torrejón de Ardoz (un 2,1 %), Alcobendas (un 1,7 %) y Coslada (un 1,3 %). En términos relativos, pueblos como Fresnedillas de la Oliva, Gargantilla del Lozoya y Pinilla de Buitrago, Lozoya, Olmeda de las Fuentes, Pelayos de la Presa o Zarzalejo presentan proporciones de inmigrantes entre el 20 % y el 33 %.

De acuerdo al padrón municipal del INE los veinte municipios más poblados de la Comunidad en 2019 fueron:

Además de su realidad metropolitana, la Comunidad de Madrid ofrece el fuerte contraste de zonas despobladas, con un marcado carácter rural. Prueba de ello son las cifras demográficas de Madarcos (48), La Hiruela (57), Robregordo (58), Puebla de la Sierra (65), Robledillo de la Jara (80) y La Serna del Monte (81), los seis municipios menos poblados de la región. La provincia de Madrid es la 4.ª en que existe un mayor porcentaje de habitantes concentrados en su capital (49 %, frente al 31,85 % del conjunto de España).

Según el Barómetro Autonómico publicado por el CIS (Centro de Investigaciones Sociológicas) de julio de 2019, la afiliación religiosa en Madrid es:


El 20,4 % de la población es practicante.

El Estatuto de Autonomía de la Comunidad de Madrid, norma fundamental de la comunidad, establece que la Asamblea de Madrid, el Gobierno y el Presidente de la Comunidad son los órganos que ejercen los poderes de la Comunidad de Madrid:

La Asamblea «representa al pueblo de Madrid, ejerce la potestad legislativa de la Comunidad, aprueba y controla el Presupuesto de la Comunidad, impulsa, orienta y controla la acción del Gobierno y ejerce las demás competencias que le atribuyen la Constitución, el Estatuto y el resto del ordenamiento jurídico», según se señala en el Estatuto de Autonomía de la Comunidad de Madrid (LO 3/1983, de 25 de febrero, BOE del 1 de marzo de 1983). Su sede actual, que se encuentra en el madrileño distrito de Puente de Vallecas y ocupa una superficie de 7148,87 m². 

Anteriormente, entre 1983 y 1998 estuvo ubicada en el viejo Caserón de la antigua Universidad de la calle de San Bernardo del centro de la capital.

El gobierno de la Comunidad de Madrid es «el órgano colegiado que dirige la política de la Comunidad de Madrid, correspondiéndole las funciones ejecutivas y administrativas, así como el ejercicio de la potestad reglamentaria en materias no reservadas a la Asamblea». Está compuesto por el presidente, los vicepresidentes y los consejeros. En 2007, se contabilizan dos Vicepresidencias y trece Consejerías. La Administración autonómica cuenta, para su asistencia jurídica con el Cuerpo de Letrados de la Comunidad de Madrid.

El presidente «ostenta la suprema representación de la Comunidad Autónoma y la ordinaria del Estado en la misma, preside y dirige la actividad del Gobierno, designa y separa a los Vicepresidentes y Consejeros y coordina la Administración». La Presidencia de la Comunidad de Madrid se encuentra actualmente ubicada en la Real Casa de Correos, en la Puerta del Sol, de Madrid. El edificio, que fue sede de la Dirección General de Seguridad, fue adquirido por la Comunidad de Madrid en 1985 la cual encargó al arquitecto Ramón Valls Navascués las obras de adaptación para instalar algunas de sus dependencias. En 1992, se volvieron a hacer obras de rehabilitación y finalmente entre 1996 y 1998 se hizo la última gran reforma y el edificio fue restaurado y acondicionado para embellecer la sede y recuperar parte de la arquitectura original.

Anteriormente, entre 1983 y 1985 la sede de la Presidencia se encontraba en el Palacio de Borghetto (actual sede de la Delegación del Gobierno en la Comunidad de Madrid), cuyo edificio albergó la sede de la Diputación Provincial de Madrid.Durante el periodo de las obras de renovación y acondicionamiento entre 1996 y 1998, la sede de la presidencia volvió de forma provisional al Palacio de Borghetto.

La Comunidad de Madrid se rige por el mismo calendario electoral que las restantes comunidades autónomas, excepción hecha de Andalucía, Cataluña, País Vasco, Comunitat Valenciana y Galicia, que, dado su carácter histórico, tienen facultad para convocar elecciones al margen del citado calendario. Tras las elecciones de mayo de 2011, el Partido Popular renovó la mayoría absoluta para los próximos cuatro años. Tras las elecciones a la Asamblea de Madrid de 2015 el Partido Popular fue la lista más votada pero perdió la mayoría absoluta, quedándose con tan solo 48 escaños, consecuentemente llegó a un pacto de investidura con Ciudadanos, con 17 escaños, obteniendo la mayoría necesaria para investir un nuevo presidente. Tras las elecciones a la Asamblea de Madrid de 2019, el Partido Socialista se convirtió en la lista más votada en la Comunidad, pero el Partido Popular y Ciudadanos alcanzaron un pacto de gobierno que fue aprobado por mayoría absoluta gracias al apoyo externo de Vox.

En 2011 el presupuesto de gastos de la Comunidad Autónoma alcanzó 18,8 mil millones de euros (el tercer mayor presupuesto autonómico). En 2010 el endeudamiento autonómico alcanzó 13,492 mil millones de euros.

En 2018, Madrid es la comunidad autónoma con mayor PIB per cápita, con 35 041 €, por delante del País Vasco, de Navarra y de Cataluña, superando claramente la media europea (establecida en 30 960 €). Además, es la segunda comunidad autónoma (por detrás del País Vasco) con mayor porcentaje de gasto en actividades de I+D sobre el PIB, con un 1,66%.

En el Informe "Plataforma de seguimiento de la Estrategia de Lisboa", promovido por la Unión Europea en 2007, se señala que los puntos fuertes de la economía madrileña son su escaso desempleo, su gasto en investigación, su desarrollo relativamente elevado y sus servicios de alto valor añadido. Entre sus puntos débiles aparecen la falta de conexiones de banda ancha (nuevas tecnologías de la información y la comunicación) y su tasa de actividad relativamente baja entre las mujeres. En este estudio, se destaca a la Comunidad de Madrid como una región-municipio preferentemente asentada en el sector de los servicios.

La Comunidad de Madrid es la cuarta comunidad autónoma con mayor porcentaje de empresas innovadoras de diez o más asalariados, con un 21,2% del total en el período 2016-2018, por encima de la media nacional, pero por debajo del País Vasco (24,3%), la Comunidad Valenciana (22,7%), y Cataluña (22,3%).

Al igual que ocurre con los datos demográficos, la renta disponible bruta municipal per cápita presenta enormes oscilaciones entre las distintas localidades de la provincia. Pero, a diferencia de las cifras poblacionales (que iban a la baja cuanto más aumentaba la distancia con el área metropolitana), se configura ahora un mapa completamente distinto: las áreas de mayor renta per cápita se sitúan preferentemente en el municipio de Madrid y en su corona metropolitana norte, noroeste y nordeste, con extensiones hacia la sierra de Guadarrama, hasta el límite con la provincia de Segovia.

Estas zonas presentan un fuerte componente residencial y, en determinados puntos, integran urbanizaciones consideradas de lujo. Pozuelo de Alarcón, Las Rozas de Madrid, Majadahonda, Boadilla del Monte, Villaviciosa de Odón, Torrelodones y Villanueva de la Cañada, que se ubican en el arco oeste del área metropolitana, repiten año tras año como los municipios de mayor renta per cápita de la Comunidad de Madrid. En 2004, en concreto, alcanzaron cifras que iban desde los 22 846 euros de Pozuelo hasta los 19 753 de Torrelodones.

En el otro extremo, con menos de 8500 euros, figuran los tres vértices del triángulo que dibuja la provincia, tal y como puede observarse en el mapa adjunto. En 2004, los municipios de menor renta per cápita fueron Madarcos (7375), Valdaracete (7746), Somosierra (7819), Prádena del Rincón (7941) y Brea de Tajo (7985 euros). Madarcos es también el pueblo menos poblado de la región (45 habitantes) y uno de los términos municipales de menor superficie (8,5 km²).

La economía se encuentra en fase expansiva desde 1993, con porcentajes de crecimiento entre el 3 % y el 4 % año tras año. En 2005, lideró el crecimiento económico del país con un 4 %, seis décimas más que la media nacional y, en 2006, prácticamente se repitieron las mismas cifras (un 3,9 %, un punto por encima del promedio europeo) —datos del INE—.

El incremento tanto del consumo privado como de la inversión en vivienda y en bienes de equipo se encuentra en la base de esta secuencia de crecimiento. Especialmente relevantes son los datos relativos a la vivienda: en 2006 se construyeron alrededor de 127 000 viviendas, de las cuales 58 000 se concluyeron en el citado año. Expertos y políticos destacan, además, el fenómeno de la inmigración como uno de los principales motores de esta tendencia alcista de la economía madrileña.

Desde 2018 la Comunidad de Madrid es la primera comunidad autónoma en el ranking nacional de contribución al Producto Interior Bruto (PIB) estatal, representando un 19,2% del conjunto del total español, superando de esta forma a Cataluña, que pasa a segunda posición (19,0%). Recién constituida la autonomía, el crecimiento del PIB regional se situó en una media del 4,6 % frente al 4,7 % nacional, con el sector de la construcción como uno de los más pujantes.

A lo largo del siglo , el PIB regional evoluciona igualmente en magnitudes muy similares a las del conjunto estatal. En 2005 se localizan las máximas desviaciones: en este año Madrid se despega en casi un punto del porcentaje nacional (un 4,3 % sobre un 3,5 %, respectivamente). Pero en 2006 ambos datos se equiparan en un 3,9 %, como puede apreciarse en el gráfico adjunto. La construcción se destaca, también en estos años, como uno de los sectores de mayor empuje, tanto en la comunidad autónoma como en el país.

El PIB madrileño se distribuye sectorialmente de la siguiente forma: un 75,8 % corresponde a los servicios, un 13 % a la industria, un 11 % a la construcción y un 0,2 % a la agricultura (fuente: Contabilidad Regional de España, 2006).

La población activa de la Comunidad de Madrid es de 3 320 300 personas, de las cuales 2 688 500 están ocupadas y 631 800 paradas (datos correspondientes al segundo trimestre de 2014, según la Encuesta de Población Activa). En términos relativos, la tasa de paro se sitúa en el segundo semestre de 2014 en el 19 % de la población activa. La tasa de actividad se cifra en un 52,74 %.

El sector agrícola-ganadero posee un peso relativo escaso dentro de la economía de la región (apenas un 0,2 % del PIB). Sin embargo, presenta magnitudes absolutas muy similares a las de las provincias limítrofes, aspecto que resulta especialmente significativo si se tiene en cuenta que la Comunidad de Madrid ocupa una superficie menor y que integra una importante área metropolitana, que resta recursos a este sector.

La agricultura madrileña posee, además, un grado de variedad mucho mayor que el de las provincias colindantes. Ello es consecuencia de las tres unidades de relieve que definen el medio físico de la región y que permiten la existencia de bosques, pastos, cultivos herbáceos de secano, viñedo, olivar y cultivos hortofrutícolas de regadío, dentro de una superficie relativamente reducida, con dos áreas de especial actividad: las comarcas serranas y los valles interfluviales.

A pesar de su potencial, el sector agrícola-ganadero madrileño se encuentra en regresión, ante la expansión del área metropolitana y el empuje de otras actividades, como la construcción. En 1985, existían 251 498 hectáreas de tierras de cultivo, en 2001 la superficie productiva labrada desciende a 199 687 hectáreas.

En lo que respecta a la minería, esta resulta irrelevante en relación al total de su economía. Existen más de un centenar de minas, entre las que figuran las de sepiolita de Vicálvaro (Madrid) y Parla, y las de sulfato sódico de Colmenar de Oreja. La comunidad posee yacimientos de sepiolita que atesoran el 80 % de las reservas mundiales, y la producción minera de la región representa el 4 % de la nacional. El área minera más activa es la de Colmenar de Oreja, con seis minas activas. En el término municipal de Colmenarejo existieron minas de cobre, abandonadas en la actualidad.

Por último, la Comunidad de Madrid presenta un fuerte déficit energético. Las centrales hidroeléctricas que se encuentran al pie de los embalses son insuficientes, razón por la cual la región importa electricidad de Castilla y León, Castilla-La Mancha y Extremadura, entre otras comunidades. La región consume el 11,4 % del total de la energía nacional y sólo produce el 0,49 % (datos correspondientes a 2006). En términos absolutos, la demanda madrileña de energía en el citado año se cifra en 30 598 GW, mientras que la producción regional apenas llega a 1330 GW.

La Comunidad de Madrid es la segunda región industrial del país. Este sector, que ocupa el 28 % de la población activa madrileña, muestra síntomas de recuperación tras varios años en retroceso, como prueba el crecimiento experimentado en 2006 —un 3,3 % (INE)—. La industria supone el 13 % de la economía madrileña.

Los principales subsectores industriales de la región son los siguientes:

La construcción representa el 11 % de la economía madrileña. Se trata del sector más dinámico y pujante en los últimos años. Es, de hecho, el que más crece en 2006, con un 5,3 %, impulsado tanto por la edificación residencial como por las infraestructuras civiles. Debe tenerse en cuenta que, en ese año, se acometieron en la región proyectos de gran envergadura, como el soterramiento de la autovía M-30, la fase final de las obras de la Terminal 4 del aeropuerto de Madrid-Barajas o la ampliación del red de Metro.

El sector terciario es, sin duda, el más relevante de la economía madrileña, en la que representa casi un 77 % (porcentaje sobre el PIB, año 2006). Su crecimiento en 2006 es del 3,5 %, según el INE. Su importancia viene dada por la radicación en la región de la mayor parte de las grandes empresas del país, tanto nacionales como extranjeras, que, además de su peso específico, generan alrededor de sí un tejido de servicios.

La provincia concentra el mayor volumen de compañías de nuevas tecnologías, como Indra, Everis, Ericsson, Lucent Technologies, Telefónica, Microsoft e IBM. Esta última decidió en 2005 establecer en la capital su nueva sede para Europa, África y Oriente Medio.

El turismo se ha perfilado como una de las actividades económicas más pujantes de la región. En enero de 2007, la ciudad de Madrid tuvo 511 892 viajeros alojados en sus establecimientos hoteleros, lo que le confirma como el punto turístico con mayor número de viajeros y pernoctaciones de España.
Más concretamente, hay que destacar la importancia alcanzada por el turismo de negocios. En este subsector, la Feria de Madrid, IFEMA, juega un papel transcendental. Esta institución es artífice de las ferias y exposiciones de mayor peso del país, algunas de las cuales se encuentran entre las primeras del continente europeo, caso de SIMO o Fitur. Las instalaciones de IFEMA, en el Campo de las Naciones, cerca del Parque Juan Carlos I, son el lugar más visitado de toda la comunidad, por encima de monumentos como el Museo del Prado o el Monasterio de San Lorenzo de El Escorial. Tuvo 3,8 millones de visitantes en 2006.

En otro orden, la inauguración de las nuevas terminales T4 y T4S del Aeropuerto Internacional de Barajas, de diseño vanguardista y con una elevada capacidad de operaciones, consolida sus instalaciones como uno de los más importantes del mundo y como la puerta a Europa desde Iberoamérica. En la actualidad, es el quinto aeropuerto de Europa en volumen de viajeros.

Los lugares más visitados en 2013 en la Comunidad de Madrid fueron, por número de visitantes (nacionales y extranjeros): el Museo Reina Sofía (3,2 millones), el Museo del Prado (2,3 millones), el Parque Warner (1,2 millones), el Palacio Real de Madrid (1 millón), el Museo Thyssen-Bornemisza (944 346) y el tour del estadio Santiago Bernabéu (820 000).

La Comunidad de Madrid es el centro de la red de comunicaciones españolas, dada la estructura radial de las carreteras del Estado, que tiene su origen en el siglo . Aunque, en los últimos tiempos, la articulación radial de las carreteras españolas se ha ido desdibujando mediante la apertura de ejes transversales, Madrid sigue siendo paso obligado en las comunicaciones interprovinciales por carretera.

Extremo que se subraya aún más en el transporte por tren, que todavía mantiene la configuración radial diseñada en el siglo , y en los desplazamientos por avión, con el aeropuerto de Madrid-Barajas como punto de referencia de todos los aeropuertos españoles para las conexiones internacionales.

A esto se añaden los desplazamientos internos de los propios madrileños, que también tienen su epicentro en la ciudad de Madrid, como punto de destino y salida preferente. Estos resultan especialmente intensos en el área metropolitana.

Todo ello da lugar a una estructura de comunicaciones de gran complejidad, en cuya articulación resultan igualmente decisivas las actuaciones del Ministerio de Fomento (que gestiona las carreteras radiales y de circunvalación —excepto la M-30 y la M-45—, el transporte por tren y el aeropuerto de Madrid-Barajas), de la Comunidad de Madrid (responsable de las carreteras regionales, del Metro y de los autobuses interurbanos) y de los distintos municipios metropolitanos, con especial mención al Ayuntamiento de Madrid (del que dependen la M-30 y el servicio de autobuses urbanos de la capital).

A partir de la creación del Consorcio Regional de Transportes de Madrid en 1985, las citadas administraciones se coordinan en el establecimiento de servicios y tarifas en los medios de transporte público de toda la región. Entre sus iniciativas más destacadas, figura la creación del Abono Transportes y de los billetes combinados.

La Comunidad de Madrid cuenta con una amplia red de autovías y autopistas. Todas son de uso gratuito, excepción hecha de las radiales R-2, R-3, R-4 y R-5, la AP-6 y la M-12, que son de peaje.


De Madrid parten las autovías A-1, A-2, A-3, A-4, A-5 y A-6, A-42, cuyos puntos kilométricos empiezan a contabilizarse desde el llamado Kilómetro Cero, situado en la Puerta del Sol. En torno a estas carreteras se han formado grandes núcleos urbanos, así como áreas industriales y empresariales.



Debido a los significativos problemas de tráfico de las vías anteriormente descritas, el Ministerio de Fomento inauguró en 2004 cuatro autopistas de peaje (R-2, R-3, R-4 y R-5), que parten de la autovía de circunvalación M-40. Sus longitudes van desde los 28,3 km de la R-5 a los 61 de la R-2 y su función es servir de alternativa a las autovías radiales de las que toman el cardinal indicativo.



Existen además autovías gratuitas de circunvalación, que comunican las diferentes autovías y autopistas radiales, entre otras carreteras. La M-30 depende del Ayuntamiento de Madrid, mientras que la M-40 y la M-50 son de titularidad estatal. Por su parte, la M-45 es autonómica. Junto a ellas, la M-21 y la M-31 enlazan, a modo de ejes troncales, las distintas vías de circunvalación.


En este apartado destacan la A-42 (Madrid-Toledo), la M-607 (Madrid-Colmenar Viejo), la M-500 (carretera de Castilla) y la M-501 (conocida popularmente como la "carretera de los pantanos"), así como las autopistas de peaje y autovías gratuitas que acceden al Aeropuerto de Madrid-Barajas (la M-11, la M-12 y la M-13).


Los puertos más importantes de la Comunidad de Madrid, por cuanto forman parte de la red principal de carreteras, son el de Navacerrada, a 1858 m de altitud, el de Guadarrama o los Leones, a 1511 m, y el de Somosierra, a 1434 m. En la red secundaria se encuentran el de Canencia, el de la Morcuera, el de la Cruz Verde, el de Cotos, el de Fuenfría, el de la Puebla, el de Galapagar y el de San Juan. Todos ellos están situados en la sierra de Guadarrama (incluida su zona más oriental, Somosierra), excepto el de San Juan, en las primeras estribaciones de la sierra de Gredos.

El Metro de Madrid es uno de los más antiguos de Europa. Fue inaugurado en 1919 por el rey Alfonso XIII. Su red, una de las más extensas y modernas del mundo, no solo da servicio a la ciudad de Madrid, sino también a otros municipios de la región. Cuenta con un total de 317 estaciones y 317 km de vías distribuidas en doce líneas, más un ramal, y tres líneas de metro ligero.

La Comunidad de Madrid es uno de los sectores de la red ferroviaria española por la que más trenes circulan. La región disfruta de ferrocarril desde 1851, cuando la reina Isabel II inauguró la línea Madrid-Aranjuez. Se trata del segundo tramo ferroviario más antiguo de la España peninsular, después del de Barcelona-Mataró.


Desde la ciudad de Madrid parten todos los ejes de red de alta velocidad (AVE) que se encuentran actualmente en funcionamiento en España: el de Madrid-Córdoba-Sevilla, el de Madrid-Córdoba-Málaga, el de Madrid-Zaragoza-Lérida-Tarragona-Barcelona, el de Madrid-Toledo, el de Madrid-Segovia-Valladolid-León, el de Madrid-Cuenca-Valencia y el de Madrid-Albacete-Alicante.

La red de Cercanías de la Comunidad de Madrid es la de mayor tráfico de viajeros de toda España. Está integrada por diez líneas, que comunican radialmente la capital y las zonas más pobladas de la región. Todas las líneas tienen correspondencia en la estación de Atocha Cercanías, excepto la C-9, que discurre por las laderas de la Sierra de Guadarrama, atravesando los puertos de montaña de Navacerrada y Cotos. Esta línea, que salva una pendiente media del 60 %, una de las más acusadas de Europa en materia ferroviaria, fue inaugurada en 1923 por el rey Alfonso XIII.

Las líneas de Media Distancia en Madrid componen la red de trenes regionales Media Distancia de la Red Nacional de los Ferrocarriles Españoles que circulan en la Comunidad de Madrid. Los servicios llegan hasta Extremadura, Andalucía, Castilla-La Mancha, Castilla y León, Aragón, País Vasco y Comunidad Valenciana.

En la ciudad de Madrid confluye la red radial de vías férreas de España, que data del siglo . Las estaciones de Chamartín y Puerta de Atocha distribuyen el tráfico ferroviario de los tres tramos básicos: Madrid-Venta de Baños (Palencia), Madrid-Alcázar de San Juan (Ciudad Real) (que se extiende a Sevilla y a Cádiz) y Madrid-Zaragoza-Barcelona.

A estos tres tramos se les suma una red complementaria, cuyas cabeceras principales son Aranjuez, con una bifurcación hacia Valencia; Collado Villalba, hacia Segovia y Burgos; y la propia capital, con un tramo hacia Soria y Logroño, otro hacia Toledo y Puertollano (Ciudad Real) y otro hacia Talavera de la Reina (Toledo) y Cáceres.

El aeropuerto de Madrid-Barajas empezó a funcionar en 1928 y se trata del aeropuerto más importante de España y el cuarto de Europa en tránsito de pasajeros (52,2 millones de pasajeros en 2007). Cuenta con el mayor número de vuelos directos a Latinoamérica de todo el continente. Está integrado por cuatro terminales (T1, T2, T3 y T4 —esta última inaugurada en 2006—), a las que hay que añadir la T4-S, satélite de la T4. Es el mayor aeropuerto del mundo por superficie de terminales, con casi un millón de m².

De menor importancia son la base aérea de Torrejón de Ardoz, la base aérea de Getafe y el aeropuerto de Madrid-Cuatro Vientos, inaugurado en 1911 y la instalación aeroportuaria más antigua de España. Existe también una serie de pequeños aeródromos privados de menor tamaño.

Madrid es la comunidad líder de España en el sector de los medios de comunicación, no solo en número de empresas, sino también en volumen de facturación. Concentra los principales grupos de radio y televisión, tanto operadores como productoras, y de prensa. La mayor parte de las agencias de información del país —entre ellas la Agencia EFE— también tienen su sede en la comunidad autónoma.

En la región se encuentra el grupo PRISA, propietario de "El País", el diario de mayor difusión nacional y ventas, con 2 182 000 lectores diarios en toda España (octubre de 2006 a mayo de 2007), según el Estudio General de Medios (EGM). Asimismo, PRISA es accionista de la Cadena SER, líder de la radiodifusión española, que alcanza una audiencia acumulada de 4 643 000 oyentes diarios (datos correspondientes a la segunda oleada de 2007 del EGM).

Gestevisión Telecinco, el primer operador de televisión del país en inversión publicitaria y número de espectadores, también radica en la Comunidad de Madrid. Esta sociedad explota la señal de Telecinco, el canal más visto en España entre 2004 (un 22,1 % de "share") y 2008 (un 18,1 %), según datos de TNS Audiencia de Medios (antes Sofres). Globomedia, que tiene su sede en el distrito de Fuencarral, es la productora audiovisual líder en horas de emisión en televisión. Asimismo, es la que produce un mayor número de series de ficción y una de las propietarias de la cadena de televisión La Sexta.

La Comunidad de Madrid alumbró, en el siglo , la "Gaceta de Madrid", considerado el primer periódico de la historia de la prensa española. En la actualidad, sirve de soporte al "Boletín Oficial del Estado".

Periódicos de difusión regional

A diferencia de otras comunidades autónomas, Madrid carece de prensa regional. Los intentos de lanzar un periódico de actualidad regional han sido tan escasos como fallidos (es el caso del desaparecido Cisneros). La información sobre la región madrileña se sustenta en forma de secciones o separatas incluidas en los diarios de difusión nacional, en algunos periódicos digitales y, sobre todo, en los diarios gratuitos. Madrid es una de las provincias pioneras en este tipo de prensa, que se reparte preferentemente en comercios y a las puertas de las estaciones de metro y tren. "20 minutos", "Qué!" y "ADN" son los periódicos gratuitos más relevantes.

Periódicos de difusión local y comarcal

Destacan el "Diario de Alcálá", de pago y que también cuenta con una edición digital, y otros comarcales como Sur-Madrid el periódico gratuito con 24 años de historia con cabeceras en las ciudades de Alcorcón, Móstoles, Fuenlabrada, Getafe, Leganés, etc... así como otros como "Crónica Norte". El semanario alcalaíno "Puerta de Madrid", de venta cada viernes en la ciudad complutense y en algunos municipios de su comarca y de la provincia de Guadalajara, tiene una historia de más de 2150 números y 42 años.

Periódicos de difusión nacional

En la Comunidad de Madrid se editan los diarios de difusión nacional más importantes de España, tanto los de información general ("El País", "El Mundo", "ABC" o "La Razón") como los especializados. En este último apartado destacan los deportivos "As" y "Marca", así como los económicos "Expansión", "Cinco Días" y "La Gaceta de los Negocios", entre otros. El más antiguo de todos ellos es "ABC", fundado el 1 de enero de 1903 por Torcuato Luca de Tena y Álvarez-Ossorio. Todos estos diarios cuentan con ediciones digitales.

Radio Ibérica de Madrid fue la primera emisora de radio española en emitir (año 1923), aunque carecía de licencia. Le siguió Radio España de Madrid, desaparecida en 2001, que arrancó el 10 de noviembre de 1924, si bien le fue adjudicada la licencia después que a Radio Barcelona (cuyas emisiones comenzaron el 14 de noviembre del mismo año). Por esta razón, esta última emisora, hoy perteneciente a la SER, es considerada oficialmente como la radio más antigua del país.

Radio Madrid, de la SER, es la emisora más escuchada de España.


A diferencia de la prensa, la región madrileña sí que cuenta con cadenas radiofónicas de cobertura estrictamente regional. En lo que respecta a la radio generalista, hay dos emisoras regionales: la privada comercial Radio Intercontinental y la pública Onda Madrid, dependiente del ente público Radio Televisión Madrid —que explota, en el terreno de la televisión, los canales Telemadrid y La Otra—. Asimismo, existen varias radios temáticas, con un ámbito de emisión limitado a la Comunidad de Madrid. En los últimos tiempos, han proliferado numerosas radios locales y comarcales, generalmente impulsadas por organismos municipales o supra-municipales, especializadas en temas localistas.


En Madrid tienen su sede las principales cadenas de radio generalistas del país, que emiten para toda España, y estrictamente para el territorio madrileño, en determinadas franjas horarias. Aquí se engloban las cadenas privadas comerciales SER, COPE, Onda Cero y Punto Radio, así como la pública Radio Nacional de España, que pertenece al ente RTVE.

La SER tiene emisoras en Madrid (Radio Madrid), Alcalá de Henares (SER Henares), Alcobendas (SER Madrid Norte), Aranjuez (Radio Aranjuez), Móstoles (SER Suroeste) y Parla (SER Madrid Sur). Por su parte, la COPE explota diferentes licencias en Madrid (Radio Popular de Madrid), en Collado Villalba (COPE de la Sierra), en Getafe y en Fuenlabrada (COPE Sur). La programación de Punto Radio se difunde a través de las emisoras que esta cadena tiene en Madrid, Alcalá de Henares y El Escorial.

Madrid también acoge la mayor parte de las cadenas temáticas de difusión nacional del país, que emiten preferentemente a través de la FM. Se trata de "radios-fórmula" mayoritariamente musicales (Los 40 Principales, M80 Radio, Cadena Dial, Máxima FM, Radiolé, Kiss FM, Hit FM, Top Radio) y, en menor medida, económicas (Radio Intereconomía), deportivas (Radio Marca), informativas (Radio 5 Todo Noticias, del grupo RNE) y religiosas (Radio María). También emiten 18 emisoras en radio digital DAB.

Madrid cuenta con una amplia presencia de emisoras sin ánimo de lucro pertenecientes a organizaciones sociales, las llamadas radios libres y comunitarias, agrupadas en el llamado Tercer Sector de la Comunicación. En la capital, la más conocida es Radio Vallekas, aunque también cuentan con presencia en la villa Radio Enlace, R.K.20, Desencadena Usera, Onda Diamante, Onda Merlín Comunitaria, Radio Almenara, Radio Cigüeña, Radio Jabato, Radio Morata y Radio Paloma. Además, también hay radios del Tercer Sector en Getafe (Radio Ritmo) y Aranjuez (Radio Fuga).

Muchas de ellas están agrupadas en la Unión de Radios Libres y Comunitarias de Madrid que, en 2009, ganó un recurso en el Tribunal Supremo contra la adjudicación de licencias de radio por la Comunidad de Madrid en 2003. La URCM como red, al igual que varias emisoras a título particular, está unida a la Red de Medios Comunitarios.

Al igual que en prensa y radio, la Comunidad de Madrid acogió las primeras emisiones televisivas de España, primero en pruebas (año 1952) y, a partir de 1956, de forma regular, con el arranque de TVE.

La televisión se recibe en la Comunidad de Madrid preferentemente por vía analógica, que representa el 63,3 % del consumo televisivo en 2008. La Televisión Digital Terrestre (TDT) supone un 22,9 %, el cable un 8,4 % y el satélite digital un 4,7 %, según datos de GECA.

La Ciudad de la Imagen concentra numerosas empresas relacionadas con el sector audiovisual. Este polígono, situado en el término municipal de Pozuelo de Alarcón, fue promovido por la propia Comunidad de Madrid con el fin de dotar a la región de un parque tecnológico en el terreno de la televisión y el cine. En él tiene su sede Telemadrid, varios canales temáticos, diferentes productoras audiovisuales (entre ellas, Videomedia y la delegación madrileña de la empresa catalana Mediapro), parte del archivo de la Filmoteca Española, la Escuela de Cinematografía y del Audiovisual de la Comunidad de Madrid (ECAM) y la Entidad de Derechos de los Productores Auidovisuales (EGEDA), entre otras muchas empresas y entidades audiovisuales. Alrededor de la Ciudad de la Imagen, se ha desarrollado un área comercial y de ocio, en el que destacan los megacines Kinépolis, que albergan la sala de cine más grande del mundo, según figura en el "Libro Guinness de los Récords".

La región cuenta con dos canales autonómicos públicos, que dependen del ente Radio Televisión Madrid, integrado en la Federación de Organismos de Radio y Televisión Autonómicos (FORTA). Telemadrid inició sus emisiones en 1989 y en 2005 arrancó La Otra. A finales de la década de 1990, Telemadrid, junto con la catalana TV3, llegó a ser la televisión autonómica de mayor audiencia del país, con "shares" superiores al 20 %. Tras una severa pérdida de audiencia, su cuota de pantalla en 2008 se situaba en un 10,5 %, a más de nueve puntos de Telecinco (un 19,7 % en el mismo año), líder en la Comunidad de Madrid. En el último mes de 2008, Telemadrid alcanzó un 10,7 % de "share", mientras que laOtra se instaló en un 0,3 % —datos de TNS Audiencia de Medios (antes Sofres)—. En 2009 la audiencia de Telemadrid bajó a 9,7 %, en 2010 a 8 % y en 2011 a 6,4 %.


Además de los tres canales de Radio Televisión Madrid, existen canales privados de cobertura autonómica, aunque su audiencia es inferior a la obtenida por los medios públicos. Entre otros se encuentran 8madrid, canal de cine perteneciente al empresario Enrique Cerezo; Kiss TV, emisora musical del Grupo Kiss; Business TV, controlado por Intereconomía TV, y otros canales como 13TV Madrid (antigua Popular TV), Libertad Digital TV, Ver-T Madrid y Aprende Inglés TV. Dentro de las televisiones locales, se encuentran canales como Canal 33 Madrid y Tele K, televisión comunitaria de Vallecas.

Anteriormente, la Comunidad de Madrid ha contado con otros canales privados. El primero fue Onda 6, perteneciente al grupo Vocento (antes Grupo Correo), que explota una licencia en TDT y también contó con licencia analógica. Sin embargo, el canal desapareció cuando La 10 se convirtió en una televisión con cobertura nacional. Madrid contó también con una versión local de Localia TV, gestionado por Grupo PRISA, que no obtuvo licencia para emitir en TDT y desapareció en 2009.


El nuevo marco legal introducido por la TDT ha multiplicado la oferta de televisión local en la región madrileña. A lo largo de 2007 y 2008 se han puesto en marcha canales que, como 8madrid, Libertad Digital TV y es.madrid.tv, explotan una licencia digital de TDT, concedida por el gobierno autonómico.

Junto a estos nuevos canales, aún continúan emitiendo vía analógica cadenas que, como Canal 7, carecen de licencia. Localia Madrid también se encontraba en esta situación de alegalidad, hasta diciembre de 2008, cuando anunció el cierre de sus actividades.

Tras la polémica adjudicación de licencias de TV, la televisión comunitaria Tele K corrió riesgo de desaparición, pero el 4 de diciembre de 2007, el Senado reconoció a Tele K, como emisora local de proximidad, la posibilidad de obtener licencias locales para la TDT, al margen del concurso de la comunidad autónoma, dentro del marco de la Ley de Impulso a la Sociedad de la Información. Por lo tanto, la cadena vallecana recuperó su legalidad y pudo optar a una concesión local de Televisión Digital Terrestre.


Como sucede con la prensa y la radio, los operadores nacionales de televisión tienen su sede en la Comunidad de Madrid. Las instalaciones de TVE están en Torrespaña, en la ciudad de Madrid, y en Prado del Rey, en Pozuelo de Alarcón. En esta última localidad también se encuentran las dependencias de La Sexta, en concreto en la Ciudad de la Imagen. Las de Antena 3 están en San Sebastián de los Reyes y las de Telecinco en Madrid, en el distrito de Fuencarral. Cuatro y Digital+ emiten desde Tres Cantos. En la Comunidad de Madrid también radican Net TV y Veo TV, dos operadores de cobertura nacional que difunden sus canales a través de la TDT. Con la excepción de TVE -entre cuyos canales se encuentran La Primera, La 2, 24 Horas y Teledeporte—, todas estas cadenas son de titularidad privada comercial.

En la Comunidad de Madrid se encuentran ubicadas algunas de las empresas más relevantes del país dentro del sector de la publicidad y del "marketing". Es el caso de Carat España, cuyo presidente es Miguel Ángel Rodríguez, antiguo portavoz del gobierno durante la presidencia de José María Aznar.

El gasto que la Comunidad de Madrid realiza en campañas publicitarias ronda los 160 millones de euros, según datos correspondientes al año 2007.

El sector del turismo se ha convertido en una de las actividades más pujantes de la economía madrileña. La comunidad recibió la visita de 8 651 891 turistas en 2006, un 9,41 % más que el año anterior. Con esta cifra, la más alta en la historia del turismo madrileño, la región superó en número de visitantes a países como Brasil, Croacia, Suiza o Egipto.

La Comunidad de Madrid añade a su relevante patrimonio histórico-artístico una variada oferta cultural, museística y de ocio. Esta base turística se completa con diferentes infraestructuras dirigidas a captar el llamado turismo de negocios, uno de los subsectores que han experimentado un mayor crecimiento en número de visitantes.

La región cuenta con tres Patrimonios de la Humanidad: el Monasterio y Real Sitio de San Lorenzo de El Escorial, el Paisaje Cultural de Aranjuez y la Universidad y recinto histórico de Alcalá de Henares. 

Madrid es una región de contrastes medioambientales, consecuencia de su relieve configurado alrededor de tres grandes unidades (la sierra, el piedemonte y la llanura del Tajo). A pesar de integrar el área metropolitana más importante del país, la región posee un rico patrimonio natural, en el que destacan la sierra de Guadarrama y su extensión oriental Somosierra, así como las comarcas situadas en el piedemonte, conocido, en términos geomorfológicos, como la rampa de la sierra.

El noroeste de la comunidad autónoma alberga dos parques regionales —Cuenca Alta del Manzanares y Curso Medio del río Guadarrama, creados respectivamente en 1985 y 1999— y uno nacional, el parque nacional de la Sierra de Guadarrama, que data de 2013 y que absorbió al parque natural de la Cumbre, el Circo y las Lagunas de Peñalara. En la parte del parque nacional ubicada dentro de la Comunidad de Madrid —pues una fracción del área protegida pertenece a Castilla y León— se encuentran zonas como el circo de Peñalara, la Cuerda Larga, el macizo granítico de La Pedriza y el cordal de Siete Picos. En la sierra también se encuentran el monumento natural de las Peñas del Arcipreste de Hita, al este del puerto de los Leones, una formación granítica con algunas frases dedicadas al Arcipreste de Hita, así como el paraje pintoresco del Pinar de Abantos y Zona de La Herrería en terrenos próximos al monasterio de San Lorenzo de El Escorial, con bosques de pinos, robles y encinas.

En el propio municipio de Madrid se encuentra el Monte de El Pardo, una valiosa área de bosque mediterráneo gestionada por Patrimonio Nacional y de acceso restringido. A orillas del Jarama y en el municipio de Montejo de la Sierra, al norte de la comunidad autónoma, se encuentra el sitio natural de interés nacional del Hayedo de Montejo, uno de los más meridionales de Europa.
Al sur y al este de la capital se extienden el parque regional de los Cursos Bajos de los Ríos Manzanares y Jarama, creado en 1994 en un intento de recuperar ecosistemas como las llanuras de ribera, los sotos, los cursos fluviales y, especialmente, las numerosas lagunas de la zona, producto de la filtración de los ríos a antiguas graveras; y los Sotos del Henares, un espacio bajo protección preventiva. En el sur de la provincia están también repartidos espacios de menor entidad, como la reserva natural de El Regajal-Mar de Ontígola, la reserva natural del Carrizal de Villamejor y el refugio de fauna de la Laguna de San Juan. Existe un encinar adehesado muy bien conservado en el municipio del Sevilla la Nueva, con el estatus de espacio natural de protección temporal.

Junto con estos espacios naturales, existen otros enclaves en la región con un importante valor paisajístico y medioambiental, como los bosques de La Acebeda, el Pinar de Peña Pintada, situado en el entorno de la vía de ascenso al puerto de Navacerrada, y los cerros de El Viso y del Ecce-Homo, que se encuentran en la campiña del río Henares. Hay catalogados 23 humedales, y está en estudio su ampliación. Los embalses de Picadas, Pinilla, El Atazar, Riosequillo, San Juan y Valmayor, entre otros, reciben un elevado número de visitantes. La cueva del Reguerillo, a la que se accede desde la garganta de Patones, es un punto de referencia para los aficionados a la espeleología. Valles como el del Alto Jarama, el del Alberche o el del Lozoya, sierras como la de La Cabrera o la del Rincón y comarcas como la Vega de Aranjuez completan la lista de enclaves de interés medioambiental.

Además, cabe citar las estepas de la Sagra madrileña, al sur de la comunidad autónoma, donde se concentra una valiosa población de aves esteparias. Aquí se integran municipios como Griñón, Humanes de Madrid o Torrejón de Velasco y parajes como Los Estrágales (Pinto), los cerros de El Espartal (Valdemoro), el cerro de La Cantueña (Parla) o el arroyo Humanejos (Parla), con el último bosque de ribera de la parte meridional de la región bien conservado, en el que habitan aves forestales.

La Comunidad de Madrid celebra su festividad el día 2 de mayo, en conmemoración de los actos heroicos que dieron lugar a la Guerra de la Independencia, en 1808. Posee un carácter marcadamante institucional.

Entre los festejos más destacados de la región, destacan los de carácter taurino. La fiesta de los toros en la Comunidad de Madrid está declarada Bien de Interés Cultural. La Feria de San Isidro, que se celebra en mayo en la plaza monumental de Las Ventas, es una de las citas de mayor interés del mundo taurino.

Por su parte, los encierros de San Sebastián de los Reyes están considerados como los segundos más importantes de España, después de los de San Fermín en Pamplona. Su origen se remonta al siglo .

Alcalá de Henares cuenta con tres fiestas declaradas de Interés Turístico Regional: su Semana Santa, el Don Juan Tenorio y el Octubre Cervantino. Así mismo cuenta con unas Ferias y Fiestas Populares que se remontan a 1184, las patronales de Los Santos Niños el 5 y 6 de agosto, las patronales de la Virgen del Val el tercer domingo de septiembre, San Antón en enero y Santa Lucía en diciembre.

La ciudad de Madrid alberga una celebración del Orgullo LGTB desde 1978 cuando fue promovido por el Frente de Liberación Homosexual de Castilla con un carácter reivindicativo y recreativo. Es una de las más multitudinarias del mundo congregando cada año a más de un millón de asistentes.

Las bases de la cocina regional madrileña se sientan en el siglo , cuando el rey Felipe II proclama a Madrid como capital, a partir de dos niveles bien diferenciados: el de la aristocracia y el de las clases populares. Del segundo surge el pastel de liebre, que, pese a su procedencia humilde, se convirtió en uno de los platos más solicitados por la nobleza en los siglos posteriores a la capitalidad.

A partir del siglo , aparecen en la ciudad de Madrid las primeras fondas, casas de comidas y restaurantes modernos (el célebre Lhardy, que aún sigue funcionando, se fundó en 1839), así como cafés y confiterías, que toman el relevo de los antiguos mesones. La apertura de estos establecimientos suaviza las diferencias entre esos dos niveles y empieza a tomar forma lo que hoy en día se entiende como cocina madrileña. De esta época datan platos como el cocido de tres vuelcos (conocido en la actualidad como cocido madrileño), los soldaditos de Pavía, el besugo a la madrileña, el potaje de vigilia o los bartolillos.

En los siglos y , la cocina madrileña se suma a las corrientes renovadoras y experimentales de las gastronomías catalana y vasca. En los actuales restaurantes madrileños, conviven los platos más tradicionales y las creaciones más vanguardistas, en diferentes manifestaciones (cocina de autor, cocina de fusión, cocina creativa, etc.), en lo que constituye una de las ofertas restauradoras más cuantiosa, prestigiosa e importante de España. A ello se añaden los bares, tascas y tabernas, que mantienen en pie la cultura de la tapa, de los populares bocadillos de calamares y de las raciones; y la existencia de numerosos restaurantes internacionales y regionales, especializados en otras cocinas de España.

La región madrileña cuenta con una importante oferta agroalimentaria, reconocida legalmente en las siguientes denominaciones de origen:


A estas denominaciones de origen se añaden otros alimentos que han alcanzado fama nacional, caso del queso de Campo Real, los ajos de Chinchón, el requesón de Miraflores de la Sierra, los melones de Villaconejos, los garbanzos de Navalcarnero, la repostería de los conventos de Alcalá de Henares o los fresones, fresas y espárragos de Aranjuez.

El fútbol acapara el interés deportivo de los madrileños, al igual que ocurre en el resto de España. De hecho, fue Madrid uno de los primeros lugares en los que se introdujo el fútbol en España, con la constitución en 1879 del Cricket y Foot-ball Club. La Comunidad de Madrid es la organizadora de la Copa Mundial de Clubes Sub-17 a nivel internacional y está representada por una selección autonómica.

La región cuenta con tres equipos de gran tradición y repercusión global, el Real Madrid (fundado en 1902), el Atlético de Madrid (fundado en 1903) -disputando ambos el derby madrileño-, y el Rayo Vallecano (fundado en 1924), a los que se les suman el Club Deportivo Leganés, que se fundó en 1928, y el Getafe Club de Fútbol (sucesor del Club Getafe Deportivo, fundado en 1946) -que disputan el derby del sur de Madrid-. Todos ellos juegan en la Primera División, siendo Madrid la única provincia española que ha contado en una misma temporada con cinco clubes en la máxima división nacional de fútbol. A los anteriores hay que añadir a que militan en la Segunda División del fútbol profesional español: actualmente estos son la Agrupación Deportiva Alcorcón, que se fundó en 1971, y el Rayo Majadahonda, fundado en 1976. 

Siempre ha contado la Comunidad con representantes al máximo nivel en el fútbol femenino, desde que el C.D. Oroquieta Villaverde dominara el panorama deportivo nacional en los años 90. Actualmente juegan en la Liga Iberdrola el Club Atlético de Madrid Femenino (vigente campeón y sucesor del Atlético Villa de Madrid), el Rayo Vallecano Femenino y el Madrid CFF. 

También goza de raigambre el fútbol sala, siendo la región hogar del Inter FS (que ha jugado en Madrid, Alcobendas, Alcalá de Henares y actualmente en Torrejón de Ardoz), el club más laureado del mundo en este deporte, y del Futsi Atlético de Madrid Navalcarnero, varias veces campeón de Europa en la modalidad femenina.

Otro deporte muy popular en la comunidad autónoma es el baloncesto. Clubes como el Real Madrid, el Estudiantes o el Fuenlabrada se encuentran en la primera línea del baloncesto español. 

El ciclismo reúne también a un gran número de aficionados. En la región se disputan tradicionalmente las etapas finales de la Vuelta ciclista a España, que discurre por los pueblos y puertos de montaña de la comunidad hasta su conclusión en el paseo de la Castellana, en la capital.

La sierra de Guadarrama es un punto de referencia para los numerosos clubes ciclistas con los que cuenta la región, a la que se suman los diferentes carriles y circuitos urbanos de uso ciclista y peatonal. En mayo de 2007 se ha puesto en marcha el Anillo verde ciclista, que circunvala el término municipal de Madrid con un trazado de más de 64 km.

El boxeo, la hípica, el golf, el tenis, las artes marciales, el rugby (donde el equipo nacional juega sus partidos como local), la natación y el balonmano (siendo digno de mención el fenecido BM Atlético de Madrid, campeón del mundo y heredero de la histórica sección de balonmano del Atlético de Madrid) se encuentran también entre las preferencias deportivas de los madrileños, como avala la existencia de numerosas escuelas, clubes, asociaciones y federaciones, así como la celebración de distintos campeonatos internacionales (caso del Masters de Madrid, torneo de tenis que se disputa en las canchas del Madrid Arena).

Mención aparte merecen los llamados deportes de naturaleza, con un elevado número de aficionados, dadas las idóneas condiciones orográficas de la región. Hay varias escuelas y federaciones de escalada, alpinismo, montañismo y senderismo, entre ellas la Real Sociedad de Alpinismo Peñalara, creada en 1912, una de las más antiguas de España en esta especialidad. La Pedriza, La Maliciosa, las formaciones graníticas de Torrelodones y los "rocódromos" de la capital se encuentran entre los lugares más frecuentados por los amantes de la escalada.

Por su parte, los embalses, estanques y ríos de la región permiten la práctica de diferentes deportes náuticos. Aquí destacan el embalse de San Juan, en San Martín de Valdeiglesias y Pelayos de la Presa, el lago de la Casa de Campo y la ría del Parque Juan Carlos I, ambos en Madrid, así como el río Tajo, a la altura de Aranjuez.

Entre los deportistas más destacados nacidos en la Comunidad de Madrid se pueden señalar a Manuel Santana, Francisco Fernández Ochoa, Carlos Sainz, Blanca Fernández Ochoa, Emilio Butragueño, Jesús Rollán, Pedro García Aguado, Javier Fernández López, Fernando Torres, Raúl González Blanco, Estela Giménez, Iker Casillas o Alberto Contador.

La Consejería de Deportes, creada en junio de 2007, tras las elecciones autonómicas de mayo, planifica y gestiona la política deportiva de la Comunidad de Madrid. De esta entidad administrativa, anteriormente integrada en la Consejería de Cultura y Deportes, dependen algunas de las principales instalaciones deportivas de la región, a las que se añaden los complejos de titularidad municipal y los de carácter privado o asociativo.

La comunidad madrileña alberga diversas disciplinas deportivas de motor, tanto automovilísticas como motociclistas. En primer término cuenta con el circuito del Jarama, que llegó a albergar pruebas de la Fórmula 1 y en el pasado el Circuito de Guadarrama que albergó el primer Gran Premio de España. En la disciplina de rally se han disputado pruebas puntuables para el Campeonato de España de Rally de asfalto como el Rally Shalymar, el Rally Valeo, el Critérium Luis de Baviera, el Rally de Madrid o el Rally Comunidad de Madrid; como el Campeonato de España de Rally de Tierra. El Rally RACE de España, prueba que puntuó en el pasado para el Campeonato de Europa de Rally, contó en varias de sus ediciones con tramos por la comunidad de Madrid. Desde 2009 la misma se reconvirtió en un prueba de históricos, también puntuable para la categoría continental.

La Comunidad de Madrid dispone de una red de hospitales públicos (algunos de ellos de gestión privada) repartidos por la capital y otros municipios de la región, aparte de los centros privados, que también se encuentran distribuidos por diferentes localidades.

Se zonifican en 11 grandes áreas, si bien el gobierno regional contempla su ampliación a 15. Estas se articulan alrededor de otros tantos hospitales de cabecera, en las que se integran un total de 35 grandes centros hospitalarios.

No dispone de una Policía Autonómica propia, pero subvenciona a policías locales de varios ayuntamientos, para que se encarguen de competencias en materia de seguridad, lo que se conoce bajo el nombre de BESCAM (Brigadas Especiales de Seguridad de la Comunidad Autónoma de Madrid).

En los 21 distritos de la ciudad de Madrid hay 520 guarderías (98 públicas y 422 privadas), 235 colegios públicos de educación infantil y primaria, 106 institutos de educación secundaria, 309 colegios privados (con y sin concierto) y 24 centros extranjeros. En los 43 municipios de la zona este de la Comunidad de Madrid hay 180 guarderías (78 públicas y 102 privadas), 138 colegios públicos de educación infantil y primaria, 51 institutos de educación secundaria y 28 colegios privados (con y sin concierto). En los 40 municipios de la zona norte de la Comunidad de Madrid hay 180 guarderías (78 públicas y 102 privadas), 78 colegios públicos de educación infantil y primaria, 23 institutos de educación secundaria, 23 colegios privados (con y sin concierto) y 8 centros extranjeros. En los 29 municipios de la zona oeste de la Comunidad de Madrid hay 177 guarderías (42 públicas y 135 privadas), 78 colegios públicos de educación infantil y primaria, 32 institutos de educación secundaria, 53 colegios privados (con y sin concierto) y 11 centros extranjeros. En los 39 municipios de la zona sur de la Comunidad de Madrid hay 258 guarderías (121 públicas y 137 privadas), 234 colegios públicos de educación infantil y primaria, 103 institutos de educación secundaria y 77 colegios privados (con y sin concierto).

En la Comunidad de Madrid hay un total de catorce universidades, seis de ellas son públicas. La primera universidad en el territorio fue la Universidad Complutense, fundada en 1499 en Alcalá de Henares y desaparecida como tal en la actualidad. Hoy en día existen, entre las públicas, la Universidad Complutense de Madrid (heredera de las anteriores Universidad Central y Universidad de Madrid), la Universidad Politécnica de Madrid, la Universidad Autónoma de Madrid, la Universidad de Alcalá, la Universidad Rey Juan Carlos y la Universidad Carlos III de Madrid.




</doc>
<doc id="3605" url="https://es.wikipedia.org/wiki?curid=3605" title="1 de marzo">
1 de marzo

El 1 de marzo es el 60.º (sexagésimo) día del año en el calendario gregoriano y el 61.º en los años bisiestos. Quedan 305 días para finalizar el año. En el calendario romano (hasta el año 153 a. C.) este era el primer día del año.



















</doc>
<doc id="3607" url="https://es.wikipedia.org/wiki?curid=3607" title="Antonio Meucci">
Antonio Meucci

Antonio Santi Giuseppe Meucci (Florencia, 13 de abril de 1808-Nueva York, 18 de octubre de 1889)fue un inventor italiano, creador del teletrófono, posteriormente bautizado como «teléfono», entre otras innovaciones técnicas. Desarrolló un teléfono neumático (precursor de su teletrófono) que hoy todavía se utiliza en el Teatro de la Pergola de Florencia y que luego perfeccionó en el teatro Tacón de La Habana. Creó un nuevo sistema de galvanizado, un sistema de filtros para la depuración del agua e introdujo el uso de la parafina en la fabricación de velas. También desarrolló un sistema de electroshocks terapéuticos que administraba en La Habana. El gobierno de Italia lo honra con el título de "Inventore ufficiale del telefono".

Estudió ingeniería química e ingeniería industrial en la Academia de Bellas Artes de Florencia, que además de formar artistas plásticos como pintores o escultores también poseía profesorado y laboratorios de física y química. 

Se casó el 7 de agosto de 1834 con Ester Mochi. Luego fue acusado de participar en una conspiración del Movimiento de Unificación Italiana con la sociedad secreta de los carbonarios y fue encarcelado tres meses.

En octubre de 1835, Meucci y su esposa dejaron Florencia para nunca regresar. Emigraron al continente americano, parando primero en Cuba donde Meucci aceptó un trabajo en el Gran Teatro Tacón (en La Habana). En 1839, Meucci y su esposa emigraron a los Estados Unidos, y llegaron a Clifton (en Staten Island), que por ferry quedaba a 3 km frente al distrito de Brooklyn, y a 10 km del distrito de Manhattan, en la ciudad de Nueva York, donde Meucci vivió el resto de su vida.

En su nuevo hogar, Meucci fue siempre respetado como un prohombre de la comunidad italiana de Nueva York. Había levantado una fábrica de velas y acogía a cualquier italiano que necesitara ayuda. Garibaldi pasó por casa de Meucci durante su periplo americano.

El inventor italiano Antonio (pronunciado [meúcchi]) fue el inventor del "«telettrófoni»", posteriormente bautizado como «teléfono».

En 1854,
Meucci construyó un teléfono para conectar su oficina (en la planta baja de su casa) con su dormitorio (ubicado en el segundo piso), debido a que su esposa estaba inmovilizada por el reumatismo. Sin embargo, Meucci carecía del dinero suficiente para patentar su invento, aunque sí patentó otros inventos que él creía más redituables, como un filtro económico para la depuración del agua y el uso de la parafina en la fabricación de velas (que hasta ese momento se fabricaban con grasa de animales, muy contaminantes y sucias).

En 1860 Antonio Meucci hizo público su invento, el teletrófono. En una demostración pública, la voz de un cantante se trasmitió a una considerable distancia. La prensa italiana de Nueva York publicó una descripción del invento y un tal Sr. Bendelari se llevó a Italia una copia del prototipo, y la documentación necesaria para producirlo allí, pero no se volvió a saber de él, como tampoco se materializó ninguna de las ofertas que surgieron tras la demostración.

Consciente de que alguien podía robarle la patente, pero incapaz de reunir los 250 dólares (unos 7900 dólares de 2016)
que costaba la patente definitiva, tuvo que conformarse con un "cáveat" (‘aviso’, trámite preliminar de presentación de documentación para el patentamiento, con vigencia de un año) que registró el 28 de diciembre de 1871 y que pudo permitirse renovar ―por 10 dólares (o 314 dólares de 2016)―
solo en 1872 y 1873.

Un accidente, la explosión del vapor Westfield, del que sale con severas quemaduras, obliga a su esposa a vender los trabajos de Antonio a un prestamista, por 6 dólares. Cuando, una vez repuesto, vuelve para recuperarlos la casa de empeño dice haberlos vendido a un hombre joven al que nunca se pudo identificar.

En cuanto tuvo el acuse de recibo de Patentes, Antonio Meucci volvió a empeñarse en demostrar el potencial de su invento. Para ello, ofreció una demostración del «telégrafo parlante» a un empresario llamado Edward B. Grant, vicepresidente de una filial de la Western Union Telegraph Company. Cada vez que Meucci trataba de avanzar, se le decía que no había hueco para su demostración, así que a los dos años, Meucci pidió que le devolvieran su material, a lo que le contestaron que se había perdido.

En 1876, Alexander Graham Bell registró una patente que realmente no describía el teléfono pero lo mencionaba como tal.
Cuando Meucci ―que vivía cerca de Nueva York― se enteró, pidió a un abogado que reclamara ante la oficina de patentes de los Estados Unidos en Washington, algo que nunca sucedió. Sin embargo, un amigo que tenía contactos en Washington, se enteró de que toda la documentación referente al telégrafo parlante registrada por Meucci se había perdido.

Una investigación posterior puso en evidencia un delito de prevaricación por parte de algunos empleados de la oficina de patentes con la compañía de Bell. En un litigio posterior entre la empresa Bell Telephone Company (creada en 1877) y Western Union, afloró que existía un acuerdo por el cual Bell pagaría a la Western Union un 20 % de los beneficios derivados de la comercialización de su invento durante 17 años.

Diez años después, en un proceso legal de 1886, Meucci tuvo que demandar incluso a su propio abogado, sobornado por el poderoso Bell. Sin embargo, Meucci supo hacer entender al juez que no cabía duda en cuanto a la autoría del invento registrado. Pese a la declaración pública del entonces secretario de Estado: «Existen suficientes pruebas para dar prioridad a Meucci en la invención del teléfono».

A pesar de que el Gobierno de Estados Unidos inició acciones legales por fraude contra la patente de Alexander Graham Bell, el proceso embarrancó en el arenal de los recursos por los abogados de Bell, hasta cerrarse en 1889 debido a la muerte de Meucci.

Meucci falleció pobre y amargado y jamás vio la gloria y el reconocimiento de su talento, que chocó con su escaso conocimiento del inglés y su poca desenvoltura ante las artimañas legales y los ingentes intereses económicos de las grandes corporaciones de Estados Unidos.

El 11 de junio de 2002, el "Boletín Oficial de la Cámara de Representantes de los Estados Unidos" publicó la Resolución n.º 269, por la que se honra la vida y el trabajo del inventor italoestadounidense. En la misma se reconoce que fue más bien Antonio Meucci en vez de Alexander Graham Bell quien inventó el teléfono.
Reconoció además que Meucci demostró y publicó su invento en 1860 y concluye con un reconocimiento a su autoría en dicha invención.




</doc>
<doc id="3610" url="https://es.wikipedia.org/wiki?curid=3610" title="Monte Katahdin">
Monte Katahdin

El monte Katahdin es la cumbre más elevada del estado de Maine, además del punto culminante del Parque Estatal Baxter, un área protegida de 80.000 ha localizada en Montañas Blancas (Nuevo Hampshire), Estados Unidos. El parque cuenta con más de 45 cumbres, explorables a través de una red de senderos que suman 240 km aproximadamente.


</doc>
<doc id="3618" url="https://es.wikipedia.org/wiki?curid=3618" title="19 de septiembre">
19 de septiembre

El 19 de septiembre es el 262.º (ducentésimo sexagésimo segundo) día del año en el calendario gregoriano y el 263.º en los años bisiestos. Quedan 103 días para finalizar el año.










</doc>
<doc id="3619" url="https://es.wikipedia.org/wiki?curid=3619" title="18 de septiembre">
18 de septiembre

El 18 de septiembre es el 261.º (ducentésimo sexagésimo primer) día del año en el calendario gregoriano y el 262.º en los años bisiestos. Quedan 104 días para finalizar el año.







































</doc>
<doc id="3620" url="https://es.wikipedia.org/wiki?curid=3620" title="17 de septiembre">
17 de septiembre

El 17 de septiembre es el 260.º (ducentésimo sexagésimo) día del año en el calendario gregoriano y el 261.º en los años bisiestos. Quedan 105 días para finalizar el año.








</doc>
<doc id="3621" url="https://es.wikipedia.org/wiki?curid=3621" title="Karl Popper">
Karl Popper

Karl Raimund Popper (Viena, 28 de julio de 1902-Londres, 17 de septiembre de 1994) fue un filósofo y profesor austriaco, aunque más tarde se convirtió en ciudadano británico.

Es considerado como uno de los filósofos de la ciencia más importantes del siglo XX. Popper argumentó que una teoría en las ciencias empíricas nunca puede ser probada, pero puede ser falsada, lo que significa que puede y debe ser examinada por experimentos decisivos para distinguir la ciencia de la no-ciencia. En el discurso político, es conocido por su vigorosa defensa de la democracia liberal y los principios de crítica social que creía que hacían posible una floreciente sociedad abierta. Su filosofía política abarca ideas de todas las principales ideologías políticas democráticas e intenta conciliarlas, como la socialdemocracia, libertarismo / liberalismo clásico y conservadurismo.

Fue hijo del abogado Simon Siegmund Carl Popper y Jenny Schiff, descendientes de judíos. La familia de Popper se había convertido del judaísmo al luteranismo antes de que él naciera en 1902. Popper mismo se caracterizaba como agnóstico.

De la familia Schiff provenían varias personalidades significativas de los siglos XIX y XX tales como el director de orquesta Bruno Walter. 

Su abuelo paterno tenía una formidable biblioteca en la que él, desde niño, contraería la pasión de la lectura. Nunca se consoló de haber tenido que venderla cuando se desplomaron las finanzas de su familia que, durante su infancia, había sido muy próspera. 

En la Viena multicultural de principios del siglo XX, que vio nacer a Karl Raimund Popper, la situación de los judíos era compleja. Por un lado, pertenecían a las capas medias y altas de la sociedad, y ocupaban con frecuencia posiciones destacadas en la economía y la política, pero, por otra parte, eran habituales las manifestaciones de antisemitismo. Popper se destacó pronto por un precoz rechazo a toda forma de nacionalismo —la regresión a la tribu— lo que lo llevó a oponerse al sionismo y siempre pensó que la creación del Estado de Israel fue «un trágico error».

En el borrador de su "Autobiografía" escribió: «Inicialmente me opuse al sionismo porque yo estaba contra toda forma de nacionalismo. Pero nunca creí que los sionistas se volvieran racistas. Esto me hace sentir vergüenza de mi origen, pues me siento responsable de las acciones de los nacionalistas israelíes». Pensaba entonces que los judíos debían integrarse a las sociedades en las que vivían, como había hecho su familia, porque la idea del «pueblo elegido» le parecía peligrosa. Presagiaba, según él, las visiones modernas de la «clase elegida» del marxismo o de la «raza elegida» del nazismo.

Cuando Karl Popper comenzó sus estudios universitarios, en la década del 1920, la escena política estaba dominada efímeramente por la izquierda política en Viena. Florecía entonces la llamada Viena Roja. También Popper, interesado principalmente en la pedagogía política, se implicó en este movimiento, e ingresó en las juventudes socialistas. 

Tras presentar en 1928 una tesis doctoral fuertemente matemática dirigida por el psicólogo y lingüista Karl Bühler, Popper adquirió en 1929 la capacitación para dar lecciones universitarias de matemáticas y física. En estos años tomó contacto con el llamado Círculo de Viena. No obstante su cercanía con este, Popper cuestionó siempre algunos de los postulados más significativos de este grupo de pensadores, lo que dificultó su integración en él. 

En cualquier caso, el Círculo se vio influido por la fundamentada crítica de Popper y, de hecho, "La lógica de la investigación científica" (en alemán "Logik der Forschung"), principal contribución de Popper a la teoría de la ciencia, apareció por primera vez en una serie de publicaciones del propio círculo vienés, a pesar de que contenía una moderada crítica al positivismo de esta comunidad de filósofos. La obra fue recibida como fruto de las discusiones del círculo, lo que llevó a muchos a calificar equivocadamente a Popper como positivista.

El ascenso del nacionalsocialismo en Austria llevó finalmente a la disolución del Círculo de Viena. En 1936 su fundador Moritz Schlick fue asesinado por un estudiante. En 1937, tras la toma del poder por los partidarios de Hitler, Popper, ante la amenazante situación política se exilió en Nueva Zelanda, tras intentar en vano emigrar varias veces a Estados Unidos y Reino Unido.

En el Canterbury College en Christchurch, Popper vivió aislado y hasta cierto punto desconectado de un mundo que se precipitaba entonces en el torbellino de la Segunda Guerra Mundial. En este entorno Popper redactó "La sociedad abierta y sus enemigos" (en alemán "Die offene Gesellschaft und ihre Feinde"). También de aquella época data su amistad y colaboración con el neurobiólogo John C. Eccles, junto al que escribiría "El Yo y el cerebro" en 1977.

Tras la guerra, en 1946, Popper ingresó como profesor de filosofía en la "London School of Economics and Political Science". El sociólogo y economista liberal Friedrich August von Hayek fue uno de los principales valedores de Popper para la concesión de esa plaza. Sin embargo, la relación entre ambos pensadores es aún controvertida. 

A pesar de que ambos mantenían posiciones metodológicas parecidas y de que Popper hizo suyos algunos conceptos fundamentales de las obras de Hayek, tales como el principio del orden espontáneo, lo cierto es que Popper desconfiaba de los mecanismos puros del libre mercado que abanderaba Hayek, y predicaba más bien cierto intervencionismo del Estado pero que no desembocara, en cualquier caso, en el control o en la propiedad estatal.

En 1969 se retiró de la vida académica activa y pasó a la categoría de profesor emérito, a pesar de lo cual continuó publicando hasta su muerte, el 17 de septiembre de 1994 en East Croydon (Londres).

Los logros filosóficos de Karl Popper le valieron numerosos reconocimientos, tales como ser nombrado caballero por la reina Isabel II del Reino Unido en 1969. Recibió la insignia de Compañero de Honor (Companion of Honour) en 1982, el premio Lippincott de la Asociación Norteamericana de Ciencias Políticas y el premio Sonning. Fue miembro de la Sociedad Mont Pelerin, una comunidad de estudios fundada por Hayek para promover una agenda política liberal, así como de la Royal Society de Londres, con el rango de miembro, y de la Academia Internacional de la Ciencia. Entre otras, cultivó la amistad del canciller alemán Helmut Schmidt. Algunos conocidos discípulos de Popper fueron Hans Albert, Imre Lakatos, y Paul Feyerabend.

Expuso su visión sobre la filosofía de la ciencia en su obra, ahora clásica, "La lógica de la investigación científica", cuya primera edición se publicó en alemán ("Logik der Forschung") en 1934. En ella el filósofo austríaco aborda el problema de los límites entre la ciencia y la metafísica, y se propone la búsqueda de un llamado "criterio de demarcación" entre las mismas que permita, de forma tan objetiva como sea posible, es decir, a partir de criterios (epistémicos, metodológicos, reglas y normas) bajo los cuales se evalúe la teoría y así, distinguir las proposiciones científicas de aquellas que no lo son. 

Es importante señalar que el criterio de demarcación no decide sobre la veracidad o falsedad de una afirmación, sino sólo sobre si tal afirmación ha de ser estudiada y discutida dentro de la ciencia o, por el contrario, se sitúa en el campo más especulativo de la metafísica. Para Popper una proposición es científica si puede ser refutable, es decir, susceptible de que en algún momento se puedan plantear ensayos o pruebas para refutarla, independientemente de que salgan airosas o no de dichos ensayos. Esto último a partir del Método Científico que también se hace valer de la conjetura y la coyuntura. Sin embargo, cabe señalar que la demarcación científica también incluye el criterio de verdad (explícita e implícitamente), al igual que el Inductivo Lógico, puesto que el objetivo de la ciencia es la verdad, la verosimilitud... 

En este punto Popper discrepa intencionadamente del programa positivista, que establecía una distinción entre proposiciones contrastables (positivas), tales como "Hoy llueve" y aquellas que, según los positivistas, no son más que abusos del lenguaje y carecen de sentido, por ejemplo "Dios existe". Para Popper, este último tipo de proposiciones sí tiene sentido y resulta legítimo discutir sobre ellas, pero han de ser distinguidas y separadas de la ciencia. Su criterio de demarcación le trajo sin querer un conflicto con Ludwig Wittgenstein, el cual también sostenía que era preciso distinguir entre proposiciones con sentido y las que no lo tienen. El criterio de distinción, para Wittgenstein, era el del "significado": solamente las proposiciones científicas tenían significado, mientras que las que no lo tenían eran pura metafísica.

Era tarea de la filosofía desenmascarar los sinsentidos de muchas proposiciones autodenominadas científicas, a través de la aclaración del significado de las proposiciones. A Popper se le encuadró en dicha escuela cuando formuló su idea de la demarcación, pero él mismo se encargó de aclarar que no estaba de acuerdo con dicho planteamiento, y que su tesis no era ningún criterio de significación (Popper siempre huyó de cualquier intento por aclarar significados antes de plantear teorías). Es más, Popper planteó que muchas proposiciones que para Wittgestein tenían significado no podían calificarse como ciencia como, por ejemplo, el psicoanálisis o el marxismo, ya que ante cualquier crítica se defendían con hipótesis "ad hoc" que impedían cualquier refutación.

Lo cierto es que Popper era consciente del enorme progreso en el conocimiento científico que se experimentó en los siglos que le precedieron, en tanto que problemas como la existencia de Dios o el origen de la ley moral parecían resistirse sin remedio, puesto que no mostraban grandes avances desde la Grecia clásica. Por ello, la búsqueda de un criterio de demarcación aparece ligada a la pregunta de ¿qué propiedad distintiva del conocimiento científico ha hecho posible el avance en nuestro entendimiento de la naturaleza? 

Algunos filósofos de la época habían buscado respuesta en el inductivismo, según el cual cuando una ley física resulta repetidamente confirmada por nuestra experiencia, podemos darla por cierta o, al menos, asignarle una gran probabilidad. Ya que se trata de una forma en que los positivistas comprueban. Pero tal razonamiento, como ya fue notado por David Hume, no puede sostenerse en criterios estrictamente lógicos, puesto que éstos no permiten extraer (inducir) una ley general (universal) a partir de un conjunto finito de observaciones particulares. En ese aspecto se dice que el científico se enfrenta a cosas finitas. 

Popper supera la crítica de Hume abandonando por completo el inductivismo y sosteniendo que lo primero son las teorías, y que sólo a la luz de ellas nos fijamos en los hechos. Nunca las experiencias sensibles anteceden a las teorías (cuya base empírica contiene elementos físicos para comprobar), por lo que no hay necesidad de responder cómo de las experiencias particulares pasamos a las teorías al hacernos suponer de una manera falsacionista. Con ello, Popper supera la polémica entre empirismo y racionalismo, sosteniendo que las teorías anteceden a los hechos, pero que las teorías necesitan de la experiencia (en su caso, de las refutaciones) para distinguir qué teorías son aptas de las que no.

La salida a este dilema, propuesta en "La lógica de la investigación científica", es que el conocimiento científico no avanza confirmando nuevas leyes, sino descartando leyes que contradicen la experiencia. A este descarte Popper lo llama "falsación". De acuerdo con esta nueva interpretación, la labor del científico consiste principalmente en criticar (acto al que Popper siempre concedió la mayor importancia) leyes y principios de la naturaleza para reducir así el número de las teorías compatibles con las observaciones experimentales de las que se dispone. 

El criterio de demarcación puede definirse entonces como la capacidad de una proposición de ser refutada o "falsada". Solo se admitirán como proposiciones científicas aquellas para las que sea conceptualmente posible un experimento o una observación que las contradiga. Así, dentro de la ciencia quedan por ejemplo la teoría de la relatividad y la mecánica cuántica, y fuera de ella, el marxismo o el psicoanálisis. En este sentido, resulta extremadamente revelador el pensamiento que Popper escribió en las primeras páginas de su autobiografía "Búsqueda sin término":

Sin embargo, la tesis de Quine-Duhem, también llamada holismo confirmacional u holismo epistemológico, argumenta que no es posible "probar" que un enunciado ha sido falsado.

Hay dos aspectos del holismo confirmacional. El primero es que "las observaciones dependen de la teoría". Antes de aceptar las observaciones del telescopio se debe mirar la óptica del telescopio, el modo en que está montado, con el fin de asegurar que el telescopio esté apuntando en la dirección correcta y que la luz viaje a través del espacio en línea recta (que a veces no es tal, como Einstein demostró). El segundo es que "la evidencia por sí sola es insuficiente para determinar qué teoría es correcta". Cada una de las alternativas mencionadas "podría" haber sido correcta, pero solo una de ellas fue finalmente aceptada.

Que las teorías solo puedan ser probadas por su relación con otras teorías, implica que siempre se puede declarar que los resultados de las pruebas que parecen refutar una teoría científica no la refutan en absoluto. En lugar de eso, se puede sostener que esos resultados chocan con las predicciones porque alguna otra teoría es falsa o desconocida. Quizá el equipo de pruebas esté desalineado o quizá haya materia oscura en el universo que sea la causante de los extraños movimientos de algunas galaxias.

El hecho de que no sea posible determinar qué teoría es refutada por datos inesperados significa que los científicos deben consensuar qué teorías aceptar y cuáles rechazar. La lógica por sí sola no sirve de guía en estas decisiones.

Para Popper, tanto el psicoanálisis como la teoría de la historia de Karl Marx no eran científicas. Karl Marx argumentaba que las sociedades industrializadas darían lugar al socialismo, y en último término, al comunismo. Pero cuando esto no ocurría, en lugar de admitir que la teoría de Marx era incorrecta, los marxistas se inventarían una explanación "ad hoc" para demostrar que lo sucedido era perfectamente consistente con la teoría. 

Por ejemplo, podrían argumentar que el inevitable progreso del comunismo se había visto temporalmente ralentizado por las mejoras del estado de bienestar, lo cual "ablandecía" al proletariado y debilitaba su entusiasmo revolucionario. Así, la teoría se podría compatibilizar con cualquier sucesión de acontecimientos, igual que ocurría con el psicoanálisis, motivo por el cual no las consideraba científicas.

Así pues, en realidad, y según, entre otros filósofos de la ciencia, Thomas Kuhn, la "falsación" ocurriría cuando la comunidad científica se pone "de acuerdo", consensúa en que ha sido falsado, es decir, correspondería a una "moda"

Según Bachelard, la ciencia no puede producir verdad. Lo que debe hacer es buscar mejores maneras de preguntar. Para ejemplificarlo, utiliza una metáfora: ""el conocimiento de lo real es una luz que siempre proyecta alguna sombra"".

Cada superación de algún obstáculo epistemológico conlleva necesariamente otro obstáculo más complejo, contrariamente a lo supuesto por Popper, quien posteriormente abandonó el simple falsacionismo como una "lógica de la ciencia", puesto que se dio cuenta de que cualquier teoría lo suficientemente rica puede eludir ser falsada recurriendo a hábiles movimientos de prestidigitación lógica, y finalmente admitió que las continuas modificaciones "ad hoc" de una teoría le permitirían evitar ser falsada.

Así pues, el falsacionismo, en todas y cada una de sus múltiples formas, es una idea interesante, pero "insuficiente como para caracterizar qué es lo que es ciencia o para resolver el problema de demarcación". Sufre de una serie de dificultades lógicas y epistemológicas que deberían hacernos detener si lo que buscamos es obtener una respuesta en cuanto a qué es buena ciencia y qué no.

En el sistema de Popper se combina la racionalidad con la extrema importancia que la crítica tiene en el desarrollo de nuestro conocimiento. Por eso, tal sistema fue bautizado como racionalismo crítico.

Las ideas de Popper sobre el conocimiento científico pueden considerarse como la base que sustenta el resto de sus contribuciones a la filosofía. Además han gozado de enorme popularidad desde que fueron publicadas por primera vez y, al menos entre la comunidad científica, el concepto de falsabilidad ha enraizado fuertemente y es comúnmente aceptado como criterio válido para juzgar la respetabilidad de una teoría. Consciente de ello, y de las críticas que suscitaron sus teorías, Popper amplió y matizó su trabajo originario en sucesivas ediciones y postscripta.

Popper hace mención en este escrito que para cada conjetura existe, ha existido y siempre existirá una refutación, lo que significa que si algo tiene la posibilidad de ser falso, puede ser cierto. Sin embargo, cuando algo no puede ser falso, es tan utópico que nunca podría ser verdadero, ya que para que exista la posibilidad de que sea real, necesita su contraparte de ser falso, ya que para que exista algo real debe existir su lado irreal. Y es mediante su dilema del falsacionismo como Popper logra explicar que para que exista ciencia deben existir modelos científicos que expliquen sucesos o verdades y que sean totalmente aplicables a la realidad para que funcionen en la mayoría de los casos. Y por esto deja fuera a todas las ciencias sociales, ya que estas no están metódicamente explicadas por modelos: simplemente se basan en la observación de patrones y fundamentos.

En cuanto a su idea del conocimiento, para Popper cuanto más específico y complejo sea el modelo científico, más apegado a la realidad estará, sin olvidar nunca que para que existan modelos y teorías verdaderas, siempre tendrán que existir sus contrapartes y más teorías que las invaliden, que son igualmente verdaderas. Ello significa que solo se puede generar una verdad, (o lo que se define como conocimiento) a partir de modelos científicos o hipótesis perfectas, pero como la creación de estas es algo utópico, Popper se conforma con que el modelo sea lo suficiente aproximado para que funcione en la mayoría de los escenarios, siempre haciendo énfasis y reiterando en que existe lo falso en lo verdadero, y que una idea o concepto nunca será completamente verdadera porque existirán otras ideas o conceptos que la invaliden.

Popper expresa así que todo el tiempo estamos elaborando teorías e hipótesis de acuerdo con nuestras expectativas, y la mayor parte del tiempo las estamos experimentando, a las cuales las llama conjeturas. Al momento en que una teoría puede ser contrastable, aunque no se pueda verificar, es falsable. Cuando se generaliza algo y puede haber una excepción, una refutación, se convierte en teoría científica. Así él confirma que no se trata de verificar infinitamente una teoría, sino de encontrar algo que la convierta en falsa; haciéndolo lógico y no metodológico. Con esta idea el crecimiento del conocimiento científico se encarga de eliminar teorías y crear una división entre la ciencia y la metafísica, por medio de conjeturas, que se ponen a prueba y refutan principalmente por científicos.

En su libro "Realism and the Aim of Science: From the Postscript to The Logic of Scientific Discovery", Popper niega que exista el método científico:

Además de sus notables contribuciones a la epistemología, Popper es recordado por muchos como un filósofo, teórico del liberalismo y defensor de la "sociedad abierta" frente a los sistemas que, según su concepción, resultaban totalitarios, tales como el comunismo y el nacionalsocialismo. Sin embargo, para comprender sus posiciones políticas, es preciso partir de sus aportaciones a la teoría del conocimiento (véase epistemología).

La obra más conocida de Karl Popper es "La sociedad abierta y sus enemigos", escrita durante la Segunda Guerra Mundial desde su exilio en Nueva Zelanda. En ella el autor se propone aplicar a la política sus teorías sobre la ciencia y el avance del conocimiento. Al tiempo, Popper indaga en la historia de la filosofía para trazar los orígenes del totalitarismo que había desembocado en la guerra y en la radical crisis del pensamiento occidental. Es notable que, desde sus primeras páginas, Popper aborda el problema armado de un firme optimismo respecto a la naturaleza humana, pues afirma que el pensamiento totalitario y la destrucción asociada a él, nacen del empeño sincero de los hombres en mejorar su condición y la de sus semejantes, si bien su buena voluntad descarrila al ser guiada por filosofías utópicas y metodológicamente equivocadas.

Este reconocimiento moral que Popper otorga a sus adversarios ideológicos es particularmente visible en la consideración con la que trata a Karl Marx puesto que, si bien puede considerarse a "La sociedad abierta y sus enemigos" una acerada crítica al marxismo, el pensador vienés reconoce en Marx un sincero interés en mejorar las condiciones de las clases humildes, así como valiosas aportaciones a la sociología, en el sentido de convertirla en una ciencia autónoma que dispone de sus propias categorías (tales como las instituciones) y que queda felizmente despojada del psicologismo de Stuart Mill.

Popper plantea una interpretación de la historia del pensamiento político basada en la confrontación entre dos escuelas o visiones del mundo: a) una reaccionaria, que añora una comunidad cerrada y perfecta, heredera de la tribu. Platón (tomando los antecedentes de Heráclito) es su máxima expresión, seguido de Aristóteles y reeditado en el pensamiento moderno por Hegel (al cual, aparte del tono claramente sarcástico y cómico de su análisis, no le reconoce absolutamente nada) y b) otra racional y crítica, que nació en la Antigüedad clásica con la "Gran Generación" de la época de Pericles, a la cual pertenecen Sócrates y Demócrito. Dicha visión reconoce el limitado conocimiento humano a la cual atribuye el auténtico espíritu de la ciencia.

Popper hace una exégesis de la obra de Platón, y le atribuye la acuñación del esencialismo en la teoría del conocimiento y del historicismo en la teoría política. Partiendo de la teoría de las formas y las ideas, la tesis de Platón es que existe un mundo de las ideas que es perfecto, y que la realidad material en la que vivimos no es más que una copia imperfecta que tiende a la degeneración. Esta visión no solo se aplica a la realidad natural, sino también a la política y social. La ciudad (la polis), modelo de sociedad fundamental en el mundo griego, tiende a la degeneración y decadencia al alejarse en el tiempo de la polis originaria y perfecta. La democracia, para Platón, es tan sólo un estadio más en la degeneración, de la cual la tiranía no será más que la última expresión.

Platón de este modo plantea la situación en Atenas, su ciudad natal, como de decadencia al haberse instaurado la democracia y ver como en sus avatares se desliza a la tiranía. Para Platón el modelo ideal de ciudad es Esparta, una aristocracia de nobles que gobierna sobre el resto que no tiene más función de obedecer. Para mantener la unidad y la estabilidad la aristocracia debe mantenerse unida sin dar lugar a lujos ni disensiones. Mantiene que las disensiones políticas es el origen de la decadencia y que deben existir una radical división entre los hombres: entre los que dirigen y el resto que debe obedecer En el mantenimiento de este orden "perfecto" gira toda su concepción sociológica de Justicia. A Aristóteles Popper no le concede más que el desarrollo de la Teoría de las Ideas en la doctrina de "potencia y acto" y en la instauración del esencialismo metodológico.

Aristóteles no hace más que adoptar el mundo de las Ideas de Platón pero en vez de modo pesimista de un modo optimista. El desarrollo del mundo material no tiene por qué ser decadencia y alejamiento de la Idea Originaria sino de desarrollo de las potencialidades de las Ideas de modo que las cosas materiales no hacen más que desarrollar la esencia de la cual surgen en su devenir histórico. Esta idea sería repetida por Hegel al cual no le atribuye más mérito que reeditarlas viejas ideas de Heráclito, Platón y Aristóteles para construir una espantosa teoría política con la única finalidad de legitimar el poder absoluto de Guillermo de Prusia.

A Hegel le atribuye también la consolidación moderna del "historicismo" que sería la versión oficial de las ciencias sociales en el continente europeo durante todo el siglo XIX. Marx, aunque políticamente opuesto al modelo de Estado de Hegel, no haría más que aplicar el método historicista al análisis de la sociedad industrial de la época, llegando a la conclusión de que la lucha de clases es el auténtico motor de la historia, y que aplicando un "método científico" se llega a la profecía de que la sociedad se dirige inexorablemente a la crisis final del capitalismo para la instauración de una sociedad sin clases en la que el Estado se disuelva y el hombre alcance la auténtica libertad.

En dicha obra existen críticas recurrentes de Popper tanto al esencialismo metodológico como al historicismo pero además crítica al "sociologismo del conocimiento" o "historismo" que no hay que confundir con el "historicismo". Según dicha doctrina nuestro conocimiento no es más que consecuencia de nuestra circunstancia histórica, de nuestra época con sus tensiones y conflictos de intereses y por ello nuestro estado actual de conocimiento no es ni mejor ni peor que otro cualquiera, negando así la existencia de cualquier verdad, no ya moral, sino incluso científica. Popper, radicalmente opuesto a dicha doctrina, sostiene que el conocimiento humano puede plantearse la búsqueda de la verdad, no entendida como verdad absoluta sino como acercamiento cada vez mejor a la verdad a través de teorías que explican la realidad mejor que otras y que puedan refutarse. 

Otra conocida obra de Karl Popper es el opúsculo "La Miseria del historicismo", cuyo título parafrasea el de la obra de Karl Marx "La miseria de la filosofía", a su vez una burlesca crítica a la "Filosofía de la miseria" de Proudhon. El libro lo dedica "en memoria de los incontables hombres y mujeres de todos los credos, naciones o razas que cayeron víctimas de la creencia fascista y comunista en las 'leyes inexorables del destino histórico'". En dicha obra Popper crítica a un grupo de doctrinas que él denomina "historicistas" y que tienen en común la capacidad de la sociología de predecir el curso de la historia. 

Divide dichas doctrinas en dos grupos: a) las anticientíficas y b) las procientíficas. a) Las primeras sostienen que la realidad social es de naturaleza radicalmente diferente a la realidad física debido a su naturaleza cambiante y compleja por lo que los métodos científicos que tanto éxito han tenido en las ciencias naturales no son aplicables. La alternativa es la aplicación del método "holístico" es decir, la intuición "esencialista y total" de las realidades sociales y el análisis de dichas realidades en su evolución histórica que es lo único que nos puede hacer captar todas sus cualidades y descubrir su esencia (que no es más que la repetición de la idea Aristotélica de potencia y acto). 

Dichas categorías esencialistas (Los Grandes Estados e Imperios, las Civilizaciones, La Lucha de Clases, Los Ejércitos), son las únicas de interés para la sociología y el estudio de su interacción y evolución histórica puede intuitivamente hacernos prever su devenir futuro. Popper critica de frente el "método holístico" como incapaz de analizar ninguna realidad. "Las totalidades en tal sentido no pueden ser objeto de estudio científico alguno". Es dicho método el que ha llevado a las grandes utopías modernas y a los grandes planes sociales totales que según el autor jamás han alcanzado sus objetivos ya que impiden cualquier control científico al pretender "transformar la sociedad" en su totalidad. 

Popper propone el método nominalista con alternativa, es decir, denominar a las realidades según las necesidades de nuestras teorías sin pretender que las cosas esconden una "esencia" detrás de ellas que hay que captar con las definiciones. Además propone la "ingeniería social gradual" o "piecemeal social technology" como alternativa a las grandes utopías transformadoras de la sociedad y de la historia b) las doctrinas pro-científicas sostienen que al igual que la ciencia ha sido capaz de predecir el curso de los planetas en el sistema solar, del mismo modo la sociología es capaz de, copiando los métodos científicos y a través de la historia, calcular el futuro devenir de la historia. Popper crítica dichas doctrinas partiendo de la equivocada idea de ciencia que tienen estas doctrinas. 

La ciencia para Popper no es más que un conjunto de teorías o hipótesis provisionales, que aunque estén inicialmente sostenidas por evidencias se deben tratar de refutar para sostener su validez. Dichas teorías están presentes siempre en la explicación causal de los acontecimientos y solemne cuando la realidad se opone a ellas surge un problema que puede servir de base para refutar una antigua teoría y plantear nuevas hipótesis que solventen dicho problema. Popper sostiene que todas las ciencias (incluidas la sociología) hacen lo mismo pero no del modo que sostienen las doctrinas pro-científicas. Los acontecimientos históricos no pueden explicarse a través de una sola teoría o incluso varias porque son únicos y en ello si intervienen infinidad de teorías de diferente naturaleza. 

Las doctrinas pro-científicas confunden lo que es una tendencia (o condiciones iniciales) con leyes universales inexorables. Popper concluye sosteniendo la unidad de todas las ciencias (incluidas las sociales) en su método de planteamiento de teorías, ensayo y error que eliminan las no aptas, en el hecho de que es imposible predecir la historia futura simplemente porque es imposible predecir los descubrimientos científicos futuros y por último que la historia como la realidad tiene infinidad de vertientes y solamente las teorías y nuestros puntos de vista sobre ella, nos permiten escribir infinidad de "historias".

Thomas Kuhn, en su influyente obra "La estructura de las revoluciones científicas" argumentó que pocas veces los científicos han actuado siguiendo estrictamente los postulados popperianos del falsacionismo. Por el contrario, Kuhn defiende la tesis de que la ciencia ha avanzado a través de "paradigmas" que dominan la mentalidad de cada época: los nuevos desarrollos científicos son únicamente examinados a la luz del paradigma en uso y sólo raramente ocurre una revolución que cuestiona el paradigma mismo. Imre Lakatos, discípulo de Popper, trató de reconciliar esta postura con la de su maestro mediante la introducción de "programas de investigación" que serían el objeto de crítica y falsación, en lugar de las más concretas "proposiciones universalmente válidas" de las que hablaba Popper. En este contexto, la tesis de Quine-Duhem afirma que es imposible contrastar una hipótesis aislada, puesto que esta siempre forma parte de una red interdependiente de teorías. Otro discípulo de Popper, Paul Feyerabend tomó una posición mucho más radical: no existe ningún método general para ampliar o examinar nuestro conocimiento y la única descripción del progreso científico es "anything goes" ("todo sirve").

En las ciencias sociales, Popper mantuvo una viva controversia conocida como la "disputa positivista" (Positivismusstreit) de la sociología alemana. El enfrentamiento fue abierto por un ensayo titulado "Lógica de las ciencias sociales" que fue presentado por Popper en 1961 en el congreso de la Sociedad Alemana de Sociología en Tubinga Tübingen. El filósofo vienés y su discípulo Hans Albert afirmaron que toda teoría con pretensiones científicas, aun dentro de las ciencias sociales, debía ser falsable. A esta visión de la Sociología se opusieron los dialécticos de la Escuela de Fráncfort, Theodor Adorno y su discípulo Jürgen Habermas. En este contexto ha de entenderse una carta de Popper, publicada sin su consentimiento en 1970 en el semanario alemán "Die Zeit" y titulada "Contra las grandes palabras". En ella, Popper ataca duramente la obra de Adorno y Habermas acusándolos de emplear un lenguaje inflado y pretencioso pero vacío de contenido.

En la historia del pensamiento se ha criticado la utilización de categorías ahistóricas por parte de Karl Popper en su evaluación de la obra de autores clásicos, en especial Platón. Klosko, ("Philosophy of the Social Sciences".1996; 26: 509-527) destaca que en "La sociedad abierta y sus enemigos" Popper presenta a Platón como un antecesor del totalitarismo moderno, una categoría que sólo cobra sentido en el mundo posterior a la Gran Guerra y que en la Antigüedad podría aplicarse con cautela al caso particular de la sociedad espartana. Según esta crítica, Popper subordina el conocimiento histórico a su uso político inmediato, y se desentiende de las preocupaciones metodológicas de la buena historia del pensamiento, como la necesidad de recurrir a la filología y la reconstrucción contextualizada del sentido para acercarse a una realidad antropológicamente lejana como la Grecia clásica.







</doc>
<doc id="3622" url="https://es.wikipedia.org/wiki?curid=3622" title="16 de septiembre">
16 de septiembre

El 16 de septiembre es el 259.º (ducentésimo quincuagésimo noveno) día del año en el calendario gregoriano y el 260.º en los años bisiestos. Quedan 106 días para finalizar el año.








</doc>
<doc id="3623" url="https://es.wikipedia.org/wiki?curid=3623" title="15 de septiembre">
15 de septiembre

El 15 de septiembre es el 258.º (ducentésimo quincuagésimo octavo) día del año —el 259.º (ducentésimo quincuagésimo noveno) en los años bisiestos— en el calendario gregoriano. Quedan 107 días para finalizar el año.









</doc>
<doc id="3624" url="https://es.wikipedia.org/wiki?curid=3624" title="14 de septiembre">
14 de septiembre

El 14 de septiembre es el 257.º (ducentésimo quincuagésimo séptimo) día del año —el 258.º (ducentésimo quincuagésimo octavo) en los años bisiestos— en el calendario gregoriano. Quedan 108 días para finalizar el año.










</doc>
<doc id="3625" url="https://es.wikipedia.org/wiki?curid=3625" title="Calendario gregoriano">
Calendario gregoriano

El calendario gregoriano es un calendario originario de Europa, actualmente utilizado de manera oficial en casi todo el mundo, denominado así por ser su promotor el papa Gregorio XIII, quien promulgó su uso por medio de la bula Inter Gravissimas. A partir de 1582, sustituyó gradualmente en distintos países al calendario juliano, utilizado desde que Julio César lo instaurara en el año 46 a. C. El calendario juliano era, básicamente, el calendario egipcio, el primer calendario solar conocido que estableció la duración del año en 365,25 días.

El calendario gregoriano se originó a partir de un primer estudio realizado en 1515 por científicos de la Universidad de Salamanca, y de un segundo en 1578. Del primero, se hizo caso omiso y del segundo, finalmente, surgió el actual calendario mundial, aunque el mérito se atribuyó a otros personajes.

Los primeros países en adoptar el calendario actual fueron España, Italia y Portugal en 1582. Sin embargo, Gran Bretaña y sus colonias americanas no lo hicieron hasta 1752.

La reforma gregoriana nace de la necesidad de llevar a la práctica uno de los acuerdos del Concilio de Trento: ajustar el calendario para eliminar el desfase producido desde el primer Concilio de Nicea, celebrado en 325, en el que se había fijado el momento astral en que debía celebrarse la Pascua y, en relación con esta, las demás fiestas religiosas móviles. Lo que importaba, pues, era la regularidad del calendario litúrgico, para lo cual era preciso introducir determinadas correcciones en el civil. En el fondo, se trataba de adecuar el "calendario civil" al año trópico.

En el Concilio de Nicea se determinó que la Pascua debía conmemorarse el domingo siguiente al plenilunio posterior al equinoccio de primavera en el hemisferio norte (equinoccio de otoño en el hemisferio sur). Aquel año 325 el equinoccio había ocurrido el día 21 de marzo, pero con el paso del tiempo la fecha del acontecimiento se había ido adelantando hasta el punto de que en 1582, el desfase era ya de 10 días, y el equinoccio se fechó el 11 de marzo.

El desfase provenía de un inexacto cómputo del número de días con que cuenta el año trópico; según el calendario juliano que instituyó un año bisiesto cada cuatro, consideraba que el año trópico estaba constituido por 365,25 días, mientras que la cifra correcta es de 365,242189, o lo que es lo mismo, 365 días, 5 horas, 48 minutos y 45,16 segundos. Esos más de 11 minutos contados adicionalmente a cada año habían supuesto en los 1257 años que mediaban entre 325 y 1582 un error acumulado de aproximadamente 10 días.

Se constituyó la "Comisión del Calendario", en la que destacaron los astrónomos Cristóbal Clavioy Luis Lilio. Clavio, quien pertenecía a la orden jesuita, era un reputado matemático y astrónomo a quien Galileo Galilei requirió como aval científico de sus observaciones telescópicas. En cuanto a Lilio, sabemos que fue el principal autor de la reforma del calendario. Murió en 1576 sin ver culminado el proceso. En las Tablas alfonsíes, realizadas por iniciativa del monarca Alfonso X de Castilla, fue asignado al año-trópico un valor de 365 días 5 horas 49 minutos y 16 segundos el cual fue tomado como correcto por la Comisión del Calendario. Pedro Chacón, matemático español, redactó el "Compendium" con el dictamen de Lilio, apoyado por Clavio, y se aprueba la reforma el 14 de septiembre de 1580, para llevarla a la práctica en octubre de 1582.

Al jueves -juliano- 4 de octubre de 1582 le sucedió el viernes -gregoriano- 15 de octubre de 1582. Así, diez días desaparecieron debido a que ya se habían contado de más en el calendario juliano.

El calendario se adoptó inmediatamente en los países donde la Iglesia católica tenía influencia. Sin embargo, en países no católicos, como los protestantes, anglicanos, ortodoxos, y otros, este calendario no se implantó hasta varios años (o siglos) después, e incluso en algunos, se sigue llamando calendario juliano, para no reconocer la autoridad del papa de Roma en su implantación. A pesar de que en sus países el calendario gregoriano es el oficial, las iglesias ortodoxas (excepto la de Finlandia) siguen utilizando el calendario juliano (o modificaciones del mismo, diferentes al calendario gregoriano). Sin embargo, fuera del mantenimiento de un calendario eclesiástico diferente en algunos países, el calendario gregoriano es el que se considera como base para el establecimiento del año civil en todo el mundo, incluyendo los países con un año eclesiástico o religioso diferente al que se estableció en la reforma gregoriana del siglo XVI.

El calendario gregoriano ajusta este desfase cambiando la regla general del bisiesto cada cuatro años, y hace que se exceptúen los años múltiplos de 100, excepción que a su vez tenía otra excepción, la de los años múltiplos de 400, que sí eran bisiestos. La nueva norma de los años bisiestos se formuló del siguiente modo: la duración básica del año es de 365 días; pero serán bisiestos (es decir tendrán 366 días) aquellos años divisibles por 4, exceptuando los múltiplos de 100 (1700, 1800, 1900..., que no serán bisiestos), de los que se exceptúan a su vez aquellos que también sean divisibles por 400 (1600, 2000, 2400..., que serán bisiestos). El calendario gregoriano ajusta a 365,2425 días la duración del año, lo que deja una diferencia de 0,000300926 días al año de error, es decir, "adelanta" cerca de 1/2 minuto cada año (aprox. 26 s c/año), lo que significa que se requiere el ajuste de un día cada 3300 años. Esta diferencia procede del hecho de que la traslación de la Tierra alrededor del Sol no coincide con una cantidad exacta de días de rotación de la Tierra alrededor de su eje. Cuando el centro de la Tierra ha recorrido una vuelta completa en torno al Sol y ha regresado a la misma «posición relativa» en que se encontraba el año anterior, se han completado 365 días y un poco menos de un cuarto de día (0,242189074 para ser más exactos). Para hacer coincidir el año con un número entero de días se requieren ajustes periódicos cada cierta cantidad de años.

Sin embargo, intentar crear una regla para corregir este error de un día cada 3300 años es complejo. En tan largo tiempo la Tierra se desacelera en su velocidad de rotación (y también se desacelera el movimiento de traslación) y ello crea una nueva diferencia que es necesario ir corrigiendo. La Luna ejerce un efecto de retraso sobre esta velocidad de giro por la excentricidad creada por las mareas. La disminución de la velocidad de giro creada por esa excentricidad es similar a la que se produce cuando hacemos girar un frisbee poniéndole un poco de arena mojada en un lado del borde inferior: cuando el platillo se hace girar, su velocidad de giro es mucho menor a la que tiene cuando no existe tal excentricidad. Este efecto todavía se encuentra en análisis y medición por parte del mundo científico y adicionalmente existen otros efectos que complican definir reglas con tal precisión. Este error es solo de una parte por millón. Lo más práctico será que cuando la diferencia sea significativa, es decir, cuando llegue a ser de un día, se declare que el siguiente año bisiesto no lo sea. De todas maneras, quedan casi dos mil años de análisis y discusión antes de necesitar este ajuste.

Otro problema distinto, como ya se ha señalado, es la disminución de la velocidad de rotación terrestre (y también de la traslación terrestre), la cual se puede medir con gran precisión con un reloj atómico. Es un problema distinto porque no tiene que ver nada con el cálculo del calendario y, por lo tanto, con los ajustes que se le tengan que hacer al calendario. Más bien es al contrario: es el reloj atómico el que tiene que ajustarse a los movimientos de la Tierra, es decir, a la duración del día solar y del año terrestre. El reloj atómico mide un tiempo uniforme que, por lo tanto, no existe en la naturaleza, donde los movimientos del mundo físico son uniformemente variados.

Pese a ser el más utilizado, el calendario gregoriano presenta diversas deficiencias. La primera, ya señalada, es su diferencia con el año trópico, pero no es importante para efectos prácticos. De mayor importancia es la diferencia en la duración de los meses (28, 29, 30 ó 31 días) y el hecho de que la semana, que es utilizada casi universalmente como unidad laboral de tiempo, no está integrada en los meses, de tal forma que el número de días laborables de un mes puede variar entre 24 y 27. Además en los países cristianos, el hecho de que la Pascua se rija por una regla lunisolar (según el concilio de Nicea tal festividad debe celebrarse el domingo siguiente a la primera luna llena posterior al equinoccio de primavera, fijado el 21 de marzo para el hemisferio norte) origina alteraciones en diversas actividades (por ejemplo en la educación, turismo, etc.).


Año 1582

Año 1583


Año 1584

Año 1587

Año 1590

Año 1605

Año 1610

Año 1682

Año 1700

Año 1701

Año 1752

Año 1753

Año 1867

Año 1873

Año 1875

Año 1912 o 1929

Año 1914 o 1926

Año 1916

Año 1918

Año 1919

Año 1923

El "calendario gregoriano" distingue entre :

Es "año bisiesto" el que sea múltiplo de 4, con excepción de los años seculares. Respecto a estos, es bisiesto el año secular múltiplo de 400.

De esta manera, el calendario gregoriano se compone de ciclos de 400 años:

Haciendo el cómputo en días:
Esto hace un total de 146 097 días en los 400 años, de modo que la duración media del año gregoriano es de 365,2425 días.

En los 400 años del ciclo del calendario gregoriano, estos 146 097 días, que son 20.871 × 7 días, hay un número entero de semanas 20 871, de tal modo que en cada ciclo de 400 años no solo se repite exactamente el ciclo de años comunes y bisiestos, sino que el ciclo semanal también es exacto, esta congruencia da lugar a que tomando un grupo de 400 años seguidos, el siguiente ciclo de 400 años es exactamente igual.

La primera semana del año, la número 1, es la que contiene el primer jueves de enero.
Las semanas de un año van de la 1 a la 52, salvo que el año termine en jueves, o bien en jueves o viernes si es bisiesto, en cuyo caso se añade una semana más: la 53.

Una regla nemotécnica consiste en cerrar los dos puños y juntarlos con los nudillos hacia arriba. Los nudillos sobresalientes representarán a los meses de 31 días, y los huecos entre nudillos los meses de menos de 31 días. El primer nudillo (el del dedo meñique) representa a enero (y por ser sobresaliente equivale a 31 días). El hueco próximo (entre los nudillos del meñique y del dedo anular) representa a febrero (y por ser hueco tiene menos de 31 días, en este caso 29 o 28 días). El segundo nudillo (del dedo anular) representa a marzo (y por ser sobresaliente equivale a 31 días) y así sucesivamente hasta llegar a julio, representado por el nudillo del dedo índice (que por ser sobresaliente equivale a 31 días). Luego se pasa a la otra mano y se cuenta desde el nudillo del dedo índice, que al igual que el anterior representará a agosto (y por ser sobresaliente equivaldrá a 31 días). Se continúa la cuenta hasta llegar a diciembre, representado por el nudillo del dedo anular (que por ser sobresaliente dice que diciembre tiene 31 días).

Otra manera de visualizar la anterior nemotécnica es como sigue: con el puño cerrado de cualquier mano, pose su dedo índice de la otra mano en el nudillo del dedo índice de su puño; ese nudillo indica el mes de enero. Desplace su dedo índice al intersticio entre los nudillos del dedo índice y medio de su puño, ese intersticio representa a febrero, desplace su índice al siguiente nudillo (dedo medio) "marzo" y así sucesivamente considerando cada nudillo e instersticio hasta llegar al nudillo del meñique que representa a julio, una vez aquí vuelva a llevar su índice al nudillo del dedo índice del puño que ahora indicará el mes de agosto y siga la cuenta nuevamente hasta el nudillo anular que será diciembre. Cada mes caído en nudillo es de 31 días y cada mes caído en instersticio es de 30 días a excepción de febrero.

Los romanos contaron el tiempo con diferentes cómputos. Uno de ellos consistió en empezar a contar a partir del año de la fundación de Roma, es decir, "ab urbe condita", abreviadamente a.u.c. Otra modalidad fue el sistema consular y las llamadas eras provinciales, como la Era de Diocleciano, la Era cesarea de Antioquía o la Era hispánica que empezaba el 38 antes de Cristo.

En la era cristiana, con el papa Bonifacio IV en 607, el origen de la escala pasó a ser el nacimiento de Cristo. Un monje rumano, Dionisio el Exiguo, matemático, basándose en la Biblia y otras fuentes históricas, entre los años 526 y 530, había fechado el nacimiento de Cristo el día 25 de diciembre del año 753 a.u.c. Dicho año pasó a ser el año 1 A. D., "Anno Domini", año 1 del Señor, pero los años anteriores a este seguían siendo años a.u.c. Finalmente en el siglo XVII se nombran los años anteriores al 1 A. D. como años "antes de Cristo", a. C., y los posteriores son años "después de Cristo", d. C..

Cuando empieza la cuenta de la era cristiana, no existía el concepto matemático de cero y los años se contaban ordinalmente (esto es: primer año, segundo, etc.). El origen del calendario gregoriano, es pues el 1 de enero del primer año (año 1 d.C.), que da comienzo a la primera década, el primer siglo (s. I) y el primer milenio. El año anterior fue el primero antes de Cristo (año 1 a.C.). No hay año 0. Establecido así el origen del calendario, el primer milenio (primeros 1000 años) transcurrió entre el 1 de enero del año 1 hasta el 31 de diciembre del año 1000. De la misma forma, el primer siglo transcurrió entre el 1 de enero del año 1 hasta el 31 de diciembre del año 100.

El calendario gregoriano, al saltarse tres días bisiestos cada 400 años, mejora la aproximación hecha por el calendario juliano, dando un año promedio de 365,2425 días solares medios. Esta aproximación tiene un error de aproximadamente un día cada 3300 años con respecto al valor actual del año tropical medio. Sin embargo, debido a la precesión de los equinoccios, que no es constante, y al movimiento del perihelio (que afecta a la velocidad orbital de la Tierra) el error con respecto al equinoccio vernal "astronómico" es variable; si se considera un intervalo promedio aproximado entre equinoccios de primavera con una duración de 365,24237 días en ciclos de 2000 años, esto implica un error cercano a 1 día cada 7700 años. En cualquier caso, el calendario gregoriano es sustancialmente más preciso que el calendario juliano (que con un año promedio de 365,25 días, incurre en 1 día de error cada 128 años).

En el siglo XIX, Sir John Herschel propuso una modificación al calendario gregoriano con 969 días bisiestos cada 4000 años, en lugar de los 970 días bisiestos que el calendario gregoriano insertaría en el mismo período. Esto reduciría el año promedio a 365,24225 días. La propuesta de Herschel haría el año 4000, y sus múltiplos, comunes en lugar de bisiestos. Si bien esta modificación se ha propuesto ya varias veces, nunca se ha adoptado oficialmente.

En escalas de tiempo de miles de años, el calendario gregoriano se atrasa respecto a las estaciones astronómicas debido a la ralentización de la rotación de la Tierra, que hace cada día un poco más largo con el paso del tiempo (ver aceleración de las mareas y segundo intercalar), mientras que el año mantiene una duración más uniforme.

La imagen muestra la diferencia entre el calendario gregoriano y las estaciones astronómicas.

El eje vertical es la fecha en junio y el eje horizontal son los años en el calendario gregoriano.

Cada punto es la fecha y la hora del solsticio de junio de ese año en particular. El error se desplaza alrededor de un cuarto de día por año. Las centurias son años ordinarios, a menos que sean divisibles por 400, en cuyo caso se incluirán en los años bisiestos. Esto provoca una corrección en los años 1700, 1800, 1900, 2100, 2200 y 2300.

Por ejemplo, estas correcciones causan que el 23 de diciembre de 1903 fuese el solsticio más tardío en un mes de diciembre, mientras que el 20 de diciembre de 2096 sea el solsticio de diciembre más temprano, con -2,25 días de diferencia con respecto a la fecha teórica del acontecimiento estacional.

Norma ISO 8601 para la escritura de fechas y horas.


Además, la Real Academia Española recomienda las escritura de fecha en los siguientes términos: se escribirá "30 de diciembre de 2005", o bien "30 de diciembre del año 2005", aunque esta recomendación no implica que se considere incorrecto utilizar el artículo en estos casos: "30 de diciembre del 2005". Evidentemente, en este último caso, el término año se encuentra sobreentendido.




</doc>
<doc id="3626" url="https://es.wikipedia.org/wiki?curid=3626" title="13 de septiembre">
13 de septiembre

El 13 de septiembre es el 256.º (ducentésimo quincuagésimo sexto) día del año —el 257.º (ducentésimo quincuagésimo séptimo) en los años bisiestos— en el calendario gregoriano. Quedan 109 días para finalizar el año.









</doc>
<doc id="3627" url="https://es.wikipedia.org/wiki?curid=3627" title="12 de septiembre">
12 de septiembre

El 12 de septiembre es el 255.º (ducentésimo quincuagésimo quinto) día del año —el 256.º (ducentésimo quincuagésimo sexto) en los años bisiestos— en el calendario gregoriano. Quedan 110 días para finalizar el año.

























</doc>
<doc id="3628" url="https://es.wikipedia.org/wiki?curid=3628" title="11 de septiembre">
11 de septiembre

El 11 de septiembre es el 254.º (ducentésimo quincuagésimo cuarto) día del año —el 255.º (ducentésimo quincuagésimo quinto) en los años bisiestos— en el calendario gregoriano. Quedan 111 días para finalizar el año.








</doc>
<doc id="3630" url="https://es.wikipedia.org/wiki?curid=3630" title="9 de septiembre">
9 de septiembre

El 9 de septiembre es el 252.º (ducentésimo quincuagésimo segundo) día del año —el 253.º (ducentésimo quincuagésimo tercero) en los años bisiestos— en el calendario gregoriano. Quedan 113 días para finalizar el año.








</doc>
<doc id="3631" url="https://es.wikipedia.org/wiki?curid=3631" title="8 de septiembre">
8 de septiembre

El 8 de septiembre es el 251.º (ducentésimo quincuagésimo primer) día del año —el 252.º (ducentésimo quincuagésimo segundo) en los años bisiestos— en el calendario gregoriano. Quedan 114 días para finalizar el año.








</doc>
<doc id="3632" url="https://es.wikipedia.org/wiki?curid=3632" title="7 de septiembre">
7 de septiembre

El 7 de septiembre es el 250.º (ducentésimo quincuagésimo) día del año —el 251.º (ducentésimo quincuagésimo primero) en los años bisiestos— en el calendario gregoriano. Quedan 115 días para finalizar el año.











</doc>
<doc id="3633" url="https://es.wikipedia.org/wiki?curid=3633" title="6 de septiembre">
6 de septiembre

El 6 de septiembre es el 249.º (ducentésimo cuadragésimo noveno) día del año —el 250.º (ducentésimo quincuagésimo) en los años bisiestos— en el calendario gregoriano. Quedan 116 días para finalizar el año.




















</doc>
<doc id="3634" url="https://es.wikipedia.org/wiki?curid=3634" title="5 de septiembre">
5 de septiembre

El 5 de septiembre es el 248.º (ducentésimo cuadragésimo octavo) día del año —el 249.º (ducentésimo cuadragésimo noveno) en los años bisiestos— en el calendario gregoriano. Quedan 117 días para finalizar el año.











</doc>
<doc id="3635" url="https://es.wikipedia.org/wiki?curid=3635" title="4 de septiembre">
4 de septiembre

El 4 de septiembre es el 247.º (ducentésimo cuadragésimo séptimo) día del año —el 248.º (ducentésimo cuadragésimo octavo) en los años bisiestos— en el calendario gregoriano. Quedan 118 días para finalizar el año.














</doc>
<doc id="3636" url="https://es.wikipedia.org/wiki?curid=3636" title="3 de septiembre">
3 de septiembre

El 3 de septiembre es el 246.º (ducentésimo cuadragésimo sexto) día del año en el calendario gregoriano y el 247.º en los años bisiestos. Quedan 119 días para finalizar el año.






(s. IV)



</doc>
<doc id="3637" url="https://es.wikipedia.org/wiki?curid=3637" title="2 de septiembre">
2 de septiembre

El 2 de septiembre es el 245.º (ducentésimo cuadragésimo quinto) día del año en el calendario gregoriano y el 246.º en los años bisiestos. Quedan 120días para finalizar el año.











</doc>
<doc id="3638" url="https://es.wikipedia.org/wiki?curid=3638" title="1 de septiembre">
1 de septiembre

El 1 de septiembre es el 244.º (ducentésimo cuadragésimo cuarto) día del año en el calendario gregoriano y el 245.º en los años bisiestos. Quedan 121 días para finalizar el año.


















</doc>
<doc id="3639" url="https://es.wikipedia.org/wiki?curid=3639" title="31 de agosto">
31 de agosto

El 31 de agosto es el 243.º (ducentésimo cuadragésimo tercer) día del año en el calendario gregoriano y el 244.º en los años bisiestos. Quedan 122 días para finalizar el año.

El mes de agosto tenía 30 días hasta que César Augusto hizo que tuviera 31 días, como julio.









</doc>
<doc id="3640" url="https://es.wikipedia.org/wiki?curid=3640" title="Monoplacophora">
Monoplacophora

Los monoplacóforos (Monoplacophora) son una clase de moluscos primitivos que se consideraba extintos, ya que se conocía el grupo por fósiles del período Cámbrico y Devónico, hasta que en 1952 se encontraron ejemplares vivos del género "Neopilina" los cuales se extrajeron de una profunda fosa submarina en las costas de Costa Rica en América Central. 

Desde el descubrimiento en 1952 de "Neopilina", se han colectado ejemplares de Monoplacophora de diferentes especies, pero siempre en aguas profundas (2.000 a 7.000 m de profundidad). Se han colectado especies de "Neopilina" en el Océano Pacífico Oriental, Océano Atlántico Meridional y Golfo de Adén.

Al grupo de los Monoplacophora se les considera como el grupo basal del cual derivaron los moluscos de las clases Gastropoda, Bivalvia y Cephalopoda.

Los monoplacóforos son moluscos de simetría bilateral con pseudometamerismo y con una concha simple tipo pateliforme la cual cubre al palio, y se extiende totalmente sobre el dorso del animal. En general, los Monoplacophora se asemejan a los Polyplacophora, pero con seis pares de nefridios dispuestos metaméricamente, que descargan desechos a través de nefridioporos asociados con cinco o seis pares de branquias simples (dependiendo de la especie), dispuesta de manera alterna con ocho pares de músculos dorsoventrales, los cuales están adheridos a la concha y pie.

El celoma, que es algo grande, consiste de una cavidad pericardial caudal y en la cual se hallan dos pares de gonoceles ventrales grandes. El sistema circulatorio consiste principalmente de aurículas pares que drenan en ventrículos (también pares), uno cada lado del intestino. El sistema digestivo es similar al observado en el resto de los moluscos.

El sistema reproductor (carente de glándulas accesorias ni órganos copuladores) consiste de dos pares de gónadas grandes (una en cada gonocele) las cuales desembocan respectivamente en el tercer y cuarto nefridios, saliendo los gametos a través de los gonoporos.

Los monoplacóforos son un grupo pequeño, primitivo y sus registros son hallazgos para la ciencia como resultado de expediciones científicas. Esta clase contiene actualmente 20 especies vivivientes, distribuidas en 7 géneros.




</doc>
<doc id="3641" url="https://es.wikipedia.org/wiki?curid=3641" title="Aplacophora">
Aplacophora

Los aplacóforos (Aplacophora, del griego "a", "no", "plakós", "lámina" y "phoros", "que lleva") son una antigua clase del filo de los moluscos. Hoy se considera un grupo parafilético, ya que los dos subgrupos que incluía (Caudofoveata y Solenogastres) solo tienen en común la ausencia de concha, no compartiendo ninguna apomorfía.

Existen alrededor de 250 especies. Tienen espículas calcáreas. Carecen de ojos y tentáculos pero tienen una muesca pedal que se cree tiene el mismo origen que el pie de otros moluscos.

Algunos de estos moluscos viven en anémonas de mar y corales, se encuentran a más de 200 metros de profundidad.


</doc>
<doc id="3642" url="https://es.wikipedia.org/wiki?curid=3642" title="Polyplacophora">
Polyplacophora

Los poliplacóforos (Polyplacophora, del griego "polýs" πολύς, muchas, "plax" πλαξ, placa o valva y "phorós" φορός, portador; más la terminación neutra de plural "-a"), son una clase de moluscos. Existen unas 800 especies, llamadas quitones en los libros por castellanización y generalización del nombre de uno de los géneros más comunes, "Chiton". En la lengua común se les llama a veces cucarachas de mar o cochinillas de mar, y en Perú con el nombre de barquillos.

La anatomía de los poliplacóforos aparece simplificada respecto a otros moluscos, como los gasterópodos. Tienen una cabeza indiscernible carente de tentáculos y de ojos. En ella se abre la boca, dotada de una rádula cubierta por filas de dientecillos, 17 cada una, reforzados con un recubrimiento de magnetita. En la superficie inferior se encuentra el pie musculado por cuyos movimientos se deslizan lentamente.

Su manto secreta una serie de ocho placas imbricadas (como las tejas en un tejado) que le sirven de protección, rodeadas en su parte exterior por un cinturón carnoso que es el borde del manto. Las placas de los poliplacóforos tienen una estructura compleja, con una base totalmente calcárea y una capa superior mixta organomineral, con conquiolina en su composición, según la regla general de los moluscos. El componente mineral es en este caso aragonito. Las placas tienen además engrosamientos y surcos que son funcionales, porque concuerdan con órganos sensoriales táctiles y visuales situados bajo ellas en la epidermis y llamados "estetos". Así, aunque no suelen tener el par de ojos que es general a los moluscos, pueden llegar a tener miles de otros más pequeños en su superficie superior. A veces estos ojos están dotados de una estructura refringente, es decir un cristalino. Entre el pie y el cinturón hay una cavidad paleal en forma de surco, y es donde se encuentran las branquias.

Son dioicos (con sexos separados) y generalmente una sola gónada, ovario o testículo. Las hembras sueltan los huevos para ser fecundados cuando perciben el esperma liberado por los machos. En unos pocos casos la fecundación es interna, desarrollándose entonces las larvas en los conductos ováricos, donde se ha producido la fecundación.

La mayoría de estos moluscos son herbívoros ramoneadores de algas. Habitan sustratos rocosos en la línea de la costa, incluida la zona intermareal, con una ecología semejante a la de las lapas, aunque también se conocen especies de aguas profundas, sobre todo en las latitudes más frías. Son animales poco conspicuos, crípticos por sus colores y su actitud, que es la de permanecer mucho tiempo inmóviles, encajados en un rincón propio de la roca, al que vuelven regularmente y al que terminan incluso ajustando su anatomía. Son gregarios y de hábitos nocturnos, con una taxia fotonegativa marcada, así que buscan el lado inferior de las rocas. Tienen la habilidad de enroscarse, formando una bola protegida por las placas. 



</doc>
<doc id="3643" url="https://es.wikipedia.org/wiki?curid=3643" title="Scaphopoda">
Scaphopoda

Los escafópodos (Scaphopoda, del griego "skaphe", "bote" y "podos", "pie") o conchas colmillo son una clase de moluscos con simetría bilateral y el cuerpo alargado dorsoventralmente, que a su vez, está rodeado por un manto que segrega una concha tubulosa, abierta por ambos extremos, ligeramente curvada y cónica que recibe el nombre vulgar de conchas "colmillo de elefante", “conchas dientes” o “caninos”. Se estima la existencia de 900 especies vivas a nivel mundial.

Actualmente, se reconocen dos órdenes: Dentaliida, con caparazones pequeños a largos, pie cónico, dientes anchos, caparazón acanalado y siempre más ancho en el extremo anterior y; , con caparazones más pequeños, pie vermiforme con terminación en disco, caparazones lisos y a menudo, más anchos detrás de la apertura.

Poseen una cabeza poco desarrollada, en la que hay dos lóbulos (uno a cada lado) de los que se desprenden agrupamientos de tentáculos, denominados captáculos. Los escafópodos no poseen ojos ni branquias (), pues el amplio manto que poseen en la parte ventral, les sirve para la respiración (“El intercambio de gases se produce a través de la superficie del manto”). Al estar abierto por ambos extremos, el caparazón pálido de los escafópodos permite la retracción de la cabeza (y sus captáculos) y pie a través del extremo anterior (la abertura más ancha), que a su vez está ubicado bajo el sustrato. Por su parte, el extremo posterior (la abertura más angosta) permanece por fuera del sustrato. El ensanchamiento del pie en forma de disco, le permite al escafópodo anclarse al sustrato (la misma característica se presenta en almeja primitivas).

Los escafópodos son marinos y bentónicos en su totalidad, miden entre 4 mm y 25 cm de longitud, la mayoría entre 2 y 5 cm. Viven en aguas poco profundas o, hasta profundidades de 4.500 m. Generalmente, se hallan enterrados parcialmente, en la arena o el barro. También, se conoce su preferencia por sedimentos como arena gruesa, escombros finos de coral o barro arcilloso.

Los captáculos que poseen son delicados, ciliados, contráctiles, sensitivos y prensiles. Esta herramienta le permite a los escafópodos capturar y manipular sus presas. Su dieta microfágica les permite alimentarse principalmente de foraminíferos, ostrácodos, otros animales, diatomeas y microplantas. En el interior de la boca poseen una rádula con dientes grandes aplanados, que le permiten asir y raspar el alimento.

La parte terminal de la cabeza, se encuentra formada por la proboscis y, a su vez, ésta contiene en el interior la masa bucal conformada por la rádula. Después de pasar por la degradación de la rádula, el material alimenticio pasa por el esófago hasta el estómago globular, que actúa como una molleja. El estómago y la glándula digestiva se encuentran en la parte media del cuerpo. La digestión sucede de manera extracelular. Después del estómago el material se dirige a través del intestino hacia su parte anterior, y luego hacia la parte posterior del cuerpo (el intestino tiene forma de “u”), por la que finalmente desemboca en el ano a través de la cavidad del manto.

Es simple, ya que carece de corazón y vasos sanguíneos. El sistema nervioso es ganglionado y no concentrado. En cuanto a los nefridios, su ubicación está cerca del ano, en donde el material es expulsado a través de nefridioporos.

Los escafópodos son dióicos, con una única gónada que emite su contenido (huevos o espermatozoides) a través de un nefridioporo. Estos organismos poseen fecundación externa y ponen los huevos por separado. El desarrollo embrionario se da a través de una larva trocófora de vida libre, que se transforma en una larva velígera con simetría bilateral. Dicha larva se asienta en el bentos dando inicio a su fase adulta. La metamorfosis es gradual y se acompaña de un alargamiento del cuerpo.



</doc>
<doc id="3645" url="https://es.wikipedia.org/wiki?curid=3645" title="Bivalvia">
Bivalvia

Los bivalvos (Bivalvia, bi = dos; valvia = valva o placa), lamelibranquios (Lamellibranchia) o pelecípodos (Pelecypoda) son seres vivos clasificados biológicamente en el filo (tipo de organización) Mollusca (de "molusco"). Existen aproximadamente unas 13 000 especies de moluscos bivalvos, la mayoría marinas. Presentan un caparazón con dos valvas laterales, que se cierran por acción de uno o dos músculos aductores. Estas son simétricas, generalmente, unidas por una bisagra y ligamentos. 

Se les encuentra enterrados en fondos blandos (infauna), como habitantes fijos de superficies y estructuras rígidas o libres sobre los fondos (epifauna). Algunas especies perforan el sustrato (roca o madera) y algunas más son comensales o parásitas.

Carecen de cabeza diferenciada y de tentáculos. No tienen maxilas ni rádula. La boca presenta palpos labiales carnosos. Respiran mediante uno o dos pares de branquias, generalmente laminares.
son invertebrados, es decir que no tiene huesos.
En las conchas de los bivalvos se observa gran variedad de tamaños, formas, colores y dibujos esculpidos en la superficie. El tamaño fluctúa desde conchas diminutas (2 mm) hasta especies que pueden alcanzar 15 dm de largo y un peso de 250 kg. Entre los moluscos bivalvos más conocidos podemos nombrar: ostra, almeja, navaja, mejillón, broma de los barcos, coquina, etc.

En el borde anterior del manto se distinguen tres pliegues: interno, medio y externo. El pliegue interno es muscular, el medio destaca por su función sensorial y el externo está relacionado con la secreción de la concha. La concha está dividida en dos valvas unidas dorsalmente en la charnela, que consiste en un ligamento elástico formado por conquiolina y secretado por el manto; no está muy calcificado, por lo que permanece flexible y elástico. El músculo o "pie" característico de los moluscos, en los bivalvos puede presentarse modificado o muy reducido según el hábito de las diferentes especies.

Todos los representantes de esta clase son acuáticos, tanto marinos como dulceacuícolas, y pueden encontrarse desde los límites superiores de la pleamar hasta las zonas abisales. La protección de las conchas permite que algunas especies especializadas soporten las condiciones de la franja costera intermareal.

Típicamente, las especies que viven sobre sustratos blandos como fangos y arenas, presentan un pie que les permite excavar y tiene forma de hacha. Las especies sésiles se mantienen adheridas al sustrato, bien sea por cimentación, como las ostras, o mediante la secreción de una serie de filamentos que conforman el biso (mejillones).

Aquellos bivalvos que viven bajo la arena (suelo blando), se alimentan filtrando pequeñas cantidades de agua durante las mareas altas, de donde extraen el alimento. No suelen viajar lejos, ya que sus órganos están adaptados para filtrar el agua a través de las valvas de su concha en vez de que el bivalvo mismo se traslade para introducir agua en su interior. En las ocasiones en que se mueve, utiliza su pie, un músculo que le permite enterrarse en la arena.

Los Bivalvia son moluscos esencialmente acuáticos, en su mayoría marinos, que habitan con preferencia la región bentónica sublitoral, aunque también se los encuentra hasta las profundidades abisales.La salinidad y la temperatura se cuentan entre los factores que más influyen en su distribución. Tienen gran valor en los análisis paleoambientales debido a su estrecha relación con el tipo de sustrato, la buena presencia en el registro fósil desde el Cámbrico, su gran diversidad y abundancia en variados paleoambientes, y excelente representación en los medios acuáticos actuales. Estos moluscos viven en un amplio rango de temperaturas del agua; en general el tamaño y espesor de la conchilla decrecen al disminuir la temperatura. Son más comunes en aguas bien oxigenadas, aunque algunas especies oportunistas o con adaptaciones especiales pueden vivir en ambientes pobres en oxígeno hasta completamente anóxicos. Es amplio también el rango de salinidad que soportan, desde aguas dulces continentales hasta mares hipersalinos, pero son mucho más diversos en condiciones de salinidad marina normal.

La salinidad y la temperatura tienen poca influencia sobre los hábitos de vida, en cambio la depredación condiciona notablemente algunos aspectos generales de la morfología funcional de los bivalvos, como la secreción del biso o la excavación, aunque se conoce menos su influencia en el desarrollo de variantes morfológicas específicas para protegerse de depredadores (cementación, espinas sifonales). Los factores más importantes que afectan el modo de vida de los bivalvos son la disponibilidad de alimento, el tipo de sustrato y el movimiento del agua. La morfología funcional de las valvasestá muy relacionada con el carácter del sustrato. En cuanto a la turbulencia del agua, puede decirse que en general prefieren ambientes con movimiento del agua moderado; en condiciones turbulentas predominan los cementantes, nidificadores, perforantes y excavadores rápidos.

El estudio de la diversidad morfológica de los bivalvos en relación a distintas estrategias de modo de vida ha demostrado que existen morfologías adaptativas recurrentes, tipificadas cada una por conjuntos de caracteres específicos. Muchos bivalvos (especialmente los infaunales) viven con su comisura orientada en posición perpendicualr a la interfase entre el sustrato y el agua, pero existen algunos (comunes entre los epifaunales, aunque también los hay infaunales) que lo hacen con una de las valvas contra el sustrato o de manera que la comisura queda aproximadamente paralela u oblicua a la interfase. Estos últimos bivalvos se denominan pleurotéticos, y se reconocen por ser inequivalvos en algún grado (valvas derecha e izquierda con convexidad, espesor, ornamentación o aun coloración diferentes), pueden poseer valvas torsionadas, cuya comisura no se halla en un plano y, en el caso de los pleurotéticos epifaunales, son casi siempre monomiarios. Los pleurotéticos incluyen a todos los bivalvos cementantes, la mayoría de los apoyados, muchos bisados y algunos excavadores.

Sin embargo, estos hábitos no son ejercidos con exclusividad, presentándose frecuentes combinaciones de los mismos. Algunas especies excavadoras (especialmente árcidos) emplean un débil biso para aumentar la estabilidad en los estratos blandos. Otras especies que viven fijas por el biso, pertenecientes a los géneros Isognomun y Barbatia, habitan introducidas a la manera de una cuña entre colonias de corales o rocas, igual que algunas especies nidificadoras libres. Muchas formas nadadoras, como los pectínidos, pasan buena parte de su existencia apoyadas en el fondo o fijas por el biso. Petricola pholadiformis normalmente vive como un excavador en sedimentos blandos o perforador en sustratos duros. En el canal de Beagle, Hiatella solida se fija por el biso a los sustratos rocosos y al cachiyuyo, en latitudes menores es perforadora en arenas duras o incrustante de bivalvos y gastrópodos, y en Brasil vive fija sobre rocas, estrellas de mar y briozoos o entre tubos de poliquetos.

Estas variaciones en hábito pueden producirse a lo largo de la ontogenia, como se deduce a veces de cambios morfológicos registrados en la conchilla.

La diversidad de formas que presentan las conchillas se correlaciona con la diversidad de hábitats ocupados y la variedad de modos de vida adoptados, y así grupos de bivalvos poco relacionados filogenéticamente pueden desarrollar conchillas con caracteres similares.



</doc>
<doc id="3646" url="https://es.wikipedia.org/wiki?curid=3646" title="Gastropoda">
Gastropoda

Los gasterópodos, gastrópodos o univalvos (Gastropoda, del griego γαστήρ "gastér", "estómago" y πούς "pus", "pie") constituyen la clase más extensa del filo de los Moluscos. Presentan área cefálica (cabeza), un pie musculoso ventral y una concha dorsal (que puede reducirse o hasta perderse en los gasterópodos más evolucionados); además, cuando son larvas, sufren el fenómeno de torsión, que es el giro de la masa visceral sobre el pie y la cabeza. Esto les permite esconder antes la cabeza en la concha, dándoles una clara ventaja evolutiva. Los gasterópodos incluyen especies tan populares como caracoles y babosas marinas y terrestres, las lapas, las orejas y liebres de mar, etc. 

Existen aproximadamente más de 75000 especies vivas y 15.000 f por los focilesfósiles descritas. Se pueden encontrar en casi todo tipo de ambientes (inclusive desiertos), pero mayoritariamente en aguas saladas o dulces, aunque unos pocos han logrado colonizar el medio terrestre, siendo el único grupo de moluscos con representantes en tierra firme.

Los gasterópodos se caracterizan por la torsión, un proceso en que la masa visceral gira sobre el pie y la cabeza durante el desarrollo.

Típicamente tienen una cabeza bien definida, con dos o cuatro tentáculos sensoriales, y un pie ventral, de donde deriva su nombre. Los ojos, que pueden estar situados en el extremo de tentáculos retráctiles, varían de simples ocelos que solo detectan claridad y oscuridad, sin formar imagen definida, a complejos ojos con lentes. La larva de los gasterópodos se denomina protoconcha.

Muchos gasterópodos poseen concha de una pieza y enrollada en espiral, que usualmente se abre hacia la derecha
(cuando se observa la concha con el ápice hacia arriba). Muchas especies poseen un opérculo que actúa como tapadera para cerrar la concha; en general es de material córneo, pero en algunas especies es calcáreo. En algunos grupos, como las babosas y los opistobranquios, la concha está reducida o completamente atrofiada y el cuerpo es alargado, con lo que la torsión es poco evidente.

A pesar de que los gasterópodos más conocidos son los terrestres, más de dos tercios de las especies viven en el mar. Los gasterópodos marinos incluyen herbívoros, detritívoros, carnívoros e incluso especies que atraen el alimento gracias al movimiento de cilios y, en tal caso, la rádula está reducida o ausente. La rádula está adaptada al régimen alimenticio de cada especie. Los gasterópodos más simples, como las lapas y las orejas de mar, son herbívoros que utilizan sus duras rádulas para raspar las algas de las rocas. Muchos gasterópodos marinos son excavadores y poseen sifones o tubos que extienden más allá del manto e incluso de la concha, con el fin de conseguir oxígeno y alimento; los sifones se usan también para detectar presas a distancia.

Los gasterópodos marinos respiran por branquias, pero algunos dulceacuícolas y todos los terrestres han desarrollado pulmones, y forman el grupo de los Pulmonados (Pulmonata), cuya monofilia se encuentra en discusión. Sin embargo el clado Panpulmonata sí es monofilético.

Las nudibranquios poseen extravagantes colores, tanto aposemáticos (que anuncian que son organismos venenosos o peligrosos) como crípticos (que sirven para camuflarse en el entorno).

Los primeros gasterópodos fueron exclusivamente marinos, y aparecieron a finales del Cámbrico ("Chippewaella", "Strepsodiscus"); formas del Cámbrico inferior, como "Helcionella" y "Scenella" no son considerados ya como gasterópodos, y la diminuta y espiralada "Aldanella", también del Cámbrico inferior, probablemente no sea ni un molusco. 
Durante el Ordovícico los gasterópodos fueron un grupo diverso, presente en hábitats acuáticos.

En general, los gasterópodos de las rocas del Paleozoico inferior se encuentran en un estado de conservación demasiado pobre como para realizar una correcta identificación. A pesar de ello, el género "Poleumita", del Silúrico, contiene 15 especies descritas. Los fósiles de gasterópodos son menos comunes en el Paleozoico que los bivalvos.

La mayoría de los gasterópodos paleozoicos pertenecen a grupos primitivos, unos pocos de los cuales sobreviven hoy día. Durante el Carbonífero se observan muchos de los modelos presentes de los gasterópodos actuales, pero a pesar de estas similitudes, la mayoría de estas antiguas formas no están directamente relacionadas con las actuales.

Fue durante el Mesozoico cuando evolucionaron los ancestros de la mayoría de los gasterópodos actuales.

Uno de los primeros gasterópodos terrestres conocidos es "Maturipupa", que se halló en Europa en el Coal Measures del Carbonífero, pero los parientes de los caracoles actuales son raros antes del Cretácico, cuando apareció el conocido género "Helix".

En rocas del Mesozoico, los gasterópodos son algo más comunes y mejor conservados. Sus fósiles aparecen en sedimentos depositados en entornos tanto marinos como dulceacuícolas. El mármol de Purbeck, del Jurásico, y el mármol de Sussex del Cretácico inferior, ambos del sur de Inglaterra, son calizas que contienen abundantísimos restos del caracol lacustre "Viviparus".

Las rocas del Cenozoico proporcionan un gran número de gasterópodos fósiles, la mayoría de los cuales estrechamente relacionados con las formas actuales. La diversidad aumentó marcadamente al principio de esta era, junto con la de los bivalvos.

Ciertos rastros preservados en rocas sedimentarias antiguas se pensó que fueron causadas por gasterópodos arrastrándose sobre el barro y la arena. Aunque tales rastros son de origen debatido, algunos se parecen a los rastros que dejan los modernos gasterópodos.

Los gasterópodos fósiles pueden ser confundidos a veces con los amonites u otros cefalópodos con concha. Un ejemplo es "Bellerophon", de las calizas del Carbonífero de Europa

Los gasterópodos son uno de los grupos que mejor documentan los cambios en la fauna causados por el avance y el retroceso de los hielos durante el Pleistoceno.

La taxonomía de los Gastropoda está bajo constante revisión, y poco a poco se va abandonando la antigua clasificación. No obstante, términos como opistobranquios y prosobranquios se usan todavía en sentido descriptivo. Realmente puede hablarse de una "jungla taxonómica" cuando descendemos a niveles taxonómicos inferiores, ya que la clasificación de los gasterópodos varía según los autores. Además, con el estudio creciente de secuencias de ADN pueden esperarse nuevas revisiones en la clasificación de los niveles taxonómicos superiores.

Según la taxonomía tradicional, los gasterópodos se subdividen en tres subclases:

Según nuevos estudios (Ponder & Lindberg, 1997), la taxonomía de los gasterópodos debería ser rehecha para basarse solo en grupos monofiléticos. Este es el reto en los estudios taxonómicos para los próximos años. Hoy por hoy es imposible presentar una clasificación de los gasterópodos en grupos consistentes. La evolución convergente, observada con mucha frecuencia, puede explicar las diferencias entre las filogenias obtenidas a partir de datos morfológicos de aquellas obtenidas de datos moleculares (secuenciación de genes).





</doc>
<doc id="3647" url="https://es.wikipedia.org/wiki?curid=3647" title="Schmidtia">
Schmidtia

Schmidtia, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de África tropical y del sur, Cabo Verde y Pakistán.
El número cromosómico básico es x = 9, con números cromosómicos somáticos de 2n = 36. 



</doc>
<doc id="3648" url="https://es.wikipedia.org/wiki?curid=3648" title="Epigrama">
Epigrama

El epigrama (del griego antiguo «"ἐπί-γραφὼ"»: literalmente, «sobre-escribir» o «escribir encima») es una composición poética breve que expresa un solo pensamiento principal festivo o satírico de forma ingeniosa.

Se pueden rastrear las raíces del género hasta muy atrás, en la lírica arcaica (no por nada incluye Meleagro en su "Corona" o "Guirnalda" a Arquíloco y a Simónides), aunque es más propio hablar del epigrama como un género netamente helenístico, pues caracteriza, tanto por su contenido como por su extensión y por su forma, a esa época. Los epigramas helenísticos constituyen un verdadero crisol de aquellas sociedades, vemos desfilar a heteras, navegantes, carpinteros, tejedoras con su vida simple y ardua, también al amor heterosexual y lésbico, las fiestas, la religiosidad, el cortejo, la sexualidad, la inocencia, las artes plásticas, la crítica literaria, hasta las mascotas; todo esto, pasado por el filtro de erudición y cultura (pues portadores de tales eran los poetas). En resumen, producen testimonios diversos, bellos y muy vívidos de las principales preocupaciones del hombre helenístico.

El epigrama se creó en la Grecia clásica y, como su nombre indica en griego, era una inscripción que se ponía sobre un objeto, que podía ser un exvoto, un regalo (xenion), una estatua o una tumba; los epigramas sobre las tumbas formaron clase aparte y se denominaron epitafios o epicedios, por lo que el vocablo pasó a designar el poema ingenioso que poseía la calidad de ser breve para poder pasar por rótulo o inscripción. La mayoría de los epigramas griegos puede encontrarse en la llamada "Antología Palatina". Tras los griegos, destacaron en la composición de epigramas los romanos, singularmente Catulo y Marco Valerio Marcial.

En sus "Poetices libri septem" (Lyon, 1561), el humanista Giulio Cesare Scaligero define el epigrama como:

Y establece para él dos características definitivas: «"brevĭtas et argutĭa"» (brevedad y argucia). En la literatura barroca española el epigrama fue muy utilizado al ser una forma apropiada para la exhibición cortesana del ingenio. El escritor conceptista barroco Baltasar Gracián, en su obra "Agudeza y arte de ingenio" (1648), realiza un estudio y antología de epigramas escritos en castellano y latín. También el jesuita Joseph Morell en "Poesías selectas de varios autores latinos" (Tarragona, 1684) hizo una excelente antología. Durante el siglo XVIII el género no decayó, como hubiera podido esperarse, sino que tomó una intención menos cortesana y más educativa y moral. Autores de la Ilustración como León de Arroyal compusieron libros de epigramas y lo definieron como:

Por otra parte, y siempre según el prólogo de Arroyal a sus "Epigramas" (1784), la belleza del epigrama consistiría en dos virtudes fundamentales:

Y, dentro del mismo siglo XVIII, Juan de Iriarte lo definió usando la misma forma del epigrama:

Posteriormente, algunas formas literarias, como el artículo breve de prensa, las greguerías de Ramón Gómez de la Serna o los "membretes" de Oliverio Girondo se aproximan al género epigramático, así como las inscripciones anónimas populares en muros o retretes denominadas grafitos o pintadas, que desde las ruinas de Pompeya hasta la actualidad resultan una fuente inestimable sobre la opinión popular de una época. Algunas de estas inscripciones son recogidas por Pío Baroja en su obra "Vitrina pintoresca" o Camilo José Cela en su "San Camilo 1936". A veces, en la lírica del siglo XX puede adoptar un tono elegiaco (Jaime Gil de Biedma) o forma de versos de amor, como es el caso de "Epigramas", del sacerdote nicaragüense Ernesto Cardenal. Federico Carlos Sainz de Robles compuso además una historia del epigrama español en "El epigrama español del siglo I al siglo XX" (Madrid, 1946).

En otras naciones el epigrama fue cultivado con extraordinario acierto. En el Reino Unido, sobresalen John Donne, Jonathan Swift, Alexander Pope (creador en el siglo XVIII de una forma de pareado epigramático) y Óscar Wilde. En Francia destacan especialmente Voltaire y Nicolás Boileau. En Alemania, G. E. Lessing. El epigrama también se encuentra en literaturas no occidentales, como la china y japonesa; en esta última puede decirse que el epigrama se encuentra emparentado con la forma poética conocida como haikú. Puede aplicarse el término a cualquier aforismo o dicho más o menos sentencioso, e incluso hasta a cierto tipo de narración hiperbreve.

En 2018, el productor de cine Pedro Alonso Pablos realizó una mini-serie de dibujos animados titulada "Epigramas del Doctor Pelayo", ofreciendo un repaso por la historia del epigrama como género literario y mencionando muchos de ellos. Esta obra utilizó imágenes del archivo de la Biblioteca Nacional de España.












</doc>
<doc id="3649" url="https://es.wikipedia.org/wiki?curid=3649" title="Epístola">
Epístola

Epístola (del griego: "ἐπιστολή", "epistolē") es un sinónimo de carta: un texto cuya función principal es la comunicación entre el remitente o emisor (el escritor que la redacta y envía) y el destinatario o receptor. El uso del término suele implicar un registro culto o un contexto literario (el género epistolar).

En la actualidad es un término arcaico, por lo general restringido en su uso a las cartas didácticas sobre ética o religión; y particularmente para referirse a las obras del Nuevo Testamento que reciben el nombre de "epístolas", y donde se recogen las escritas por algunos apóstoles destinadas a las comunidades cristianas primitivas. Las tradicionalmente atribuidas a Pablo de Tarso se conocen como "epístolas paulinas" y el resto con el nombre genérico de "epístolas católicas" (es decir, "universales" o "generales").

El género epistolar fue común en el Antiguo Egipto como parte del trabajo de los escribas, y están recogidas bajo el nombre de "Sebayt" ("instrucciones"), estando datadas las más antiguas en el siglo XXV a. C. 
Los griegos cultivaron el género y dejaron un amplio "corpus" epistolar, que presenta notables problemas críticos. Por ejemplo, muchas de las "Cartas" del filósofo Platón son de atribución insegura; lo mismo ocurre con bastantes de los autores reunidos en los "Epistolographi Graeci" (París, 1874) de Rudolf Hercher. Son interesantes las de Filóstrato, Epicuro, Juliano; es una falsificación bastante antigua la correspondencia mantenida entre San Pablo y Séneca; otro fraude largo tiempo mantenido es el de las "Cartas" del tirano Falaris, que fue completamente desmontado por el filólogo inglés del siglo XVIII Richard Bentley. Los Santos Padres de habla griega nos dejaron también un epistolario muy abundante, por ejemplo San Basilio, San Juan Crisóstomo, San Gregorio Nacianceno, San Gregorio de Nisa. También griego es el tratado de Demetrio "Sobre el estilo", fechable entre el siglo I a. C. y el I d. C., donde se define el género en los párrafos 223-235. No hay manuales de teoría epistolar más antiguos. Allí formula ya los elementos del género:
próxima al diálogo, pero más elaborada.


Ningún romano, pese al cultivo asiduo del género en el Imperio, se detiene sino ocasionalmente en precisar más el género hasta el siglo IV, en que le dedica una breve preceptiva la "Rhetorica" de Julio Victor, que distingue cartas "negotiales" y "familiares", y la obra anónima "Excerpta Rhetorica", de entre los siglos III y IV, que distingue entre cartas públicas y privadas, religiosas o no, personales o ajenas, grandes o moderadas. La caída del Imperio Romano de Occidente y la llegada de la Edad Media fragmenta la retórica para adaptarla a nuevas necesidades desarrollando preceptivas específicas para la carta ("ars dictaminis"), la composición poética ("ars poetria") y la predicación ("ars praedicandi"). El "ars dictaminis" aparece en Montecasino y se extiende rápidamente por Europa a través de las escuelas de Bolonia y de Orleáns. Su preceptiva tiene como rasgo característico la adecuación absoluta al destinatario, con largas listas de fórmulas específicas para cada estamento social y la configuración del formato aprobado de las partes de la carta, modelado sobre la preceptiva del discurso de la "Rhetorica ad Herennium" y el "De inventione" de Cicerón, y la utilización de las figuras del libro IV del primero. Más tarde, la aparición de la burguesía en las ciudades obligó a adaptar de nuevo el género, como se aprecia en la obra de Giovanni de Bonandrea, precursor de los cambios que realizarán los humanistas. Ya en el siglo XV, se muestran los diferentes intentos de recuperar la preceptiva epistolar clásica y, al mismo tiempo, de dar respuesta a las necesidades de la nueva sociedad: Niccolò Perotti, que dedica su "De conscribendis epistolis" a la carta familiar, de acuerdo con la concepción ciceroniana; Giovanni Sulpizio da Veroli, que inserta su teoría en una preceptiva retórica del discurso; el intento fallido de Juan Luis Vives; y, por último, la configuración definitiva de la teoría epistolar humanística realizada por Erasmo de Róterdam que abarca tanto la carta familiar como la oficial y ofrece fórmulas y consejos para la redacción de la epístola.

En suma, la conocida definición de Cicerón "conloquia amicorum absentium" , «conversación de amigos ausentes», obliga al escritor de epístolas a llenar un vacío de la amistad: pide respuesta, se queja por la falta de las noticias del otro, y se pide concreción, contenido, aunque Cicerón admite que la carta puede no tener ninguno, que se escriba lo primero que se ocurra aunque no se tenga nada que contar, cuando se trata de cartas familiares o entre amigos. La carta debe ser capaz de reflejar, para el destinatario, la personalidad del ausente, de modo que, a través de ella, se produce el conocimiento del «ethos» del autor. Además en la carta se vierte la absoluta franqueza, como afirma Cicerón: "epistula non erubescit", «la carta no siente vergüenza» Por ello son privadas: no deben mostrarse, copiarse ni divulgarse sin permiso. La lengua en que se escriben debe adecuarse a los temas, pero en los generales debe ser el "sermo cotidianus": lo recomiendan Cicerón, Séneca y Plinio:

Por eso son frecuentes las irrupciones de frases o palabras en griego, las frases hechas, las máximas de carácter general, las expresiones coloquiales, los diminutivos, las expresiones parentéticas o aclaratorias, las elisiones y los anacolutos. Se usa también el diálogo fingido o dialogismo. La brevedad, la claridad, el uso del imperfecto epistolar...

Nos han llegado de la antigüedad romana las "Epístolas familiares" en prosa de Cicerón y los dos libros de "Epístolas" en verso del romano Horacio, del siglo I  a. C. Una de ellas, la "Epistula ad Pisones", recibe el nombre de "Arte poética", y ha sido durante siglos considerada como la normativa de principios literarios; el poeta latino Ovidio compuso asimismo numerosas cartas en verso: las "Heroidas", cartas ficticias de famosas heroínas, y las reales que forman las colecciones o epistolarios en verso en que protesta por su exilio, llamadas "Tristia" y "Ponticas". De época neroniana son las "Cartas a Lucilio" del hispanolatino Séneca, de carácter filosófico y moral; de entre los siglos I y II d. de Cristo es el "Epistolario" en prosa de Plinio el Joven: nueve libros con un total de 248 epístolas enviadas a 105 destinatarios, algunos de ellos célebres, como el historiador Tácito o el emperador Trajano; muchas son famosas por el tema que tratan, como la erupción del Vesubio en que pereció su tío Plinio el Viejo o aquella en que pregunta al emperador Trajano sobre el trato que hay que dar a los cristianos. Del siglo IV son dos libros de epístolas en verso de Ausonio.

Las Epístolas bíblicas son la parte del Nuevo Testamento que consiste en cartas enviadas a las primeras comunidades cristianas por los apóstoles Santiago, Judas, Pedro y Juan, y también por San Pablo (las Epístolas paulinas).

En la liturgia de la misa, "la Epístola" o lectura de la Epístola es una de sus partes, en la que se procede a la lectura de un fragmento de alguna de las Epístolas bíblicas. El libro litúrgico que recoge estas lecturas se denomina "epistolario". En la disposición física de las partes de la iglesia, "la Epístola" o el "lado de la Epístola" es el lado derecho (desde el punto de vista de los fieles), por oposición al "lado del Evangelio".

En el humanismo renacentista, la epístola se transformó en un género literario ensayístico, dignificado por un estilo exigente y formal, muy a menudo provisto de intención didáctica o moral. Petrarca, aislado en los siglos oscuros, escribió cartas a escritores paganos y cristianos de la Antigüedad para sentirse menos solo (a Cicerón y a San Agustín); ya en el siglo XVI, Erasmo compuso cientos de epístolas, y los humanistas españoles (Hernando del Pulgar, con sus "Letras", o fray Antonio de Guevara, con sus amenas "Epístolas familiares") contribuyeron también al género. Cuando se escogía la forma poética, se hacía casi siempre en tercetos encadenados (con destinatario real, como la "Epístola a Mendoza" de Juan Boscán y la "Epístola a Arias Montano" del capitán Francisco de Aldana, o con destinatario ficticio y simbólico, como la "Epístola moral a Fabio" de Andrés Fernández de Andrada, por citar solo dos ejemplos clásicos). Más raramente se usaba el verso blanco ("Epístola a Boscán" de Garcilaso de la Vega). Así, durante el siglo XVI se prodigaron las epístolas en prosa y en verso por el afán comunicativo y abierto que tenían ambos géneros y su afinidad con los ideales antropocéntricos de la sociabilidad y la estética renacentista. La subjetividad del antropocentrismo no siempre tenía por qué tener un destinatario, pues podía ser ficticio como pretexto para el desahogo personal. Otras veces, en prosa, la epístola revestía un mero carácter informativo, como las "Cartas de relación" de Hernán Cortés, que narraban los progresos de la conquista de México y como tales constituyen hoy en día un documento histórico. De ámbito privado y no destinadas a publicarse son las "Cartas" de Santa Teresa de Jesús, a las que el estilo oral y desenvuelto de su autora dan una gracia especial.

Ya en el siglo XVII aumentan las de carácter satírico, como las "Epístolas del caballero de la Tenaza" de Francisco de Quevedo, sobre la tacañería de un señor respecto a su amante, o la "Epístola satírica y censoria al Conde-duque de Olivares", también de Quevedo, donde se pide una reforma política y moral de la sociedad, ya no en prosa, sino en tercetos. Apócrifas o falsas son las contenidas en el llamado "Centón epistolario" del bachiller Fernán Gómez de Cibdad Real, atribuidas a un médico del siglo XV, pero compuestas en el siglo XVII. Forma ensayística y erudita tomaron también en ese siglo las "Cartas filológicas" de Francisco Cascales o las de sor Juana Inés de la Cruz.

En el siglo XVIII, sobre el modelo preensayístico de Cascales, escribió el "novator" fray Benito Jerónimo Feijoo sus "Cartas eruditas y curiosas" (1742-1760, 5 vols.) pequeños ensayos menos extensos que los que él había publicado anteriormente como "discursos" (de "discurrir") en su "Teatro crítico universal", con la intención de desterrar el oscurantismo y los que él llamaba "errores comunes", recordando los "Errores celebrados" de Juan de Zabaleta. Escribió entonces también un notable epistolario el jesuita expulso Juan Andrés.

Se establecieron ya varios géneros concretos a partir de una edición bilingüe dieciochesca de las "Epístolas familiares" de Cicerón. Se dividen con un criterio temático-retórico en "narratorias" o "narrativas", "cuyo fin es dar noticia a un ausente"; "comendaticias", comendatorias o "cartas de favor", para encomendar cosas ajenas; "petitorias" para encomendar cosas propias; "expostulatorias" las que exponen quejas; "gratulatoria" o de acción de gracias las que reflejan "alegría por prósperos sucesos"; "exhortatorias", "consolatorias" las que confortan por alguna desgracia; "jocosas" las que tratan de burlas y donaires; "excusatorias" y de diversos asuntos. También las había "nuncupatorias", esto es, dedicatorias de alguna obra, de forma que servían en cierta manera de prólogo a ellas; esa forma tenía, por ejemplo, en el siglo XV, la "Carta proemio al condestable don Pedro de Portugal" del marqués de Santillana, que es en realidad una historia de la poesía de su tiempo y antecedía a un manuscrito de sus obras enviado a dicho personaje. María Romero Masegosa y Cancelada tradujo al parecer la novela epistolar de Françoise de Graffigny "Lettres d une peruvienne" (1747) y la publicó en Valladolid en 1792 con notas, supresiones, y alguna carta añadida más. Y es que en el siglo XVIII la novela epistolar fue un género muy cultivado desde que la puso de moda Samuel Richardson con sus "Pamela" (1740) y "Clarissa" (1748); bastará recordar que Montesquieu lo había utilizado como recurso literario para la crítica socio-política en sus "Cartas persas" (1721), que José de Cadalso imitó en sus "Cartas marruecas" (1789) divulgando los ideales de la Ilustración, y que Choderlos de Laclos usó también el género para condenar el libertinismo en su "Las amistades peligrosas" (1782); Voltaire fue un infatigable escritor de misivas, que terminaba siempre con la misma frase: "Ecrasez l'infame!". Entre las de otros ilustrados españoles destacan las humorísticas (y un poco escabrosas y escatológicas) "Cartas de Juan del Encina" (1804) de José Francisco de Isla y el "Epistolario" de Leandro Fernández de Moratín; pero otras veces las cartas presentaban grandes proyectos de reforma política y económica, como las "Cartas económico-políticas" (1786-1790) de León de Arroyal o las "Cartas sobre los obstáculos que la Naturaleza, la opinión y las leyes imponen a la felicidad pública" (1792) y la "Carta al Príncipe de la Paz" (1795) de Francisco Cabarrús.

El siglo XIX español se abre con las "Seis cartas a Irénico en que se dan claras y distintas ideas de los derechos del hombre" (Barcelona, 1817) que Félix Amat escribió con pseudónimo; contenido social y político tienen las "Cartas de España" de José María Blanco White, publicadas en su exilio inglés, así como las "Cartas de los lamentos políticos de un pobrecito holgazán" (1820) de Sebastián de Miñano y, en sentido reaccionario, las "Cartas críticas" (1824-1825, 5 vols.) del Filósofo Rancio. Hay que mencionar también las "Cartas a Elpidio sobre la impiedad, la superstición y el fanatismo en sus relaciones con la sociedad" (1836) del cubano Félix Varela. y destacan como un modelo de prosa las "Cartas desde mi celda" (1864) y las "Cartas literarias a una mujer" (1860-1861) de Gustavo Adolfo Bécquer. Ya en el realismo, además de Galdós y, sobre todo, Juan Valera, hay que reseñar la colección de "Modelos para cartas" (1899) de Rafael Díez de la Cortina. Ya en el siglo XX los ejemplos son numerosos; por su carácter oportunista y la polémica que despertó cabe recordar la "Carta al General Franco" de Fernando Arrabal escrita en 1971 a salvo en su exilio francés.

Una estructura habitual de las epístolas incluye las siguientes partes:


También puede utilizarse la epístola como mecanismo narrativo o recurso literario que permite escribir novelas en forma de cartas o epístolas, ejemplo de las novelas epistolares son el "Proceso de cartas de amores" de Juan de Segura, "Pamela, o La virtud recompensada" de Samuel Richardson, "Las amistades peligrosas" de Pierre Choderlos de Laclos o la primera parte de "Pepita Jiménez" de Juan Valera, así como la novela Pobres gentes y el relato Novela en nueve cartas, ambas del escritor ruso Fiódor Dostoyevski.

La correspondencia literaria es el intercambio epistolar entre escritores, especialmente notable en algunos casos (Max Aub).

La correspondencia científica, que se remonta a la ciencia griega (cartas de Arquímedes), fue trascendental para la fijación del concepto de publicación científica a partir del siglo XVII.

Las cartas o epístolas que constituyen una correspondencia pueden reunirse en colecciones llamadas epistolarios; estos pueden ser de distintos tipos, según agrupen las cartas por autores, corresponsales, temas o fechas; los epistolarios más completos recogen también las epístolas que escriben los corresponsales, que a menudo son excluidos (bien porque no tienen tanta importancia, fama o calidad literaria como el autor a quien están consagradas estas colecciones, o bien porque no se ha conservado -es muy difícil que se haya conservado este tipo de literatura efímera-).





</doc>
<doc id="3650" url="https://es.wikipedia.org/wiki?curid=3650" title="Himno">
Himno

Un himno, es un canto o un texto lírico que expresa sentimientos positivos, de alegría y celebración.

En la antigüedad era una composición coral en honor a una divinidad y es retomado con pleno valor litúrgico en la literatura latina cristiana de la Edad Media (por ejemplo, el "Pange lingua" escrito por Tomás de Aquino para conmemorar el día del Corpus). El vocablo deriva del idioma griego "" ("hymnos") y pasó a casi todas las lenguas de Europa en un mismo sentido o significación.

Es, además, la representación musical o literaria de un acontecimiento tan elevado que produce la necesidad de plasmarlo en música o texto.

Un himno puede estar dedicado a dioses, un santo, un héroe o a una persona célebre. También puede estar dedicado a celebrar una victoria u otro suceso memorable o a expresar júbilo o entusiasmo, en cuyo caso conviene mejor llamarlo oda. Asimismo puede ser una composición musical que identifica a una colectividad, una región, un pueblo o una nación y que une a quienes la interpretan. Estos últimos himnos suelen ser, o bien marchas, o bien poemas líricos.

Entre los himnos más antiguos se encuentran los himnos reales sumerios y griegos (finales del III milenio a. C.), el Gran Himno a Atón, compuesto por el faraón Ajenatón en el Antiguo Egipto, los Vedas, una colección de himnos en la tradición del hinduismo, los salmos, una colección de canciones del judaísmo, los Gathas, himnos en honor a Ahura Mazda y el himno de Ugarit.

La más antigua obra anotada completa de música antigua es una canción hurrita, un himno escrito en cuneiforme ugarítico silábico dedicado a la diosa Nikkal.

La tradición occidental de los himnos se inicia con los himnos homéricos, una colección de himnos antiguos griegos, que fueron escritos en el siglo VII a. C., alabando a las deidades de la religión de la Antigua Grecia. Se ha conservado una colección de seis himnos literarios ("Ὕμνοι") del poeta alejandrino Calímaco del siglo III a. C..

Los griegos engalanaron el himno con los ritmos de la poesía y con los melodiosos encantos de la música. Tenían muchos tipos de himnos: el "invocativo", el "laudativo", el "admirativo", el "votivo", el "teogónico" y el "filosófico". Los himnos de Orfeo pertenecen al género "invocativo". Los compusieron también de diferentes géneros Homero, Cleanto, Calímaco, Teócrito, Anacreonte, Tirteo, Safo, Simónides, Píndaro y otros. Los coros de la tragedia griega no eran otra cosa que himnos o invocaciones.

El himno profano llegó al más alto grado de perfección con "Carmen saeculare" de Horacio, compuesto por orden de Augusto para la celebración de los "ludi saeculares" del año 17 a. C., en el que un coro de mancebos y de doncellas cantaban alternativamente este himno de alabanza a los dioses Apolo y Diana.

Los himnos más antiguos que se conocen son los de Moisés y de Débora, la profetisa que cantó uno en acción de gracias al Dios hebreo, 2710 años antes de Cristo. Esdras ha recogido en la Biblia el mayor número de cánticos hebreos con este epígrafe: "Sepertheillim", es decir, libro de las alabanzas.

Se cantaban los himnos al son de las arpas y de las liras (solo las cuerdas acompañaban las voces) por coros alternativos; el primero cantaba el himno, y el otro, en determinados intervalos, repetía un dístico de intercalo o un refrán, imitando de este modo a los serafines, a quienes los profetas habían oído cantar alternativamente: "Santo, Santo, Santo, Señor Dios de los ejércitos". Cuatro mil levitas, cuyo jefe era Asaph, celebraban por turno estos cánticos en el templo de Yahvé bajo los reinados de David y de Salomón, dos celebérrimos himnógrafos de Israel, porque si.

Desde los primeros siglos de la Era cristiana se introdujo en las ceremonias religiosas el uso de cantar salmos e himnos. La creación de la himnodia se atribuye a San Ambrosio (397), que compuso una infinidad de ellos llenos de unción, sublimidad y energía. Posteriormente le seguirían San Benito y el movimiento monástico por él impulsado hasta que el himno se aceptó para los oficios divinos.

Algunos papas como Inocencio III, Clemente VII y San Gregorio los hicieron de una majestad sublime. Entre los cientos que usa la Iglesia católica citaremos el "Stabat Mater", producción de Inocencio III, que también compuso el "Veni Sancte Spiritus"; el "Dies irae", composición del franciscano Tomás Celano; o el "Ave maris stella", que salió de la pluma de San Bernardo.

Pero los himnos que descuellan por la majestad, sublimidad y augusta locución en las ideas son los que compuso el doctor de la Iglesia Santo Tomás de Aquino para el rezo del Santísimo sacramento y festividad del día del Corpus; así, el "Pange lingua".

De los primitivos himnos, cuando la música era puramente melódica se pasó posteriormente a melodías de canto llano y no se compondrán himnos en estilo polifónico hasta el siglo XIII. La obra más famosa será la compuesta por Palestrina en 1589: "Hymni totius anni".





</doc>
<doc id="3651" url="https://es.wikipedia.org/wiki?curid=3651" title="Provincia de Segovia">
Provincia de Segovia

Segovia es una provincia española perteneciente a la comunidad autónoma de Castilla y León, situada al norte del sistema Central que divide en dos la altiplanicie del centro de la península ibérica. Su capital es la ciudad homónima de Segovia. Tiene una superficie de 6920,65 km², siendo la provincia de menor extensión de Castilla y León, y cuenta con una población de habitantes (INE ).

Consta de 209 municipios y 17 entidades locales menores. En materia judicial, la provincia está dividida en cuatro partidos judiciales: el de Cuéllar, de Santa María la Real de Nieva, de Segovia y de Sepúlveda. Para elegir los diputados que forman el pleno de la Diputación Provincial, a estos cuatro se añade el Partido Judicial de Riaza.


El escudo heráldico de la diputación incluye en sus cuarteles las armas de las cabezas de partido de los partidos judiciales de Sepúlveda, Cuéllar, Riaza, Santa María la Real de Nieva y un escusón en abismo con las armas de la ciudad de Segovia.

La provincia, que tiene una superficie de 6920,65 km², limita al norte con las provincias de Valladolid y Burgos, al este con Soria y Guadalajara, al sur con la Comunidad de Madrid, y al oeste con Ávila.

La provincia está surcada por diferentes cursos de agua que nacen en las alturas de la sierra de Guadarrama y que, con la excepción de algunos pocos arroyos que discurren en dirección suroeste pertenecientes a la cuenca hidrográfica del Tajo, discurren en dirección sureste-noreste por el llano de la Meseta Norte, todos ellos pertenecientes a la cuenca hidrográfica de Duero. Entre ellos se encuentran el río Eresma, que pasa por la capital provincial, y el Duratón con sus conocidas «hoces», además del Cega, el Riaza y el Voltoya.

Al río Riaza desaguan el Aguisejo (también llamado Grado o Ayllón), el Riaguas y los demás arroyos que bañan el partido de su nombre, saliendo de la provincia por el término de Montejo de la Vega de Serrezuela y entrando en el Duero cerca de Roa. Entre los afluentes del Duratón están el Serrano, Castilla, Prádena y los demás arroyos del partido de Sepúlveda; sale por el término de Laguna de Contreras, y entra en el Duero cerca de Peñafiel. El Cega recibe las aguas del Cerquilla y multitud de arroyos de Cuéllar y Segovia; sale por el término de la Mata de Cuéllar y entra en el Duero, cerca de Puente Duero. El Pirón tiene como tributario al arroyo de Polendos, el Maluca y otros en los mismos partidos de Segovia y Cuéllar, reuniéndose con el Cega al salir este de la provincia. A la cuenca del Eresma pertenecen el Riofrío, Milanillos, Moros y el Voltoya con todos los arroyos del partido de Santa María la Real de Nieva y otros muchos del partido de Segovia; sale de la provincia, siendo línea divisoria entre el partido de Santa María y el de Cuéllar. Este río entra en la provincia de Valladolid y desemboca en el Adaja.

El clima es mediterráneo continentalizado, con inviernos prolongados, secos y fríos, y veranos calurosos pero cortos.

La unidad administrativa básica en la que se divide la provincia son los municipios. Existen 234 en la actualidad. El municipio con más habitantes es la capital provincial. El resto de municipios no alcanzan la cifra de 10 000 ciudadanos empadronados. Es destacable un elevado número de ellos con poblaciones por debajo de los 500 habitantes. La extensión promedio del municipio en la provincia es de 33,11 km². Entre las localidades de la parte noroeste de la provincia destacan en cuanto a población Cuéllar —el segundo municipio en población de la provincia—, Coca, Navas de Oro o Nava de la Asunción. En el sur de la provincia, en la vertiente noroccidental de la sierra de Guadarrama los municipios con más habitantes son, junto a la propia ciudad de Segovia (que cuenta con municipios limítrofes de relativa importancia como Palazuelos de Eresma, La Lastrilla, Hontanares de Eresma o San Cristóbal de Segovia), los de El Espinar, Real Sitio de San Ildefonso (La Granja) o Villacastín.

El centro de la provincia destacan los municipios de Cantalejo, Carbonero el Mayor o Turégano. En la poco poblada parte noreste de la provincia los municipios de mayor población son Sepúlveda, Riaza y Ayllón.

En la provincia de Segovia se contabilizan :

Los veinte municipios más poblados de la provincia de Segovia son los siguientes (INE de 1 de enero de 2019):

La provincia de Segovia está dividida aunque de manera no oficial, en diferentes comarcas históricas que engloban a varios municipios y pedanías, las denominadas Comunidades de Villa y Tierra que surgieron para llevar a cabo la Repoblación. Estas comunidades son:


Varias de estas comunidades de Villa y Tierra siguen vigentes en la actualidad como mancomunidades. Además, existen otras comarcas y mancomunidades en la provincia que han sido creadas para la gestión de servicios o aprovechamiento de bienes comunes.

En 2018, la población estimada (INE) de la provincia de Segovia ascendía a 153 342 personas. Entre los años 2000 y 2009, la provincia recuperó población, posiblemente debido al impulso económico que supone estar cerca de la Comunidad de Madrid, y en los años 2010 a 2017, se ha pasado a una fase de estancamiento a causa de la crisis económica y el envejecimiento de la población. Segovia sigue siendo una provincia muy despoblada, con una densidad demográfica muy baja, de poco más de 22 hab./km². En el conjunto de las provincias españolas, solo Soria (88 903 habitantes) y Teruel (135 562 habitantes) tienen menos población absoluta (INE, 2018). En el contexto de Castilla y León, en los últimos quince años se ha recortado notablemente el diferencial de población con respecto a las provincias de Ávila (160 700 habitantes) y Palencia (163 390 habitantes).

Los principales núcleos de población de la provincia de Segovia son: la capital provincial Segovia, Cuéllar, El Espinar, La Granja de San Ildefonso, Cantalejo, San Rafael, Nava de la Asunción, Carbonero el Mayor, Riaza, Coca, Villacastín, Navas de Oro, Ayllón, Cantimpalos y Turégano. Más de la mitad de los municipios tienen menos de 200 habitantes. El crecimiento demográfico más dinámico de la provincia lo ha experimentado la veintena de municipios que forman el llamado "alfoz" de Segovia, es decir aquellos que se sitúan a una distancia relativamente corta de la capital provincial estando dentro de su zona de influencia, y que, por ello, reciben preferencialmente la migración de la población que busca viviendas menos costosas sin alejarse excesivamente del núcleo urbano y de las fuentes de trabajo. 

De acuerdo a los datos del INE, el alfoz de Segovia ha crecido en forma sostenida en las últimas dos décadas, aumentado su población más del 100 %, entre los años 2000 y 2010 hasta alcanzar los 20 000 habitantes. La capital provincial creció solamente un 0,7 % en dicho período, e inclusive redujo su población en 466 individuos (0,8 %) entre los años 2005 y 2006 (INE). En contraste, los incrementos demográficos más marcados en el mismo período, se han producido en varios municipios del alfoz, destacando un primer cinturón distante hasta 10 km con: Espirdo, La Lastrilla, San Cristóbal de Segovia, Palazuelos de Eresma, Trescasas y Torrecaballeros, así como un segundo cinturón de municipios, distante entre 10 y 20 km con: Hontanares de Eresma, Valverde del Majano, Garcillán, Abades, Real Sitio de San Ildefonso, Bernuy de Porreros, Encinillas y Roda de Eresma entre otros.

El municipio de El Espinar es un caso especial en la provincia. Por su condición de municipio segoviano más próximo y mejor comunicado con Madrid, está experimentando notables crecimientos demográficos que se vienen dando durante el siglo (aproximadamente un 5% anual). Incrementos de población motivados por dicha proximidad a Madrid se detectan también en otros municipios del suroeste provincial, como Ituero y Lama, Otero de Herreros, Navas de San Antonio, Zarzuela del Monte o Marugán.

El municipio de Cuéllar es otro caso especial en la provincia, ya que se sitúa en la zona denominada ``El Carracillo´´, lugar donde hay extensiones grandes de superficie cultivada, gracias al acuífero que toma el mismo nombre. Es una zona que atrae población, y de ello, es beneficiaria la propia ciudad de Cuéllar. Otra razón es que se encuentra a medio camino entre Segovia y Valladolid y la única con una población de 10 000, por lo que es un centro de referencia comarcal. Actualmente cuenta con alrededor de 10 000 habitantes.


Fuente: INE. Unidades: millares.

El crecimiento sostenido de la población residente en la provincia de Segovia desde el año 2000 al 2008, está relacionado con el fuerte incremento de la inmigración, la cual representa en la actualidad más del 10 % de la población. Destaca la comunidad búlgara con más de 7000 residentes. Los polos de asentamiento de los emigrantes son la capital y su alfoz, la Tierra de Pinares y el Carracillo, así como los municipios lindantes con la provincia de Madrid. En 2009 y 2010, la crisis económica ha provocado una reducción drástica de nuevos empadronados foráneos, y como consecuencia de ello al estancamiento poblacional actual.

Su economía en la actualidad, está basada fundamentalmente, en el sector servicios, destacando el turismo ya que la provincia cuenta con importantes recursos culturales, fruto de un pasado muy importante, y el sector primario, sobre todos explotaciones de ganadería porcina.

Desde las Elecciones Municipales de 2019, el PP tiene mayoría absoluta en la Diputación.


Sus comunicaciones con la capital de España son bastante buenas: por carretera a través de la autopista AP-61, y por ferrocarril a través y la línea 53 de Renfe Media Distancia Segovia-Madrid y por la línea de alta velocidad Madrid-Segovia-Valladolid.

Las conexiones con el resto de lugares son algo precarias, bien a través de carreteras nacionales, o bien a través de carreteras autonómicas tanto de la Junta de Castilla y León tanto como de la Diputación de Segovia. En el año 2008, se ha puesto en servicio una autovía autonómica que une Segovia con Valladolid, a través de Cuéllar.


El patrimonio histórico de la provincia es muy rico y variado. En la capital se encuentran joyas romanas como el acueducto de Segovia, único en la provincia pues de la misma época sólo se conocen los mosaicos de algunas villas excavadas en Aguilafuente o Paradinas. 

Del gótico medieval, aunque alterados e incluso en ruinas, quedan numerosas construcciones; en su momento inicial dejó sus huellas en los monasterios de Sacramenia y San Pedro de las Dueñas, pero el gótico tardío, más pujante, levantó una espléndida catedral, los conventos de Santa María la Real de Nieva y San Francisco de Cuéllar y Santa Cruz, El Parral y San Francisco de Segovia, así como notables templos parroquiales, a veces incompletos, en El Espinar, Villacastín, La Losa, Martín Muñoz de las Posadas, Coca, Carbonero el Mayor o Cantimpalos. También destaca la pintoresca Ermita del Santo Cristo de la Moralejilla en Rapariegos del S. VI, declarada Monumento Histórico Artístico en 1994, y en la misma localidad el Convento Inmaculada Concepción de Rapariegos y la Iglesia de arquitectura Múdejar tardío.

De la arquitectura militar del mismo estilo se han conservado los castillos de Turégano, Pedraza y Cuellar, el alcázar de Segovia y las torres-fortaleza de Lastras del Pozo y Valdeprados. En lo que respecta a la arquitectura civil, existen numerosos palacios y casonas de fachadas blasonadas en localidades como Ayllón, Sebúlcor, Sauquillo de Cabezas, Pedraza, La Armuña, Segovia y Villacastín.

El patrimonio arquitectónico segoviano no se entiende sin el complejo que los Borbones levantaron en San Ildefonso, formado por el palacio y los jardines de La Granja, el palacio de Riofrío y el templo parroquial de Trescasas.

La provincia de Segovia, destaca principalmente por sus asados, tanto cordero como cochinillo de Segovia. Otros platos típicos son el chorizo de Cantimpalos, los judiones de La Granja, las sopas de ajo, y como postre el afamado ponche segoviano. El producto hortícola segoviano más conocido es el guisante.

En cuanto a bebidas, en la provincia de Segovia, se encuentran municipios pertenecientes a dos Denominaciones de Origen: Denominación de Origen de Rueda, y Denominación de Origen Ribera del Duero; además, cuenta con el vino de calidad de Valtiendas.






</doc>
<doc id="3652" url="https://es.wikipedia.org/wiki?curid=3652" title="Moral de Hornuez">
Moral de Hornuez

Moral de Hornuez es un municipio y localidad española de la provincia de Segovia, en la comunidad autónoma de Castilla y León.

Situado en el centro de una circunferencia equidistante unos 24 km de pueblos importantes de la zona como Sepúlveda, Riaza o Aranda de Duero, a él se accede por la carretera SG-V-9322; por el Sur desde Fuentemizarra o por el Norte desde Valdevacas, tras cruzar el arroyo Valdemiro

La primera manifestación escrita de la existencia de Moral de Hornuez es del siglo XIII, al comenzar la leyenda de la "Virgen de Hornuez". En datos reales aparece El Moral en el Censo de la Corona de Castilla de 1591. Situado en el Vecindario de Segovia, pertenecía a la Tierra de Maderuelo. Contaba con 58 vecinos, de los cuales 56 eran pecheros y 2 clérigos. Los lugares de Tamarón (despoblado) y La Nava (2 vecinos), aparecen independientes, siendo incorporados más tarde. 

La siguiente manifestación es de 1787.
En el diccionario de Sebastián Miñano (1827) figura:

La pertenencia a la provincia de Burgos se debió a un intento de reforma de los límites provinciales tras la invasión francesa.

En 1918 aparece definido como
En los años 60 del pasado siglo llegó a su máximo esplendor, con 560 habitantes. En la actualidad cuenta con 93 habitantes en 126 viviendas, perteneciendo al partido judicial de Riaza. Estuvo integrada en la Comunidad de Villa y Tierra de Maderuelo.

Se trata de un bien de interés cultural situado a unos 2 km al norte de la localidad. La ermita tiene planta de cruz latina y fue edificada en el mismo lugar donde, según cuenta la tradición, en 1246 unos pastores trashumantes que trataban de encender una hoguera quedaron deslumbrados con la luz que emanaba de la imagen de la Virgen que se les apareció. Tanto los habitantes de Moral como los de Maderuelo trataron de llevarla a sus localidades, pero la imagen regresaba a este lugar.

En el archivo parroquial de Segovia se encuentra el acta judicial de fecha 19 de junio de 1697 en la que se atestigua por 10 testigos y reconoce como cierta la «tradición antiquísima de la Virgen de Hornuez».

Aquella imagen primitiva se quemó en un incendio ocurrido el 3 de octubre de 1913 y en la actualidad se encuentra otra esculpida ese mismo año. Se trata de una Virgen con el Niño Jesús en el brazo izquierdo y un cetro en la mano derecha.

La ermita se asienta una pradera entre sabinas milenarias que se encuentra acondicionada como parque recreativo, con columpios, barbacoas, fuentes de agua, bar y frontón.

Todas las actividades referentes a esta ermita y su devoción son llevadas a cabo por la «cofradía de la Virgen del Milagro de Hornuez», de tradición milenaria y actualmente formada por más de mil cofrades, moraliegos principalmente.

Fiesta mayor y romería el último domingo de mayo, cuando son tradicionales la procesión y jota en honor a la Virgen María, y fiesta el segundo domingo de septiembre. Además son características las bodegas, donde todavía se hace el vino de manera tradicional.

La gastronomía local se basa principalmente en la carne de cordero, elaborada en todas sus variedades —asado, guisado, etc— que se acompaña de vino casero. Es tradicional el «Chupo» que se toma en las bodegas acompañado con crema de queso.

Bajo corona real, blasón dividido con la figura de un enebro (sabina) a la derecha y la imagen de una estrella lloviendo a la península a la izquierda.




</doc>
<doc id="3657" url="https://es.wikipedia.org/wiki?curid=3657" title="Cnidaria">
Cnidaria

Los cnidarios (Cnidaria, del griego <nowiki>"kníde"</nowiki>, ortiga) son un filo de animales diblásticos relativamente simples, que viven exclusivamente en ambientes acuáticos, mayoritariamente marinos. Agrupa alrededor de 10 000 especies. 

El nombre del filo alude a una característica diagnóstica propia de estos animales, la presencia de unas células urticantes llamadas cnidocitos, presentes en los tentáculos de todos los miembros del filo y que es inyectada cuando se roza el cnidocilio del cnidocito. Tienen simetría radial y su plan corporal es en forma de saco. Son los animales más simples que presentan células nerviosas y órganos de los sentidos (estatocistos, ocelos).

Son un grupo antiguo, con una larga historia fósil que se remonta, probablemente, a la fauna de Ediacara, alrededor de unos 600 millones de años atrás. No obstante, análisis genéticos del reloj molecular de sus mitocondrias sugieren una edad muy anterior para el grupo corona de todos los cnidarios, estimada en unos 741 millones de años, mucho antes de que haya fósiles que hayan perdurado hasta la actualidad..
Ejemplos de cnidarios: Medusozoa - medusas. Cubozoa - cubomedusas. Hydrozoa - hidropólipos, hidromedusas. Scyphozoa - medusas. Staurozoa - estauromedusas.

Los cnidarios son animales diblásticos (epidermis y gastrodermis) con simetría radial primaria. Sus células se organizan en dos capas que actúan como unidades funcionales (tejidos), aunque muchas células todavía guardan cierta independencia y cierta totipotencia. Tienen 2 hojas embrionarias, ectodermo y endodermo e indicios de mesodermo; aunque a veces existe un tejido análogo (ectomesodermo) de origen ectodérmico, no de origen endodérmico como el auténtico mesodermo de los triblásticos, y del cual nunca derivan órganos internos complejos.

Su organización corporal es en forma de saco; el aparato digestivo tiene un solo orificio que actúa como boca y ano al mismo tiempo, y una cavidad gastrovascular en forma de saco donde se realiza la digestión y que se utiliza también como sistema de distribución de los nutrientes y del oxígeno, y como sistema excretor.

La pared del cuerpo consta de:

Tienen uno o varios tentáculos alrededor de la boca. El sistema nervioso forma de una red o plexo; en muchos grupos hay protoneuronas no polarizadas, aunque también puede haber neuronas polarizadas, células sensoriales e incluso agrupación de las mismas en órganos sensoriales.

Tienden al polimorfismo, en especial formas coloniales. No hay aparato excretor, aparato respiratorio, ni aparato circulatorio. Estas funciones se realizan a través de la cavidad gastrovascular o de la ectodermis.

Son esencialmente marinos (99%). El resto son de agua dulce, como la hidra, o ciertas medusas de grandes lagos africanos como la medusa "Craspedacusta". Son siempre acuáticos.

Pueden vivir de forma individual o en colonias, fijados al sustrato o libres, incluso los hay nadadores. En ocasiones, parte de las fases son planctónicas (móviles, pero arrastrados por las corrientes).

Se conocen aproximadamente 10.000 especies, de tamaño variable, de 1-2 mm hasta 1 m de diámetro en algunas medusas, o hasta 3 m de diámetro en algunos pólipos.

El color también es variable aunque muchas formas del plancton son transparentes. Otras son coloreadas, y presentan prácticamente todos los colores.

Sin atender a las peculiaridades de cada grupo aparecen en dos formas fundamentales :

En términos generales (ver por grupos para más información), es un animal de paredes finas y amplia cavidad gastrovascular. Mesoglea muy poco desarrollada. Cuerpo más o menos columnar. Por el lado aboral (ver simetría radial) se relaciona con el sustrato. De adultos, pueden ser sésiles o libres, solitarios o coloniales.

Pueden ser dioicos o hermafroditas, y tienen una reproducción sexual y otra asexual.

Tamaño variable:

Presenta dos superficies perfectamente definidas; una cóncava donde se sitúa la boca (lado oral, véase simetría radial) y otra la opuesta (aboral). Al conjunto se le conoce como umbrela, donde se diferencian la exumbrela (aboral) y subumbrela (oral).

La boca se prolonga en un manubrio. Del manubrio pueden salir (o no, según los grupos) tentáculos. Además, la umbrela puede alargarse en tentáculos umbrelares (que pueden o no presentar cavidad tentacular, una extensión de la cavidad gastrovascular).

La boca desemboca en la cavidad gastrovascular de la que parten canales radiales hacia las paredes de la medusa. El canal umbrelar da la vuelta a toda la medusa. Si los tentáculos son huecos, también aparece el canal tentacular.

Toda la cavidad (también los canales) está tapizada por gastrodermis. El resto por ectodermis. Hay mesoglea muy desarrollada en la exumbrela, mientras que en la subumbrela está muy poco desarrollada.

Tamaño variable, desde pequeñas medusas de pólipos coloniales como "Obelia" (1-2 mm) hasta grandes medusas como la "nomura" que puede llegar a 2 m de diámetro y pesar hasta 220 kilogramos. Las medusas gigantes son animales míticos.

Veamos los tipos celulares que aparecen en los cnidarios:

La ectodermis consta de diversos tipos de células:






No está claro si las células mioepiteliales descansan sobre una verdadera lámina basal, como los tejidos epiteliales auténticos de otros metazoos, aunque puede interpretarse que la propia mesoglea es la lámina basal del epitelio de los cnidarios.

Sustancia gelatinosa que separa las dos capas epiteliales. Puede variar de una membrana delgada, no celular, hasta una gruesa capa gelatinosa con amebocitos errantes o sin ellos. Presenta abundante colágeno y precolágeno. Contiene numerosas células, como escleroblastos.


Entre ellas, hay células sensoriales (en menor número que en la ectodermis) y células basales. También pueden aparecer cnidocitos. Plexo nervioso poco desarrollado.

En algunos grupos aparecen receptores de diversa índole (táctiles, químicos, luminosos, gravitatorios, etc.)

Los receptores táctiles son células ciliadas con prolongaciones típicas como las ya vistas en la anatomía interna.

Receptores para estímulos luminosos

Los más sencillos son las llamadas manchas ocelares, o manchas oculares, que son manchas pigmentadas que aparecen en ciertas medusas en la umbrela. Constituidas por grupos de células de dos tipos: sensoriales ciliadas y células en cuyo interior se acumula un pigmento (rodoxina). El pigmento se sitúa en la base del cilio de forma simétrica o asimétrica, en un replegamiento de membrana. La célula pigmentada envía información a la sensorial típica.

Más avanzados son los ocelos, que son un pequeño entrante de la epidermis en el cual las células fotorreceptoras quedan en el centro, rodeadas de células pigmentadas.

Este ocelo se complica hacia un ocelo en copa, en el que hay un entrante grande, en cuyo fondo las fotorreceptoras y rodeando las pigmentadas.

Un poco más allá, en algunas medusas se desprende la parte distal de la célula y se forman prolongaciones vacuolares que rellenan la hendidura, la "copa", formando una especie de lente.

Estos receptores no les permiten distinguir objetos. sólo luces y sombras. Esto es lo que llamamos encelofalitis.

También han evolucionado diversos órganos del equilibrio.

Los estatos son estructuras que presentan filas de dos tipos de células, unas sensoriales ciliadas, y otras (litocitos) que acumulan en su interior una bolita calcárea (estatolito). Si el animal gira, como la célula con el estatolito cuelga por gravedad y pesa, se mueve y toca una célula sensorial de las que la rodea. Así el animal se mantiene informado de su posición.

Derivado de esto, tenemos los estatocistos, con una hendidura mayor que puede estar incluso cerrada y no comunicar con la umbrela. En otros casos, los estatolitos se desprenden de los litocitos o se usan partículas extrañas para la misma función.

También existen los estatorabdos, que son pequeños tentáculos con uno o más estatolitos. Este tentáculo cuelga rodeado de células sensoriales.

En el orden escifozoos hay un órgano muy desarrollado, la ropalia, que es un "centro quimio-estato-fotorreceptor". Es la estructura sensorial más compleja de los cnidarios, y sólo aparece en la fase móvil. La ropalia más compleja de todas se encuentra en el orden cubozoos.

El ciclo varía mucho según los grupos, por lo que para tener más información véanse las distintas clases. Aunque se caracteriza por la alternancia de generaciones. 

De manera general, el huevo posee poco vitelo y sufre segmentación total e igual. En muchos la gastrulación es por delaminación para alcanzar un estado larvario general, la larva plánula, ciliada, nadadora, que buscará en el sustrato un lugar para fijarse, dando lugar al pólipo, que crece y en un momento determinado, por reproducción agamética (normalmente gemación)por un proceso denominado estrobilación, origina las medusas, en las que madurarán los gametos que formarán el nuevo huevo.

Este ciclo completo es un ciclo metagenético. Dentro de los grupos hay desviaciones; por ejemplo hay pólipos que dan pólipos ("Hydra" por gemación crea un pólipo que crece y se separa). También hay medusas cuyo huevo se desarrolla a plánula, pero que se desarrollará muy rápido a medusa. También hay fases intermedias; los sifonóforos son coloniales, y hay polipoides (derivados de pólipo) y medusoides (derivados de medusa) que coexisten en la colonia. Todos estos ciclos que no son completos se llaman hipogenéticos.

La clasificación tradicional de los cnidarios reconoce cuatro clases y supone que los antozoos son la clase más primitiva y de la que han derivado las demás:

Solo pólipos (antopólipos), conocidos como anémonas y corales. No hay forma medusa. El pólipo da, por reproducción asexual o sexual, pólipos. Hay cnidocitos en la cavidad gastrovascular, a veces muy potentes, incluso en filamentos que salen por la pared del cuerpo. Las gónadas endodérmicas. La cavidad gastrovascular está dividida de forma completa (los tabiques provienen de la gastrodermis y de la mesoglea). El lado oral se introduce en la cavidad gastrovascular y origina un estomodeo o faringe.

Poseen exclusivamente medusas (cubomedusas), de forma cúbica, con cnidocitos especiales, muy potentes. Hasta hace poco se le consideraba como un orden de los escifozoos. Se caracterizan por su división tetrámera, que separa la cavidad gastrovascular en cuatro bolsas. El borde umbrelar no es festoneado y el margen de la subumbrela se pliega al interior para formar un velario.

Tienen hidropólipos e hidromedusas.
Las medusas tienen velo (son medusas craspédotas). No tienen cnidocitos en la cavidad gastrovascular. Las gónadas son siempre de origen ectodérmico. En una sección transversal, la cavidad gastrovascular es sencilla, sin dividir. La mesoglea de los pólipos está poco desarrollada.

Posee pólipo pequeño e inconspicuo (escifopólipo o escifostoma) y medusa (escifomedusa), que carece de velo (acraspedota). Presentan cnidocitos en la cavidad gastrovascular. Las gónadas son endodérmicas. Una vez maduradas, de todas maneras, pueden almacenarse en la endodermis. 
Su cavidad gastrovascular está dividida incompletamente por 4 tabiques o septos incompletos en posición inter-radial (ver simetría radial); los tabiques separan 4 bolsas gastrales.

Actualmente, el Registro Mundial de Especies Marinas incluye también las siguientes clases en Cnidaria:

Son una clase de animales parásitos microscópicos, clasificados durante mucho tiempo como protozoos dentro de los esporozoos. La parasitación ocurre por esporas, con válvulas que contendrían uno o dos esporoblastos y una o dos cápsulas con filamentos que anclarían la espora al hospedador. En 2015 un estudio reveló que los mixozoos son en realidad cnidarios extremadamente reducidos en tamaños y con un genoma simplificado.

Es una clase monoespecífica de cnidarios parásitos. Es uno de los pocos cnidarios que viven en el interior de las células de otros animales. Los datos moleculares sugieren que podría estar relacionado con los mixozoos (también parásitos), pero este punto es controvertido, por lo que su clasificación es temporal, ya que sus relaciones filogenéticas no están claras.

Se trata de formas sésiles. La superficie aboral, correspondiente a la exumbrela de otras medusas, se prolonga en un tallo gracias al cual se fijan al sustrato, en especial algas y rocas. Hasta hace poco se consideraban un orden dentro de la clase Scyphozoa, pero se elevaron a la categoría de clase tras un estudio cladístico.

Se han propuesto muchas teorías filogenéticas para relacionar a los distintos grupos de cnidarios. La mayor parte de los autores defienden que el posible pre-cnidario era un organismo planuloide adaptado a la vida sésil, adquiriendo simetría radial, que habría sufrido una invaginación que posteriormente dará a la cavidad gastrovascular. El pre-cnidario ¿tenía forma medusa o pólipo? Existen 2 teorías:

Estudios basados en filogenia molecular
corroboran la monofilia de los cnidarios y que la forma pólipo probablemente precedió a la forma medusa en la evolución de los cnidarios. También sugiere que los cnidarios están formados por dos grupos que podrían tener categoría de subfilos, los antozoos y los medusozoos; estos últimos agrupan los cubozoos, los escifozoos, los hidrozoos y las estauromedusas, por lo que las relaciones genómicas más recientes dan el siguiente resultado:


</doc>
<doc id="3658" url="https://es.wikipedia.org/wiki?curid=3658" title="Anthozoa">
Anthozoa

Los antozoos (Anthozoa, del griego ανθος "anthos", flor, y ζωον "zoon", animal) son una clase del filo Cnidaria que presentan exclusivamente forma de pólipo. Incluye especies tan conocidas como las anémonas de mar, los corales y las plumas de mar; pueden ser solitarios o coloniales, con esqueleto o sin esqueleto. Se conocen más de 6000 especies, todas marinas.

El pólipo de los antozoos, de simetría hexarradial u octorradial, presenta las siguientes características: 

La anatomía de un antozoo se basa en una estructura tubular, eje boral - aboral, es decir sin ano, epidermis interior, y cavidad gastrovascular, llamada también celenteron. En el caso de los antozoos, por no presentar alternancia de generaciones, como en Scyphozoa y Cubozoa, posee una superficie de estabilización que la une al suelo, justo después del momento en que la larva (plánula) encuentra el nuevo sitio para el establecimiento.

Tanto la mesoglea, como la epidermis y el celenteron, se forman tempranamente en la gastrulación, a partir de la invaginación del epiblasto en el blastocele. Sin embargo, a diferencia de los demás metazoos, los cnidarios en general, y entre ellos los antozoos, no forman el eje boca-ano, lo que en términos prácticos significa, un único lugar de alimentación y de excreción.

La mesoglea, mesenterio, está compuesta por dos tipos de músculos, que se encuentran alrededor de la cavidad gastrovascular. Tanto los músculos longitudinales, como transversales, realizan contracciones que facilitan el movimiento de la presa, a través del celenteron, y su posterior asimilación.

La simetría de los pólipos es radial. El principal eje del cuerpo discurre, longitudinalmente, desde la boca, zona boral o apical, hasta la base del pólipo, zona aboral o basal. Es decir, que si se cortara de arriba abajo al pólipo, sacando una tajada del mismo, a lo largo de este corte se podrían ver las mismas estructuras, en este caso de la mesoglea. La zona aboral puede además desarrollar un disco basal, adaptado especialmente para cavar y 'atenazarse' en sustratos blandos, o, en el caso de las formas coloniales, permitirles a los tallos erigirse desde una misma base.

En anémonas y corales duros, el pólipo libera los gametos, estos se transforman en larvas plánulas, las cuales viven de forma pelágica hasta encontrar un sitio para establecerse. En el caso de los corales solitarios, no lo hacen cerca de los pólipos de los padres, ni de las esponjas, porque la competencia por recursos, como plancton o luz, y los depredadores, respectivamente, les imposibilita su desarrollo.

Los antozoos se subdividen en tres clados: Octocorallia, Hexacorallia y Ceriantharia. Sus relaciones se muestran en el cladograma siguiente:

La organización taxonómica de las especies, géneros, familias, órdenes y subclases de la clase Anthozoa viene siendo, desde el siglo XIX, materia de debate para los científicos.

Actualmente, hay un consenso general entre los taxónomos en otorgar categoría de subclase a los tres clados, clasificación aceptada también por el Registro Mundial de Especies Marinas; nótese que lla antigua subclase Ceriantipatharia ha sido abandonada:
Subclase Hexacorallia

Subclase Octocorallia 

Subclase Ceriantharia


</doc>
<doc id="3681" url="https://es.wikipedia.org/wiki?curid=3681" title="Guanche">
Guanche

Guanche hace referencia a varios artículos:


</doc>
