<doc id="41372" url="https://es.wikipedia.org/wiki?curid=41372" title="Gaélico escocés">
Gaélico escocés

El gaélico escocés ("Gàidhlig" ) es una lengua indoeuropea de la rama celta, miembro de las lenguas goidélicas, que llegó a Escocia alrededor del siglo V, cuando los escotos de etnia celta y provenientes del norte de Irlanda se asentaron en la costa occidental, llevando una variedad del irlandés antiguo que sustituyó a la antigua lengua de los pictos hablada en la zona hasta entonces. De ahí su similitud con el gaélico hablado en Irlanda y la Isla de Man. Más tarde, los préstamos lingüísticos procedentes de los anglos y las invasiones vikingas irían relegando cada vez más el idioma, hasta que alrededor del 1500, durante el reinado de Jacobo IV, se crearon en las islas Hébridas las cortes locales y las escuelas de bardos, que fueron cuna del Sistema de Clanes de las Tierras altas y un refugio para la cultura y el idioma gaélico, fuertemente reprimido durante siglos. 

En la actualidad es hablado por unas 60 000 personas en las regiones norteñas de Escocia, cifra que representa menos del 1 % de la población escocesa —de un total de 5,1 millones—; para los que lo hablan, hay varios periódicos y programas de radio disponibles. El 21 de abril de 2005 se aprobó en el Parlamento de Escocia la ley que convierte al gaélico escocés en una de las lenguas oficiales de Escocia, junto al inglés. Se denomina siempre gaélico escocés [Scottish Gaelic] y no gaélico (para diferenciarlo del irlandés y el manés) o escocés [Scottish] (para no confundirlo con el escocés [Scots], lengua germánica cercana al inglés).

El gaélico escocés es una de las lenguas tradicionales de los escoceses y la lengua histórica de la mayor parte de Escocia. No está claro desde cuándo se habla gaélico en Escocia. Aunque hay quien afirma que se hablaba en Argyll antes de la llegada de los romanos, parece que la fecha más segura es el siglo IV, con el reino de Dalriada, que unió la antigua provincia del Ulster, en el norte de Irlanda, con el oeste de Escocia, acelerando así la expansión del gaélico, a lo que también contribuyó el establecimiento de la iglesia gaélico parlante. La toponimia parece indicar que el gaélico ya se hablaba en el siglo V. 

Esta lengua acabó por desplazar la de los pictos al norte del río Forth y, hasta finales del siglo XV, se conocía en inglés como "Scottis". Su declive comenzó en el continente del siglo XIII; dos siglos más tarde existía ya la divisoria Tierras Altas/Tierras Bajas. 

A comienzos del siglo XVI, los hablantes de "Inglis" le llamaban al gaélico Erse, esto es, irlandés, pasando a ser "Scottis" la colección de dialectos de inglés medio hablados en el reino de Escocia, y de ahí el moderno Scots o escocés. No obstante, el gaélico ocupa aún un lugar especial en la cultura escocesa y no fue nunca desposeído totalmente de su consideración como lengua nacional. Muchos escoceses, lo hablen o no, consideran que forma parte fundamental de su cultura nacional, si bien también hay quien lo considera una mera lengua regional de las Tierras Altas y de las islas. 

El gaélico cuenta con una tradición oral ("beul aithris") y escrita muy rica, habiendo sido la lengua de la cultura bárdica de los clanes de la Tierra Alta durante varios siglos. La lengua preservó el patrimonio y adhesión a leyes y costumbres pre-feudales (como por ejemplo en el uso de las expresiones "tuatha" y "dùthchas"). Sufrió especialmente con las persecuciones padecidas por los habitantes de las Tierras Altas tras la Batalla de Culloden en 1746 y durante los Desalojos de las Highlands. Ciertas actitudes prefeudales aún se ven en las quejas y reclamaciones de la Liga Agraria de las Highlands de finales del siglo XIX. 

Es posible distinguir entre el gaélico de las Tierras Altas, que correspondería con el que se conoce como gaélico escocés y el gaélico de la Tierras Bajas, ahora extinto. Este último se hablaba en el sur de Escocia antes de la introducción del escocés de la Tierras Bajas. Sin embargo, no hay pruebas de la existencia de una frontera lingüística entre el norte y el sur, como tampoco entre Argyll y Galloway. Los dialectos en las dos márgenes del Estrecho de Moyle, que ligaban el gaélico escocés con el irlandés, también están extintos hoy en día.

El censo británico del año 2001 mostró un total de 58 652 hablantes en Escocia (1,2 % de la población mayor de tres años). Comparado con el censo del año 1991 ha habido una disminución de 7300 personas (11 % del total), esto significa que el gaélico sigue declinando en Escocia. Hasta ahora se hacen esfuerzos por revertir la situación, pero han tenido un éxito limitado.
Considerando los datos relativos a las parroquias civiles (lo cual permite un continuo estudio del estatus del gaélico desde el siglo XIX) dos circunstancias nuevas han tomado lugar que son relativas al declive del gaélico:

Ninguna parroquia civil tiene una proporción de hablantes del gaélico superior al 75 %, siendo Barvas ("Barabhas") y Lewis ("Leòdhas") las que tienen un mayor porcentaje (74 %).

El principal baluarte del gaélico continúan siendo las islas Hébridas Exteriores ("Na h-Eileanan Siar") donde el porcentaje de hablantes va desde el 61 % al 50 % en todas las parroquias. La parroquia de Kilmuir en el norte de la isla de Skye ("An t-Eilean Sgitheanach") está también sobre el umbral del 50 %.

Fuera de las Hébridas Exteriores las únicas áreas con un porcentaje significativo de hablantes son la isla de Tiree ("Tiriodh") (41,4 %) de las Islas Hébridas Interiores ("Na h-Eileanan a-staigh") y la isla de Lismore ("Lios Mór") en el centro de las Tierras Altas occidentales (28,8 %). A pesar de esto, el peso del gaélico es muy reducido. De un total de casi 900 parroquias civiles en Escocia:


Fuera de los principales "gàidhealtachd" (áreas donde se habla el gaélico) existe un número relativamente alto de hablantes, pero aislados de otros hablantes del gaélico tienen pocas oportunidades de usar la lengua corrientemente.

El gaélico escocés se escribe con el alfabeto latino, usándose solo 18 letras para escribirlo:

a, b, c, d, e, f, g, h, i, l, m, n, o, p, r, s, t, u

La letra h, utilizada ahora sobre todo para indicar la lenición de una consonante y la aspiración, no se empleaba en la ortografía antigua, ya que la lenición se indicaba con un punto sobre la consonante. Las letras del alfabeto recibían nombres tradicionales de árboles: "ailm" (olmo), "beith" (abedul), "coll" (avellano), "dair" (roble), etc., aunque ya no se sigue esta norma. 

La calidad de las consonantes se indica en parte con las vocales que las rodean. Las vocales se clasifican como "caol" (‘delgadas’, o sea, "e" e "i") o "leathann" (‘anchas’, o sea, "a", "o" y "u"). La regla ortográfica es "caol ri caol is leathann ri leathann" (‘delgada a delgada y ancha a ancha’). Las consonantes delgadas se palatalizan, mientras que las anchas se velarizan. 

Debido la esta regla ortográfica, un grupo consonántico interior se debe rodear de vocales de la misma calidad para indicar su pronunciación sin ambigüedad, dado que algunas consonantes mudan su pronunciación dependiendo de si se rodean con vocales anchas o delgadas. Como por ejemplo, la "t" de "slàinte" ([slaːntʃə]) con "t" de "bàta" ([paːtə]).

Esta regla no afecta la pronunciación de las vocales. Como por ejemplo, los plurales en gaélico se forman habitualmente con el sufijo "an": "bròg", [proːk] (‘zapato’)/"brògan", [proːkən] (zapatos). Pero debido a la regla ortográfica, el sufijo se escribe "-ean" (aunque pronunciado igual) después de una consonante delgada, como en "taigh", [tʰɤj] (‘casa’)/"taighean", [tʰɤjən] (‘casas’).

A partir de 1976, la Comisión de Exámenes de Escocia introdujo determinadas modificaciones a esta regla. Como por ejemplo, el sufijo del participio de pasado siempre se escribe "-te", incluso después de una consonante ancha, como en "togte" (‘elevado’), y no el tradicional "togta". Cuando coinciden pares de vocales, no siempre está claro qué vocal se debe pronunciar y cuál se introdujo para satisfacer esta regla ortográfica. 

Las vocales acentuadas omitidas en el habla también se pueden omitir en la escrita informal. Como por ejemplo: 

Una vez aprendidas las reglas ortográficas, la pronunciación de los textos escritos resulta bastante predecible. 

Los signos diacríticos utilizados por el gaélico son el acento grave (`) y el acento agudo (´). El acento grave es el más usado para marcar las vocales largas y marca en el caso de la ò [] y è [] un sonido distinto al de la é [] y ó [].

La mayoría de las letras se pronuncia de manera semejante a otras lenguas europeas. Las consonantes anchas "t" y "d" y, con frecuencia, "n" tienen una articulación dental (como el irlandés y las lenguas románicas y eslavas, en contraste con la articulación alveolar típica del inglés y otras lenguas germánicas. La "r" no palatal es una vibración alveolar, como la "rr" del gallego. 

Las consonantes oclusivas «sonoras» "b", "d", "g" no lo son en el gaélico, sino más bien no aspiradas sordas. Las oclusivas «sordas» "p", "t", "c" son sordas y se pronuncian con una aspiración fuerte (postaspiradas en posición inicial, preaspiradas en posición medial o final). El gaélico comparte esta propiedad con el islandés. En algunos dialectos gaélicos, las oclusivas al inicio de una sílaba acentudada sonorizan si las sigue una consonante nasal. Como por ejemplo, "taigh" (‘una casa’) es [tʰɤi], pero "an taigh", (‘la casa’) es [ən dʰɤi]. También ocurre "tombaca" (‘tabaco’), [tʰomˈbaxkə].

Las consonantes con lenición tienen pronunciaciones especiales: "bh" y "mh" son [v]; "ch" es [x] ou [ç]; "dh", "gh" es [ʝ] o [ɣ]; "th" es [h], [ʔ] o muda. La lenición de "l", "n" y "r" no se muestra en la escrita. 

"fh" es casi siempre mudo, con solo tres excepciones: "fhèin" (‘mismo’), "fhathast" (‘aún’) y "fhuair" (forma independiente de pasado del verbo irregular "faigh", "a’ faighinn" ‘encontrar’, ‘obtener’), en que se pronuncia [h].

Hay determinados rasgos que conviene hacer notar: 


El sustantivo tiene dos géneros (masculino y femenino) y su número puede ser singular y plural. El gaélico escocés, al igual que las demás lenguas celtas, carece de artículo indefinido, y la propia forma del sustantivo puede indicar, a su vez, la forma indefinida ("dùthaich", ‘país’ o ‘un país’). Esta lengua tiene artículo definido, el cual varía considerablemente de forma (y provoca lenición) dependiendo del caso del sustantivo y del género: "a’ chlann" (‘los niños’), "an t-ainm" (‘el nombre’), "an dotair" (‘el doctor’), "am bràthair" (‘el hermano’).

Al igual que las demás lenguas celtas es un idioma flexivo que tiene los siguientes casos y que se manifiestan mediante la lenición (en la mayoría de los casos): nominativo/acusativo, dativo, genitivo y vocativo. En cuanto al orden de la frase, el verbo va al principio, luego el sujeto y al final el objeto (V+S+O): "Bidh" (V) "mi" (S) "a' dol" (OD) "dhan oilthigh" (CCL) "a-màireach anns a' mhàdainn" (CCT) ('Iré a la universidad mañana por la mañana'). 

Cabe destacar la importancia de los llamados pronombres preposicionales, formados mediante la forma de los pronombres tónicos y alguna preposición; por ejemplo: "agam" (‘en mí’ ← "aig"+"mi"), "annam" (‘en mí’ ← "ann"+"mi"), "leam" (‘conmigo’, ‘por mí’ ← "le"+"mi"), etcétera, que sirven para expresar acciones como «tener» ("A bheil bràthair no piuthar agad?", ‘¿Tienes hermano o hermana?’; "Chan eil Beurla aige", ‘(Él) no sabe inglés’), definirse como un sujeto ("'S e oileannach a th’ annam", ‘Soy un estudiante’), preguntar cómo se llama un sujeto ("Dè an t-ainim a th’ort?", ‘¿Cómo te llamas?’) o presentar las aflicciones o condiciones ("Tha an t-acras orm", ‘Tengo hambre’), etcétera. Literalmente, ‘hay alguna cosa en/con/hacia/etc. alguien’: "Tha seann chù agam" (lit. ‘Hay un perro viejo en mí’ ← ‘Tengo un perro viejo’).

El verbo gaélico tiene voz (activa y pasiva), modo (indicativo, subjuntivo e imperativo), tiempo, número y persona. Además, los verbos tienen dos formas: la forma independiente y la forma dependiente (además, la forma de futuro presenta una forma relativa). Carece de una forma infinitiva distinta de las otras (el infinitivo debe ser construido desde la raíz verbal para ser usado en las frases y puede tener una forma aspirada y una no aspirada). El gaélico además del verbo "bhith" ('ser/estar'), tiene solo 10 verbos irregulares.

Para el verbo "ser/estar", en gaélico escocés existen tres formas de expresarlo:

Al igual que el galés, el gaélico escocés tiene dos sistemas de cómputo, el decimal y el vigesimal.

"Tha gach uile dhuine air a bhreth saor agus co-ionnan ann an urram ’s ann an còirichean. Tha iad air am breth le reusan is le cogais agus mar sin bu chòir dhaibh a bhith beò nam measg fhein ann an spiorad bràthaireil."

‘Todos los seres humanos nacen libres e iguales en dignidad y derechos y, dotados como están de razón y conciencia, deben comportarse fraternalmente los unos con los otros.’


A pesar de haber sido prohibido y relegado de la enseñanza pública y uso público durante siglos por parte de las autoridades inglesas y también escocesas, en el año 2005 fue promulgada el Acta del idioma gaélico ("Gaelic Language (Scotland) Act") alcanzando un grado de reconocimiento oficial por parte del Gobierno de Escocia, dándole igual respeto que al inglés y encomendando su promoción y desarrollo a Bòrd na Gàidhlig.

El gaélico es enseñado en las escuelas y el gobierno escocés mediante Bòrd na Gàidhlig intenta aumentar el número de alumnos que lo estudian tanto en las Tierras Altas como en las Tierras Bajas, para ello ha iniciado un plan para reclutar a más profesores que puedan enseñarlo y a aumentar el número de establecimientos educacionales de todos los niveles donde se enseña.
Desde 2008 comenzó a funcionar BBC Alba que transmite gran parte de sus contenidos en gaélico. La BBC también opera la "Radio nan Gàidheal".

Gradualmente se ha ido introduciendo señalética vial bilingüe gaélico-inglesa en áreas donde se habla gaélico en las Tierras Altas, las Islas y Argyll. En muchos casos es solo la readopción de la forma ortográfica tradicional (como por ejemplo "Ràtagan" o "Loch Ailleart" en vez de las formas anglizadas de "Ratagan" o "Lochailort" respectivamente). Recientemente el Highland Council ("Comhairle na Gàidhealtachd") ha tenido la intención de introducir señaletica vial bilingüe en toda su área de jurisdicción, a lo que se han resistido algunos residentes.





</doc>
<doc id="41374" url="https://es.wikipedia.org/wiki?curid=41374" title="Economía de la República Popular China">
Economía de la República Popular China

La economía de la República Popular China, más conocida simplemente como China, es la más grande del mundo en términos de producto interior bruto nominal. y la en paridad de poder adquisitivo, según el Fondo Monetario Internacional. Es el país de más rápido crecimiento económico en el mundo desde la década de 1980, con un promedio de crecimiento anual de casi el 10% en los últimos 38 años. Según datos oficiales del Buró Nacional de Estadísticas, la economía del país creció un 6,9% en 2017, lo que supone un aumento del 0,2% con respecto a 2016. 

China es el centro mundial para la fabricación de todo tipo de productos y la indiscutible mayor potencia industrial y de bienes a nivel mundial. China es el y con una mayor tasa de crecimiento en consumo, además de de mercancías.

China tiene una economía ligada a los mercados internacionales tanto industriales como financieros, según la Federación de Industria y Comercio de China el PIB está producido en más del 60% por empresas del sector privado, las estimaciones de otros académicos tanto chinos como extranjeros calculan que la aportación del sector privado al PIB chino está entre el 70% y el 80%. Además los recursos son asignados y los precios determinados en su mayoría por mecanismos de mercado. Las empresas privadas, además, representan el 90% del total de empresas del país. El estado chino a su vez controla y monopoliza sus sectores estratégicos y mantiene una notable regulación económica, siendo las empresas más grandes de China mayoritariamente estatales; a esta forma de economía el gobierno chino la denomina «economía de mercado socialista» o «socialismo con características chinas». China es el país con mayor volumen de comercio y juega un papel determinante en el comercio internacional. En las últimas décadas ha ido ingresando en organizaciones y tratados comerciales. Por ejemplo, el país entró en la Organización Mundial del Comercio en el año 2001 y en la Asociación de Naciones del Sudeste Asiático en el 2010. China también tiene acuerdos de libre comercio bilaterales con varios países, como Suiza o Pakistán.

China ocupa el puesto 82º en ingreso per cápita nominal y el 89º en PIB PPA, según datos de 2013 del FMI. Las provincias costeras tienden a estar más desarrolladas e industrializadas, mientras que las regiones del interior del país son más rurales y poseen un menor desarrollo. El , Xi Jinping, proclamó en el año 2013 el Sueño Chino y fijó como meta a corto plazo lograr que en el año 2021, la sociedad china viva al menos modestamente acomodada, y como meta a largo plazo que China se convierta en un país completamente desarrollado para el año 2049. Ambas fechas tienen una importante carga simbólica, pues en 2021 será el 100º aniversario de la fundación del Partido Comunista de China y en 2049 el centenario de la fundación de la República Popular China. En 2018, Oficina de Reducción de la Pobreza y Desarrollo del Consejo de Estado a través del Ministerio de Comercio anunció que habían sacado de la pobreza a 68 millones de personas en los últimos cinco años, o lo que es lo mismo, una reducción anual de al menos 13 millones. El gobierno se ha marcado como objetivo eliminar la pobreza absoluta para finales de 2020.

Desde 1980, China ha ido estableciendo zonas económicas especiales, donde establece economías basadas en experiencias exitosas en distintas áreas. El gran desarrollo en infraestructuras del país quedó documentado en un informe de KPMG de 2009.
Por otro lado, China ha sido criticada por los medios de comunicación de Occidente por practicar comercio desleal, incluida la devaluación artificial del yuan, espionaje industrial y robo de propiedad intelectual, proteccionismo y favoritismos locales.

En 1949 China era un país rural con un desarrollo industrial muy deficiente. La agricultura era de carácter intensivo, con una tasa de inversión muy baja, la ausencia de una política hidrológica, provocaba grandes fluctuaciones entre los años secos y lluviosos, lo que producía grandes desequilibrios en los mercados, acentuados por una mala red de transportes. Según estimaciones de la ONU, en 1947 la renta per cápita china era de 40 dólares al año, la mitad de la renta India y muy por debajo del promedio mundial, fijado en torno a los 250 dólares.

A partir del final de la guerra se extendió a todo el país, la reforma agrícola iniciada durante el conflicto, ésta consistió en una primera fase, en la confiscación de las tierras de los terratenientes para su redistribución entre los pequeños agricultores y los asalariados agrícolas. Posteriormente se pasó a un proceso de cooperativización y colectivización. La coincidencia de este proceso junto a algunos fenómenos climatológicos adversos provocaron entre 1959 y 1961, una bajada de la producción agrícola y unas fuertes hambrunas.

Tras el final de la guerra se nacionalizaron las principales industrias, la banca y el comercio al por mayor. Permaneció en manos privadas, aunque muy controladas, la pequeña y mediana industria. En 1953 se aprobó el primer plan quinquenal, siguiendo el modelo soviético, su objetivo era la construcción de 694 centros fabriles. Los resultados permitieron incrementar la producción y formar un gran número de técnicos. A partir de 1957 el gobierno chino se fue separando de las directrices soviéticas que coincidió en el tiempo con una fase denominada de las cien flores que supuso un periodo de apertura hacia la crítica.

A partir de 1958, con el segundo plan quinquenal, se inició el conocido como "Gran Salto Adelante", conjunto de medidas tendentes a forzar el crecimiento económico, que pretendía ser una vía más rápida de incrementar la producción. En la agricultura supuso la colectivización de las explotaciones agrícolas y en la industria la combinación de grandes inversiones siguiendo el modelo soviético con otras industrias de menor tamaño, en las que se trataba de implicar a todo el país. El plan obtuvo inicialmente resultados cuantitativos significativos, consiguiendo duplicar en un año la producción de hierro y acero. En la agricultura, el enorme tamaño de las comunas establecidas unido a los desastres naturales, sequía e inundaciones, que afectaron a China, provocaron una bajada de la producción y una gran hambruna.

En 1961 se inicia una fase de depresión en el que se deshacen las grandes comunas agrícolas, faltas de eficiencia. También se produce en este año la ruptura con la Unión Soviética y se cancelan todos los proyectos iniciados con su cooperación y se reducen las ambiciosas metas de crecimiento que se habían demostrado inútiles, programa de reformas económicas llamado "Socialismo con características chinas" en la República Popular China (PRC), que se inició en diciembre de 1978 por los reformistas dentro del Partido Comunista de China (CPC) dirigidos por Deng Xiaoping. La meta de la reforma económica china era transformar la economía planificada para generar un fuerte crecimiento económico e incrementar el bienestar de los ciudadanos chinos. La primera fase, a finales de los 70 y principios de los 80, involucraba la descolectivización de la agricultura, la apertura del país a la inversión extranjera y el permiso a emprendedores de iniciar empresas. La segunda fase de la reforma, a finales de los 80 y 90, involucraba la industria pesada aunque los monopolios públicos en sectores como la banca y el petróleo permanecieron. El sector privado creció notablemente, reflejando casi el 70% del PIB de China para 2005, De 1978 a 2010, sucedió un crecimiento sin precedentes, con el aumento de la economía en 9,5% anual.

Cuando se introdujeron las reformas a fines de la década de 1970, China era un país en vías de desarrollo que luchaba por alimentar a su vasta población de casi mil millones. Por lo tanto, la reforma se enfrentó al doble desafío de proporcionar incentivos económicos racionales al tiempo que sostenía una red de seguridad social básica para superar los temores del cambio. Este es un enfoque en el que los derechos existentes se "protegen" para reducir la resistencia a la reforma, al tiempo que se crean fuertes incentivos para la creación de riqueza. Esta fue la filosofía de la "reforma de doble vía", también conocida como "la reforma sin perdedores".

La reforma de la doble vía se introdujo por primera vez en el sector agrícola, que sirvió como modelo para las reformas en otros sectores. En este enfoque, la primera vía (la red de seguridad social) eran las cuotas de producción obligatorias existentes a precios fijos bajos; mientras que la segunda vía era toda nueva producción a precios de mercado (mucho más altos). Esta reforma, combinada con la concesión de "derechos de propiedad casi privados" creó tasas de ganancia marginal del 100% para cualquier nueva producción, y tuvo tanto éxito que el "Plan" fue abandonado en la década de 1980 y las dos vías se unificaron en 1992.

En este nuevo sistema económico, las empresas públicas o danwei están luchando por encontrar su lugar y se enfrentan cada vez a más dificultades. Sus pérdidas alcanzaron un máximo de 102.600 millones de yuanes (12.750 millones de dólares) en 2005, un aumento del 56,7 por ciento anual, según cifras de la Oficina Estatal de Estadísticas (BES). En los dos primeros meses de 2006, las pérdidas de las empresas estatales o controladas por el estado alcanzaron los 26.200 millones de yuanes, o 3.250 millones de dólares estadounidenses. El aumento de los costos de producción, un sistema de fijación de precios ineficiente, exceso de capacidad y deficiencias tecnológicas significativas fueron en la década pasada las principales causas de esta situación, de acuerdo con Jiang Yuan, miembro de la Oficina Estatal de Estadísticas. El rápido crecimiento del sector privado ha desplazado la importancia de los activos de las empresas públicas de casi el 99% a fines de la década de 1970 al 25,2% en 2013, particularmente porque sus resultados se han mantenido muy por debajo de las compañías privadas.

En 2016, el FMI alertó a las autoridades chinas sobre el endeudamiento excesivo de las empresas del país, especialmente las empresas públicas: mientras que la deuda estatal y de los hogares sigue siendo razonable (40% del PIB cada una), las empresas alcanzan el 145% del PIB; sin embargo, solo las empresas estatales representan el 55% de la deuda corporativa total, mientras que su participación en la producción total en el país es solo del 22%.

La presencia de empresas extranjeras en suelo chino es en gran parte responsable de la fuerte aceleración del crecimiento de las exportaciones. El establecimiento del "socialismo de mercado" ha implantado muchas fábricas en China, lo que convirtió al país en el "Taller del Mundo", debido al dumping social de sus fábricas. Han atraído mano de obra calificada a las áreas costeras donde se han establecido la mayoría de las industrias. Solo el 41% de las exportaciones provienen de empresas totalmente chinas. En 2006, el 39% de las exportaciones de China proviene de compañías cuyo capital es 100% extranjero y el 20% es el resultado de la asociación entre compañías extranjeras y compañías locales. La China continental mantiene su atractivo para las empresas que demandan mano de obra barata, no sindicalizada y dócil. La falta de organización de los trabajadores chinos representa un beneficio sustancial para los empresarios, que encuentran allí una flexibilidad de empleo imposible de implementar en las democracias liberales.

Dicho esto, el impacto de las empresas extranjeras va más allá de su papel como exportadores y empleadores de trabajadores domésticos baratos. De hecho, las compañías extranjeras están creando enormes sinergias al establecer estándares visibles en términos de prácticas de gestión, control de calidad y métodos de capacitación, que las empresas nacionales han sido rápidas de emular. Entonces, cuando las compañías extranjeras regresaron a China en la década de 1980, prácticamente todos los técnicos y gerentes, así como las piezas de repuesto, fueron importadas. Pero, con el tiempo, las empresas y el personal doméstico rápidamente comenzaron a cumplir estos roles, impulsados por un sistema educativo que proporciona 2 millones de graduados en ciencias e ingeniería al año y la atracción de más de 100 parques científicos 2014. De hecho, el mercado chino se ha vuelto demasiado grande para ignorarlo, con la presencia de 200 compañías Fortune 500, además de ser la sede de estas 52 compañías en Beijing en 2014.

China es desde el año 2006 el país más contaminante del mundo, superando a Estados Unidos y con proyecciones que apuntan a que esta tendencia de emisiones crecientes continuará hasta al menos 2025. En el año 2012, sus emisiones eran ya casi el doble que las de EE.UU. Al contrario que el resto de países industrializados, China obtiene la mayor parte de su energía de la quema de carbón, mineral que incrementó su peso en la producción entre 1970 y 2010, un hecho sin precedentes. Para remediar esto, el gobierno comenzó a invertir en nuevos proyectos para el aprovechamiento de la energía de fuentes renovables, como la energía hidroeléctrica, la energía eólica, la energía solar, la energía geotérmica, la biomasa y los biocombustibles.
De hecho, China es uno de los países donde la energía eólica y solar ha experimentado un crecimiento más vertiginoso y, en pocos años, se ha convertido en el país con mayor potencia instalada de ambas tecnologías.

El país tiene además el reto de tener que alimentar a su población, que equivale a uno de cada cinco seres humanos, con sólo un 7% de su superficie en condiciones de ser empleada para la agricultura. Debido en parte a la meta gubernamental de reubicar a 400 millones de chinos hacia las ciudades en los próximos 25 años, dicha superficie cultivable disminuye al ritmo de 2 millones de hectáreas al año. Por ello desde la década de 2000, China ha estado comprando vastas extensiones de terreno en África —al igual que están haciendo multinacionales occidentales—, América del Sur o países europeos, destacando el caso de Ucrania, que dio a China 3 millones de hectáreas —un 5% de la superficie del país— para su explotación por varios miles de millones de dólares; las tierras agrícolas ucranianas (Chernozem) son de las más fértiles del mundo.

En los últimos años, el gobierno, ante la presión social, ha puesto más énfasis en atajar el problema de la polución, intentado conciliar el crecimiento económico con el control de las emisiones de carbono y el cambio climático. En los últimos años, más de 1000 plantas energéticas consideradas ineficientes han sido cerradas, pero como se ha indicado, las proyecciones siguen apuntando a una tasa de emisiones creciente. A mediados de 2014, China firmó con Estados Unidos una serie de acuerdos para la reducción de emisiones de las dos principales potencias. Los acuerdos contemplan que para 2025, Estados Unidos haya reducido sus emisiones entre un 26 y un 28% respecto a niveles de 2005, mientras que China aumentará sus emisiones hasta 2030, momento en él que empezará a reducirlas.

De acuerdo a su tesis, la estrategia económica China nunca será sostenible si a estos niveles, por ejemplo, ya se han drenado la totalidad de los lagos a las afueras de ciudades como Pekín y el país es sede de 16 de las 20 ciudades más contaminadas del planeta. Además, señalan que la negativa actual de los Estados Unidos a participar en el Protocolo de Kyoto es una prueba fehaciente de que una nación industrializada no necesariamente se compromete a reparar el daño que ocasiona su desarrollo económico.
China es el mayor productor y consumidor de productos agrícolas del mundo. Prácticamente toda la tierra cultivable se usa para cultivos destinados a la alimentación humana. China es el mayor productor mundial de arroz y se encuentra entre las principales fuentes de trigo, maíz, tabaco, soja, papa, sorgo, maní, té, mijo, cebada, semillas oleaginosas, carne de cerdo y pescado. Los principales cultivos no alimentarios, incluido el algodón, otras fibras y semillas oleaginosas, proporcionan a China una pequeña proporción de sus ingresos de comercio exterior. Las exportaciones agrícolas, como verduras y frutas, pescado y mariscos, cereales y productos cárnicos, se exportan a Hong Kong. China espera aumentar aún más la producción agrícola a través de mejoras en las semillas, fertilizantes y tecnología. Según las estadísticas del gobierno publicadas en 2005, desde el año 2000 la producción ha experimentado un aumento anual constante.

Según el Programa Mundial de Alimentos de las Naciones Unidas, en 2003, China alimentó al 20 por ciento de la población mundial con solo el 7 por ciento de las tierras cultivables del mundo. China ocupa el primer lugar a nivel mundial en producción agrícola y, como resultado de factores topográficos y climáticos, solo alrededor del 10-15 por ciento de la superficie total del país es apta para la agricultura. De esto, un poco más de la mitad es irrigada, y el resto se divide aproximadamente por igual entre los arrozales y las áreas de regadío. Sin embargo, alrededor del 60 por ciento de la población vive en las zonas rurales, y hasta la década de 1980 un alto porcentaje de ellos vivía directamente de la agricultura. Desde entonces, muchos han sido alentados a abandonar los campos y realizar otras actividades, en la industria ligera, el comercio y el transporte, por citar algunos ejemplos; y para mediados de la década de 1980, la agricultura representaba menos de la mitad del valor de la producción rural. Hoy en día, la agricultura contribuye con solo el 13% del PIB de China.

La cría de animales constituye la segunda actividad más importante del sector. China es el principal productor mundial de cerdos, pollos y huevos, y también tiene grandes rebaños de ovejas y ganado vacuno. Desde mediados de la década de 1970, se ha puesto mayor énfasis en aumentar la producción ganadera. China tiene una larga tradición de pesca oceánica y fluvial y de acuicultura. La captación de estanques siempre ha sido importante y ha ido cobrando protagonismo como forma de complementar las capturas costeras y continentales amenazadas por la sobrepesca y para proporcionar productos de exportación tan valiosos como los langostinos.

Problemas ambientales como inundaciones, sequías y la erosión plantean serias amenazas para la agricultura en muchas partes del país. El país cuenta actualmente con más de 2.500 empresas de maquinaria agrícola, y la producción agrícola del país emplea generalmente máquinas de cultivo, teniendo una tasa de mecanización del 66 por ciento. Los bosques principales se encuentran en las montañas de Qin y las montañas centrales y en la meseta de Sichuan-Yunnan. Debido a que son inaccesibles, los bosques de Qinling no se explotan en su totalidad. Gran parte de la madera del país proviene de Heilongjiang, Jilin, Sichuán y Yunnan.

El oeste de China, que comprende el Tíbet, Xinjiang y Qinghai, tiene poca importancia agrícola a excepción de las áreas de floricultura y ganadería. El arroz, el cultivo más importante de China, es dominante en las provincias del sur y muchas de las granjas aquí rinden dos cosechas al año. En el norte, el trigo es de la mayor importancia, mientras que en el centro de China, el trigo y el arroz compiten entre sí por el primer puesto. El mijo y el kaoliang (una variedad de sorgo de grano) se cultivan principalmente en el noreste y algunas provincias centrales, que, junto con algunas áreas del norte, también proporcionan cantidades considerables de cebada. La mayor parte del cultivo de soja proviene del norte y el noreste; el maíz se cultiva en el centro y el norte, mientras que el té proviene principalmente de las zonas montañosas cálidas y húmedas del sur. El algodón se cultiva extensamente en las provincias centrales, pero también se encuentra en menor medida en el sureste y en el norte. El tabaco proviene del centro y partes del sur. Otros cultivos importantes son las papas, la remolacha azucarera y las semillas oleaginosas.

En la última década, el gobierno ha estado fomentando la mecanización agrícola para aumentar los rendimientos y compensar la pérdida de trabajadores rurales que han emigrado a las ciudades. Según las estadísticas más recientes de la Organización de las Naciones Unidas para la Alimentación y la Agricultura, la tasa de crecimiento anual de la mecanización agrícola en China es del 6,38 por ciento. Para 2014, la tasa de mecanización integrada había aumentado a casi el 60 por ciento, con una producción de trigo que sobrepasaba el 90 por ciento y una producción de maíz cercana al 80 por ciento. Además del equipamiento agrícola estándar, como los tractores, las cooperativas agrícolas de China han comenzado a utilizar equipos de alta tecnología, incluidos vehículos aéreos no tripulados, que se utilizan para tratar los cultivos con pesticidas.

Durante el XIX Congreso Nacional del Partido Comunista de China fue presentada la nueva estrategia de desarrollo rural con el objetivo de mejorar el sector agrícola y elevar las condiciones de vida de los agricultores. El gobierno ha reconocido la brecha de desarrollo que existe entre lo urbano y lo rural y para ello ha fijado un plan de intervención con las siguientes fechas: para 2020 se deberá haber establecido un marco institucional y un plan político. Para 2035 se ha fijado como objetivo que todos los habitantes del medio rural puedan acceder a los mismos servicios que los habitantes de las ciudades como educación, sanidad e infraestructuras. Para 2050 se espera que la nueva estrategia haya alcanzado sus objetivos de desarrollo rural.

La industria y la construcción representan el 46,8% del PIB de China. Entre los años 2011 y 2013, China utilizó más cemento del que consumió Estados Unidos durante todo el siglo XX. En 2009, alrededor del 8% de la producción industrial total en el mundo provenía de China y ese año ocupó el tercer lugar mundial en producción industrial (el primero fue en la UE y el segundo en Estados Unidos). La investigación de "IHS Global Insight" indica que en 2010 China contribuyó con el 19,8% de la producción manufacturera mundial y se convirtió en el mayor fabricante del mundo ese año, después de que EE. UU. y mantuvo esa posición durante aproximadamente 110 años.

En noviembre de 2012, el Consejo de Estado de la República Popular de China ordenó una "evaluación del riesgo social" para todos los proyectos industriales importantes. Este requisito siguió a protestas públicas masivas en algunos lugares para proyectos o expansiones planificadas.

Las principales industrias incluyen el procesamiento de minerales; hierro y acero; aluminio; carbón; maquinaria; armamento; textiles y prendas de vestir; petróleo; cemento; químico; fertilizantes; procesamiento de alimentos; automóviles y otros equipos de transporte, incluidos vagones y locomotoras, barcos y aeronaves; productos de consumo, incluidos calzado, juguetes y productos electrónicos; telecomunicaciones y tecnología de la información. China se ha convertido en un destino preferido para la reubicación de instalaciones de fabricación globales. Su fortaleza como plataforma de exportación ha contribuido a los ingresos y el empleo en China.

Desde la fundación de la República Popular, se ha prestado considerable atención al desarrollo industrial; a partir de 2011, el 46% de la producción nacional de China siguió dedicada a la inversión; un porcentaje mucho más alto que cualquier otra nación. Entre las diversas ramas industriales, las industrias metalmecánica y de construcción de maquinaria han recibido la más alta prioridad. Estas dos áreas solas ahora representan alrededor del 20-30 por ciento del valor bruto total de la producción industrial. En estos, como en la mayoría de las otras áreas de la industria, sin embargo, la innovación en general ha sufrido a manos de un sistema que ha recompensado los aumentos en la producción bruta en lugar de mejoras en la variedad, sofisticación y calidad. China, por lo tanto, todavía importa cantidades significativas de aceros especializados. La producción industrial general ha crecido a una tasa promedio de más del 10 por ciento anual, superando a todos los demás sectores en cuanto al crecimiento económico y el grado de modernización. Algunas industrias y productos pesados ​​que se consideran de importancia estratégica nacional siguen siendo propiedad del Estado, pero una proporción cada vez mayor de industrias ligeras y orientadas al consumidor son empresas privadas o empresas conjuntas privadas.

En 2012, China fue el mayor productor de acero en el mundo, produciendo el 45% del total, 683 millones de toneladas, un aumento del 9% con respecto a 2010. 6 de cada 10 productores de acero más grandes del mundo están en China. Las ganancias son bajas a pesar de la continua alta demanda debido a la alta deuda y la sobreproducción de productos de alta gama producidos con los equipos financiados por la alta deuda. El gobierno central es consciente de este problema, pero no existe una manera fácil de resolverlo, ya que los gobiernos locales apoyan firmemente la producción local de acero. Mientras tanto, cada empresa aumenta agresivamente la producción. La producción de mineral de hierro se mantuvo a la par de la producción de acero a principios de la década de 1990, pero pronto fue superada por el mineral de hierro importado y otros metales a principios de la década de 2000. La producción de acero, estimada en 140 millones de toneladas en el año 2000, aumentó a 419 millones de toneladas en 2006. Gran parte de la producción de acero del país proviene de un gran número de centros de producción en pequeña escala, uno de los más grandes es Anshan en Liaoning.

China fue el principal exportador de acero del mundo en 2008. Los volúmenes de exportación en 2008 fueron de 59,23 millones de toneladas, un 5,5% menos que el año anterior. El declive puso fin al crecimiento de las exportaciones de acero de China, que duró una década. A partir de 2012, las exportaciones de acero se enfrentaron a impuestos antidumping generalizados y no habían vuelto a los niveles previos a 2008. La demanda interna se mantuvo fuerte, particularmente en el oeste del país, que ha experimentado un notable desarrollo, donde la producción de acero en Xinjiang se estaba expandiendo.

El 26 de abril de 2012, el regulador bancario de China emitió una advertencia de cautela con respecto a prestar dinero a compañías siderúrgicas que, como las ganancias de la fabricación y venta de acero han disminuido, algunas veces han usado dinero prestado con fines especulativos. Según la Asociación China del Hierro y el Acero, la industria siderúrgica china perdió 1 mil millones de Rmb en el primer trimestre de 2012, su primera pérdida desde 2000.

China recortó 45 millones de toneladas de capacidad de producción de acero en 2016 y 250 millones de toneladas de carbón en 2016. El Ministerio de Industria e Información publicó el 12 de agosto de 2016 un proyecto de nueva reglamentación y aplicación de normas más estrictas para resolver el problema de sobreproducción de acero, carbón, cemento, vidrio y aluminio: se aplicará una "política de crédito diferencial" a las empresas que pueden reestructurarse, mientras que las empresas que no se ajusten a los planes se verán privadas de financiación.

En 2006, China se convirtió en el tercer fabricante mundial de vehículos automotores (después de EE. UU. Y Japón) y el segundo consumidor más grande (solo después de los EE. UU.). La fabricación de automóviles se ha disparado durante el período de la reforma. En 1975, solo se produjeron 139.800 automóviles al año, pero en 1985 la producción había llegado a 443.377, luego aumentó a casi 1,1 millones en 1992 y aumentó de manera bastante uniforme cada año hasta 2001, cuando alcanzó los 2,3 millones. En 2002 la producción aumentó a casi 3.25 millones y luego saltó a 4.44 millones en 2003, 5.07 millones en 2004, 5.71 millones en 2005, 7.28 millones en 2006, 8.88 millones en 2007, 9.35 millones en 2008 y 13.83 millones en 2009. China se ha convertido el fabricante de automóviles número uno en el mundo en 2009. Las ventas nacionales han seguido el ritmo de la producción. Después de respetables aumentos anuales a mediados y finales de la década de 1990, las ventas de automóviles se dispararon a principios de la década de 2000. En 2006, se vendieron un total de 7.22 millones de automóviles, incluyendo 5.18 millones de unidades de automóviles y 2.04 millones de unidades de vehículos comerciales.

En 2010, China se convirtió en el mayor fabricante de vehículos automotores del mundo y en el mayor consumidor por delante de EUA o Estados Unidos, con aproximadamente 18 millones de automóviles nuevos vendidos. Sin embargo, las ventas de automóviles nuevos crecieron solo en un 1% estimado entre 2011 y 2012 debido a la escalada en la disputa de las Islas Spratly, que involucró a Japón, el tercer productor mundial de vehículos.

La industria automotriz de China comenzó a exportar piezas de automóviles en 1999. China comenzó a planificar grandes movimientos en el negocio de exportación de automóviles y componentes a partir de 2005. Una nueva planta de Honda en Guangzhou se construyó en 2004 únicamente para el mercado de exportación con capacidad de despachar 30.000 vehículos de pasajeros anuales a Europa desde el 2005 en adelante. Para el 2004, 12 de los principales fabricantes de automóviles extranjeros tenían plantas de "joint-venture" en China. Produjeron una amplia gama de automóviles, minivans, vehículos utilitarios deportivos, autobuses y camiones. En 2003, China exportó vehículos y componentes por valor de 4.700 millones de dólares estadounidenses. La exportación de vehículos fue de 78.000 unidades en 2004, 173.000 unidades en 2005 y 340.000 unidades en 2006. La exportación de vehículos y componentes superó los 70 mil millones de dólares en 2010.

El gobierno chino anunció el objetivo de 5 millones de vehículos eléctricos e híbridos en circulación para el año 2020. Para lograr este objetivo, las autoridades ya han realizado una inversión de 54 mil millones de dólares en la década de 2000-2010 para desarrollar la tecnología baterías de vehículos eléctricos, y se comprometen a invertir otros 10 mil millones adicionales para desarrollar la infraestructura de carga para 2020.

Se realizaron importantes inversiones en la fabricación de paneles solares y generadores eólicos por parte de varias empresas, respaldadas por préstamos liberales de bancos y gobiernos locales. Sin embargo, para 2012 la capacidad de fabricación había superado con creces la demanda interna y mundial de ambos productos, en particular los paneles solares, que fueron objeto de sanciones antidumping tanto por los Estados Unidos como por Europa. La sobreoferta global ha provocado recortes de producción tanto dentro como fuera de China. China ha presupuestado 50 mil millones de dólares para subsidiar la producción de energía solar durante las dos décadas posteriores a 2015 pero, incluso a un precio muy reducido como resultado del exceso de oferta, a partir de 2012 el costo de la energía solar en China sigue siendo tres veces mayor que la energía no renovable.

China es el mayor productor mundial de juguetes sexuales y representa el 70% de la producción mundial total. Hay más de 1.000 fabricantes activos en esta industria, que en conjunto generan alrededor de dos mil millones de dólares al año.

Desde 2011, China es el mayor mercado mundial de computadoras personales.

Desde 1980, la producción de energía de China ha crecido de manera significativa. Alrededor del 80 por ciento de toda la energía se genera a partir de combustibles fósiles en plantas térmicas, con alrededor del 17 por ciento en instalaciones hidroeléctricas; solo alrededor del dos por ciento proviene de la energía nuclear, principalmente de plantas ubicadas en Guangdong y Zhejiang. Aunque China tiene un rico potencial energético, todavía no ha alcanzado su máximo potencial de producción. Además, la distribución geográfica de la energía aleja a la mayoría de estos recursos de sus principales usuarios industriales. Básicamente, el noreste es rico en carbón y petróleo, la parte central del norte de China tiene abundante carbón y el suroeste tiene un inmenso potencial hidroeléctrico. Pero las regiones industrializadas alrededor de Guangzhou y la región del Bajo Yangtze alrededor de Shanghái tienen muy poca energía, mientras que hay relativamente poca industria pesada localizada cerca de las principales áreas de recursos energéticos que no sean en la parte sur del noreste.

Debido en gran parte a las preocupaciones ambientales, el país ha querido reducir la dependencia del carbón, que representa el 70-75% de la producción energética hacia una mayor dependencia del petróleo, gas natural, energía renovable y nuclear. China ha cerrado miles de minas de carbón en la última década para reducir la sobreproducción. Según las estadísticas chinas, esto ha reducido la producción de carbón en más del 25%.

Desde 1993, China ha sido un importador neto de petróleo, gran parte del cual proviene de Próximo Oriente. El petróleo importado representa el 20% del crudo procesado en China. Hay un interés en diversificar las fuentes de sus importaciones de petróleo y ha invertido en yacimientos petrolíferos en todo el mundo. Está desarrollando importaciones de petróleo de Asia Central y el país ha invertido en yacimientos petrolíferos de Kazajistán. Beijing también planea aumentar la producción de gas natural de China, que actualmente representa solo el 3% del consumo total de energía de China e incorporó una estrategia de gas natural en su décimo plan quinquenal (2001-2005), con el objetivo de expandir el uso de gas desde un 2% de la participación en la producción total de energía hasta el 4% para 2005 (el gas representa el 25% de la producción de energía en los Estados Unidos).

El XI Programa Quinquenal (2006-10), anunciado en 2005 y aprobado por el Congreso Nacional del Pueblo en marzo de 2006, solicitó mayores medidas de conservación de energía, incluido el desarrollo de fuentes de energía renovables y una mayor atención a la protección ambiental. Las directrices pedían una reducción del 20 por ciento en el consumo de energía por unidad de PIB para 2010. Alejarse del carbón hacia fuentes de energía más limpias, incluyendo petróleo, gas natural, energía renovable y energía nuclear es un componente importante del programa de desarrollo de China. Beijing también tiene la intención de seguir mejorando la eficiencia energética y promover el uso de tecnología de carbón limpio. China tiene abundantes recursos hidroeléctricos; la presa de las Tres Gargantas, por ejemplo, tiene una capacidad instalada de 22 500 MW. Además, se prevé que la participación de la electricidad generada por energía nuclear aumentará del 1% en 2000 al 5% en 2030. La ley de energía renovable de China, que entró en vigor en 2006, exige que el 10% de su energía provenga de energía renovable hacia 2020.

China posee importantes reservas minerales, de las cuales la más importante es el carbón. Los recursos minerales de China incluyen además grandes reservas mineral de hierro y de casi todos los demás minerales industriales. Aunque los depósitos de carbón están muy dispersos (se encuentra algo de carbón en cada provincia), la mayor parte se encuentran al norte del país. La provincia de Shanxi, de hecho, se cree que contiene aproximadamente la mitad del total; otras provincias importantes que contienen carbón incluyen Heilongjiang, Liaoning, Jilin, Hebei y Shandong. Además de estas provincias del norte, hay cantidades importantes de carbón en Sichuan, y hay algunos depósitos de importancia en Guangdong, Guangxi, Yunnan y Guizhou. Una gran parte de las reservas del país se compone de buen carbón bituminoso, pero también hay grandes depósitos de lignito. La antracita está presente en varios lugares (especialmente Liaoning, Guizhou y Henan), pero en general no es muy significativa.

Para garantizar una distribución más uniforme de los suministros de carbón y reducir la presión sobre una red de transporte todavía inadecuada, las autoridades presionaron para que se desarrollara un gran número de pequeñas minas de alcance local por todo el país. Esta campaña fue enérgicamente perseguida después de la década de 1960, con el resultado de que se han establecido miles de pequeños pozos y producen más de la mitad del carbón del país. Sin embargo, esta salida suele ser costosa y se usa para el consumo local. También ha llevado a una implementación menos que estricta de las medidas de seguridad en estas minas no reguladas, que causan varios miles de muertes cada año.

El carbón representa la mayor parte del consumo de energía de China (el 70% en 2005), y China es el mayor productor y consumidor de carbón en el mundo. A medida que la economía de China continúa en expansión, se prevé que la demanda de carbón de China aumente significativamente. Aunque la participación del carbón en el consumo total de energía de China disminuirá, el consumo de carbón seguirá aumentando en términos absolutos mientras se desarrollan otras alternativas. La creciente dependencia de China del carbón como fuente de energía ha contribuido significativamente a poner a China en el camino de convertirse en el mayor emisor mundial de dióxido de azufre causante de la lluvia ácida y gases de efecto invernadero, incluido el dióxido de carbono.

La caída de los precios del carbón en 2015 provocó despidos en las minas de carbón en el noreste.

Los recursos petrolíferos en tierra de China se encuentran principalmente en el noreste y en las provincias de Xinjiang, Gansu, Qinghai, Sichuan, Shandong y Henan. La lutita bituminosa se encuentra en varios lugares, especialmente en Fushun en Liaoning, donde los depósitos se encuentran en las reservas de carbón, así como en Guangdong. Se ha encontrado petróleo liviano de alta calidad en el estuario del río de las Perlas en el Mar de China Meridional, la Cuenca de Qaidam en Qinghai y la Cuenca de Tarim en Xinjiang. El país consume la mayor parte de su producción de petróleo, pero exporta petróleo crudo y productos derivados del petróleo. China ha explorado y desarrollado yacimientos de petróleo en el Mar Meridional de China y el Mar Oriental de China, el Mar Amarillo, el Golfo de Tonkin y el Mar de Bohai.

En 2013, el ritmo del crecimiento económico de China excedió la capacidad nacional de producción de petróleo y a mediados de ese mismo año las inundaciones dañaron los campos petrolíferos de la nación. En consecuencia, China importó petróleo para compensar la reducción de la oferta y superó a los EE. UU. en septiembre de 2013 para convertirse en el mayor importador mundial de petróleo.

Se desconoce la extensión total de las reservas de gas natural de China, ya que todavía no se ha explotado de manera extensiva. Sichuán representa casi la mitad de las reservas y la producción de gas natural conocidas. La mayor parte del resto es gas asociado producido en los principales yacimientos petrolíferos del noreste, especialmente en el campo petrolero Daqing. Se han encontrado otros depósitos de gas en la Cuenca de Qaidam, Hebei, Jiangsu, Shanghái y Zhejiang, y en el litoral al suroeste de la isla de Hainan. Según un artículo publicado en "Energy Economics" en 2011 por los economistas Mete Feridun (Universidad de Greenwich) y Abdul Jalil (Universidad de Wuhan en China), el desarrollo financiero ha conducido a disminución de la contaminación ambiental. Los autores concluyen que las emisiones de carbono están determinadas principalmente por los ingresos, el consumo de energía y la apertura comercial, y sus hallazgos confirman la existencia de una curva de Kuznets ambiental en el caso de este país.

China tiene un gran potencial para la producción de energía hidroeléctrica debido a su considerable red fluvial y terreno montañosos. La mayor parte de la capacidad hidroeléctrica total se encuentra en el suroeste del país, donde el suministro de carbón es deficiente pero donde la demanda de energía está aumentando rápidamente. El potencial en el noreste es bastante pequeño, pero fue allí donde las primeras centrales hidroeléctricas fueron construidas por los japoneses durante su ocupación de Manchuria. Debido a las considerables fluctuaciones estacionales de las precipitaciones, el flujo de los ríos tiende a disminuir durante el invierno, obligando a muchas centrales a operar a una capacidad inferior a la normal, mientras que en el verano, por otra parte, las inundaciones a menudo interfieren con la generación. Tras trece años de construcción a un costo de 24 mil millones de dólares, la inmensa presa de las Tres Gargantas al otro lado del río Yangtsé se completó esencialmente en 2006 y comenzó a operar en 2012.

Pese al aumento de la producción debido en parte gracias a la modernización de los procesos y equipos mineros, la rápida industrialización de China requiere importaciones de minerales del exterior. En particular, las importaciones de mineral de hierro de Australia y los Estados Unidos se han disparado a principios de la década de 2000, ya que la producción de acero superó rápidamente la producción nacional de mineral de hierro. También China se ha vuelto cada vez más activa en varios países africanos para explotar las reservas que requiere para el crecimiento económico, especialmente en países como la República Democrática del Congo y Gabón.

Las principales áreas de producción en 2004 fueron carbón (casi 2 mil millones de toneladas), mineral de hierro (310 millones de toneladas), petróleo crudo (175 millones de toneladas), gas natural (41 millones de metros cúbicos), mineral de antimonio (110,000 toneladas), concentrados de estaño (110,000 toneladas), mineral de níquel (64,000 toneladas), concentrados de tungsteno (67,000 toneladas), sal sin refinar (37 millones de toneladas), vanadio (40,000 toneladas) y mineral de molibdeno (29,000 toneladas). En orden de magnitud, los minerales producidos fueron bauxita, yeso, barita, magnesita, talco y minerales relacionados, mineral de manganeso, espato flúor y zinc. Además, China produjo 2.450 toneladas de plata y 215 toneladas de oro en 2004. El sector minero representó menos del 0,9% del empleo total en 2002, pero produjo alrededor del 5,3% de la producción industrial total.

Las reservas de mineral de hierro se encuentran en la mayoría de las provincias, incluida Hainan. Las provincias de Gansu, Guizhou, Sichuan meridional y Guangdong tienen abundantes depósitos. Las mayores reservas minadas se encuentran al norte del río Yangtsé y abastecen a las empresas siderúrgicas vecinas. Con la excepción del níquel, el cromo y el cobalto, China está bien provista de ferroaleaciones y manganeso. Las reservas de tungsteno también son bastante grandes. Los recursos de cobre son moderados, y el mineral de alta calidad solo está presente en algunos depósitos. El plomo y el zinc están disponibles, y se cree que los recursos de bauxita son abundantes. Las reservas de antimonio de China son las más grandes del mundo. Los recursos de estaño son abundantes e importantes reservas de oro. China es el quinto mayor productor mundial de oro y, a principios del siglo XXI, se convirtió en un importante productor y exportador de metales raros necesarios para las industrias de alta tecnología.

China también produce una amplia gama de minerales no metálicos. Uno de los más importantes es la sal, que se deriva de sitios de evaporación costera en Jiangsu, Hebei, Shandong y Liaoning, así como de extensos campos de sal en Sichuan, Ningxia y la Cuenca de Qaidam. Hay depósitos importantes de roca de fosfato en varias áreas; Jiangxi, Guangxi, Yunnan y Hubei. La producción se ha estado acelerando cada año. A partir de 2013, China está produciendo 97,000,000 de toneladas métricas de roca de fosfato por año. Las piritas están presentes en varios lugares; Liaoning, Hebei, Shandong y Shanxi tienen los depósitos más importantes. China también posee grandes recursos de fluorita (espato flúor), yeso y amianto, y posee las reservas y la producción de cemento, clínker y piedra caliza más grandes del mundo.

Desde 2010, China ocupa el tercer lugar en el mundo en el sector servicios, después de los Estados Unidos y Japón, y la alta densidad de telecomunicaciones y de alta potencia ha asegurado que el país se mantenga en una trayectoria de alto crecimiento en el largo plazo. En 2010, el sector de servicios produjo el 43% del PIB anual de China, solo superado por las manufacturas. Sin embargo, su proporción del PIB sigue siendo baja en comparación con la proporción en los países más desarrollados, y el sector agrícola todavía emplea una fuerza de trabajo más grande.

Antes del inicio de las reformas económicas en 1978, el sector de servicios de China se caracterizaba por tiendas estatales, racionamiento y precios regulados; con la reforma llegaron los mercados privados, los empresarios individuales y un sector comercial. El comercio mayorista y minorista se ha expandido rápidamente, con numerosos centros comerciales, tiendas, cadenas de restaurantes y hoteles construidos en áreas urbanas. La administración pública sigue siendo un componente principal del sector de los servicios, mientras que el turismo se ha convertido en un factor importante en el empleo y una fuente de divisas.

Chengdu, China, posee edificios singulares como el New Century Global Centre, con 100 metros de altura, 500 m de largo y 400 m ancho, alberga tiendas, un cine de 14 salas, oficinas, hoteles, el parque acuático "Paradise Island", una playa artificial, una pantalla LED de 150 m, pista de patinaje, barco pirata, una recreación de una aldea mediterránea, sol artificial las 24 horas y 15.000 plazas de párking.

China posee un sistema de comunicaciones diversificado que conecta todas las partes del país por Internet, teléfono, telégrafo, radio y televisión.

El número de usuarios de Internet o cibernautas de China superó los 137 millones hacia fines de 2006, un aumento del 23,4% respecto del año anterior y 162 millones hacia junio de 2007, lo que convierte a China en el segundo usuario de Internet después de Estados Unidos, según Ministerio de Industria de la Información de China (MII). La tasa de penetración de teléfonos móviles de China fue del 34% en 2007. En 2006, los usuarios de teléfonos móviles enviaron 429 mil millones de mensajes de texto (en promedio, 967 mensajes de texto por usuario). Para 2006, el número de líneas fijas aumentó en un 79%, principalmente en las zonas rurales.

La industria turística de China es una de las actividades económicas de más rápido crecimiento en la economía nacional y también es una de las industrias con mayor ventaja competitiva global. Según el Consejo Mundial de Viajes y Turismo, los viajes y el turismo aportaron directamente 16 mil millones de dólares a la economía china (alrededor del 2,6% del PIB). En 2011, el total de llegadas de turistas internacionales fue de 58 millones, y los ingresos por turismo internacional fueron de 48 mil millones de dólares.

El turismo nacional representa más del 90% del tráfico turístico del país y contribuye con más del 70% de los ingresos totales del sector. En 2002, los turistas nacionales llegaron a 878 millones y los ingresos por turismo fueron de 46.9 mil millones de dólares. Una clase media emergente con un fuerte poder de consumo está cobrando protagonismo en China, especialmente en las ciudades principales. Los turistas de China que salieron del país llegaron a 20.22 millones en 2003, superando a Japón por primera vez.

La Organización Mundial del Turismo prevé que la industria turística de China asuma hasta el 8,6% de la cuota de mercado mundial para convertirse en la principal industria turística del mundo para 2020.

China superó en 2014 a los EE. UU. en el gasto por viajes de negocios. Según un estudio de "Global Business Travel Association", el gasto total en viajes de negocios llegó a 195 mil millones de dólares en 2012.

Debido a su impresionante peso demográfico dentro del planeta y al incremento de sus principales ratios económicos, China está llamada a ser la primera potencia económica mundial en los próximos años. Sin embargo, es necesario contextualizar ese liderazgo económico: así como en las otras grandes potencias actuales (especialmente las occidentales, como Estados Unidos y la Unión Europea) existe una correlación clara entre variables económicas absolutas (PIB) y relativas (renta per cápita) en el caso de China esto no es así. Aunque haya experimentado un aumento del 485% de su renta per cápita en la última década (pasando de 930$ en el año 2000 a 5.445$ en el año 2010), su riqueza se encuentra muy desigualmente repartida, generando grandes brechas sociales y mermando la competitividad media por habitante. Por tanto, su ascenso económico se apoya de momento en su ingente cantidad de habitantes (la quinta parte del planeta) al igual que otro país con características demográficas similares, como la India. A pesar de ello, es necesario recalcar que China mejora constantemente indicadores macroeconómicos relevantes como el índice de competitividad global, generado por el Foro Económico Mundial, el porcentaje de usuarios con acceso a Internet o incluso, el promedio de días para montar una empresa. La siguiente tabla muestra un resumen de la situación:

Las principales iniciativas económicas chinas a nivel internacional establecen tres grandes objetivos: (I) Implantar un modelo económico alternativo de economía global frente al liderazgo de Occidente, especialmente de EE. UU.; (II) favorecer la integración económica de Eurasia y del eje Indo-Pacífico para reordenar a largo plazo la estructura de la economía mundial; y (III) consolidar un nuevo orden en Asia, con el liderazgo de China.

Se presentan a continuación las mercancías de mayor peso en las importaciones de China para el período 2010-hasta mayo 2015. Las cifras están expresadas en dólares estadounidenses valor FOB.

Se presentan a continuación los principales socios comerciales de China para el periodo 2010-hasta mayo 2015. La mayoría de sus importadores están en Asia salvo Estados Unidos, Alemania y el Reino Unido. Las cifras expresadas son en dólares estadounidenses valor FOB.





</doc>
<doc id="41375" url="https://es.wikipedia.org/wiki?curid=41375" title="MVRDV">
MVRDV

MVRDV es una oficina de Arquitectura y Urbanismo fundada en Róterdam en 1993. La palabra está formada por las iniciales de los apellidos de sus fundadores Winy Maas, Jacob van Rijs y Nathalie de Vries.

MVRDV produce diseños y estudios en los campos de la arquitectura, el urbanismo y el diseño del paisaje. Sus primeros proyectos, como la sede de la emisora pública neerlandesa VPRO y las viviendas WoZoCo para ancianos en Ámsterdam les llevaron al reconocimiento internacional.
MVRDV actualmente trabaja en diversos proyectos: viviendas en los Países Bajos, España, China, Francia, Austria, el Reino Unido, Estados Unidos, Corea y otros países, un centro de televisión en Zúrich, una biblioteca pública de Spijkenisse (Países Bajos), un mercado central de la sala Róterdam, una plaza de la cultura en Nanjing (China), planes directores de gran escala urbana en Oslo (Noruega) y en Tirana (Albania), Alemania, un plan director para una eco-ciudad en Logroño (España) y un plan director de investigación para el futuro crecimiento de París.

Iniciaron la empresa después de ganar el Premio Europan Berlín. Su sede fue integrada en una antigua imprenta diseñada por Kromhout.


Además son finalistas en los concursos para la construcción de la villa Olímpica de Nueva York y el Proyecto Les Halles de París.



</doc>
<doc id="41377" url="https://es.wikipedia.org/wiki?curid=41377" title="Cortocircuito">
Cortocircuito

Se denomina cortocircuito al fallo en un aparato o línea eléctrica por el cual la corriente eléctrica pasa directamente del conductor activo o fase al neutro o a tierra en sistemas monofásicos de corriente alterna, entre dos fases o igual al caso anterior para sistemas polifásicos, o entre polos opuestos en el caso de corriente continua. Es decir: es un defecto de baja impedancia entre dos puntos de potencial diferente y produce arco eléctrico, esfuerzos electrodinámicos y esfuerzos térmicos.

El cortocircuito se produce normalmente por los fallos en el aislante de los conductores, cuando estos quedan sumergidos en un medio conductor como el agua o por contacto accidental entre conductores aéreos por fuertes vientos o rotura de los apoyos.

Debido a que un cortocircuito puede causar importantes daños en las instalaciones eléctricas e incluso incendios en edificios, estas instalaciones están normalmente dotadas de fusibles o interruptores magnetotérmicos a fin de proteger a las personas y los objetos. Además cabe señalar que en sistemas de corriente alterna se producen por diferencia en ángulos de desfase eléctrico.

En un circuito cerrado el voltaje, corriente eléctrica y la resistencia deben tener valores debidamente controlados para un buen funcionamiento del sistema. Una condición de cortocircuito queda determinada al eliminarse, desde el punto de vista práctico, la resistencia de consumo del circuito. Según la ley de Ohm se tiene que

Por tanto, si la resistencia se disminuye aproximadamente a cero la intensidad de la corriente tiende a infinito. Esta situación se da, por ejemplo, al caer una barra de metal sobre los conductores y formar un puente. En este caso se dice que han quedado "puenteados" el vivo o fase y el neutro del circuito, oponiendo este una resistencia prácticamente igual a 0 al paso de corriente eléctrica.

Según el efecto Joule la corriente que circula por un conductor genera un calor que puede determinarse según la relación:

Por lo que si la corriente adquiere valores excesivos, la cantidad de calor puede ser tal que puede fundir casi instantáneamente los conductores del circuito, siendo este el fenómeno más apreciable en un cortocircuito.



</doc>
<doc id="41378" url="https://es.wikipedia.org/wiki?curid=41378" title="Albert Speer">
Albert Speer

Berthold Konrad Hermann Albert Speer (Mannheim, 19 de marzo de 1905 — Londres, 1 de septiembre de 1981) fue un arquitecto alemán que ejerció como Ministro de Armamento y Producción de Guerra de la Alemania nazi durante gran parte de la Segunda Guerra Mundial. Aliado cercano de Adolf Hitler, fue condenado en los juicios de Núremberg y sentenciado a veinte años de prisión.

Arquitecto de formación, Speer se afilió al Partido Nazi en 1931. Sus conocimientos de arquitectura le permitieron destacar dentro del partido y llegó a convertirse en miembro del círculo más cercano de Hitler. El "Führer" le encargó el diseño y construcción de edificios como la Cancillería del Reich y el Campo Zeppelín para los Congresos de Núremberg. En 1937 Hitler lo nombró Inspector general de edificios de Berlín, un cargo desde el que fue el responsable del Departamento Central de Reasentamiento que desalojó a propietarios judíos de sus hogares en la capital alemana. En febrero de 1942, Speer fue designado Ministro de Armamento y Producción de Guerra del Reich. Sirviéndose de estadísticas manipuladas, se promocionó a sí mismo como el responsable del «milagro del armamento» que permitió que Alemania se mantuviera en la guerra. En 1944 creó un equipo de trabajo para aumentar la producción de aviones de combate. También fue uno de los responsables principales en la explotación de trabajadores forzados en beneficio del esfuerzo de guerra alemán.

Después de la guerra, Speer estuvo entre los 24 «principales » arrestados y acusados de los crímenes del régimen nazi en los juicios de Núremberg. Fue declarado culpable de crímenes de guerra y crímenes de lesa humanidad, principalmente por el uso de trabajadores forzados, y evitó por poco la pena de muerte. Tras cumplir toda su condena, fue puesto en libertad en 1966. Utilizó sus escritos de prisión como base de dos libros autobiográficos, "Memorias: Hitler y el Tercer Reich vistos desde dentro" y "Diario de Spandau". Sus libros fueron un éxito porque a los lectores les fascinaba tener una visión del interior del Tercer Reich. Albert Speer falleció por un infarto cerebral durante una visita a Londres en 1981. Se conserva muy poco de sus obras de arquitectura.

A través de sus autobiografías y entrevistas, Speer construyó cuidadosamente una imagen de sí mismo como un hombre que lamentaba profundamente no haber descubierto los monstruosos crímenes del Tercer Reich. Continuó negando el conocimiento explícito y la responsabilidad del Holocausto, una imagen de su figura que predominó en la historiografía en las décadas posteriores a la guerra, durante las cuales se creó «El mito de Speer»: era visto como un tecnócrata apolítico responsable de revolucionar la máquina de guerra alemana. El mito comenzó a desmoronarse en la década de 1980, cuando el milagro armamentístico se atribuyó a la propaganda nazi. El historiador británico Adam Tooze escribió que la idea de que Speer era un tecnócrata apolítico era «absurda», mientras que Martin Kitchen declaró que gran parte del aumento en la producción de armas de Alemania se debió en realidad a los sistemas creados por su predecesor Fritz Todt y, además, que Speer estuvo íntimamente involucrado en la «Solución final».

Albert Speer nació en Mannheim, en el seno de una familia adinerada de clase media–alta, segundo de los tres hijos de Luise Máthilde Wilhelmine (Hommel) y Albert Friedrich Speer. En 1918 la familia arrendó su residencia en Mannheim y se trasladó a una casa que tenían en Heidelberg. De acuerdo con Henry T. King, fiscal adjunto en los juicios de Núremberg que más tarde escribió un libro sobre Speer, «el amor y la calidez escaseaban en el hogar de juventud de Speer». Sus hermanos Ernst y Hermann lo acosaron durante la infancia. Speer fue un activo deportista que practicó esquí y montañismo. Siguió los pasos de su padre y su abuelo y estudió arquitectura. 

Comenzó sus estudios de arquitectura en la Universidad de Karlsruhe en lugar de en una institución más prestigiosa por culpa de la crisis de hiperinflación de 1923, que limitó los ingresos de sus padres. Al año siguiente, con el alivio de la crisis, se trasladó a la «mucho más reputada» Universidad Técnica de Múnich y en 1925 se trasladó de nuevo, en este caso a la Universidad Técnica de Berlín, donde estudió con Heinrich Tessenow, a quien Speer admiraba. Tras aprobar sus exámenes en 1927, Speer se convirtió en asistente de Tessenow, un gran honor a sus 22 años porque así pudo impartir algunas de las clases de Tessenow mientras continuaba con sus estudios de posgrado. En Múnich y Berlín comenzó una estrecha amistad que duraría medio siglo con Rudolf Wolters, quien también estudió con Tessenow.

A mediados de 1922 Speer empezó a cortejar a Margarete (Margret) Weber (1905–1987), hija de un próspero artesano que empleaba a 50 trabajadores. Esta relación no era aprobada por la conciencia de clase de su madre, que sentía que los Weber eran de clase inferior, a pesar de lo cual la pareja contrajo matrimonio en Berlín el 28 de agosto de 1928; pasaron siete años hasta que Margarete Speer fue invitada a quedarse en casa de sus suegros. La pareja tuvo seis hijos, pero Speer se distanció progresivamente de su familia a partir de 1933 y también después de salir de prisión en 1966, a pesar de los esfuerzos de él por forjar vínculos más estrechos.

En enero de 1931, Speer solicitó la afiliación al Partido Nazi y, el 1 de marzo de 1931, se convirtió en el miembro número 474 481. Ese mismo año, con los estipendios reduciéndose en medio de la Gran Depresión, renunció a su puesto como asistente de Tessenow y se mudó a Mannheim, con la esperanza de ganarse la vida como arquitecto. Tras fracasar en este empeño, su padre le dio un trabajo a media jornada como gerente de sus propiedades. En julio de 1932, los Speer visitaron Berlín para dar apoyo al Partido antes de las elecciones al Reichstag y mientras estaban allí su amigo, el oficial del partido nazi Karl Hanke, recomendó al joven arquitecto a Joseph Goebbels para ayudar a renovar la sede central del partido en Berlín. Cuando completó este encargo, Speer regresó a Mannheim mientras Hitler asumía el cargo de canciller en enero de 1933.
Los organizadores del congreso del partido Nazi en Núremberg en 1933 llamaron a Speer para que presentara sus diseños y le pusieron en contacto con Hitler por primera vez. Ni los organizadores ni Rudolf Hess estaban dispuestos a decidir qué plan era aprobado, por lo que Hess envió a Speer al apartamento de Hitler en Múnich para buscar su aprobación. Este trabajo le dio su primer puesto nacional como «Comisionado para la Presentación Artística y Técnica de los Congresos y Demostraciones del Partido».

Poco después de llegar al poder, Hitler comenzó a hacer planes para reconstruir la cancillería y a finales de 1933 contrató a Paul Troost para renovar todo el edificio y a Speer, cuyo trabajo para Goebbels lo había impresionado, para gestionar las obras. Como canciller, Hitler tenía una residencia en el edificio e iba todos los días para ser informado por Speer sobre el progreso de la construcción. Tras uno de estos encuentros, Hitler lo invitó a almorzar, para gran excitación del arquitecto. Este se convirtió rápidamente en parte del círculo íntimo de Hitler, y esperaba la llamada del canciller por la mañana para pasear o charlar, ofrecer asesoramiento en materia arquitectónica y discutir sus ideas. La mayoría de los días lo invitaban a cenar.

En sus memorias Speer afirma que el partido nazi le ofreció una «nueva misión» y en una entrevista con William Hamsher dijo que se unió al partido para «salvar a Alemania del comunismo». Después de la guerra, afirmó haber tenido poco interés en la política y que se había unido casi por casualidad. Al igual que muchos de los que ostentaron poder en el Tercer Reich, él no fue un ideólogo, sino un «un antisemita instintivo». El historiador Magnus Brechtken dijo que Speer no pronunciaba discursos antisemitas, que su antisemitismo se puede entender mejor a través de sus acciones antisemitas y que, a lo largo de su vida, sus motivaciones fueron acumular poder, gobernar y adquirir riqueza.

A la muerte de Troost el 21 de enero de 1934, Speer lo reemplazó como arquitecto jefe del partido. Hitler lo nombró dirigente de la Oficina Principal de la Construcción, puesto con el que entró nominalmente en el equipo de Rudolf Hess. Uno de sus primeros encargos tras la muerte de Troost fue el Campo Zeppelín, el campo de desfiles militares que aparece en el documental de Leni Riefenstahl "El triunfo de la voluntad" y que tenía capacidad para 340 000 personas. Speer insistió en que se celebraran de noche el mayor número de espectáculos posibles tanto para resaltar los efectos de iluminación como para ocultar a los miembros del Partido Nazi, muchos de los cuales tenían sobrepeso. Núremberg fue sede de muchos de los edificios oficiales del Partido Nazi, pero algunos nunca se construyeron, como el "Deutsches Stadion", que podría haber acomodado a 400 000 espectadores. Modificó los planes de Werner March para el Estadio Olímpico de los Juegos Olímpicos de Berlín de 1936 añadiendo un exterior de piedra que gustó a Hitler. También diseñó el pabellón alemán para la Exposición Internacional de París de 1937. Mientras planeaba estas estructuras, inventó la teoría del «valor de la ruina» –"Die Ruinenwerttheorie"–: el edificio no solo está construido para ser utilizado por sus contemporáneos, sino también para suscitar la admiración de quienes lo encontrarán en el estado de vestigio mil años después.

En 1937 Hitler nombró a Speer Inspector General de los edificios de la capital del Reich con el rango de subsecretario de estado en el gobierno del Reich. Este puesto suponía extraordinarios poderes sobre el gobierno de la ciudad de Berlín y lo hizo responsable solo ante Hitler. También hizo al arquitecto miembro del Reichstag, aunque este organismo tenía entonces muy poco poder efectivo. El canciller le ordenó elaborar planos para reconstruir Berlín como capital del mundo, "Welthauptstadt Germania". Speer elaboró un trazado que se basaba en una larga avenida de cinco kilómetros que corría de norte a sur y que el arquitecto llamó "Prachtstrasse", la "Calle de la Magnificencia", o también el «Eje norte-sur». En el extremo norte de esta avenida dispuso la "Volkshalle", un enorme edificio de asamblea cerrado por una gigantesca cúpula de más de 200 metros de altura y con espacio interior para 180 000 personas. En el extremo sur de la avenida habría un arco de triunfo, también de colosales proporciones, de al menos 120 metros de altura y capaz de contener el Arco de Triunfo de París. La principal estación de ferrocarril existente iba a ser demolida y en su lugar se levantarían otras dos grandes estaciones. Contrató a Wolters para su equipo de diseño y le dio una especial responsabilidad para diseñar la "Prachtstrasse". El estallido de la Segunda Guerra Mundial en septiembre de 1939 obligó a posponer y finalmente abandonar estos planes.
Desde 1934 se estaba planificando la construcción de una nueva Cancillería del Reich. El terreno se adquirió a finales de 1934 y a comienzos de marzo de 1936 se derribaron algunos edificios para ensanchar la calle "Voßstraße". Speer estuvo implicado virtualmente en este proyecto. Tras la Noche de los cuchillos largos se le había encargado renovar el Palacio Borsig en la esquina de "Voßstraße" con "Wilhelmstraße" como sede de las "SA". Los trabajos preliminares para la nueva cancillería estuvieron completos en mayo de 1936. En junio de ese año el arquitecto cobró unos honorarios personales de 30 000 Reichsmark y estimó que la cancillería estaría terminada en un plazo de tres o cuatro años. Los planes detallados estuvieron listos en julio de 1937 y la primera estructura se completó el 1 de enero de 1938. Poco después, el 27 de enero, Speer recibió plenos poderes por parte de Hitler para que finalizara la nueva cancillería en la fecha límite del 1 de enero de 1939. Con fines propagandísticos, Hitler afirmó durante la ceremonia de finalización el 2 de agosto de 1938 que había ordenado a Speer que terminara la cancillería ese año. Debido a la escasez de mano de obra, Speer obligó a los obreros a trabajar en dos turnos de diez y doce horas. Las "Schutzstaffel" (SS) construyeron en 1938 dos campos de concentración y obligaron a los internos a extraer piedras para su construcción. A instancias de Speer, se construyó una fábrica de ladrillos cerca del campo de concentración de Oranienburg; cuando le comentaron las pobres condiciones del lugar, el arquitecto dijo: «Los judíos se acostumbraron a hacer ladrillos cuando eran esclavos de los egipcios». La cancillería estuvo terminada a comienzos de enero de 1939 y el edificio fue aclamado por Hitler como «la gloria suprema del gran imperio político alemán».
Durante el proyecto de la nueva cancillería se produjo el pogromo de la Noche de los cristales rotos ("Kristallnacht"). Speer no hizo mención a este suceso en el primer borrador de sus "Memorias", y fue solo tras el aviso urgente de su editor que añadió una reseña que afirma que vio desde su coche las ruinas de la Sinagoga Central de Berlín. La "Kristallnacht" aceleró los esfuerzos de Speer por desahuciar de sus hogares a los judíos de Berlín. De 1939 en adelante el Departamento que dirigía aplicó las Leyes de Núremberg para desalojar a los inquilinos judíos en Berlín, para así hacer sitio a inquilinos no judíos desplazados por la reurbanización o los bombardeos. Finalmente, 75 000 judíos resultaron desplazados por estas medidas. Speer negó que supiera que los estaban subiendo a los trenes del Holocausto y afirmó que los desplazados eran «completamente libres y que sus familias todavía estaban en sus apartamentos». También dijo: «... de camino a mi ministerio en la autopista de la ciudad, pude ver ... multitudes de personas en los andenes de la cercana Estación de ferrocarril Nikolassee. Sabía que estos debían ser judíos de Berlín que estaban siendo evacuados. Estoy seguro de que un sentimiento opresivo me golpeó al pasar. Presumiblemente sentí eventos sombríos». El historiador alemán Matthias Schmidt afirmó que Speer en persona inspeccionó campos de concentración y describió sus relatos como «una farsa absoluta». Martin Kitchen escribió que este repetía a menudo de manera hueca que no sabía nada de las «cosas terribles», porque no solo era plenamente consciente del destino de los judíos, sino que participaba activamente en su persecución.

Cuando Alemania hizo estallar la Segunda Guerra Mundial en Europa, Speer puso en marcha escuadras de reacción rápida para construir carreteras o limpiar escombros. Recurrió a trabajadores forzados para estos trabajos junto a obreros alemanes libres. Los trabajos de construcción se detuvieron en Berlín y Núremberg con el estallido de la contienda. Aunque el almacenamiento de materiales y otros trabajos continuaron, esto también se detuvo a medida que se necesitaban más recursos para la industria armamentística. Las oficinas de Speer emprendieron trabajos de construcción para cada rama del ejército y para las SS utilizando mano de obra forzada. El negocio de la construcción lo convirtió en uno de los miembros más ricos de la élite nazi.

El 8 de febrero de 1942 el Ministro de Armamento Fritz Todt moría en un accidente de avión poco después de despegar del cuartel oriental de Hitler en Rastenburg. Speer, que llegó a Rastenburg la noche anterior, había aceptado el ofrecimiento de Todt para volar con él a Berlín, pero lo canceló pocas horas antes del despegue porque había estado hasta tarde en una reunión nocturna con Hitler. El "Führer" designó a Speer sustituto de Todt, una elección nada sorprendente según el historiador británico Martin Kitchen porque el arquitecto era un hombre fiel a Hitler que además tenía experiencia en la construcción de campos de prisioneros y otras estructuras militares. Además, lo designó jefe de la Organización Todt, una enorme empresa constructora estatal. Como era habitual, el canciller no dio a Speer ningún mandato claro, sino que dejó que se abriera hueco entre la élite del nazismo en la lucha por el poder y el control, un empeño en el que se mostraría ambicioso, implacable y despiadado. El nuevo ministro se propuso no solo obtener el control de la producción de armamento para el ejército, sino para todas las fuerzas armadas. No mostró a sus rivales políticos que sus llamadas a la racionalización y la reorganización escondían su deseo de apartarlos y tomar el control.

Speer era festejado en esa época, como también lo fue en la posguerra, por el «milagro del armamento» que consistió en un espectacular aumento en la producción de guerra alemana. Este milagro se detuvo en el verano de 1943 por, entre otras causas, los persistentes bombardeos aliados sobre Alemania. A aquel aumento productivo probablemente contribuyeron más otros factores que el propio Speer. La fabricación de armas en Alemania ya había comenzado a aumentar antes, bajo dirección de su predecesor Todt. El armamento naval no estuvo bajo control de Speer hasta octubre de 1943 y el de la fuerza aérea hasta junio del año siguiente, a pesar de lo cual todos mostraron un aumento comparable al que controlaba el ministro de armamento. Otro factor que contribuyó al auge de las municiones fue la política de asignar más carbón a la industria del acero. La producción de cada tipo de arma alcanzó su punto máximo en junio y julio de 1944, pero a partir de agosto de 1944 el crudo rumano dejó de estar disponible y por tanto el combustible empezó a escasear. La producción de petróleo decayó tanto que imposibilitó cualquier acción ofensiva y el armamento quedó almacenado.
Como Ministro de Armamento, era el responsable de suministrar armas al ejército. Tras acordarlo con Hitler, decidió priorizar la construcción de carros de combate y se le dieron plenos poderes para asegurar su éxito. Hitler estuvo muy involucrado en el diseño de los tanques, pero cambiaba continuamente de opinión sobre sus especificaciones, lo cual retrasó el programa sin que Speer pudiera remediar la situación. En consecuencia, a pesar de que la producción de tanques tenía la máxima prioridad, se gastó relativamente poco del presupuesto de armamento en ella y eso llevó a una derrota importante del ejército alemán en la batalla de Projorovka, un importante punto de inflexión en el frente oriental contra el Ejército Rojo soviético.

Como jefe de la Organización Todt, estuvo directamente involucrado en la construcción y modificación de campos de concentración. Acordó ampliar Auschwitz y algunos otros campos, asignando 13,7 millones de Reichsmark para esos trabajos. Esto permitió construir 300 barracones adicionales en Auschwitz, aumentando su capacidad total hasta 132 000 internos. En las obras se incluyó material para construir cámaras de gas, crematorios y morgues. Las SS llamaron a esto el «Programa Especial del Profesor Speer».

Speer se dio cuenta de que, con seis millones de trabajadores alistados en las fuerzas armadas, había una escasez importante de mano de obra en la economía de guerra e insuficientes trabajadores para sus fábricas. La respuesta de Hitler fue designar a Fritz Sauckel como un «dictador de mano de obra» para conseguir nuevos trabajadores, empeño en el que colaboró con el ministro. Hitler dio a Sauckel total libertad para conseguir trabajadores, algo que deleitó a Speer, quien había solicitado un millón de trabajadores «voluntarios» para satisfacer las necesidades de la industria armamentista. Sauckel ordenó que se detuviera por la fuerza a los habitantes de pueblos enteros de Francia, Holanda y Bélgica y se los enviara a las fábricas. En otras ocasiones usó métodos todavía más brutales, como en zonas de la Unión Soviética donde habían actuado grupos de partisanos y en las que hombres y mujeres fueron detenidos en masa y enviados a trabajar por la fuerza en Alemania. Hacia abril de 1943, Sauckel había conseguido para Speer 1 568 801 trabajadores «voluntarios», que en realidad eran trabajadores forzados, prisioneros de guerra e internos de campos de concentración que el ministro usó en sus fábricas de armas. Fue principalmente por el maltrato a esta gente por lo que a Speer lo condenaron en los juicios de Núremberg.

El nombramiento como Ministro de Armamento le dio a Speer únicamente el control sobre la producción de armas para el ejército, pero él codiciaba controlar la producción de suministros también para la para la "Luftwaffe" (fuerza aérea) y la "Kriegsmarine" (armada). Comenzó a extender su poder e influencia con una ambición inesperada sirviéndose de su estrecha relación con Hitler, lo cual le proporcionó protección política para ser capaz de burlar y superar a sus rivales en el régimen. El gabinete de Hitler no vio con buenos ojos sus tácticas, a pesar de lo cual fue capaz de acumular nuevas responsabilidades y más poder. En julio de 1943 ya había obtenido el control de la producción de armamentos para la "Luftwaffe" y la "Kriegsmarine". En agosto de 1943, tomó el control de la mayor parte del Ministerio de Economía, para convertirse, en palabras del almirante Karl Dönitz, en «el dictador económico de Europa». Su título formal cambió a «Ministro del Reich para Armamento y Producción de Guerra». Se había convertido en una de las personas más poderosas de la Alemania nazi.

Speer y el elegido como director de construcción se submarinos, Otto Merker, creían que la industria naval estaba siendo frenada por métodos anticuados, y que los revolucionarios nuevos enfoques que impusieran ellos mejorarían drásticamente la producción. Esta creencia resultó incorrecta, y el intento de ambos de construir la nueva generación de submarinos de la Kriegsmarine, los Tipo XXI y Tipo XXIII, a base de secciones prefabricadas en diferentes instalaciones en lugar de en astilleros individuales contribuyó al fracaso de este programa estratégicamente importante. Los diseños se mandaban demasiado rápido a producción, por lo que los nuevos submarinos estaban plagados de fallos por culpa de las nuevas técnicas de construcción. Aunque se construyeron decenas de submarinos, muy pocos entraron en servicio.
En diciembre de 1943, Speer visitó a los trabajadores de la Organización Todt en Laponia, pero estando allí se dañó gravemente la rodilla y estuvo incapacitado durante varios meses. Se puso bajo los dudosos cuidados del doctor Karl Gebhardt en una clínica médica llamada "Hohenlychen", donde los pacientes «misteriosamente no lograban sobrevivir». A mediados de enero de 1944, sufrió una embolia pulmonar y cayó gravemente enfermo. Deseoso por retener su poder, no designó a un delegado y continuó dirigiendo el trabajo del Ministerio de Armamento desde su cama. Su enfermedad coincidió con la «Gran Semana» Aliada, una oleada de bombardeos aéreos sobre las fábricas alemanas que fueron un golpe devastador para la producción de aeronaves. Sus rivales políticos aprovecharon la oportunidad para socavar su autoridad y dañar su reputación ante Hitler, cuyo apoyo incondicional perdió junto con gran parte de su poder.

En respuesta a los bombardeos de los aliados, Adolf Hitler autorizó la creación de un comité de cazas de combate con el objetivo era garantizar la preservación y el aumento de la producción de aviones de combate. El grupo de trabajo se creó por una orden de Speer del 1 de marzo de 1944 con apoyo de Erhard Milch, del Ministerio del Aire del Reich. El resultado fue que la producción de aviones de combate se duplicó en Alemania entre 1943 y 1944, aunque este crecimiento se debió en gran parte a la construcción de aeronaves ya obsoletas que demostraron ser presa fácil para los cazas de los aliados. El 1 de agosto de 1944, el ministro fusionó este organismo con el recién creado comité de armamento ("Rüstungsstab").

El comité de cazas de combate fue fundamental en el aumento de la explotación de trabajadores forzados en la economía de guerra. Las SS proporcionaron 64 000 prisioneros para veinte proyectos distintos de varios campos de concentración, incluyendo Mittelbau-Dora. Los prisioneros trabajaban para empresas como Junkers, Messerschmitt, Henschel y BMW, entre otras. Para aumentar la producción, Speer introdujo un sistema de castigos para sus trabajadores, por el cual a los que fingían enfermedad, se relajaban, saboteaban la producción o intentaban escapar, se les negaba la comida o se los enviaba a campos de concentración. En 1944 esto ya era habitual y más de medio millón de trabajadores fueron arrestados. En esa época 140 000 personas trabajaban por la fuerza en las fábricas subterráneas, que eran verdaderas trampas mortales por la disciplina brutal y las habituales ejecuciones. Por ejemplo, se acumularon tantos cadáveres en la fábrica subterránea de Dora que se saturó el crematorio. El propio personal de Speer describió las condiciones allí como un «infierno». 
El mayor avance tecnológico bajo su mando se produjo en el programa de cohetes, que había arrancado en 1932 pero no había suministrado ninguna arma reseñable. Speer lo apoyó con entusiasmo y en marzo de 1942 hizo un pedido de cohetes A4, el predecesor del primer misil balístico del mundo, el cohete V2. La investigación para su desarrollo, así como el de la bomba voladora V1, se hizo en las instalaciones de Peenemünde. El primer objetivo del V2 fue París el 8 de septiembre de 1944. El programa, aunque ya avanzado, demostró ser un impedimento para la economía de guerra porque requirió de una importante inversión de capital sin efectividad militar. Los cohetes fueron construidos en una fábrica subterránea en Mittelwerk, donde la mano de obra para construir los cohetes A4 provino del campo de concentración Mittelbau-Dora. De los 60 000 internos en este campo, 20 000 murieron debido a las pésimas condiciones.

En el verano de 1944, Speer ya había perdido el control de la Organización Todt y de la producción de armamento. Se opuso al atentado del 20 de julio de 1944 contra Hitler, no participó en el complot y desempeñó un papel menor en los esfuerzos del régimen para recuperar el control sobre Berlín después de que el "Führer" sobreviviera. Destapado este complot, los rivales del ministro atacaron a algunos de sus aliados más cercanos y su sistema de gestión cayó en desgracia ante los radicales del partido, con lo que perdió aún más autoridad.

Las pérdidas territoriales y la incesante campaña de bombardeos estratégicos de los aliados causaron el colapso de la economía alemana a fines de 1944. Los ataques aéreos a la red de transporte fueron particularmente efectivos, ya que cortaron los principales centros de producción de los suministros esenciales de carbón. En enero de 1945, Speer le dijo a Goebbels que la producción de armamento podría mantenerse durante al menos un año. Sin embargo, concluyó que la guerra estaba perdida después de que las fuerzas soviéticas capturaran la importante región industrial de Silesia a finales de ese mes. A pesar de todo, el ministro creía que Alemania debía continuar en la guerra el mayor tiempo posible con el objetivo de obtener mejores condiciones de los Aliados que la rendición incondicional en la que insistían. Durante enero y febrero, afirmó que su ministerio entregaría «armas decisivas» y aumentaría la producción de armamento, lo cual «provocaría un cambio dramático en el campo de batalla». Speer obtuvo el control sobre los ferrocarriles en febrero y le pidió a Himmler que suministrara prisioneros de los campos de concentración para trabajar en su reparación.
A mediados de marzo, Speer había aceptado que la economía de Alemania colapsaría en las próximas ocho semanas. Si bien trató de frustrar las órdenes para destruir las instalaciones industriales en áreas en riesgo de captura, para que pudieran usarse después de la guerra, aún apoyaba continuar la guerra. Le entregó a Hitler un memorando el 15 de marzo, que detallaba la grave situación económica de Alemania y buscaba su aprobación para cesar las demoliciones de infraestructuras. Tres días después, también le propuso a Hitler que los recursos militares restantes de Alemania se concentraran a lo largo de los ríos Rin y Vístula en un intento por prolongar la lucha. Todo esto ignoraba la realidad militar, que era que las fuerzas armadas alemanas no podían igualar la potencia de fuego de los Aliados y se enfrentaban a una derrota total. Hitler no solo rechazó su propuesta de cesar las demoliciones, sino que emitió la «Orden Nerón» el 19 de marzo, que pedía la destrucción de todas las infraestructuras a medida que el ejército se retiraba. El ministro se horrorizó por esta orden y convenció a varios líderes militares y políticos para que la ignoraran. Durante una reunión con Speer entre el 28 y 29 de marzo, Hitler rescindió el decreto y le dio autoridad sobre las demoliciones. Este las detuvo, aunque el ejército continuó volando puentes.

En abril, quedaba poco de la industria armamentística alemana, y Speer tenía pocos deberes oficiales. Visitó el "Führerbunker" el 22 de abril por última vez, se reunió con Hitler y recorrió la cancillería dañada antes de abandonar Berlín para regresar a Hamburgo. El 29 de abril, el día antes de suicidarse, Hitler dictó un testamento político final que dejaba a Speer fuera del nuevo gobierno; su sustituto sería su subordinado, Karl Saur. A este le decepcionó que el "Führer" no lo eligiera como su sucesor. Después de la muerte de Hitler, ofreció sus servicios al llamado Gobierno de Flensburgo, encabezado por el sucesor de Hitler, Karl Dönitz. Asumió un papel en ese régimen de corta duración como Ministro de Industria y Producción. A partir del 10 de mayo, Speer proporcionó información a los Aliados sobre los efectos de la guerra aérea y sobre otros muchos temas. El 23 de mayo, dos semanas después de la rendición de las fuerzas alemanas, las tropas británicas arrestaron a los miembros del Gobierno de Flensburgo y pusieron final formal a la Alemania nazi.

Speer fue llevado a varios centros de internamiento para funcionarios nazis e interrogado. En septiembre de 1945, le dijeron que lo juzgarían por crímenes de guerra, y varios días después, lo trasladaron a Núremberg y lo encarcelaron allí. Fue acusado de cuatro cargos: participar en un plan común o conspiración para perpetrar un crimen contra la paz, planear, iniciar y librar guerras de agresión y otros crímenes contra la paz, crímenes de guerra, y por último, crímenes de lesa humanidad.

Robert H. Jackson, juez de la Corte Suprema de los Estados Unidos y fiscal jefe estadounidense en Núremberg, alegó: «Speer se unió a la planificación y ejecución del programa para emplear prisioneros de guerra y trabajadores extranjeros en la industria de guerra alemana, que creció en producción mientras los trabajadores se morían de hambre». El abogado del antiguo ministro, Hans Flächsner, presentó a su defendido como un artista empujado a la vida política, que siempre había permanecido fuera de toda ideología.

Speer fue declarado culpable de crímenes de guerra y crímenes de lesa humanidad, aunque fue absuelto de los otros dos cargos. Había afirmado que no sabía nada de los planes de exterminio nazis y eso probablemente lo salvó de morir en la horca. Se supo que su afirmación era falsa tras el hallazgo de una carta privada escrita en 1971 y revelada públicamente en 2007. El 1 de octubre de 1946 fue sentenciado a veinte años de prisión. Aunque tres de los ocho jueces (dos soviéticos y el estadounidense Francis Biddle) inicialmente abogaron por condenarlo a muerte, el resto de jueces no lo hizo, y el acuerdo de sentencia se alcanzó tras dos días de debates.

El 18 de julio de 1947, Speer fue transferido a la prisión de Spandau en Berlín para cumplir su condena. Allí fue conocido como el Prisionero Número Cinco. Sus progenitores murieron mientras estaba encarcelado: su padre, que murió en 1947, despreciaba a los nazis y guardó silencio al encontrarse con Hitler, mientras que su madre, que murió en 1952, fue una nazi que había disfrutado mucho de cenar con el "Führer". Rudolf Wolters y su secretaria durante años Annemarie Kempf, que no tenían permitida la comunicación directa con el antiguo ministro, hicieron lo que pudieron para ayudar a su familia y llevar a cabo las solicitudes que Speer le envió a su esposa, la única comunicación escrita que se le permitió oficialmente. A partir de 1948, Speer contó con los servicios de Toni Proost, un simpatizante holandés que pasó de contrabando su correo y sus escritos.
En 1949 Wolters le abrió una cuenta bancaria a Speer y comenzó a recaudar fondos entre los arquitectos e industriales que se habían beneficiado de las actividades del ministro durante la guerra. Inicialmente, los fondos se usaron solo para mantener a la familia de Speer, pero con el tiempo el dinero se usó para otros fines, como pagarle unas vacaciones a Toni Proost y sobornar a quienes pudieran asegurar su liberación. Una vez que el recluso supo de la existencia del fondo, Speer envió instrucciones detalladas sobre qué hacer con el dinero. Wolters recaudó un total de 158 000 marcos alemanes para Speer durante los últimos diecisiete años de su sentencia.

A los prisioneros se les prohibió escribir memorias. Sin embargo, Speer se las arregló para enviar 20 000 páginas de escritos a Wolters. Había completado sus memorias en noviembre de 1953, las cuales se convertirían en la base del libro "Dentro del Tercer Reich". En "Diarios de Spandau" pretendía presentarse como un héroe trágico que había hecho un pacto con el diablo por el cual soportó una dura sentencia de prisión.

Gran parte de sus energías las dedicó a mantenerse en forma, tanto física como mentalmente, durante su largo encierro. Spandau tenía un gran patio cerrado donde a los reclusos se les asignaron terrenos para jardinería y Speer creó un elaborado jardín con césped, flores, arbustos y árboles frutales. Para hacer sus paseos diarios por el jardín más atractivos, se embarcó en un viaje imaginario alrededor del mundo. Midiendo cuidadosamente la distancia de su recorrido, las trasladó a la geografía del mundo real y llegó a caminar más de 30 000 km hasta que fue liberado, momento en el que estaba, imaginariamente, cerca de Guadalajara, México. También leyó, estudió revistas de arquitectura y repasó los idiomas inglés y francés. En sus escritos afirmó haber leído cinco mil libros mientras estuvo en prisión, una exageración porque tuvo poco más de un día para acabar cada uno de ellos.

Los defensores de Albert Speer no dejaron de pedir su liberación. Entre los que prometieron apoyo para que se conmutara su sentencia estuvieron el presidente de la república francesa Charles de Gaulle, el diplomático estadounidense George Wildman Ball y Willy Brandt, canciller de Alemania Occidental, que puso fin a los procedimientos de desnazificación en su contra que podrían haber acabado en la confiscación de sus bienes. Sus esfuerzos para que le concedieran una liberación anticipada quedaron en nada. La Unión Soviética, que había exigido una sentencia de muerte en el juicio, nunca lo consintió. Así, cumplió la totalidad de su condena y fue puesto en libertad en la medianoche del 1 de octubre de 1966.

La puesta en libertad de Albert Speer fue un acontecimiento mundial en los medios de comunicación. Los periodistas abarrotaron tanto las calles de los alrededores de Spandau como el vestíbulo del hotel berlinés en el que Speer pasó la noche. Habló poco y reservó la mayoría de sus palabras para una gran entrevista publicada en el periódico "Der Spiegel" en noviembre de 1966. Aunque afirmó que esperaba reanudar una carrera arquitectónica, su único proyecto, una colaboración para una cervecería, no tuvo éxito. En cambio, revisó sus escritos de Spandau en dos libros autobiográficos. Más tarde publicó un tercero sobre Himmler y las SS. Sus libros incluyen "Memorias" (en alemán "Erinnerungen", literalmente "Recuerdos") y "Diario de Spandau", para cuya redacción recibió el asesoramiento de Joachim Fest y Wolf Jobst Siedler, de la editorial Ullstein. Por otra parte, se encontró incapaz de retomar la relación con sus hijos, incluso con su hijo Albert Speer Jr., que también era arquitecto. Según su hija Hilde, «Uno por uno mis hermanas y hermanos renunciaron. No había comunicación». Ayudó económicamente a su hermano Hermann después de la guerra, pero su otro hermano Ernst había muerto en la batalla de Stalingrado, a pesar de las repetidas peticiones de sus padres para que lo repatriara. 

Después de ser puesto en libertad, Speer donó su diario personal al Archivo Federal de Alemania. Había sido editado por Wolters y no contenía ninguna mención a los judíos. El escritor inglés David Irving descubrió discrepancias entre el diario engañosamente editado y documentos independientes. Speer le había pedido a Wolters que destruyera el material que había omitido de su donación, pero Wolters se negó y conservó una copia original. La amistad entre ambos se deterioró y un año antes de la muerte de Speer, Wolters le dio al historiador alemán Matthias Schmidt acceso al diario sin alterar. Schmidt escribiría el libro "Albert Speer: El fin de un mito", el primero que fue muy crítico con el antiguo ministro nazi.

Las memorias de Speer fueron un éxito fenomenal. Los lectores quedaron fascinados por una visión interna del Tercer Reich y así un importante criminal de guerra se convirtió en una figura popular casi de la noche a la mañana. Es importante destacar que proporcionó una coartada a los ya ancianos alemanes que habían sido nazis, porque si el mismísimo Albert Speer, tan cercano a Hitler y poderoso, no conocía el alcance total de los crímenes del régimen nazi y solo estaba «siguiendo órdenes», ellos también podrían decirse a sí mismos y a los demás que hicieron lo mismo. Speer proporcionó un lavado de cara a toda una generación de antiguos nazis. Tan grande era para todos ellos la necesidad de creer este «mito de Speer», que Fest y Siedler pudieron fortalecerlo, incluso frente a la creciente evidencia histórica de lo contrario.
Speer estuvo a disposición de historiadores y de cualquier otro interesado. En octubre de 1973 el arquitecto alemán hizo su primer viaje al Reino Unido para ser entrevistado en el programa "Midweek" de la BBC. Ese mismo año apareció entrevistado en la serie documental "El mundo en guerra". Regresó a Londres en 1981 para participar en el programa "Newsnight" de la BBC, pero estando allí sufrió un derrame cerebral y murió el 1 de septiembre de 1981. Aunque no se había divorciado de su mujer, había entablado una relación con una alemana que vivía en Londres y se encontraba con ella en el momento de su muerte. Su hija Margret Nissen escribió en sus memorias publicadas en 2005 que después de abandonar la cárcel, su padre había empleado todo su tiempo en construir «El mito de Speer».

Después de su liberación de Spandau, Speer se construyó la imagen del «buen nazi». Era un hombre bien educado, de clase media y burgués, en claro contraste con los psicópatas y asesinos que, en el imaginario popular, tipificaban a los «malos nazis». En sus memorias y entrevistas había distorsionado tanto la verdad y ocultado tantas cosas que sus mentiras se conocieron como «mitos». Speer llevó su creación de mitos a los medios de comunicación y sus «disculpas astutas» se reprodujeron innumerables veces en la Alemania de la posguerra. Isabell Trommer escribe en su biografía de Speer que Joachim Fest y Wolf Jobst Siedler fueron coautores de las memorias de Speer y cocreadores de sus mitos a cambio de una parte de sus derechos de autor y otros incentivos financieros. Speer, Siedler y Fest habían construido una obra maestra, la imagen del «buen nazi», que permaneció inalterable durante décadas, a pesar de las evidencias históricas que indicaban que era falsa.
Speer había construido cuidadosamente una imagen de sí mismo como un tecnócrata apolítico que lamentaba profundamente no haber podido descubrir los monstruosos crímenes del Tercer Reich. Después de la muerte de Speer, Matthias Schmidt publicó un libro que demostró que había ordenado el desalojo de judíos de sus hogares en Berlín. Ya en 1999, los historiadores habían demostrado con claridad que había mentido extensamente. A pesar de ello, la reputación de Speer no cambió sustancialmente hasta que el director de cine Heinrich Breloer rodó en 2004 una producción televisiva biográfica que sería el comienzo de un proceso de desmitificación y revaluación crítica. El historiador británico Adam Tooze afirma en su libro "The Wages of Destruction" que Speer se había movido entre las filas del régimen de forma diestra y despiadada, y que la idea de que era un tecnócrata que cumplía órdenes a ciegas era «absurda». Trommer dijo que no era un tecnócrata apolítico, sino uno de los líderes más poderosos y con menos escrúpulos del régimen nazi. Martin Kitchen dijo que había engañado al Tribunal de Núremberg y a la Alemania de la posguerra, mientras que Magnus Brechtken opinó que si su profunda participación en el Holocausto se hubiera conocido en el momento de su juicio, habría sido condenado a muerte. 

La imagen del buen nazi se sustentó sobre numerosos mitos de Speer. Además del mito de que era un tecnócrata apolítico, afirmó que no tenía pleno conocimiento del Holocausto o de la persecución de los judíos. Otro mito postula que Speer revolucionó la máquina de guerra alemana después de su nombramiento como Ministro de Armamentos gracias a un aumento espectacular en el envío de armas que se publicitó como el motivo de que Alemania se mantuviera en la guerra. Otro mito era que ideó un plan falso para asesinar a Hitler con gas venenoso, una falsedad que se le ocurrió al recordar el pánico que sentía cuando los gases del automóvil entraban por el sistema de ventilación de aire, a lo cual añadió los detalles. Brechtken escribió que su mentira más descarada fue inventada durante una entrevista con un periodista francés en 1952. El periodista describió una escena inventada en la que Speer rechazaba las órdenes de Hitler y el Führer se iba con lágrimas en los ojos. A Speer le gustó tanto que la añadió a sus memorias, con lo que el periodista había colaborado involuntariamente en uno de sus mitos.

Speer también buscó retratarse a sí mismo como un oponente al liderazgo de Hitler. A pesar de su oposición al complot del 20 de julio, afirmó falsamente en sus memorias que simpatizaba con los conspiradores. Sostuvo que Hitler fue amable con él por el resto de su vida después de enterarse de que lo habían incluido en una lista de posibles ministros, otro elemento clave de sus mitos. También afirmó falazmante que se había dado cuenta muy pronto de que la guerra estaba perdida, y que por ello trabajó para preservar los recursos necesarios para la supervivencia de la población civil. En realidad, había tratado de prolongar la guerra hasta que fuera imposible resistir más, contribuyendo así a la gran cantidad de muertes y la enorme destrucción que Alemania sufrió en los últimos meses del conflicto.

Speer mantuvo en los juicios de Núremberg y en sus memorias que no tenía conocimiento directo del Holocausto. Solo admitió sentirse incómodo con los judíos en la versión publicada de los "Diarios de Spandau". En términos más generales, Speer aceptó la responsabilidad de las acciones del régimen nazi. El historiador Martin Kitchen afirma que Speer era «plenamente consciente de lo que les había sucedido a los judíos» y que estuvo «íntimamente involucrado en la 'Solución Final'». Magnus Brechtken dijo que Speer solo admitió una responsabilidad general del Holocausto para ocultar su responsabilidad directa y real. Speer fue fotografiado con trabajadores esclavos en el campo de concentración de Mauthausen en 1942 y Blaine Taylor sostiene que si la foto hubiera estado disponible en los juicios de Núremberg habría sido ahorcado. En 2005, el periódico "The Daily Telegraph" publicó que habían aparecido documentos que indicaban que Speer había aprobado la asignación de materiales para la ampliación del campo de concentración de Auschwitz después de una inspección de las instalaciones realizada por sus asistentes en un día en que casi mil judíos fueron masacrados. Heinrich Breoler, al hablar sobre la construcción de Auschwitz, dijo que Speer no era solo un engranaje en el trabajo, sino que era el «terror mismo».

Speer también negó estar presente en los discursos de Posen (Poznan) a los líderes nazis en una conferencia el 6 de octubre de 1943, en la que Heinrich Himmler dijo: «Se tuvo que tomar la grave decisión de hacer que esta gente desapareciera de la tierra»; y más tarde, «Los judíos deben ser exterminados». Himmler menciona a Speer y se dirige a él en varias ocasiones durante este discurso. En 2007, el diario "The Guardian" informó que se había encontrado una carta de Speer con fecha del 23 de diciembre de 1971 en una colección de su correspondencia con Hélène Jeanty, viuda de un combatiente de la resistencia belga, en la que el antiguo ministro admite que «No hay duda: estuve presente cuando Himmler anunció el 6 de octubre de 1943 que todos los judíos serían asesinados».

A Speer se le reconoció como el responsable del «milagro armamentístico». Durante el invierno de 1941-1942, a la luz de la desastrosa derrota de Alemania en la Batalla de Moscú, los líderes alemanes, incluidos Fromm, Thomas y Todt, llegaron a la conclusión de que la guerra no se podía ganar. La posición racional a adoptar era buscar una solución política que pusiera fin a la guerra sin derrota, pero la respuesta de Speer fue usar su habilidad propagandística para demostrar un nuevo dinamismo en la economía de guerra. Publicó unas estadísticas espectaculares en las que se alegaba que se había multiplicado por seis la producción de artillería y consiguió que esa propaganda se difundiera por todo el país. Así logró acallar la discusión sobre que la guerra debía acabar.

El «milagro» armamentístico era un mito sostenido sobre la manipulación estadística que Speer efectuó para respaldar sus afirmaciones. La producción de armamento en efecto aumentó; sin embargo, se debió a las consecuencias lógicas de la reorganización realizada antes de que Speer asumiera el cargo, a la implacable movilización de trabajadores forzados y a una reducción deliberada en la calidad de la producción en favor de la cantidad. En julio de 1943, la propaganda sobre producción de armamento de Speer se volvió irrelevante porque ya no pudieron ocultarse al pueblo alemán una serie de derrotas dramáticas en el campo de batalla que pusieron a Alemania ante la perspectiva de perder la guerra. Brechtken escribe que Speer sabía que Alemania iba a perder la guerra y deliberadamente extendió su duración, causando la muerte en los campos de exterminio y de batalla de millones de personas que de otro modo habrían vivido. Kitchen dijo: «No puede haber ninguna duda de que Speer realmente ayudó a prolongar la guerra más de lo que muchos creían posible, como resultado de lo cual murieron millones de personas y Alemania quedó reducida a un montón de escombros».

Poco queda, más allá de fotos y planos, de las obras arquitectónicas más personales de Albert Speer. En Berlín no queda casi ningún edificio diseñado por el arquitecto en la era nazi, salvo el "Schwerbelastungskörper", un cuerpo de carga pesada construido hacia 1941 que tiene forma de enorme cilindro de hormigón de 14 metros de altura que se utilizó para medir la subsidencia del suelo como parte de los estudios de viabilidad para un gigantesco arco de triunfo y otras grandes estructuras propuestas como parte de "Welthauptstadt Germania", el proyecto de renovación de posguerra de Berlín propuesto por Hitler. El cilindro ahora es un hito protegido y está abierto al público. La tribuna del Campo Zeppelín en Núremberg, aunque parcialmente demolida, se puede visitar.

La Cancillería del Reich, en cuya construcción participó Speer, resultó muy dañada durante los bombardeos aéreos de la Batalla de Berlín. Sobrevivieron sus muros exteriores, pero fueron desmantelados por los soviéticos. Rumores sin fundamento sostenían que sus restos se usaron para otras construcciones como la Universidad Humboldt de Berlín, la estación de metro de Mohrenstraße o monumentos de guerra soviéticos en Berlín.



</doc>
<doc id="41384" url="https://es.wikipedia.org/wiki?curid=41384" title="Impuesto">
Impuesto

El impuesto es una clase de tributo (obligaciones generalmente pecuniarias en favor del acreedor tributario) regido por derecho público, que se caracteriza por no requerir una contraprestación directa o determinada por parte de la administración hacendaria (acreedor tributario).

En la mayoría de legislaciones, los impuestos surgen exclusivamente por la “potestad tributaria del Estado” el que se constituye en el acreedor. Generalmente los impuestos son cargas obligatorias para las personas y empresas. Un principio rector, denominado “capacidad contributiva”, sugiere que quienes más tienen deben aportar en mayor medida al financiamiento estatal, para consagrar el principio constitucional de equidad y el principio social de la libertad.

El principal objetivo de los impuestos es financiar el gasto público: construcción de infraestructuras (de transporte, distribución de energía), prestar los servicios públicos de sanidad, educación, seguridad ciudadana, policía, defensa, sistemas de protección social (jubilación, prestaciones por desempleo, discapacidad), etc. 

En ocasiones, en la base del establecimiento del impuesto se encuentran otras causas, como disuadir la compra de determinado producto (por ejemplo, tabaco) o fomentar o desalentar determinadas actividades económicas. De esta manera, se puede definir la figura tributaria como una "exacción pecuniaria forzosa para los que están en el hecho imponible". 

La reglamentación de los impuestos se denomina sistema fiscal o fiscalidad.


Una de las preocupaciones fundamentales de la Hacienda Pública ha sido determinar los criterios y principios que deben regir un sistema impositivo, para que sea calificado de óptimo. No existe sin embargo acuerdo sobre cuáles han de ser estos principios, los más extendidos son:

Los impuestos son generalmente calculados con base en porcentajes, denominado tipo de gravamen, tasas de impuestos o alícuotas, sobre un valor particular, la base imponible. Se distingue: 


Los impuestos progresivos reducen el agobio sobre personas de ingresos menores, ya que ellos pagan un menor porcentaje sobre sus ganancias. Esto puede ser visto como algo bueno en sí mismo o puede ser hecho por razones pragmáticas, ya que requiere menores registros y complejidad para personas con menores negocios. A veces se califica de impuesto progresivo o regresivo a un impuesto cuyos efectos puedan ser más favorables o desfavorables sobre las personas de rentas menores, pero este uso informal del término no admite una definición clara de regresividad o progresividad.

La discusión sobre la progresividad o la regresividad de un impuesto está vinculada al principio tributario de “equidad”, que a su vez remite al principio de “capacidad tributaria” o contributiva. La Constitución de la Nación Argentina (art. 16) reza: “La igualdad es la base del impuesto y las cargas públicas”, lo que la doctrina entendió como “igualdad de esfuerzos” o “igualdad entre iguales”. Se desprende así el concepto de equidad horizontal y vertical del impuesto. La equidad horizontal indica que, a igual renta, consumo o patrimonio, los contribuyentes deben aportar al fisco en igual medida. La equidad vertical indica que, a mayor renta, consumo o patrimonio, debe aportarse en mayor medida, es decir, a tasas más altas, para conseguir la “igualdad de esfuerzos”. Basándose en este último concepto, es que se ha generalizado el uso del término “regresividad” para calificar a los impuestos que exigen un mayor esfuerzo contributivo a quienes menos capacidad tributaria tienen. Es el caso del IVA, que siendo un impuesto plano en su alícuota, al gravar productos de primera necesidad impone un esfuerzo tributario mayor a las clases bajas.

Impuesto directo es aquel que grava directamente las fuentes de riqueza, la propiedad o la renta, tales como los impuestos sobre la renta, los impuestos sobre el patrimonio, impuesto de sucesiones, los impuestos sobre transferencia de bienes a título gratuito, los impuestos sobre Bienes Inmuebles, sobre la posesión de vehículos (Impuesto de la tenencia o uso de vehículos, Impuesto sobre Vehículos de Tracción Mecánica), animales, etc. En sistemas fiscales históricos se daba la capitación (impuesto igual a todos los habitantes) y también eran impuestos directos muchos de los exigidos dentro del complejo sistema fiscal en torno a la renta feudal.

Impuesto indirecto o imposición indirecta es el impuesto que grava el consumo. Su nombre radica en que no afecta de manera directa los ingresos de un contribuyente sino que recae sobre el costo de algún producto o mercancía. El impuesto indirecto más importante es el impuesto al valor agregado o IVA el cual constituye una parte importante de los ingresos tributarios en muchos países del mundo. Históricamente, es el caso de la alcabala castellana del Antiguo Régimen y de los "consumos" del siglo XIX.

Existe otra posibilidad de definición de ambos tipos de imposición, teniendo en cuenta consideraciones jurídicas, según las que son directos los impuestos en los que el contribuyente "de iure" (aquel que la ley designa como responsable del ingreso del tributo al fisco), es el mismo que el contribuyente "de facto" (quien soporta la carga impositiva), al tiempo que considera indirectos a aquellos impuestos que presentan una traslación de la carga impositiva del contribuyente "de iure" al contribuyente "de facto". Si bien esta traslación puede presentarse en distintos sentidos (adelante si se la traslada a los clientes; atrás, si se la traslada a los factores de la producción; lateral, si se la traslada a otras empresas), debe considerarse, a los fines de esta concepción de impuesto indirecto, solo la traslación adelante. Esta posición es ampliamente difundida, pero presenta asimismo aspectos muy discutidos, en el sentido de que es muy difícil determinar quién soporta verdaderamente la carga tributaria y en qué medida. No obstante, esta definición suscita las más interesantes discusiones sobre los efectos económicos de los impuestos.

Son impuestos objetivos aquellos que gravan una manifestación de riqueza sin tener en cuenta las circunstancias personales del sujeto que debe pagar el impuesto, por el contrario son impuestos subjetivos, aquellos que al establecer el gravamen sí tienen en cuenta las circunstancias de la persona que ha de hacer frente al pago del mismo. Un claro ejemplo de impuesto subjetivo es el Impuesto sobre la renta, porque normalmente en este impuesto se modula diversas circunstancias de la persona como pueden ser su minusvalía o el número de hijos para establecer la cuota a pagar, por el contrario el impuesto sobre la cerveza es un impuesto objetivo porque se establece en función de los litros producidos de cerveza sin tener en cuenta las circunstancias personales del sujeto pasivo.

Los impuestos reales gravan manifestaciones separadas de la capacidad económicas sin ponerla en relación con una determinada persona. Los impuestos se convierten en personales cuando gravan una manifestación de capacidad económica puesto en relación con una persona determinada.

Por ejemplo, un impuesto sobre la renta puede ser de carácter real si grava separadamente los salarios, los beneficios de los empresarios, los alquileres o los intereses obtenidos. El gravamen sobre la renta será personal cuando recaiga sobre el conjunto de la rentas de una persona. Son típicos impuestos de carácter personal , el impuesto sobre la renta de las personas físicas, el impuesto sobre sociedades y el impuesto sobre el patrimonio. Sin embargo estos impuestos no tienen siempre este carácter, pues el impuesto sobre la renta de los no residentes que grava las rentas obtenidas por las personas en un país por personas que no son residentes en ese país, suelen ser de carácter real, los no residentes deben presentar una declaración de este impuesto por cada renta que obtienen en el país.

Los impuestos personales pueden subjetivizarse de manera más fácil y adecuada, al gravar de manera más completa a un sujeto, pero no tiene porque ser así, por ejemplo podemos adoptar un impuesto sobre los salarios (por tanto real) que tenga en cuenta en su gravamen las circunstancias familiares (número de hijos, minusvalías físicas etc) y por tanto tenga carácter subjetivo.<ref name=ferr >

En los impuestos instantáneos, el hecho imponible se realiza en un determinado momento del tiempo de manera esporádica, por ejemplo la compra de un inmueble o recibir una donación de otra persona. En los impuestos periódicos, el hecho imponible se prolonga de manera indefinida en el tiempo, en estos casos, el legislador fracciona su duración en el tiempo en diferentes periodos impositivos. El impuesto sobre la renta grava la renta que es un fenómeno continuo, pero la ley la grava anualmente.

La clasificación de impuestos de la OCDE es la siguiente:


El establecimiento de un impuesto supone una disminución de su renta disponible de un agente, esto puede producir una variación de la conducta del agente económico. En cuanto al efecto sobre la renta nacional el efecto puede ser favorable o desfavorable de acuerdo con el modelo IS-LM.

Por otro lado, algunos impuestos al incidir sobre el precio de los productos que gravan, es posible que los productores deseen pasar la cuenta del pago del impuesto a los consumidores, a través de una elevación en los precios.

Los impuestos son pagados por los sujetos a quienes las leyes imponen las obligaciones correspondientes. Este hecho no tiene solamente un significado jurídico, ya que el pago del impuesto impone al sujeto la necesidad de disponer de las cantidades líquidas para efectuarlo lo que, a veces, involucra también la necesidad de acudir al crédito en sus diversas formas. Todo ello trae con sigo consecuencias en la conducta económica del contribuyente y alteraciones en el mercado.

Se da cuando el sujeto "de iure", es decir, aquel sujeto obligado por la ley al pago del impuesto traslada a un tercero (sujeto "de facto") mediante la subida del precio, la cuantía del tributo, de modo que se resarce de la carga del impuesto. 

Este es un efecto económico y no jurídico, porque se traslada la carga económica pero no la obligación tributaria: a quien coaccionará el Estado para cobrarle será al sujeto "de iure" y no al "de facto", con quien no tiene ningún vínculo.

Se da por:

Ejemplos: 


Existen tres clásicos criterios para distinguir estos tributos:






</doc>
<doc id="41389" url="https://es.wikipedia.org/wiki?curid=41389" title="Linfa">
Linfa

La linfa es un líquido transparente que recorre los vasos linfáticos. Se produce por el exceso de líquido que sale de los capilares sanguíneos al espacio intersticial o intercelular, el cual es recogido por los capilares linfáticos que drenan a vasos linfáticos más gruesos hasta converger en conductos que se vacían en las venas subclavias. 

La linfa es un líquido claro pobre en proteínas y rico en lípidos. Se diferencia de la sangre en que no transporta oxígeno y carece de hemoglobina y glóbulos rojos, las únicas células que contiene son los glóbulos blancos (linfocitos). Puede contener microorganismos que, al pasar por el filtro de los ganglios linfáticos, son eliminados. El cuerpo humano produce alrededor de 3 litros de linfa al día que se incorpora poco a poco a la sangre. La linfa recorre el sistema linfático que está dotado de una serie de válvulas que impiden el retroceso del fluido. 

Si un vaso linfático sufre una obstrucción, el líquido intersticial se acumula en la zona afectada, produciéndose una hinchazón denominada linfedema.

La linfa realiza tres funciones principales:

Su composición es similar a la del plasma sanguíneo. Contiene agua, proteínas y grasas procedentes de la absorción intestinal en forma de quilomicrones. Asimismo, cuenta con linfocitos y, en ocasiones, gérmenes que son captados y destruidos en los ganglios linfáticos.

La composición de la linfa varia dependiendo del lugar del cuerpo en que se forma. Si procede de los miembros superiores o inferiores tiene apariencia cristalina. Sin embargo la que se forma en el intestino, especialmente después de una comida copiosa, es rica en ácidos grasos, lo que le da un aspecto blancuzco (quilo).

La linfa circula muy lentamente, pues en el sistema linfático no existe un órgano impulsor equivalente al corazón. 

El proceso de transporte se inicia en los capilares linfáticos en los que penetra el líquido intersticial gracias las diferencias de presión. Posteriormente el avance del fluido se ve facilitado por la existencia de válvulas en los conductos de mayor tamaño que impiden el retroceso y la contracción de la pared de los vasos que están dotados de fibras musculares. 

Toda la linfa del organismo desemboca en el sistema venoso que continúa hasta la vena cava superior, a través de dos conductos principales:
En el transcurso de 24 horas el organismo produce alrededor de 3 litros de linfa, la mayor parte se vierte al torrente sanguíneo a través del conducto torácico y en menor proporción mediante el conducto linfático derecho.

Por comparación, en una persona de 75 kilogramos, podemos encontrar unos 6 litros de sangre, a una velocidad de 2 kilómetros por hora.


</doc>
<doc id="41393" url="https://es.wikipedia.org/wiki?curid=41393" title="Política de Honduras">
Política de Honduras

Honduras es una república regida por una Constitución por primera vez el 11 de diciembre de 1825, ésta establece los "tres poderes" del Estado, sin relación de subordinación entre ellos; 

El Poder Legislativo: Lo ejerce el Congreso Nacional mediante 128 diputados que son elegidos mediante el
Sufragio. Se reúnen en sesiones ordinarias en la capital de la república desde el 25 de enero. Dentro de sus atribuciones están: Crear, decretar, interpretar, reformar y derogar leyes existentes.

El Poder Judicial: Tiene la potestad de impartir justicia para el pueblo, de una forma gratuita en nombre del estado por Magistrado y jueces. Está integrado por la Corte Suprema de Justicia, las cortes de apelaciones y los juzgados por la ley. 

El Poder Ejecutivo: Lo ejerce el Presidente de la República, en representación para beneficio de la población. 
El presidente de la República y tres designados Presidenciales serán electos en una forma organizada y directa por el pueblo.

El Presidente de la república es tanto el Jefe de Estado como la cabeza del gobierno y es electo por voto popular por un periodo de cuatro años. sin reelección.

El Congreso Nacional de Honduras consta de 128 diputados, electos por un periodo de cuatro años; con posibilidad de reelección continua, por representación proporcional; el número de diputados que a cada partido se le permite colocar en el Congreso es proporcional a la cantidad de votos que cada partido recibe.

La judicatura incluye una (Suprema Corte) de Justicia: la Suprema corte de Honduras, una corte de apelación, y varias cortes de auténtica jurisdicción: como laborales de impuestos, y cortes criminales. Los jueces de la Corte Suprema de Justicia, son elegidos por un periodo de 7 años por el Congreso Nacional.

En el artículo 10 de la constitución de 1880 se establece que:

"Los Ministros de las diversas sociedades religiosas no podrán ejercer cargos públicos."

Separando de esta forma la religión de estado.

En el artículo 24 de la constitución de 1880 se establece que:

"El Estado tiene el primordial deber de fomentar y proteger la instrucción pública en sus diversos ramos: la instrucción primaria es obligatoria laica y gratuita. Será también laica la instrucción media u superior. Ningún Ministro de una sociedad religiosa podrá dirigir establecimientos de enseñanza sostenidos por el Estado."

En el artículo 53 de la constitución de 1924 se establece que:

"La iglesia está separada del Estado, el cual no podrá dar subvenciones, en caso alguno, para ningún culto."

En el artículo 57 de la constitución de 1936 se establece que:

"Se prohíbe dar subvenciones para cultos o enseñanza religiosa."

En el artículo 57 de la constitución de 1936 se establece que:

"La iglesia está separada del estado."

Y en el artículo 63 se establece que:

"Son prohibidas las vinculaciones (del estado) y toda institución en favor de establecimientos religiosos."

Artículo 71.- "Los Ministros de las diversas religiones no podrán ejercer cargos públicos."

En el artículo 87 de la constitución de 1965 se establece que:

"Los Ministros de las diversas religiones no podrán ejercer cargos públicos ni hacer en ninguna forma propaganda política, invocando motivos de religión o valiéndose, como medio para tal fin, de las creencias religiosas del pueblo."

En el artículo 77 de la constitución de 1982 se establece que

"Los ministros de las diversas religiones, no podrán ejercer cargos públicos ni hacer en ninguna forma propaganda política, invocando motivos de religión o valiéndose, como medio para tal fin, de las creencias religiosas del pueblo."

Por propósitos administrativos Honduras está dividido en 18 departamentos, con oficiales municipales y departamentales electos por un término de cuatro años.

Honduras consta actualmente de nueve partidos políticos registrados ante el Tribunal Supremo Electoral (TSE), de los cuales los dos más antiguos predominaban fuertemente convirtiendo al país en un sistema esencialmente bipartidista, sobre todo al momento de la elección presidencial.

Desde 1920 Honduras ha tenido un sistema esencialmente bipartidista, con una política electoral por el Partido Liberal y el Partido Nacional. La década de los ochentas fue un periodo relativamente tranquilo, comparado con otros países de Centro América que fueron sacudidos por las guerrillas izquierdistas, a pesar de los esfuerzos de la izquierda. El gobierno de Honduras proveyó bases para los ejércitos contrarrevolucionarios respaldados por los Estados Unidos de América que operaban en la república de Nicaragua.

Entre 1981 y 1984, hubo muchas desapariciones forzadas llevadas a cabo por los militares, tal como fue probado antes de la Corte Interamericana de los Derechos Humanos y en el reporte del Comisionado Nacional para la Protección de los Derechos Humanos entre en Honduras. En 1984, las Fuerzas Armadas comandadas por el General Gustavo Álvarez Martínez fueron depuestas durante demostraciones en contra de los Estados Unidos hechas en la capital, Tegucigalpa; M.D.C.; esto marcó un decrecimiento de la actividad contrarrevolucionaria, y el gobierno continuó para asistir las actividades anti-sandinistas de los Estados Unidos en retorno de ayuda económica.

El congreso constituyente instaurado en 1824 decretó en 1825 la primera demarcación territorial y el 11 de diciembre de 1825 la primera constitución del país, donde Honduras eliminó la esclavitud varias décadas antes que Estados Unidos de América y Rusia.

En 1955 se le otorga el derecho a la mujer a poseer la categoría de ciudadana y por tanto tiene acceso y derecho a a votar y a ser electa en el Congreso Nacional.

El presidente José Manuel Zelaya Rosales, electo para gobernar del 2006 al 2010, inició una controversia en Honduras con la afiliación del país a la Alianza Bolivariana para las Américas ALBA, debido a la fuerte oposición de la oligarquía y la empresa privada hondureña por el temor al respaldo popular que ésta estaba teniendo. además de que Honduras ya tenía tratados de libre comercio con Estados Unidos y otros países que integran el CAFTA-RD.
Luego hubo más controversia cuando Zelaya se rehusó a presentar el presupuesto del Gobierno para la aprobación del Congreso. 

En mayo y abril del 2009, Zelaya dejó en claro sus intenciones de realizar una encuesta no vinculante para saber si la gente querría que se colocará una cuarta urna sobre la convocación de una asamblea nacional constituyente que reescriba la constitución de Honduras.

La Corte Suprema de Honduras mantuvo una orden judicial contra la encuesta del 28 de junio, y el 26 de junio; mientras Zelaya desobedeció dicha orden; emitió una orden en secreto para dar un golpe de Estado político-militar. 

El 28 de junio, soldados hondureños allanaron la residencia presidencial y arrestaron a Zelaya, y fue sacado del país, violando el derecho fundamental de todo ciudadano estipulado en la Constitución deteniendo la encuesta. Lo pusieron en una aeronave militar, la cual lo transportó a Costa Rica.

Subsecuentemente, el 28 de junio, el Congreso Nacional de Honduras, en una sesión extraordinaria, votó para remover a Zelaya de su cargo y colocar a su susesor constitucional, el presidente del Congreso Nacional, quien era entonces Roberto Micheletti, y que estuvo en ese cargo hasta el 27 de enero de 2010, fecha en que le correspondería a Zelaya entregar el poder.

La mayor parte de la Comunidad Internacional calificó el acto como un golpe de Estado y al mandatario Micheletti como un presidente "de facto".

Si bien la función de la política es dirigir el futuro del país, se han infiltrado en los grupos políticos muchos ladrones, criminales e incluso narcotraficantes dirigentes de bandas de criminales y sicarios, que utilizan los recursos recolectados en impuestos para robar y atacar a la misma población.

Uno de los casos más recientes fue el del Alcalde de Yoro, (Arnaldo Urbina Soto), capturado en 2014, acusado de liderar una banda de narcotráfico y sicariato que habría asesinado a más de 137 personas y abusado sexualmente de una gran cantidad de mujeres quienes se negaron a entregarles sus bienes; Casas, ganado, etc.




</doc>
<doc id="41395" url="https://es.wikipedia.org/wiki?curid=41395" title="Peste negra">
Peste negra

La peste negra o muerte negra se refiere a la pandemia de peste más devastadora en la historia de la humanidad que afectó a Eurasia en el siglo y que alcanzó un punto máximo entre 1347 y 1353. Es difícil conocer el número de fallecidos, pero modelos contemporáneos los calculan entre 75 a 200 millones, equivalente al 30-60% de la población de Europa, siendo un tercio una estimación muy optimista. La teoría aceptada sobre el origen de la peste explica que fue un brote causado por una variante de la bacteria "Yersinia pestis".<ref name="DOI10.1073/pnas.1105107108">V. J. Schuenemann, K. Bos, S. DeWitte, S. Schmedes, J. Jamieson, A. Mittnik, S. Forrest, B. K. Coombes, J. W. Wood, D. J. D. Earn, W. White, J. Krause, H. N. Poinar: "PNAS Plus: Targeted enrichment of ancient pathogens yielding the pPCP1 plasmid of Yersinia pestis from victims of the Black Death." In: "Proceedings of the National Academy of Sciences.", S. , .</ref> Es común que la palabra «peste» se utilice como sinónimo de «muerte negra», aun cuando aquella deriva del latín «"pestis"», es decir, «enfermedad» o «epidemia», y no del agente patógeno.

De acuerdo con el conocimiento actual, la pandemia irrumpió en primer lugar en Asia, para después llegar a Europa a través de las rutas comerciales. Introducida por marinos, la epidemia dio comienzo en Mesina. Mientras que algunas áreas quedaron despobladas, otras estuvieron libres de la enfermedad o solo fueron ligeramente afectadas. En Florencia, solamente un quinto de sus pobladores sobrevivió. En el territorio actual de Alemania, se estima que uno de cada diez habitantes perdió la vida a causa de la peste negra. Hamburgo, Colonia y Bremen fueron las ciudades en donde una mayor proporción de la población murió. En cambio, el número de muertes en el este de Alemania fue mucho menor.

Las consecuencias sociales de la muerte negra llegaron muy lejos; rápidamente se acusó a los judíos como los causantes de la epidemia por medio de la intoxicación y el envenenamiento de pozos. En consecuencia, en muchos lugares de Europa se iniciaron pogromos judíos y una extinción local de comunidades judías. Aun cuando líderes espirituales o seculares trataron de impedir esta situación, la falta de autoridad debido a la agitación social, que a su vez era consecuencia de la gravedad de la epidemia, generalmente no les permitía a aquellos tener éxito.

Realmente la Peste Negra no cuenta con antecedentes por su carácter multicontinental. Griegos y romanos relataron infinidad de pestilencias, algunas de gran mortalidad y/o morbilidad, como la que debió asolar el norte de África hacia 125 a.C. ; pero eran epidemias muy localizadas en una ciudad o región concreta. La Peste Negra fue un mal que atacó el norte de África, Asia, Oriente Medio y Europa, excepto Islandia y Finlandia, con una mortalidad no alcanzada después por las más graves epidemias como la viruela, diezmadora en América, o la pandemia de gripe de 1918.

La situación política encontrada por el agente infeccioso fue de relativa estabilidad comparada con lo vivido siglos antes en Europa y en buena parte de Asia. Por una parte, las grandes migraciones con sus saqueos y ataques se habían detenido, los vikingos, vándalos, húngaros o árabes se asentaron en distintos territorios de una forma más o menos definitiva. En Asia, el imperio mongol se había dividido en dos reinos que se despreciaban, indicando la poca cohesión interna según . Por su parte, toda la costa mediterránea de África ya no sufría los envites de cristianos en forma de cruzada o invasiones provenientes de la península arábiga, pues Bagdad había perdido el poder político tras los ataques de Gengis Kan. Con todo, el ambiente político en Europa y Asia distaba mucho de ser estable y en paz, entre otros motivos por la Guerra de los Cien Años, que se solaparía con la epidemia y los acontecimientos vividos en el desmembrado Imperio mongol, que terminaría arrasando Bagdad, la capital abasí.

La situación demográfica por su parte también aparentaba cierta prosperidad. Tras vivir varios años de un clima benigno y buenas cosechas, la población en el Viejo Continente aumentó hasta los 80 millones de habitantes estimados. A esto también contribuyeron las nuevas técnicas y artes agrarias, Walter indicaba varias como el empleo de caballos en lugar de bueyes, la utilización del arado con reja de hierro y la división de la tierra en tres cultivos en lugar de dos, lo que se denomina cultivo de alternancia trienal, Sin embargo, esta situación tan benigna cambió en torno al 1300. Autores como Jacques indicaban que dicho modelo comenzó a presentar signos de agotamiento por la necesidad de más tierras y más caballos disponibles para lograr alimentar a toda la población, el cultivo trienal no lograba regenerar totalmente los campos. Tampoco el ganado tenía pastos suficientes por lo que continuó la desforestación y esta la reducción de lugares para la caza con la subsiguiente reacción de nobles. Estos problemas convergieron o fueron causados por la conocida como Pequeña Edad de Hielo, según investigadores como Brian , que debió comenzar hacia 1300, produciendo una disminución en las cosechas, con el consiguiente incremento de hambrunas o malnutrición. Por tanto, la epidemia encontró a dos o más generaciones debilitadas desde la infancia por estos sucesos.

La economía por su parte había recibido cierto empuje debido a las buenas producciones agrarias, a la reanudación constante de las caravanas comerciales por la Ruta de la Seda gracias al control territorial de los mongoles y, dato de gran importancia para la propagación de la enfermedad, la mejora de las técnicas de navegación y construcción de navíos, con las que poder transportar cargamentos de gran tamaño desde puertos en mar Negro o el Mediterráneo hasta Barcelona, Marsella o las ciudades italianas. Este aumento de la riqueza se puede constatar en las construcciones de importantes catedrales europeas cada vez más grandes y más altas.

En el aspecto social, la llamada "época del gótico" trajo el crecimiento de las ciudades respecto del campo, así como el progresivo desprecio a las personas que no vivían en ellas, como eran los buhoneros, los pastores trashumantes y los gitanos que aparecen por primera vez en la Historia de Occidente. Otro cambio importante en las consecuencias traídas por la peste fueron las costumbres de diferenciar a los grupos sociales por la indumentaria. Así la casada, la soltera y la barragana pasarían a vestir de forma diferente; también los cristianos de los judíos, para desgracia de estos últimos.

Por lo que a la ciencia se refiere, realmente no existía como tal. La medicina poseía cierta independencia de la filosofía en cuanto a disciplina impartida en las universidades, pero era más empírica que científica y seguía influida en buena medida por los conocimientos aportados por otros autores griegos y latinos como Galeno de Pérgamo. El "desarrollo" se realizaba de una forma reflexiva, partiendo de los textos clásicos, y no científica, basada en la experimentación metodológica.

Por último, la religión seguía unificando a Europa bajo la Iglesia Católica, si bien existía cierta desafección debido al traslado de la corte papal a la ciudad francesa de Aviñón y al consentimiento de Clemente VI a la hora de perder su autonomía en aras de la seguridad brindada por el rey Felipe VI de Francia, como lo habían hecho los tres pontífices anteriores en el Papado de Aviñón. Por otra parte, muchos clérigos, obispos e incluso los propios papas eran dados a los placeres mundanos, poseer y pasearse con concubinas o aceptar la simonía.

No termina de haber acuerdo entre los historiadores, médicos y biólogos sobre qué agente infeccioso causó la enfermedad, por lo tanto, no hay consenso si fue o no una variedad de la peste bubónica u otra enfermedad distinta, como el carbunco, la llamada peste negra. En aquel tiempo la medicina no estaba preparada, no ya para tratar la enfermedad, ni tan siquiera para investigarla, pese a los heroicos esfuerzos y sacrificios de personas como Juan Tomás Porcell. No obstante, la mayoría de variedades de "Yersinia pestis" se han encontrado en China, lo que sugiere que la epidemia podría haberse originado en esa región 

Varios cronistas de la época indican la brusquedad con la que aparecían los síntomas. Una persona podía estar sana por la mañana y tener fiebre alta por la tarde para morir al llegar la noche. Según la literatura médica y de otra índole, los afectados padecían todos o varios de los siguientes síntomas según Giovanni y otros autores:


Giovanni Boccaccio y otros autores describen un tipo de peste casi asintomático que provocaba la muerte a las 14 horas aproximadamente. El calificativo "negra" se debe a las manchas, bubones y al aspecto producido por la gangrena en los dedos de manos y pies. La connotación de mal olor que posee la palabra "peste"  la dieron los hedores emanados al romperse los bubones, ganglios linfáticos inflamados. Según varios testimonios, el surgimiento de dichos bubones y de las manchas negras terminaba con la muerte del paciente en la inmensa mayoría de los casos. Desde notar los primeros síntomas hasta producirse la defunción pasaban cinco días habitualmente.

Por medio del ensayo y el error, las autoridades de distintas ciudades llegaron a la conclusión de que la enfermedad tardaba no más de 39 días en aparecer y los que lograban sobrevivir no volvían a contagiarse nuevamente. Esto se infiere de los cuarenta días que pasaban viajeros y navegantes confinados a la llegada de algunas ciudades italianas. Científicos del siglo indican que la enfermedad podría tener un periodo de incubación no contagioso de unos diez o doce días. A este seguiría un periodo de latencia asintomático, pero contagioso de unos veinte o veintidós días. Posteriormente aparecerían los síntomas y la enfermedad mataba en cuatro o cinco días más. De ser así, este periodo de incubación y latencia tan largo sería una de las causas que permitió su rápida propagación.

Hasta el siglo no había una propuesta sólida sobre qué organismo habría causado tal mortalidad sin comparación. Sin embargo, ese consenso se rompió en la década de 1980 y en el siglo aún perduran las discrepancias por distintos motivos. A principios del siglo , no existía consenso sobre si la peste negra fue una enfermedad emergente o reemergente. Para Christopher Duncan pertenecería al primer tipo proveniente quizá de mamíferos africanos. Para Ole J. sería del segundo, detenida durante siglos como consecuencia de la interrupción comercial con África y Oriente Medio debido a la caída del Imperio Romano.

En 1894 el gobierno francés envió a Alexandre Yersin y al japonés Kitasato Shibasaburō hasta la colonia británica de Hong Kong para enfrentar una epidemia que se había llegado a la Provincia de Cantón ese año y causaba un 80% de mortalidad en los chinos afectados. Por supuesto una epidemia así alertó a las naciones occidentales por la disminución del comercio y el peligro de que dicha plaga llegase a las distintas metrópoli, por lo que destinaron gran cantidad de recursos para indagar sobre la misma. Al examinar a los afectados ambos científicos comprobaron que desarrollaban un bubón de color oscuro, parecido a las representaciones de San Roque. Yersin escribió una carta a su familia en Gran Bretaña mostrando su regocijo por haber encontrado la enfermedad causante de la Gran Muerte. Sus informaciones serían publicadas en una revista científica con el título "La peste bubonique a Hong-Kong". El Dr. Yersin indicó como posible vehículo de transmisión del mal a las ratas.

Cuando la epidemia llegó a la India en 1905, las autoridades coloniales crearon la "Comisión de la India para la investigación de la peste" con algunos de sus mejores especialistas entre los que incluyeron al entomólogo William Glen Liston, quien llevaba estudiando las pulgas dos años antes. Según comprobaciones de Liston, la bacteria mataba a las ratas negras, anfitrión natural de las pulgas "Xenopsylla cheopis". Estas, al verse privadas de su anfitrión, se veían obligadas a buscar otro del que alimentarse, como los seres humanos. Al hacerlo inoculaban el patógeno. Cuando el humano moría, la colonia de parásitos que se había criado alimentándose de su sangre podía infectar a otros humanos u otras ratas.

Además del bubón los datos que avalan el contagio por pulgas de la rata negra eran:

La teoría de las ratas y la "Yersinia pestis" cuenta con el mayor número de partidarios. Tanto entre profesionales sanitarios, como puede ser el caso de Luis Enjuanes, Rafael Nájera, Grahan Mooney o Ken Cage, y también historiadores como María Jesús , Enrique o Ole J. .

El biólogo, zoólogo y experto en ratas Graham publicó que la peste bubónica no pudo ser la causante de la peste negra por razones como:

Esta hipótesis la defienden entre otros por el propio , Samuel Cohn o Christopher Duncan.

El a veces calificado "peor desastre biológico de la historia de la Humanidad", posee un origen desconocido. En India y China no hay datos de una epidemia especialmente relevante hacia el siglo , en este último país la referencia más antigua data del siglo , pero como descripción sintomática, no epidémica y en la gran Enciclopedia de China no se menciona hasta la década de 1640, pese a que Lien-Tê atribuye un origen chino a la enfermedad hacia 1346. Por lo tanto, solo existen hipótesis sobre si apareció en el desierto de Gobi o en su llamada "patria ancestral", es decir, los actuales Yemen, Kenya y Uganda. Los árabes dan referencias de primera mano y en especial Abu Halfs Umar Ibn al-Wardi quien indica que la epidemia surgió en la Tierra Oscura, pero sin precisar. El también musulmán Muhammad al-Maqrizi es más detallado al indicar su aparición en Kanato a lo largo del año 742 de la Hégira (1341-1342 d.C.).
Es Gabriele de Mussis quien da un lugar exacto para constatar la propagación de la plaga cuando nombra la ciudad de Caffa como el primer foco y cuenta la historia según los ejércitos mongoles que asediaron el enclave genovés lanzando cadáveres infectados con catapultas dentro de la ciudad para propagar la enfermedad y acelerar su caída. Sí se tiene constancia de que la enfermedad salió en barco de dicha colonia genovesa en la península de Crimea, en octubre de 1347 y llegó a Mesina a finales de dicho año.

Algunos barcos no llevaban a nadie vivo cuando alcanzaban las costas. En 1347 sucedió una guerra entre el Reino húngaro y el napolitano, puesto que el rey Luis I de Hungría reclamaba el trono luego del asesinato de su hermano Andrés, quien murió asesinado por su propia esposa, la reina Juana I de Nápoles. De esta manera, Luis condujo una campaña militar que coincidió con el estallido de la Peste Negra. Ante tanta muerte por la enfermedad, la campaña pronto tuvo que ser suspendida y los húngaros regresaron a casa, llevándose consigo varios de ellos la enfermedad, cobrando vidas, como la de la propia esposa del rey húngaro. Así, la peste se extendió desde Italia por Europa afectando territorios de las actuales Francia, España, Inglaterra (en junio de 1348) y Bretaña, Alemania, Hungría, Escandinavia y finalmente el noroeste de Rusia. Se considera que fue la causa de la muerte del entonces rey de Castilla Alfonso XI durante el sitio a Gibraltar en 1350.

La información sobre la mortalidad varía ampliamente entre las fuentes, pero se estima que entre el 30 % y el 60 % de la población de Europa murió desde el comienzo del brote a la mitad del siglo . Aproximadamente 25 millones de muertes tuvieron lugar sólo en Europa junto a otros 40 a 60 millones en África y Asia. Algunas localidades fueron totalmente despobladas y los pocos supervivientes huyeron y extendieron la enfermedad aún más lejos.

La gran pérdida de población trajo cambios económicos basados en el incremento de la movilidad social según la despoblación erosionaba las obligaciones de los campesinos (ya debilitadas) a permanecer en sus tierras tradicionales. La peste provocó una contracción del área cultivada en Europa, lo que hizo descender profundamente la producción agraria. Esta caída llegó a ser de un 40 % en la zona norte de Italia, en el periodo comprendido entre 1340 y 1370.

La repentina escasez de mano de obra barata proporcionó un gran incentivo para la innovación que ayudó a traer el fin de la Edad Media. Algunos argumentan que dio pie al Renacimiento, a pesar de que el Renacimiento ocurriera en algunas zonas (tales como Italia) antes que en otras. A causa de la despoblación, sin embargo, los europeos supervivientes llegaron a ser los mayores consumidores de carne para una civilización anterior a la agricultura industrial.

La peste negra acabó con un tercio de la población de Europa y se repitió en sucesivas oleadas hasta 1490, llegando finalmente a matar a unos 200 millones de personas. Ninguno de los brotes posteriores alcanzó la gravedad de la epidemia de 1348.

Al margen del análisis de sus causas obvias, tales como la presencia del bacilo "Yersinia pestis", los historiadores han buscado, desde diversas perspectivas, el significado de este gran acontecimiento. Corrientes herederas del marxismo y estudiosos como el francés Guy Bois atribuyen a esta epidemia el papel de demostración de la crisis del sistema feudal. Sin embargo, también murieron muchísimos representantes de la nobleza. Reyes como Alfonso XI de Castilla o Juana II de Navarra murieron de peste negra, así como Margarita de Luxemburgo, la reina consorte húngara esposa de Luis I y Felipa de Lancaster, la reina consorte portuguesa de Juan I de Portugal. Lo que contradice la teoría de Guy Bois, ya que no era la pobreza el lugar exclusivo donde atacaba la «peste», sino que nadie estaba a salvo. 

Así, el gran crecimiento demográfico que el mundo feudal había vivido durante la Plena Edad Media había puesto en cultivo tierras cada vez de menor calidad y de bajo rendimiento, lo que provocó una paulatina caída de la productividad y una creciente malnutrición. En este contexto llegó un bacilo que en otra situación habría sido recibido con fuertes defensas fisiológicas y no habría provocado gran mortandad, pero que esta vez encontró un sistema inmunitario debilitado.

El principal medio de contagio de la peste eran las picaduras de las pulgas, que campaban a sus anchas en una sociedad con tan poca higiene como la medieval. Pese a que es difícil constatarlo con una enfermedad que afectó a tantas personas de todo tipo y condición, sí que parece que determinadas ocupaciones estaban más expuestas a padecer peste que otras, siendo más peligroso ser comerciante de paños (las pulgas se esconden entre los tejidos) que, por ejemplo, herrero. De hecho, pronto se dieron cuenta del peligro de las vestiduras y entre las primeras medidas que se emplearon en Europa para evitar el contagio fue el de quemar la ropa de los infectados o prohibir la entrada de cargamentos de tejidos en las ciudades. Incluso en algunas ciudades se permitía la entrada al viajero solo después de haberse deshecho de las ropas que se traía puestas, cambiadas por otras «seguras» prestadas por la propia ciudad.





</doc>
<doc id="41397" url="https://es.wikipedia.org/wiki?curid=41397" title="Liberalismo">
Liberalismo

<section begin=liberalismo/>El liberalismo es una filosofía política y moral que defiende la libertad individual, la igualdad ante la ley, y una reducción del poder del Estado. Representa una corriente muy heterogénea y hay muchas formas y tipos de liberalismo, pero en general defiende los derechos individuales (fundamentalmente la libertad de expresión y la libertad de prensa), el libre mercado, el secularismo, la igualdad de género, igualdad racial, la propiedad privada, la democracia, el Estado de derecho, el capitalismo, la sociedad abierta y el internacionalismo.

El liberalismo contemporáneo surgió en la Ilustración y se popularizó rápidamente entre muchos filósofos y economistas europeos y más tarde en la sociedad en general, especialmente entre la burguesía. Los liberales buscaban eliminar la monarquía absoluta, los títulos nobiliarios, la confesionalidad del Estado y el derecho divino de los reyes y fundar un nuevo sistema político basado en la democracia representativa y el Estado de derecho. Los liberales acabaron con las políticas mercantilistas y las barreras al comercio, promoviendo el comercio libre y la libertad de mercado. Los líderes de la Revolución francesa y la Revolución estadounidense se sirvieron de la filosofía liberal para defender la rebelión contra la monarquía absoluta. En el siglo XX, el fascismo y el comunismo fueron ideologías populares que se oponían abiertamente al liberalismo. Las democracias liberales, que se hallaron en el bando vencedor en ambas guerras mundiales, se expandieron por el mundo durante el siglo XX.

Los liberales se dividen en dos grandes ramas: los moderados y los progresistas. Los moderados tienden al elitismo y al conservadurismo mientras que los progresistas defienden ciertos aspectos del Estado del bienestar. En Europa, los moderados son la rama dominante.<section end=liberalismo/>

Si bien su definición continúa siendo discutida en el ámbito académico, se entiende como "Liberalismo" a una filosofía política que tiende a la reducción del estado hasta su mínimo posible. 

Se lo identifica como una doctrina que propone la libertad y la tolerancia en las relaciones humanas. Promueve las libertades civiles y económicas, oponiéndose al absolutismo y al conservadurismo. Constituye la corriente en la que se fundamentan tanto el Estado de derecho como la democracia representativa y la división de poderes.

Desde sus primeras formulaciones, el pensamiento político liberal se ha fundamentado sobre tres grandes ideas:

El liberalismo fue un movimiento de amplia proyección (económica, política y filosófica) que defendía como idea esencial el desarrollo de la libertad personal individual como forma de conseguir el progreso de la sociedad.

Aboga principalmente por: 

El liberalismo está inspirado en parte en la organización de un Estado de derecho con poderes limitados —que idealmente tendría que reducir las funciones del gobierno a seguridad, justicia y obras públicas— y sometido a una constitución, lo que permitió el surgimiento de la democracia liberal durante el siglo XVIII, todavía vigente hoy en muchas naciones actuales, especialmente en las de Occidente.

El liberalismo europeo del siglo XX ha hecho mucho hincapié en la libertad económica, abogando por la reducción de las regulaciones económicas públicas y la no intervención del Estado en la economía. Este aspecto del liberalismo ya estuvo presente en algunas corrientes liberales del siglo XIX opuestas al absolutismo y abogó por el fomento de la economía de mercado y el ascenso progresivo del capitalismo. Durante la segunda mitad del siglo XX, la mayor parte de las corrientes liberales europeas estuvieron asociadas a la comúnmente conocida como derecha política.

Debe tenerse en cuenta que el liberalismo es diverso y existen diferentes corrientes dentro de los movimientos políticos que se autocalifican como "liberales"

Sus características principales son : 


El liberalismo normalmente incluye dos aspectos interrelacionados: el social y el económico. El liberalismo social es la aplicación de los principios liberales en la vida política de los individuos, como por ejemplo la no intromisión del Estado o de los colectivos en la conducta privada de los ciudadanos y en sus relaciones sociales, existiendo plena libertad de expresión y religiosa, así como los diferentes tipos de relaciones sociales consentidas ya sean de carácter amistoso, amoroso o sexual, así como en aspectos de moralidad.

Esta negativa permitiría (siempre y cuando sea sometida a aprobación por elección popular usando figuras como referendos o consultas públicas, ya que dentro del liberalismo siempre prevalece el Estado de derecho y este en un Estado democrático se lleva a su máxima expresión con la figura del sufragio) la libertad de paso, la no regulación del matrimonio por parte del Estado (es decir, este se reduciría a un contrato privado como otro cualquiera), la liberalización de la enseñanza, etc. Por supuesto, en el liberalismo hay multitud de corrientes que defienden con mayor o menor intensidad diferentes propuestas.

El liberalismo económico es la aplicación de los principios liberales en el desarrollo material de los individuos, como por ejemplo la no intromisión del Estado en las relaciones mercantiles entre los ciudadanos, impulsando la reducción de impuestos a su mínima expresión y reducción de la regulación sobre comercio, producción, etc. Según la doctrina liberal, la no intervención del Estado asegura la igualdad de condiciones de todos los individuos, lo que permite que se establezca un marco de competencia, sin restricciones ni manipulaciones de diversos tipos. Esto significa neutralizar cualquier tipo de beneficencia pública, como aranceles y subsidios, a favor de la ganancia de cada persona mediante el trabajo, favoreciendo la meritocracia y la producción.

Para Locke la sociedad es una creación humana, es decir por consentimiento, debido a ello puede elegir a quien(es) gobierne(n). Sin embargo, como los miembros de la sociedad o dicho de otro modo, los miembros del cuerpo político decidieron a quien elegir, por cuanto tiempo y bajo qué condiciones, si quienes gobiernan contravienen los principios del gobierno y los derechos del pueblo, el poder debe regresar a sus manos originarias.

El pueblo no está obligado a obedecer cuando se infringen las normas “Locke se refiere en todo momento a la pérdida de autoridad, a la ilegalidad como condición de posibilidad de la disolución del gobierno, ante la cual se habilita la resistencia en forma legítima" la pregunta es ¿podrá el pueblo sublevarse por cualquier cuestión que considere importante? La respuesta es NO, “Locke insiste en que el pueblo no se subleva por nimiedades, y es capaz de tolerar un gran número de injusticias. Sólo cuando las violaciones a la ley o a los fines de la sociedad se perpetúan en el tiempo los pueblos se resisten".

Otro pensador clásico liberal fue Immanuel Kant, quien también estudia la conformación de la sociedad, la libertad y la sujeción al gobierno. Para Kant la libertad está directamente relacionada con el derecho del individuo de obedecer solo aquellas leyes en las que vea reflejada su propia voluntad legisladora. Hasta este punto parece estar de acuerdo con Locke, pero si bien el pueblo es una suma de voluntades que pactan para una mejor forma de vida, «las ideas de "voluntad general" y de "contrato" no implican, en este marco, el reconocimiento de derechos inalienables del pueblo, sino que son asumidas, en todo caso, como criterios que permiten al legislador dictar leyes tales que "hubiesen podido" ser aceptadas por "la voluntad unida de todo un pueblo».

Si bien el pueblo tiene derechos, estos se pueden y deben enajenar en el momento que se conforma un gobierno, mismo que se vuelve su representante que puede diseñar y ejecutar leyes pensando en el bienestar del mismo. De ahí que «Para que una ley sea considerada legítima (y pueda reclamar el "consentimiento" de aquellos que se someten a ella), no es preciso que sea el pueblo reunido en asamblea quien dicte tal ley, ni tampoco es necesario que éste preste su consentimiento efectivo: si una ley es de tal índole que resulte "imposible" que todo un pueblo "pueda" otorgarle su aprobación, entonces no es legítima"," pero con que sea "solo posible" que alguna vez el pueblo prestara su conformidad a dicha ley establecida, entonces ésta puede ser considerada justa».

Luego entonces, para poder contar con un gobierno justo quienes lo eligen, deben conocer las cualidades y capacidades de sus elegidos, porque de acuerdo a Kant, una vez electos, no hay marcha atrás. ¿Perdió algo el liberalismo? Así es, la posibilidad de desobediencia civil.

Ahora bien, ¿Es aplicable la desobediencia civil en tiempos contemporáneos? ¿Qué dicen los nuevos abanderados del liberalismo?

Actualmente, la sociedad se encuentra inmersa en la injusticia, la pobreza y la desigualdad; que se han extendido de una manera vertiginosa. De ahí que los estudiosos de las ciencias sociales retomen al liberalismo como salida o resolución de un problema que se está agravando. Ellos sostienen «que las situaciones de pobreza extrema y miseria existentes en los países del mundo subdesarrollado constituyen un problema de justicia económica global». Una de las propuestas de John Rawls, máximo exponente del liberalismo actual, es la implementación de políticas de asistencia social, pero de ninguna manera cambiar el sistema económico.

Según Rawls, los problemas sociales actuales nada tienen que ver que las estructuras económicas internacionales, más bien son problemas locales, que los gobiernos internos han sido incapaces de resolver.

Contrario a la mayoría de los pensadores clásicos, que procuran explicar las condiciones sociopolíticas de su tiempo, pensadores contemporáneos como Rawls buscan justificar el sistema económico actual. Así pues, nos encontramos con dos posturas: una que defiende la posición del pueblo y otro que defiende la posición del gobierno.

Una división menos famosa pero más rigurosa es la que distingue entre el liberalismo predicado por Jeremías Bentham y Wilfredo Pareto propusieron otras dos concepciones para el cálculo de un óptimo de satisfacción social.

En el cálculo económico se diferencian varias corrientes del liberalismo. En la clásica y neoclásica se recurre con frecuencia a la teoría del "homo œconomicus", un ser perfectamente racional con tendencia a maximizar su satisfacción. Para simular este ser ficticio se ideó el gráfico Edgeworth-Pareto, que permitía conocer la decisión que tomaría un individuo con un sistema de preferencias dado (representado en curvas de indiferencia) y unas condiciones de mercado dadas. Es decir, en un equilibrio determinado.

Sin embargo, existe una gran controversia cuando el modelo de satisfacción se ha de trasladar a una determinada sociedad. Cuando se tiene que elaborar un gráfico de satisfacción social, el modelo benthamiano y el paretiano chocan frontalmente.

Según Wilfredo Pareto, la satisfacción de que goza una persona es absolutamente incomparable con la de otra. Para él, la satisfacción es una magnitud ordinal y personal, lo que supone que no se puede cuantificar ni relacionar con la de otros. Por lo tanto, sólo se puede realizar una gráfica de satisfacción social con una distribución de la renta dada. No se podrían comparar de ninguna manera distribuciones diferentes. Por el contrario, en el modelo de Bentham los hombres son en esencia iguales, lo cual lleva a la comparabilidad de satisfacciones y a la elaboración de una única gráfica de satisfacción social.

En el modelo paretiano, una sociedad alcanzaba la máxima satisfacción posible cuando ya no se le podía dar nada a nadie sin quitarle algo a otro. Por lo tanto, no existía ninguna distribución óptima de la renta. Un óptimo de satisfacción de una distribución absolutamente desigual sería, a nivel social, tan válido como uno de la más absoluta igualdad (siempre que estos se encontrasen dentro del criterio de óptimo paretiano).

No obstante, para igualitaristas como Bentham no valía cualquier distribución de la renta. El que los humanos seamos en esencia iguales y la comparabilidad de las satisfacciones llevaba necesariamente a un óptimo más afinado que el paretiano. Este nuevo óptimo, que es necesariamente uno de los casos de óptimo paretiano, surge como conclusión lógica necesaria de la ley de los rendimientos decrecientes.

El liberalismo, en origen, defiende la libertad individual y económica, siendo reacio a un estado fuerte (antiestatismo) y a gravar con altos impuestos a los ciudadanos. Sin embargo, a partir de esta doctrina, han surgido numerosas variantes. A continuación, se presentan las principales manifestaciones de liberalismo contemporáneo, organizadas de menor a mayor regulación (desde aceptar cierto nivel de gobierno, hasta no aceptarlo en absoluto):
"*Nota: se ha omitido en esta escala el neoliberalismo, puesto que su criterio distintivo no es ideológico, sino cronológico (aunque hay divergencia de opiniones, la acepción más generalizada es que es el mismo liberalismo tradicional, adaptado al tiempo actual)."




</doc>
<doc id="41402" url="https://es.wikipedia.org/wiki?curid=41402" title="Pichincha (Rosario)">
Pichincha (Rosario)

Pichincha es un barrio de la ciudad de Rosario (provincia de Santa Fe, Argentina). A fines del s. XIX y hasta mediados del s. XX era principalmente un barrio prostibulario; sin embargo, desde los años '90 es considerado un "Barrio de culto". Es delimitado por Vera Mujica, Santa Fe, Bv. Oroño y Av. del Valle. Oficialmente se llama Alberto Olmedo, en homenaje al humorista que nació en el barrio.

En el siglo XIX, la instalación del ferrocarril y el crecimiento de la actividad portuaria debido a la exportación de productos agrícolas, fueron hechos fundamentales para la formación del barrio Pichincha.

El centro del mencionado barrio era la estación de ferrocarriles Sunchales (actual Estación Rosario Norte), cuyo nombre aludía en realidad a una localidad del centro de la provincia de Santa Fe, ubicada a 250 kilómetros de la ciudad, donde terminaban las vías del ferrocarril, y a la que iban muchos de los envíos destinados a la ciudad de Rosario. La repetida frase de los empleados de las oficinas de encomiendas de dicha estación "¡está en Sunchales!" "
bautizó así a esa estación.

Esta estación también marcaba el límite entre la ciudad poblada y los suburbios en vías de desarrollo. Además la estación de trenes también funcionaba como terminal de tranvías hacia todos los puntos de la ciudad.

En la referida estación en la que se concentró durante varios años una nutrida cantidad de personas y personajes, albergó cuando bajaron del tren al cantor de tangos Carlos Gardel, al escritor Jorge Luis Borges y muchos otros personajes famosos de la época.

La calle principal del barrio era "Pichincha" (nombrada así en recordación a la Batalla de Pichincha, realizada en las laderas del volcán Pichincha en Ecuador, una de las principales batallas de las Guerras de Independencia Hispanoamericana), que más tarde cambiaría "para ejemplar moralización del área" su nombre por el de "Gral. Ricchieri". A principios del siglo XXI, una revisión histórica a nivel municipal propició retornar al nombre original. 

El desarrollo demográfico que experimentaba la ciudad fue el factor que propició la instalación de burdeles en toda la ciudad. Muchos funcionaban en la clandestinidad, el municipio intentó controlar y delimitar la zona para el funcionamiento de dichos establecimientos, debido a esto surge y toma auge el "imperio prostibulario" conocido como Pichincha. La mafia polaca conocida como Zwi Migdal funcionó durante décadas, dedicada a la trata de blancas. La mayor parte de estos prostíbulos, y la misma organización, desaparecieron hacia 1930, pero muchos continuaron su existencia, bajo diversos nombres, hasta el siglo XXI.
Entre los más destacados se puede mencionar:

Gran parte de las meretrices de Pichincha, mujeres explotadas como lo denunció una de ellas; Raquel Liberman, yacen sepultadas en el Cementerio de Granadero Baigorria en un lugar separado del resto de las sepulturas.

En este barrio, que conserva una gran parte de la estructura edilicia de la época, actualmente funciona un centro cultural, se encuentran varios bares y restaurantes de alto nivel, y es también un lugar de reunión de artesanos y anticuarios. Estos cambios transformaron al antiguo barrio, que ahora se perfila como un centro de cultura para la ciudad de Rosario.

Actualmente, Pichincha ha perdido su impronta prostibularia y se está transformando en un importante polo cultural de la ciudad de Rosario. Desde 2002 funciona allí la Secretaría de Cultura de la Municipalidad de Rosario (dentro del edificio de la antigua estación Rosario Norte), y el mercado de antigüedades Feria Retro «La Huella» (sobre el pavimento de la avenida Rivadavia entre avenida Ovidio Lagos y calle Rodríguez), donde cada domingo más de 150 puestos ofrecen antigüedades y objetos del pasado y de la vida cotidiana del rosarino y sus antecesores, los inmigrantes, a precios muy accesibles. 

Se agrupan por sus calles los Anticuarios de Pichincha, una veintena de comercios dedicados a la venta de antigüedades. En "la Retro" hay espectáculos callejeros de tango y teatro, radios abiertas, títeres y otras formas de expresión de cultura popular. Se asientan en las calles del barrio además, boliches y bares donde los jóvenes escuchan música, asisten a recitales u organizan performances de música electrónica.



</doc>
<doc id="41411" url="https://es.wikipedia.org/wiki?curid=41411" title="Aepyceros melampus">
Aepyceros melampus

El impala (Aepyceros melampus) es una especie de mamífero artiodáctilo de la familia Bovidae. Es un antílope de estatura mediana. Por su apariencia similar, el impala anteriormente se situaba junto a las gacelas. A causa de nuevos conocimientos, hoy se coloca en la familia de los antílopes africanos.

La palabra «impala» viene del zulú. Su nombre científico, "Aepyceros melampus", deriva del griego "aipos": alto, "keras": cuerno, "melas": negro y "pus": pie, significado "pies negros de cuernos altos".

El coche Chevrolet Impala toma el nombre de este animal.

Este esbelto animal es famoso por sus grandes saltos que pueden llegar a más de 10 m de longitud. Consigue una altura de hombro de 90 cm y un peso de 40 kg (hembra) a 65 kg (macho).

Por arriba es de color marrón claro. El vientre, el pecho, el cuello y la barbilla tienen un color blanco. En los cuartos traseros posee una línea vertical oscura y un penacho de pelo negro en las patas posteriores, por encima de las pezuñas. La cabeza es grácil, los ojos grandes y las orejas delgadas y afiladas. Los machos son los que llevan cuernos, que pueden llegar a medir hasta 90 cm de largo.

El área de distribución alcanza desde Kenia y Uganda vía Tanzania, Zambia, Mozambique y Zimbabue hasta Botsuana y en una población aislada en el sur de Angola y el norte de Namibia.

Se conocen hasta seis subespecies de impala:

El impala come hierbas, hojas y semillas. Vive en bosques poco densos y en la sabana salpicada de árboles. Este animal, en lugar de quedarse en campo abierto, como lo hacen la mayoría de los antílopes que pastan, corre a ponerse a cubierto ante cualquier amenaza.

En las épocas de hambruna, es frecuente ver a los impalas seguir a las tropas de papiones para alimentarse de las hojas y frutos que estos tiran. También los elefantes son seguidos, ya que al agitar los árboles para conseguir sus frutos, los paquidermos se dejan muchos sin recoger y es cuando aprovecha el impala para intervenir.

Durante la temporada de reproducción, por regla general, un macho de los impalas vigila un grupo de hembras, caminando de un lado a otro, exponiendo sus cuernos, con las orejas ajustadas y el rabo levantado.
La lucha de los machos por su harén se divide en tres series de combate.
Los combates entre impalas no suelen producirles heridas.

Las hembras viven con su cría en manadas de diez a cien animales. Es frecuente en la época de partos, ver a varias crías reunidas con un número muy pequeño de hembras. Estos grupos se conocen como manadas de guardería. Unas pocas hembras vigilan a las crías mientras que las demás pastan.
Aparte de estos grupos también se forman manadas de machos jóvenes y mayores, que son demasiado débiles para defender un territorio.
Los machos de mediana edad son solitarios territoriales y exigen cada una de las hembras que pase por su territorio para sí.

El impala es el animal que menos duerme en el Reino Animal, apenas 3 horas es su descanso y suele tener una especie de alarma, la cual le determina por ejemplo a qué hora debe levantarse.

Para alertar a sus demás congéneres de la presencia de un depredador o de cualquier otro peligro, los impalas emplean una serie de señales visuales, acústicas y olfativas. Las señales visuales son las más conocidas de estos animales. La parte inferior de su cola es de un color blanco inmaculado. Cuando están alertas, alzan la cola mientras van dando una serie de saltos. El color blanco de esta se puede divisar a varios metros de distancia, alertando a los demás animales de los alrededores. Las señales acústicas comprenden una serie de resoplidos, que recuerdan al disparo de un rifle. Cuando el depredador ha sido identificado, estos resoplidos resuenan por toda la sabana.

Por último, tenemos las señales olfativas. Estas son las menos conocidas. Las glándulas que los impalas portan en las patas, sueltan unas feromonas que indican la presencia de un depredador. Es probable que sea esta la razón por la cual los impalas saltan dando "coces" al aire, como si estuviesen en una mecedora flotante. Al saltar de esta manera, consiguen que el olor se vaya expandiendo por las zonas de alrededor alertando a la manada y al resto de animales de la presencia de un depredador. Esta cualidad puede ser debida a la costumbre que este antílope tiene de vivir en zonas arboladas y de vegetación espesa, donde las señales visuales no sirven en gran medida.

El impala es un antílope que debe estar en constante alerta, ya que es una pieza clave en el menú de cualquier depredador de gran tamaño de África. Sus principales depredadores incluyen a leopardos, guepardos, licaones, hienas, cocodrilos y pitónidos. Los leones también suelen cazar impalas frecuentemente sobre todo durante las épocas secas, cuando presas más substanciosas (ñues y cebras) migran a otras zonas. 
Las crías no solo tienen estos depredadores, sino también babuinos, chacales, águilas, caracales y rateles. Todo esto convierte al impala en una especie fundamental de su ecosistema.

Los impalas que pertenecen al mismo grupo social se asean los unos a los otros con la lengua. El coste del aseo —gasto de saliva y cierta reducción de la vigilancia antipredatoria— es relativamente bajo. Sin embargo, el beneficio que se obtiene por el hecho de ser aseado es considerable, ya que esta conducta sirve para eliminar ectoparásitos. Puesto que los animales no pueden asearse mutuamente al mismo tiempo, los receptores devuelven más adelante el favor a los emisores.

El impala está considerado por la UICN como especie fuera de peligro. La subespecie de cara negra ("A. m. petersi") está clasificada como vulnerable, aunque su población va en aumento.


</doc>
<doc id="41414" url="https://es.wikipedia.org/wiki?curid=41414" title="Tarragona">
Tarragona

Tarragona es una ciudad y municipio de España, en la provincia de Tarragona, y de la comarca del Tarragonés. Durante la época del Imperio romano fue una de las principales ciudades de la península ibérica, como capital sucesivamente de las provincias romanas de Hispania Citerior e Hispania Tarraconensis.

El municipio cuenta con una población de habitantes (padrón del INE a 1 de enero de 2019). Su ubicación a la orilla del Mediterráneo en la Costa Dorada, con playas de aguas cálidas, así como sus centros de recreo y tradición histórica y patrimonio artístico, la convierten en un centro de atracción turística de primer orden. Su origen se remonta a la antigua Tarraco romana, capital de la "Hispania Citerior Tarraconensis". El «Conjunto arqueológico de Tarraco» ha hecho que Tarragona sea considerada Patrimonio de la Humanidad por la Unesco. En verano de 2018 se disputaron en esta ciudad los Juegos Mediterráneos.

Tarragona posee un clima mediterráneo típico que, de acuerdo con la clasificación climática de Köppen corresponde con el clima mediterráneo Csa. La temperatura media anual es de 17,8 ºC y la precipitación supera ligeramente los 500 mm. Los inviernos son suaves y los veranos son calurosos. Las precipitaciones son irregulares, tanto dentro de un mismo año como entre diferentes años, si bien se observa un patrón según el cual la estación más seca suele ser el verano, seguido del invierno; primavera y otoño suelen registrar las mayores precipitaciones, especialmente este último. La posibilidad de precipitación se extiende durante todo el año y frecuentemente alcanza una intensidad elevada o muy elevada, aunque el episodio lluvioso suele durar poco. El carácter tormentoso de las precipitaciones suele ser más alto en verano, seguido de otoño, primavera e invierno, mientras que la precipitación tipo chubasco es más habitual en orden inverso al expuesto. La nieve no es muy rara, pero no suele caer con suficiente intensidad o duración como para cubrir el suelo, lo cual la convierte en un fenómeno subjetivamente percibido como mucho más infrecuente de lo que en realidad es. Los registros de precipitación, disponibles desde finales del aunque con discontinuidades frecuentes y a veces prolongadas, indican unos valores extremos comprendidos entre los 300 y los 700 mm anuales. El promedio anual de días lluviosos es de 56,8.

Las temperaturas presentan una pauta mucho más regular y previsible. Las heladas son infrecuentes y algunos años no llegan a registrarse. Las temperaturas máximas en los días más calurosos del año no acostumbran a superar demasiado los 30 ºC. Valores superiores a los 35 ºC no son raros, aunque son de muy breve duración. Los valores extremos registrados son: -6,5 ºC (1956, en la estación climatológica del aeropuerto de Reus) y 39 ºC (2010). El viento dominante es el Mestral (NO), seco y frío si es de componente marcado del N y más suave, incluso casi cálido, a medida que rola a componente O. En verano suele predominar un régimen de brisas.

Tarragona es famosa por su clima y patrimonio que atraen a los turistas; la ciudad es conocida, junto al municipio de Salou, como Costa Dorada.

Tarragona fue conquistada por los árabo-bereberes en una fecha variable según autores, entre el 714 y el 716. Ha sido objeto de debate si la conquista fue pacífica o tras un asedio y posterior saqueo. Esa última tesis estaría avalada por la bien documentada huida del Obispo Próspero, pero por otra parte no se han encontrado indicios arqueológicos que demuestren una interrupción súbita de la vida ciudadana. La falta de unos titulares episcopales, así como una situación excéntrica y periférica en relación a los centros de poder tanto andalusíes como cristianos, explicarían una decadencia rápida seguida de varios siglos de irrelevancia, lejos de una despoblación completa pero sin alcanzar una realidad plenamente urbana.
El valor simbólico de la antigua Tarraco, pudo actuar como incentivo para una estabilidad en el dominio cristiano, pero otros factores actuarían en contra. Hay indicios de intentos de recuperación del dominio cristiano (Reconquista) desde el Siglo IX bajo Carlomagno, pero las campañas que originarán la Marca Hispánica no alcanzaron a consolidar el dominio cristiano seguro y estable de la ciudad. Borrell II se proclamó "Príncipe de Tarragona" en el 960, pero los sucesivos intentos de consolidación del dominio cristiano sobre la ciudad demuestran lo precario que fue ese dominio, al menos hasta el siglo XI.

En 1129, san Olegario, arzobispo de Tarragona, cedió la ciudad como un principado eclesiástico al mercenario normando Robert Bordet, que había servido a las órdenes de Alfonso I de Aragón. El 14 de marzo de 1129, este caballero fue nombrado "príncipe de Tarragona" mediante un pacto de vasallaje. A partir de la infeudación del Principado de Tarragona, los normandos, comandados por Bordet, se instalaron en la ciudad. Robert Bordet aprovechó una antigua torre romana todavía en pie, la actual Torre del Pretorio, para establecer su castillo. Se iniciaba así un primer proceso de colonización de la ciudad, dirigido sobre el terreno por Robert, pero controlado desde Barcelona por el arzobispo.

La situación en la ciudad se complicó con la muerte de San Olegario. En 1146, su sucesor, Bernat Tort, un hombre de confianza del Conde de Barcelona, se estableció en la ciudad. Se iniciaba así un proceso marcado por continuos conflictos jurisdiccionales que culminaron con la extinción del principado y la restitución al Conde de Barcelona en 1151.

La Tarragona de finales del siglo XII ya era un núcleo urbano plenamente consolidado que se había convertido en el centro director de un amplio territorio. En 1148 el gobierno local se había reordenado y el consejo de habitantes de la ciudad participaba intensamente en la vida urbana. La ciudad creció y ocupó toda el área interna del Foro provincial de Tarraco, manteniendo así la estructura arquitectónica heredada de la época romana. La ciudad del siglo XII surgió, pues, en el área de grandes monumentos, alrededor de los castillos señoriales. A partir de 1146 se ocupó el área del recinto de culto de época romana, un sector que tomó especial relieve con el inicio de la construcción de la catedral en 1171, y que se convirtió en el eje vertebrador de la ciudad a partir de su consagración en 1331.

Fuera del recinto defensivo de esta primera época, había tres áreas claramente diferenciadas: en primer lugar, el "Corral", el antiguo circo romano, que se convirtió en un burgo extramuros con un mínimo de población y destinado principalmente a actividades comerciales e industriales. En segundo lugar, la "Vila Nova" que era el área que se prolongaba desde el "Corral" hasta el puerto y estaba destinada básicamente a huertos, cultivos, herrenales y molinos. A diferencia de la primera, no estaba muy habitada, excepto en el área del puerto y en la zona más próxima al "Corral". Finalmente la "huerta de Tarragona", también destinada a la explotación agraria, que se extendía a ambos lados del "Francolí" y llegaba hasta "Riu Clar".

La expansión de la peste bubónica por toda Europa marcó el inicio de un importante periodo de recesión demográfica. La epidemia llegó a la ciudad entre mayo y julio de 1348, provocando una gran mortandad. El descenso de la población y la crisis general en que se encontraba la ciudad hizo que el núcleo urbano entrara en un importante proceso recesivo. El descenso del número de fuegos se plasmó en un número menor de casas ocupadas. A pesar de esto, en 1368 la ciudad, siguiendo las directrices marcadas por la Corona, empezaba las tareas de mantenimiento y refuerzo de las murallas de la ciudad mediante la construcción de la "Muralleta" o "Mur Nou", a la altura de la fachada del circo. De esta forma el área del "Corral", el antiguo circo romano, quedó incorporada al núcleo urbano.

La situación política se agravó durante la primera mitad del siglo XV. Las diferencias entre la Generalidad de Cataluña y Juan II de Aragón provocaron una guerra civil, en la que el arzobispo se puso del lado de los realistas, mientras que el Consejo Municipal, tras un periodo de indecisión, se alió con la "Generalidad".

El 17 de octubre de 1462 las tropas de Juan II llegaron a Tarragona para sitiar la ciudad.
La guerra sumió a Tarragona en la más absoluta decadencia. Las defensas de la ciudad, especialmente en el sector del "Mur Nou", quedaron muy deterioradas, así como las del área del "Corral". La población disminuyó drásticamente y la municipalidad se declaró en quiebra. Los efectos de la guerra fueron visibles en la ciudad durante mucho tiempo.

La vida de la ciudad de Tarragona durante la época moderna está marcada por tres importantes conflictos bélicos. Desde el se construyen o consolidan fortificaciones para defender la ciudad y sus alrededores de las continuas guerras y ataques piratas. A partir de la Guerra de los Segadores y hasta mediados del Tarragona fue plaza fuerte, lo que comportaba que no se podían destruir las fortificaciones y se tenía que dejar un espacio delante de la muralla libre de edificios, con las dificultades que ello suponía para la expansión urbanística. Las epidemias fueron una constante en este periodo y provocaron grandes mortandades y el éxodo de la población.

Durante la Guerra de los Segadores (1640-1659), la situación estratégica de Tarragona le hizo sufrir dos importantes sitios, en 1641 y en 1644, que comportaron graves destrucciones de edificios y la consecuente postración y decadencia económica de la ciudad. El puerto padeció daños importantes y se abandonó durante mucho tiempo, por lo que el comercio se desvió hacia el puerto de Salou. La economía del Campo de Tarragona entró en una grave crisis de la que no se recuperó hasta finales del siglo XVIII, cuando se autorizó la reconstrucción del puerto y se concedió el permiso para comerciar libremente con América.

El segundo gran conflicto bélico que sufrió la ciudad fue la Guerra de Sucesión (1702-1714) que alcanzó Tarragona cuando todavía no se había recuperado de los estragos de la Guerra de los Segadores. Tarragona fue defendida por una guarnición británica que mejoró el sistema defensivo con la construcción de la Falsa Braga y de otros fortines y baluartes, la mayor parte actualmente desaparecidos. Cuando Felipe V accedió al poder promulgó el Decreto de Nueva Planta, que instauraba un sistema de gobierno centralizado y absolutista. En esta nueva organización, las antiguas veguerías se reagruparon en corregimientos y nacieron los ayuntamientos, al tiempo que se jerarquizó el organigrama político, se recortó el poder de los arzobispos y se suprimió la Universidad.

Otro grave problema con el que se enfrentó la ciudad durante los siglos XVI y XVII fueron las epidemias de peste y los ataques piratas. La piratería en la costa del Mediterráneo provocó la huida de la población hacia zonas más seguras del interior. Para intentar controlar los ataques piratas se construyeron torres de defensa a lo largo de la costa, como la "Torre de la Mora", o como el baluarte sobre el puerto natural de Tamarit, que data de 1617. Las batidas corsarias supusieron un importante tropiezo demográfico y económico para las zonas afectadas.

La iglesia, y más concretamente, los arzobispos jugaron un papel importante en el relanzamiento cultural, artístico y urbanístico de la ciudad en el siglo XVI, ya que estos religiosos, además de ser prelados, ocupaban importantes cargos políticos. Arzobispos como Gaspar Cervantes de Gaeta, Joan Terès y Antoni Agustín dotaron a la ciudad de una Universidad Literaria, ampliaron los límites de la ciudad amurallada hasta la actual Rambla Nova -con la construcción de la muralla de "Sant Joan"- y promovieron y financiaron obras y capillas en la catedral de Tarragona. Por otra parte, en la ciudad estaban instaladas numerosas órdenes religiosas que realizaban tareas benéficas y educativas.

La llegada del agua a la ciudad, proveniente de Puigpelat, supuso una importante mejora de la calidad de vida de la población. En este caso, también hay que destacar la contribución de la jerarquía eclesiástica al desarrollo de la ciudad, puesto que la obra fue impulsada por los arzobispos Joaquín de Santiyán y Francesc Armanyà.

Durante el siglo XVIII, la ciudad experimentó un ligero crecimiento que se verá de nuevo truncado, a principios de la centuria siguiente, por un nuevo conflicto bélico, la Guerra de la Independencia.

En el ámbito artístico, a finales del siglo XVI, se produce un renacimiento del clasicismo de la mano de la Escuela del Camp, con el apoyo del arzobispo Antoni Agustín y los canónigos humanistas.

A pesar de los acontecimientos dramáticos que marcaron la época moderna, la celebración de las fiestas tradicionales continuó siendo uno de los hitos que marcaban el calendario tarraconense. Los gremios eran los encargados del séquito que salía a la calle para las fiestas de Santa Tecla, Corpus, la llegada de los reyes y la entrada de nuevos arzobispos. A mediados del se fundó la Confraria de la Sang, cuya relevancia social sobrepasó con creces la participación en la procesión del "Santo Entierro".

En 1786 se concedió a Tarragona el permiso para comerciar libremente con América y la actividad económica se orientó hacia el comercio del vino y el aguardiente. Este hecho conllevó la expansión del cultivo de la viña en detrimento de otros productos. Con la aparición de la filoxera en Francia, hacia 1870, el cultivo se extendió de forma desmesurada hasta el punto de que se plantaban viñas en lugares poco adecuados. La ventaja de la proximidad del mercado exportador y la facilidad del transporte hacían que fuera un cultivo rentable, por lo que Tarragona se benefició mucho económicamente. Este movimiento económico motivó la aparición de una nueva clase social de obreros y menestrales, mientras que la burguesía aprovechó para invertir en diversas empresas.

El se inicia con un conflicto bélico de consecuencias devastadoras para la ciudad: la Guerra de la Independencia o Guerra del Francés. Tras un sitio largo y terrible para la población, Tarragona fue asaltada por el ejército francés el 28 de junio de 1811. A partir de ese momento, los franceses ocuparon la ciudad durante más de dos años, tras los cuales dejaron un rastro de miseria y hambre, agravados por la voladura de puntos estratégicos de la ciudad que acompañó su salida, el 19 de agosto de 1813.

La recuperación económica y demográfica fue lenta, a pesar de que se eximió a la ciudad del pago de tributos entre 1816 y 1826. Con la mejoría de la situación, se reemprenderán las obras del puerto y otras que habían quedado paradas por el conflicto. Este hecho permitirá el establecimiento de comerciantes foráneos y la formación de una pequeña burguesía comercial emprendedora que hará posible la modernización de Tarragona durante el siglo XIX.

Durante la segunda mitad del siglo XIX, las oscilaciones del precio del vino condicionaron la economía y la demografía de la ciudad, así como su expansión urbana. En periodos de euforia se incrementó la población, se fomentaron industrias auxiliares relacionadas con la exportación de vinos y se establecieron numerosas sociedades y entidades vinculadas con este comercio. A partir de mediados de siglo, el crecimiento económico posibilitó mejoras urbanas que cambiaron la fisonomía de la ciudad.

En 1868 Tarragona dejó de ser plaza fuerte, lo que permitió la construcción de edificios y viviendas fuera de la muralla. Militarmente, las murallas ya no eran necesarias, puesto que las nuevas tecnologías de guerra habían demostrado que eran inútiles. Por otra parte, la presión demográfica hacía imprescindible la urbanización de esa zona de la ciudad. Sólo a partir de 1854 y de una manera intermitente e irregular, debido a los elevados costes económicos que suponía, se inició el derribo de la muralla de "Sant Joan" que permitió la urbanización y la construcción de nuevos edificios en la actual Rambla Nova y la proyección de nuevas calles, como la de la "Unió", que harán posible la conexión de la Parte Alta con la Marina. La Parte Alta de la ciudad, más estática, continuó siendo el espacio preferido como residencia por la nobleza, por los eclesiásticos y también, por sectores, de los payeses y los artesanos. La Parte Baja o Marina, de nueva construcción, más dinámica, se convirtió en el lugar donde se establecerá la burguesía comercial con sus talleres y tiendas.

Las obras del puerto y del ensanche de la ciudad provocan el descubrimiento de numerosos restos arqueológicos. En esta época se pudieron salvar muchos restos de la antigua Tarraco, que sirvieron como base del primer Museo Arqueológico.

A lo largo de los siglos, la presencia del puerto fue determinante para el comercio de Tarragona. Las mejoras de la infraestructura del puerto durante este periodo permitieron la introducción de nuevas empresas y, por lo tanto, la modernización de la ciudad.

Durante el primer tercio del se producen en el país cambios políticos y sociales que influyeron de manera determinante en la vida de los tarraconenses: la dictadura de Primo de Rivera, la proclamación de la Segunda República y la Guerra Civil Española (1936-1939). El conflicto supuso un grave tropiezo y un retroceso en el desarrollo económico y social de Tarragona. La ciudad fue bombardeada en numerosas ocasiones, con lo que, además de sufrir un considerable número de víctimas mortales, su estructura urbanística se vio gravemente dañada con la destrucción de infraestructuras que tuvieron que reconstruirse durante los difíciles años de la posguerra.

A finales de la década de 1950 algunas industrias químicas empezaron a instalarse en la zona y en 1975 entró en funcionamiento la refinería de Enpetrol. El empuje del sector industrial también influyó de manera notable en el aspecto urbanístico y constructivo en general; ya que el aumento de población, por el incremento de la inmigración, llevó a la creación de nuevos barrios periféricos que se construyeron, al poniente, sobre la carretera de Valencia (Torreforta, Campclar, Bonavista, Icomar, Riuclar, La Floresta y la Granja) y al norte de la ciudad (Sant Salvador y Sant Pere y Sant Pau).

Tarragona pasará a ser una ciudad industrial especializada en el campo petroquímico. En estas industrias, la salida y la entrada de los productos elaborados se hace por el puerto de Tarragona que pasará a ser el segundo puerto español por cantidad de toneladas anuales.

En octubre de 2011 en la ciudad turca de Mersin la ciudad de Tarragona era seleccionada para albergar los Juegos Mediterráneos de 2017. En noviembre de 2016 en una reunión en Orán (Argelia) el Comité Internacional de los Juegos Mediterráneos, el Comité Olímpico Español y el propio alcalde de Tarragona Josep Fèlix Ballesteros decidieron pactar el aplazamiento de los Juegos hasta el verano de 2018 por la falta de 12 millones de euros que tenía que poner el Gobierno central que en ese momento estaba en funciones y al borde de unas terceras elecciones.
Tarragona tenía un total de 140 323 habitantes, según datos del padrón del 1 de enero de 2009 y retrocedió a habitantes (padrón del INE a 1 de enero de 2015). Tarragona es capital de un área metropolitana de 456 042 habitantes.<br>La población está distribuida en diferentes barrios que van de poniente a levante.

Lista de población por barrios:

Gráfico demográfico de Tarragona entre los años 1717 y 2018:

El puerto ha constituido históricamente uno de los puntales de la actividad económica. Desde mediados de la década de 1970, se encuentra siempre entre los cinco puertos españoles de mayor tonelaje. Se vincula estrechamente con el tráfico de grandes cargas a granel, especialmente petróleo y sus derivados, cereales y carbón. El tráfico de contenedores se comienza a afianzar desde la puesta en funcionamiento de una nueva terminal en el año 2008. Los cruceros sin embargo, no son todavía un tráfico frecuente. El puerto pesquero de Tarragona es el más importante de Cataluña, con un total de 8000t en 2004, que corresponde al 30 % de las capturas pertenecientes a esa comunidad.

El complejo petroquímico de Tarragona es el más importante de España, y sus factorías se extienden por el término de Tarragona y los vecinos de Vilaseca, Morell y la Pobla de Mafumet. En él se ubican empresas nacionales como Repsol o extranjeras, como Bayer y BASF. La petroquímica genera cerca de 5000 empleos, más otros 23 000 entre indirectos e inducidos. Paralelamente, existe una actividad industrial muy diversificada, con una actividad centrada en manufacturas diversas, transformados plásticos o del metal, materiales de construcción, cartonajes y embalajes, etc.

El sector terciario ocupa la parte principal de la población activa. Más allá de la actividad relacionada con la capitalidad provincial, destacan el comercio, los servicios docentes relacionados con la Universidad y otros centros educativos y, especialmente, el sector turístico.

La ciudad cuenta con una extensa cantidad de bares y restaurantes; también dispone de dos grandes centros comerciales que son El Corte Inglés inaugurado en 2010 y el Parc Central, una gran superficie que cuenta con tiendas de ropa, juguetería, tienda de electrodomésticos y sala de cines con un Eroski integrado y está situado en el centro de la ciudad. También dispone de ocho Mercadona, seis Supermercados DIA, dos Lidl y un hipermercado Carrefour situado en las afueras de la ciudad. Dispone también de un polígono comercial conocido como Polígono Comercial Les Gavarres situado por la autovía de Reus en donde se encuentran establecimientos como Decathlon, Media Markt, Leroy Merlin, McDonald's, una bolera, un cine y varios restaurantes.

Después de doce años del socialista Josep Fèlix Ballesteros como alcalde de la ciudad, los cuatro últimos gobernando con el apoyo del PP, Esquerra Republicana de Cataluña presentó junto a En Comú Podem un gobierno alternativo después de acordar un pacto de gobernabilidad y contando con los votos favorables de Junts per Catalunya y la CUP.

Tiene ocho guarderías públicas gestionadas por el Ayuntamiento y la Concejalía de Educación. La ciudad cuenta también con cuatro guarderías privadas.

Tarragona cuenta con veinte escuelas públicas de infantil y primaria y trece privadas.
En educación secundaria cuenta con ocho centros públicos y doce privados.
La formación profesional pública en Tarragona se imparte en el Complex Educatiu (Complejo Educativo) y conocida también como "Universidad Laboral" o simplemente "Laboral", que se encuentra a las afueras de Tarragona por la autovía de Salou y se distribuye en dos centros llamados "Pere Martell" y "Calipolis" y se imparten clases de grado medio y grado superior. Está gestionado por el Departamento de Enseñanza de la Generalidad de Cataluña.
Tarragona cuenta con una universidad pública: la Universidad Rovira i Virgili, fundada en 1991 y que tiene como objetivos prioritarios la docencia y la investigación. Además ha sido reconocida como Campus de Excelencia Internacional Cataluña Sur (CEICS) por el Ministerio de Educación, Cultura y Deporte. Desde 2013 dispone de una Unidad de Comunicación de la Ciencia y de la Innovación (UCC+i) -llamada ComCiència-2 para comunicar, difundir y divulgar la investigación científica que genera la universidad y los centros del CEICS con el fin de mejorar e incrementar la formación, la cultura y los conocimientos científicos de los ciudadanos.

Tarragona cuenta con un sistema sanitario muy amplio, tiene 8 centros de atención primaria y urgencias gestionados por el CatSalut.
La ciudad tiene dos hospitales públicos:

Como en el resto de la UE, en Tarragona opera el sistema de Emergencias 112, servicio gratuito mediante el cual se atiende cualquier situación de urgencias en materia sanitaria, catástrofes, extinción de incendios, salvamento, seguridad ciudadana y protección de las personas. Los Mozos de Escuadra y la Guardia Civil junto a la Guardia Urbana de Tarragona y Protección Civil, son los encargados de controlar la seguridad ciudadana.

La expedición del DNI corresponde a la Policía Nacional.

La ciudad tiene 20 líneas de autobús diurnas y 4 nocturnas, operadas por la Empresa Municipal de Transportes de Tarragona (EMTT), que está controlada por el Ayuntamiento de Tarragona.
La estación de autobuses se encuentra en el centro de la ciudad y fue inaugurada en 1988, acoge líneas regionales, nacionales e internacionales.

También cuenta con una estación ferroviaria de paso en el Corredor Mediterráneo gestionada por Adif y servida por el operador público Renfe que ofrece conexiones con todos los municipios de la provincia y con el resto de España y se encuentra a 3 km del centro de la ciudad.

Situada a 10 km del centro -también gestionada por Adif y operada por Renfe- presta los servicios AVE y Trenhotel a la ciudad, conectada a ésta por un autobús lanzadera.
Tarragona dispone de un aeropuerto situado en el municipio de Reus, a 7 km de la ciudad y conectado por un autobús lanzadera. El Aeropuerto de Reus Tarragona cuenta con vuelos nacionales e internacionales y está operado por Aena.
El Ayuntamiento tiene concedidas 93 licencias oficiales de taxi. Como en el resto de España, son vehículos blancos con una franja diagonal en cada puerta delantera sobre la que se encuentra el escudo municipal. Todos ellos deben levar visible el número de licencia y el dispositivo luminoso exterior que indica si circula "libre" o la tarifa que está siendo utilizada en caso de estar en servicio.






Cada año acuden miles de ingleses y rusos, visitan la ciudad conociendo su patrimonio, su arte, su gastronomía y su clima.



La gastronomía de Tarragona es el reflejo de su personalidad histórica y cultural: como puerto del Mediterráneo, una parte importante de su riqueza viene del mar, que se conjuga con la rica agricultura. Se pesca en Tarragona marisco y pescado azul reconocido por la Denominación de Origen Pescado Azul de Tarragona. En el "Serrallo" (el barrio de pescadores de Tarragona), se pueden encontrar varios restaurantes que ofrecen una cocina basada en los productos frescos del mar.

El plato típico por antonomasia en Tarragona es la Espineta con caracoles, especialmente consumido durante las fiestas de Santa Tecla. Es un plato que combina la espineta, lomo de atún, con caracoles, así como un completo sofrito de verduras. De cocción elaborada, necesita un reposo de una noche. Es célebre la frase ""Santa Tecla gloriosa, mare dels tarragonins, què fem avui per dinar? Espineta amb cargolins!"" (Santa Tecla gloriosa, madre de los tarraconenses, ¿qué hay hoy para comer? ¡Espineta con caracoles!)

Otros platos más típicos de Tarragona son la "cassola de romesco", el "arrossejat", el arroz negro, así como el pescado a la plancha o frito. Muchos de los restaurantes que ofrecen estos platos están situados a poca distancia del mar, lo que hace que sus platos estén hechos con pescado fresco, además de poder disfrutar de maravillosas vistas sobre el mar y del ambiente típico del Serrallo.

En cuanto a los productos agrícolas la mayoría provienen de las comarcas del interior de Tarragona. Avellanas, almendras, setas, y cítricos son también productos típicos. Merecen una citación especial y aparte los vinos de la Denominación de Origen Tarragona, en especial las mistelas y vinos rancios ideales para acompañar los postres.

En la "Parte Alta" de la ciudad muchos restaurantes se encuentran en edificios históricos que transportan al gourmet a la época del esplendor imperial o a los años de la difícil reconquista. Para completar esta ambientación histórica en el mes de mayo se celebran las jornadas gastronómico-culturales "Tarraco a Taula", que ofrecen la oportunidad de catar algunos platos extraídos de recetas romanas.

En Tarragona también son típicos los bares de tapas o "llesqueries", y con la llegada del buen tiempo se puede disfrutar en cualquier plaza o calle de la ciudad de las terraza de los bares para tomar el vermut antes de comer.

La fiesta grande de la ciudad, entre el 15 y el 24 de septiembre, declarada de interés turístico nacional por el Gobierno de España y fiesta tradicional de interés nacional por la Generalidad de Cataluña. Se realizan más de 500 actividades.

El primer domingo antes del día 23 se organiza una gran diada castellera con las mejores collas del momento, la Jove de Tarragona y los Xiquets de Tarragona están siempre presentes como las dos "collas" locales más grandes. En esta diada se ven castillos humanos espectaculares, aunque los años que hay concurso de "castells", las "collas" no ponen el listón tan alto y suelen reservarse.

El día 19 se celebra la "Santa Tecla pequeña" donde los niños y niñas salen por las calles de Tarragona recreando en miniatura el seguici popular, con sus mismas figuras, bailes, y "castells", a escala más pequeña.

Otro acto muy destacado, bastante reciente, es la Baixada de l'Àliga. La madrugada del 21 al 22 los portantes de l'Àliga (elemento del "seguici" festivo de gran importancia) solían llevarla hasta el Ayuntamiento de Tarragona, y así dejarla preparada para sacarla en la "cercavil·la", y se ha acabado convirtiendo en una tradición donde apenas cabe una aguja de lo que se llega a llenar la calle para ver este destacado acto festivo.
Tanto es así que ahora no solo l'Àliga está en la fiesta sino que también la Mulassa, el Lleó, y los gigantes se han unido junto con los músicos y ahora es una parte más del espectáculo y sobre todo la fiesta.

El día 22 por la tarde sale el "seguici" o cortejo desde el Ayuntamiento, recorre el Casco Antiguo, la rambla y vuelve al Ayuntamiento. El "seguici" está compuesto por los "correfocs" que son los diables, el Drac de Sant Roc, el Bou y la Víbria. A estos les sigue el Ball de Serrallonga (els Trabucaires), l'Àliga, la Mulassa, la Cucafera, el Lleó, el Magí de les timbales, el gegants petits (els gegantons negritos), les dues parelles de gegants grans, els Capgrossos de la ciutat y los bailes. Los bailes son: el Ball de Bastons, el dels Pastorets, el de turcs i cavallets, el Patatuf, el Ball de Cercolets, otro Ball de Bastons, el Ball de Gitanes, el de Valencians, el de cossis y el dels Set Pecats Capitals, así como la representación de la Pasión de Jesucristo denominada 'Moixiganga' o Mogiganga.

Mientras tanto, en el Balcón del Mediterráneo y otros sitios hay conciertos y verbenas y por la noche, en el Camp de Mart, se celebra L'Empalmada, que se empalma hasta el día siguiente con música y la Mamadeta, que es la bebida típica de las fiestas de Santa Tecla (granizado de limón y "chartreuse").

El día 23, a primera hora de la mañana empieza con el Toc de Matinades. Grallers de diferentes entidades o por libre, se pasean por las calles de la ciudad, desde antes de que amanezca, tocando la canción de Matinades y gozando de buenos almuerzos que les ofrecen los tarraconenses. Después de almorzar, el Seguici Festiu sale de oficio para anunciar que es Santa Tecla y al mediodía, las cuatro colles castelleres (els Xiquets de Tarragona, la Colla Jove dels Xiquets de Tarragona, el Xiquets del Serrallo y el Castellers de Sant Pere i Sant Pau) actúan en la Plaza de la Font, delante del Ayuntamiento.
El 23 de septiembre de 2011 fue una gran "diada" para los Xiquets de Tarragona que consiguieron hacer "la tripleta" sin caerse y para la Colla Jove dels Xiquets de Tarragona, que se proclamó "colla de gama extra" cargando el 5 de 9 "amb folre" (nueve pisos y cinco personas en cada piso.)

Por la tarde vuelve a salir el Seguici Popular incluidas las colles castelleres mientras el brazo de Santa Tecla es llevado en procesión. Un momento realmente espectacular es alrededor de las 20:00h cuando el brazo de Santa Tecla entra en la Catedral con todos los elementos del Seguici Popoular actuando a la vez en la misma plaza. Después se bajan las escaleras de la Catedral y el Seguici Festiu finaliza de nuevo en la Plaza de la Font con otra actuación conjunta. Es la tercera y última salida del Seguici Festiu de las Fiestas de Santa Tecla. En ese momento se enciende el castillo de fuegos artificiales por parte del ganador del Concurso de Fuegos Artificiales que se celebra en julio.

El día 24 por la mañana se celebra una actuación castellera de las cuatro "colles" y seguidamente, montan unos pilares de cuatro pisos que son llevados andando desde la Catedral hasta la Plaza de la Font.

Los Xiquets del Serrallo, consiguieron en 2016 completar el camino sin que el pilar se cayese, por 21 años consecutivos.
Santa Tecla acaba con un gran Correfoc con los elementos de fuego de toda Cataluña la noche del último domingo de Fiesta Mayor. Normalmente, por esas fechas tan notorias, los tarraconenses van a buscar caracoles por la zona del Loreto, para hacer para comer al almuerzo la tradicional "espineta con caracoles" de Santa Tecla. Es un plato compuesto por atún, caracoles, patata, cebolla, tomate, salsa de "romesco", entre otros ingredientes.

Los tarraconenses, por las fiestas de Santa Tecla, dicen:

La fiesta pequeña de la ciudad, entre el 16 y el 19 de agosto. Durante estas fiestas se traen en carros, toneles de agua de las fuentes del Brufaganya (agua supuestamente bendecida por el santo patrón) hacia la ciudad. El "seguici" popular, formado por gigantes, cabezudos, dos "balls de bastons", "grallers" y la Guardia Urbana montada a caballo, recibe los carruajes y los conduce en procesión, ya con la imagen de San Magí en un paso, hasta la ermita del Portal del Carro, dónde acabados los pilares de las cuatro "colles castelleres" de la ciudad y la espectacular traca, los tarraconenses pueden beber el agua milagrosa de San Magí.

Durante las fechas de San Magí, la ciudad goza de aspectos festivos, cada vez más abundantes. Uno de los más destacados es la "remullada", una verbena que se hace en la Plaza del Rey donde el espectáculo musical está acompañado de surtidores gigantes que mojan y riegan la plaza con agua fresca, aliviando el calor pero sobre todo provocando una lluvia de intensa diversión. Los surtidores son una sandía (por el verano, es la fruta que se come), un botijo (por la fiesta de San Magí), una almeja (por estar al lado del mar) y un tren. La "remullada" se hace la noche del día 18 y dura hasta las 3:00 de la madrugada.

El día 19 por la mañana se celebra una actuación castellera en la plaza de les Cols, dónde actúan las cuatro "collas" locales.

Evento bienal, se celebra el primer fin de semana de octubre los años pares. Es el principal certamen de esta tradición catalana.
A principios de agosto, se hace una valoración de la "clasificación castellera" y las mejores "collas" son invitadas a actuar en la Tarraco Arena Plaça (antigua plaza de toros) junto a los Xiquets del Serrallo y la Colla Castellera de Sant Pere i Sant Pau, que pese a sus dificultades para igualar a las grandes "collas", son invitadas también por ser de la ciudad, deleitando aun así con sus espectaculares castillos más pequeños pero también más trabajados y más emocionantes.

En 2012, por primera vez se añadió al evento un segundo día de concurso, en el que "collas" más pequeñas tuvieron la oportunidad de competir en el concurso de "castells" de Tarragona, participando en la misma clasificación que las más punteras.

En este evento los "castells" asumen un carácter competitivo a diferencia de en el resto de actuaciones. Es por eso que en el concurso de castillos se ven construcciones humanas realmente espectaculares. Las últimas seis ediciones han sido ganadas por los Castellers de Vilafranca, marcando así un récord más en el "mundo casteller".

El primer domingo de octubre de los años pares Tarragona se tiñe de colores de las diferentes collas y se convierte en una ciudad puramente castellera.

La Procesión del Santo Entierro es la más emblemática de las que se celebran en Tarragona durante las festividades de Semana Santa. Se celebra el día de Viernes Santo, aunque antiguamente se realizaba el Jueves Santo.

La tarde del Viernes Santo se hace la recogida de los pasos por parte de los "armats". Este acto no suele verse en otras ciudades o pueblos y en Tarragona es uno de los eventos más esperados por la mayoría de tarraconenses, ya que es algo muy dinámico y espectacular. Esta reunión empieza hacia las 16:00h. La gente puede acercarse mientras se forma la cola procesional que saldrá sobre las 19:30. Los "armats" son uno de los elementos más emblemáticos de la Semana Santa de Tarragona. Están documentados desde 1758 y ellos son los que escoltan y recogen los pasos antes de la procesión y encabezan, cuando todos los pasos están en la Plaza del Rey, el cortejo. La cohorte está formada por el centurión o "Capità Manaies", un tocador de trompeta, 6 tocadores de timbales, que van vestidos con capa blanca, y 32 "armats" con capa roja, coraza, escudo y lanza.
A principios del año 2011 se hizo público el proyecto de los armats de renovar su vestimenta, diseñándola más vistosa y más realista pero sin perder la estética tradicional de los Armats de Tarragona.

Esta procesión data de 1550, organizada por la "Reial i Venerable Congregació de la Puríssima Sang del nostre Senyor Jesucrist", que se creó en 1545 cuando los alpargateros y los esparteros de la ciudad se agremiaron. Su recorrido por las calles del casco antiguo de la población le da una estética espectacular. Intervienen alrededor de unas 5.000 personas y 20 pasos, de los que la mitad se cargan a la espalda. La Semana Santa tarraconense fue declarada "Fiesta Tradicional de Interés Nacional".

El Carnaval de Tarragona se remonta a las fiestas saturnales, lupercales y matronales que celebraban los romanos. El carnaval se continuó celebrando durante todas las épocas de la historia, pese a que fue prohibido en varios momentos, el último de ellos durante la dictadura.

Es característica del carnaval de Tarragona la colocación de la "Bóta" en la plaza del ayuntamiento que indica el principio y final (cuando se queman todos los "Ninots"). Son también características las rúas de "Ninots i Reis" (con sus respectivos séquitos), las sátiras, los "saraus", entre otros muchos elementos. Y al final de la celebración la quema de los "Ninots" y el desfile mortuorio de los "Cremallers i Ploraneres".


La semana anterior a la de Pascua: único certamen dedicado a los orígenes del jazz en España y uno de los principales de Europa.


Cada 23 de abril se venden rosas y libros en la parte central de la Rambla Nova. Además, suele ser el inicio de la temporada de "Castells" de Tarragona, por eso por la tarde de este mismo día, els Castellers de Sant Pere i Sant Pau, els Xiquets del Serrallo, els Xiquets de Tarragona y la Colla Jove Xiquets de Tarragona, actúan en la Rambla Nova, con castillos modestos que son como el "pistoletazo de salida" de la temporada.


Celebrado durante las dos últimas semanas de mayo, es un festival cultural internacional dedicado y especializado en la divulgación histórica de la época romana. Con él, la ciudad vuelve a la época clásica con legionarios, gladiadores, artesanía y comida.


El 23 de junio se celebra la noche de San Juan con petardos, "correfocs", tracas y baile hasta la madrugada.


La primera semana completa de julio es el certamen pirotéctico de referencia del Mar Mediterráneo. Los fuegos artificiales se lanzan desde el cabo del Milagro por parte de compañías pirotécnicas de cualquier parte del mundo. Aquellos que hagan las formas, los colores y los efectos más bonitos recibirán más votos por parte del público y aquel que se declare ganador del concurso, tendrá el honor de ser el encargado del castillo de fuegos artificiales que ponga fin a las fiestas de Santa Tecla.

El Club Gimnàstic de Tarragona es un club polideportivo de la ciudad de Tarragona. Es la entidad deportiva más antigua en existencia ininterrumpida de España. Aunque la entidad se fundó en 1886, el club de fútbol no se creó hasta 1914. Su primer equipo de fútbol juega actualmente en la Segunda División de España.

Tarragona fue escogida el 15 de octubre de 2011, en la ciudad de Mersin (Turquía), como la sede de los XVIII Juegos Mediterráneos de 2017. Por motivos económicos se retrasó un año y fue disputada entre el 22 de junio y el 1 de julio de 2018 con sede olímpica en el barrio de Campo Claro.






</doc>
<doc id="41415" url="https://es.wikipedia.org/wiki?curid=41415" title="Macintosh">
Macintosh

Macintosh, abreviado como Mac, es la línea de computadoras personales diseñada, desarrollada y comercializada por Apple Inc. En sus inicios fue una alternativa económica y doméstica al Lisa, un avanzado microcomputador empresarial, cuya línea de desarrollo fue absorbida por la línea Macintosh. El Mac terminó por convertirse en la línea estándar de desarrollo de los computadores de Apple, al desaparecer la línea evolutiva del Apple II.

El Macintosh 128K, llamado así a cuenta de sus 128 KiB de memoria RAM, fue lanzado el 24 de enero de 1984. Fue el primer computador personal que se comercializó con éxito que usaba una interfaz gráfica de usuario (GUI) y un ratón en vez de la línea de comandos. Sus características técnicas revolucionaron la industria de computadores a mediados de la década de 1980, manteniendo su línea evolutiva de desarrollo hasta el día de hoy.

La gama de productos Mac en la actualidad varía desde el básico Mac Mini de escritorio hasta los servidores de rango medio como Mac Pro. Los sistemas Mac tienen como objetivo principal de mercado el hogar, la educación y la creatividad profesional. La producción de Mac está basada en un modelo de integración vertical en los que Apple proporciona todos los aspectos de su hardware y crea su propio sistema operativo, que viene preinstalado en todos los Mac. Esto contrasta con los PC preinstalados con Microsoft Windows, donde un vendedor proporciona el sistema operativo y múltiples vendedores el "hardware". En ambos casos, el "hardware" permite el funcionamiento de otros sistemas operativos: A partir de 1998, los Mac son capaces de soportar sistemas operativos como Linux, FreeBSD y Windows. En la actualidad también es posible modificar el sistema operativo de Apple para hacerlo compatible con la mayoría de hardware existente; es el llamado movimiento OSx86.

Los primeros Macintosh estaban basados en los microprocesadores de la familia Motorola MC68000, con tecnología CISC. En marzo de 1994, Apple introdujo en la gama Macintosh los chips PowerPC del Consorcio Apple-IBM-Motorola, que suponían el cambio a la tecnología RISC. En 2006, Apple inició la transición desde la línea de PowerPC a los procesadores Intel con arquitectura x86. Los Mac actuales usan la serie de microprocesadores Intel Core i3, Intel Core i5, Intel Xeon e Intel Core i7. Todos los modelos de Mac actuales vienen con una versión nativa de la última versión de macOS, que en octubre de 2019 se actualizó a su última versión, macOS "Catalina".

El proyecto Macintosh arrancó a finales de los años 70's con Jef Raskin, un empleado de Apple que visualizó un ordenador de bajo costo y fácil de usar para el consumidor medio. Raskin quería que el nombre del equipo fuera el de su tipo favorito de manzana, la McIntosh, pero no pudo por razones legales, ya que estaba demasiado cerca, fonéticamente, al del fabricante de equipos de audio McIntosh. Steve Jobs pidió la liberación del nombre para que Apple pudiera usarlo, pero se le negó, obligando a Apple a comprar finalmente los derechos para usar el nombre. Se autorizó a Raskin para iniciar el proyecto en septiembre de 1979, y comenzó a buscar un ingeniero que pudiera construir un prototipo. Bill Atkinson, miembro del equipo de Apple Lisa (una máquina similar, pero de gama más alta), le presentó a Burrell Smith, un técnico autodidacta de servicio que había sido contratado a principios de ese año. Con los años, Raskin fue reuniendo un gran equipo de desarrollo que diseñó y construyó el hardware y software original del Macintosh. Además de Raskin, Atkinson y Smith, en el equipo estaban George Crow, Chris Espinosa, Joanna Hoffman, Bruce Horn, Susan Kare, Andy Hertzfeld, Guy Kawasaki, Daniel Kottke, y Jerry Manock.

La primera placa de Smith para el Macintosh se construyó con las especificaciones de diseño de Raskin: tenía 64 KiB de memoria RAM, utilizaba el microprocesador Motorola 6809E y era capaz de soportar un mapa de bits de 256 × 256 píxeles en negro-blanco. Bud Tribble, un programador de Macintosh, se interesó en que funcionaran los programas gráficos del Lisa en el Macintosh y pidió a Smith si podía incorporar el microprocesador Motorola 68000, que usaba el Lisa, en el Mac si se mantenían los costos de producción bajos. En diciembre de 1980, Smith había tenido éxito en el diseño de una placa que no solo utilizaba el 68000, sino que aumentó su velocidad de 5 MHz a 8 MHz; esta placa también tenía la capacidad de admitir una pantalla de 384 × 256 píxeles. El diseño de Smith utilizaba menos chips de RAM que el de Lisa y esto hizo que la producción de la placa fuera mucho más rentable. El diseño final de Mac era "todo-en-uno" y tenía el lenguaje completo de imagen QuickDraw y su intérprete en 64 KiB de ROM, mucho más que la mayoría de otros equipos; además tenía 128 KiB de RAM, en forma de dieciséis chips de 64 Kibit de RAM soldada a la placa lógica. Aunque no tenía ranuras de memoria, su capacidad era ampliable a 512 KiB soldando dieciséis zócalos IC de 256 Kibit chips de RAM en lugar de los chips instalados de fábrica. La pantalla del producto final era de 9 pulgadas, monocromática, de 512x342 píxeles, superior a los prototipos. La máquina fue pionera también en la incorporación de disquetes de 3 1/2" fabricados por Sony, que se establecieron durante varios años en un estándar para el manejo y transporte de archivos

El diseño llamó la atención de Steve Jobs, cofundador de Apple. Al darse cuenta que el Macintosh era más comercial que el Lisa, comenzó a centrar su atención en el proyecto. Raskin finalmente abandonó el proyecto Macintosh en 1981 durante un conflicto de personalidad con Jobs. Un miembro del equipo, Andy Hertzfeld, dijo que el diseño final de Macintosh estaba más cerca de las ideas de Jobs que las de Raskin Después de oír hablar de la pionera tecnología de interfaz gráfica de usuario, que se estaba desarrollando en Xerox PARC, Jobs había negociado una visita para ver al equipo Xerox Alto y las herramientas de desarrollo Smalltalk a cambio de opciones sobre acciones de Apple. Las interfaces de usuario del Lisa y del Macintosh se vieron influenciadas en parte por la tecnología que el equipo vio en el Xerox PARC y éstas se combinaron con las ideas propias del grupo de Macintosh. La Mac fue pionera, junto con el computador Lisa, en el empleo de un GUI, o interfaz gráfica de usuario, aunque ambas líneas de computadores no fueron compatibles. La razón de esto fue que los proyectos Macintosh y Lisa fueron líneas de desarrollo paralelas e incluso rivales dentro de la empresa Apple debido a la forma de enfocar el proceso de trabajo por parte de Jobs, que formó parte de ambos equipos. Jobs también contrató al diseñador industrial Hartmut Esslinger para trabajar en la línea Macintosh, dando como resultado la línea que se denominó en diseño "Snow White" (Blancanieves). Aunque Hartmut llegó demasiado tarde para los primeros Mac, esos conceptos de diseño se aplicaron en la mayoría de los ordenadores de Apple de mediados de los 80. Sin embargo, el liderazgo de Jobs en el proyecto Macintosh no duró mucho: en 1985,después de una lucha interna de poder con el nuevo CEO John Sculley, Jobs fue despedido de Apple, fundó NeXT,y no regresó hasta 1997, cuando Apple, doce años más tarde, adquirió NeXT y todos sus activos.

El primer Macintosh, el Macintosh 128K, se anunció a la prensa en octubre de 1983, seguido por un dosier de 18 páginas incluidas en varias revistas en diciembre del mismo año. El 22 de enero de 1984 se presentó con el famoso anuncio de televisión dirigido por Ridley Scott "1984", que se emitió en el tercer cuarto de la XVIII Super Bowl, su producción tuvo un coste de 1.5 millones de dólares americanos y a día de hoy se considera un hito y una obra maestra. El comercial "1984 "utiliza una heroína sin nombre para representar la llegada de los Macintosh (indicado por un cuadro al estilo de Picasso de la computadora Macintosh de Apple en su camiseta blanca) como un medio de salvar a la humanidad de la "conformidad" de los intentos de IBM con su producto PC para dominar la industria de la informática. El anuncio alude a la novela de George Orwell, "1984", que describe un futuro distópico gobernado por un televisado "Gran Hermano".

Dos días después de que se emitiera el anuncio "1984", el Macintosh salió a la venta. Se suministraba con dos aplicaciones diseñadas para mostrar su interfaz gráfico: MacPaint y MacWrite. La primera demostración pública fue realizada por Steve Jobs en el primero de sus famosos discursos de las Keynote, y aunque el Mac obtuvo un seguimiento inmediato y entusiasta, algunos lo calificaron de mero "juguete", debido a que el sistema operativo fue diseñado en gran medida alrededor de la interfaz gráfica de usuario, las aplicaciones existentes para los interfaces textuales basados en líneas de comandos tenían que ser rediseñadas y el código de programación, reescrito. Esa era tarea que consumía mucho tiempo y muchos desarrolladores de software decidieron no llevarla a cabo. Posiblemente esta sea la razón por la cual hubo una ausencia inicial de software para el flamante Macintosh. Además, hay que tener en cuenta que, para crear software original para el Macintosh, hacía falta tener un Apple Lisa, crear el código y luego compilarlo para el Macintosh, y eso no estaba al alcance de cualquiera. En abril de 1984 Microsoft migró desde MS-DOS la hoja de cálculo MultiPlan y en enero de 1985, Microsoft Word. En 1985, Lotus presentó Jazz para la plataforma Macintosh tras el éxito de Lotus 1-2-3 para el IBM PC, aunque fue, en gran medida, un fracaso. Apple introdujo la Macintosh Office el mismo año que el anuncio de los lemmings. Este comercial es tristemente célebre por sus insultos a los potenciales clientes. El anuncio no tuvo éxito.

Para una edición especial de la revista "Newsweek" después de las elecciones de noviembre de 1984, Apple gastó más de 2.5 millones de dólares para comprar todas las secciones de publicidad de las 39 páginas de esta edición. Apple también publicó la promoción "Test Drive a Macintosh" (prueba un Macintosh). Los potenciales compradores, con una tarjeta de crédito, podían llevarse a casa un Macintosh durante 24 horas y después devolverlo a un distribuidor. Mientras que 200 000 personas participaban, a los distribuidores no les gustaba la promoción: el suministro de ordenadores era insuficiente para la demanda y muchos fueron devueltos en condiciones tan malas que ya no podían ser vendidos. Esta campaña de marketing hizo que el CEO John Sculley tuviera que elevar el precio desde los 1 995 dólares a los 2 495 dólares (unos 5 200 dólares actuales ajustando la inflación a 2010)

En 1985, la combinación del Mac, una impresora Apple LaserWriter, y el software MacPublisher o Aldus PageMaker, junto a la tecnología PostScript de Adobe y la capacidad WYSIWYG (What You See Is What You Get) permitió a los usuarios diseñar, pre-visualizar e imprimir diseños de páginas completas con texto y gráficos, una actividad que se conoce como autoedición. Inicialmente, la autoedición era un área que se realizaba exclusivamente en los Macintosh, pero con el tiempo llegó a estar disponible para otras plataformas. Más tarde, aplicaciones como Macromedia FreeHand, QuarkXPress, Adobe Photoshop y Adobe Illustrator reforzaron la posición del Mac como ordenador gráfico y contribuyeron a ampliar el emergente mercado de la autoedición.

Las limitaciones de memoria de los primeros Mac se hicieron rápidamente evidentes, incluso en comparación con otros ordenadores personales de 1984, y no se podían ampliar con facilidad. Carecían de una unidad de disco duro y los medios para fijar una con facilidad. Surgieron muchas empresas pequeñas para hacer frente al problema de memoria; estas ofrecían actualizaciones para el Mac de 128 KiB a 512 KiB, ampliación que requería la eliminación de los 16 chips de memoria que venían soldados con el equipo y su sustitución por chips de mayor capacidad, una operación tediosa que no siempre funcionaba. En octubre de 1985, Apple aumentó la memoria del Mac a 512 KiB y se ofreció una actualización para el Mac 128K que implicaba sustituir la placa lógica. En un intento por mejorar la conectividad, Apple lanzó el Macintosh Plus, el 10 de enero de 1986, al precio en Estados Unidos de 2600 dólares. Ofrecía un mebibyte de RAM, fácilmente ampliable a cuatro por el uso de placas de RAM de socket, y una interfaz SCSI que permitía hasta siete dispositivos periféricos, tales como discos duros y escáneres, que se podían conectar externamente al Mac. Su unidad de disquete incrementó su capacidad a 800 kB. El Mac Plus fue un éxito inmediato y se mantuvo en producción, sin cambios, hasta 15 de octubre de 1990. Estuvo a la venta poco más de cuatro años y diez meses, y fue el Macintosh más longevo en la historia de Apple.

Con la introducción del Macintosh II, en 1987, Apple introdujo la nueva línea de procesadores de Motorola, el 68020, un procesador a 16 MHz que junto con el co-procesador matemático de coma flotante Motorola 68881 hacían de esta máquina una de las más rápidas. La principal mejora en el Macintosh II fue Color QuickDraw en la ROM, una versión del lenguaje gráfico en color, que era el corazón de la máquina. Entre las muchas innovaciones de Color QuickDraw, las más destacables fueron: la capacidad de manejar cualquier tamaño de pantalla, cualquier profundidad de color, y varios monitores.

El Macintosh II marcó el inicio de una nueva dirección para el Macintosh. Por primera vez tenía una arquitectura abierta con varias ranuras de expansión NuBus, soporte para gráficos en color y monitores externos, y un diseño modular, similar al PC de IBM. Tenía un disco duro interno y una fuente de alimentación con un ventilador, que inicialmente hacía bastante ruido. Un desarrollador independiente vendía un dispositivo para regular la velocidad del ventilador basado en un sensor de calor, pero anulaba la garantía. Los siguientes diseños Macintosh han ido silenciado sus fuentes de alimentación y discos duros.

En septiembre de 1986, Apple presentó el Macintosh Programmer's Workshop (Taller de Programador de Macintosh), o MPW que permitía a los desarrolladores de software crear software para Macintosh en Macintosh, en vez de la compilación cruzada desde un Lisa. En agosto de 1987 Apple dio a conocer HyperCard e introdujo el MultiFinder, que añadió multitarea cooperativa al Macintosh. Apple comenzó a servir ambos programas con todos los Macintosh.

El Macintosh SE se lanzó al mismo tiempo que el Macintosh II, en 1986. El Macintosh SE fue el primer Mac compacto con una unidad de 20 MB de disco duro interno y una ranura de expansión, que se encontraba dentro de la caja junto con el monitor CRT, lo que representaba un potencial peligro ya que para conectar algún dispositivo se podía exponer a la persona que lo realizara a la alta tensión del monitor. Por esta razón, Apple recomendaba a los usuarios que llevaran sus SE a un distribuidor autorizado de Apple para realizar estas actualizaciones. El SE también actualizó el diseño original de Jerry Manock y Terry Oyama y compartía con el Macintosh II el concepto de Snow White (Blancanieves), y también el nuevo Apple Desktop Bus (ADB ) para el ratón y el teclado que había aparecido, unos meses antes y por primera vez, en el Apple IIGS.

En 1987, Apple escindió su negocio de software hacia una nueva empresa llamada Claris. Se le dio el código y los derechos de varias aplicaciones que se habían escrito dentro de Apple, en particular MacWrite, MacPaint, y MacProject. A finales de 1980, Claris dio a conocer un número de títulos de software renovado. El resultado fue el "Pro" de la serie, incluyendo MacPaint Pro, MacDraw Pro, MacWrite Pro y FileMaker Pro. Para ofrecer una suite ofimática completa, Claris compró los derechos de la hoja de cálculo de Informix Wingz en el Mac y le cambió el nombre a Claris Resolve; además, añadió al conjunto un nuevo software de presentaciones, llamado Claris Impact. En la década de 1990, las aplicaciones de Claris se incluían con la mayoría de los Macintosh de consumo y fueron muy populares. En 1991, Claris publicó ClarisWorks, convirtiéndose rápidamente en su segunda aplicación más vendida. En 1998 Claris se reincorporó de nuevo en Apple y ClarisWorks en la versión 5.0 pasó a llamarse a partir de entonces AppleWorks.

En 1988, Apple demandó a Microsoft y Hewlett-Packard con el argumento de que infringieron los derechos de autor de Apple al incorporar en sus SO una interfaz gráfica de usuario. Su argumentación se basaba, entre otras cosas, en «el uso de ventanas con formas rectangulares, que se superponen y de tamaño variable». Después de cuatro años, el caso se decidió en contra de Apple, al igual que las apelaciones posteriores. Estas acciones de Apple fueron criticadas por algunos en la comunidad de software, incluyendo la Free Software Foundation (FSF), que sentía que Apple estaba tratando de monopolizar los interfaces gráficos de usuario en general, y boicotearon la aparición de software GNU para la plataforma Macintosh durante siete años.

El Macintosh IIx se lanzó en 1988 con el nuevo procesador Motorola 68030, que incluía importantes mejoras internas como la inclusión de MMU en el propio procesador y capacidad de direccionar 32bits de forma nativa. Le siguió en 1989 una versión más compacta, con menor número de ranuras (el Macintosh IIcx) y una versión del Mac SE, el Macintosh SE/30, que incluía un procesador 68030 de 16 MHz. Más tarde en ese mismo año, el Macintosh IIci, que funcionaba a 25 MHz, fue el primer Mac que era realmente un ordenador de 32 bits nativo o "limpios", que permitía soportar de forma nativa más de 8 MiB de memoria RAM, a diferencia de sus predecesores, que se denominaban de 32 bit "sucios" (8 de los 32 bits disponibles se reservaban para direccionar los flags (banderas) del sistema operativo). Con la introducción del Sistema 7, el primer sistema operativo de Macintosh que soportaba el direccionamiento de 32 bits, Apple ofrecía un ecosistema completo y nativo de 32 bits. Apple también presentó el primer portátil Macintosh, con un procesador 68000 de 16 MHz con una pantalla plana de matriz activa que en algunos modelos incorporaba retroiluminación (backlit). Al año siguiente se introdujo el Macintosh IIfx, una máquina con un precio a partir de 9900 dólares USA. Aparte de su rápido procesador 68030 de 40 MHz, incorporaba importantes mejoras internas en su arquitectura, incluyendo una memoria más rápida y dos CPUs de Apple dedicadas aI procesamiento de I/O, además de 6 ranuras NuBus.

En mayo de 1990 Microsoft sacó a la venta Windows 3.0. Esta versión comenzó a acercarse al sistema operativo del Macintosh, tanto en conjunto de su rendimiento como en las características, convirtiéndose en una alternativa menos costosa que la plataforma Macintosh.

La respuesta de Apple fue la introducción de una serie de Macs relativamente baratos hacia octubre de 1990:
Estas tres máquinas se vendieron bastante bien, aunque el margen de beneficio de Apple fue considerablemente más bajo que el de máquinas anteriores.

Apple fue actualizando y mejorando las características de los ordenadores Macintosh con la introducción de los nuevos modelos equipados con procesadores 68K según estaban disponibles por parte de Motorola. Al Macintosh Classic II y Macintosh LC II, que utilizaban una CPU 68030 de 16 MHz, se unieron en 1991 los Macintosh Quadra 700 y 900, los primeros Macs en emplear el procesador CISC más rápido de Motorola, el 68040.

En 1991 Apple reemplazó el Macintosh Portátil con la primera de la línea PowerBook:
Estos fueron los primeros ordenadores portátiles con espacio en el teclado para reposamanos y con un trackball situado delante del teclado como dispositivo puntero. En 1993 el PowerBook 165c se convirtió en el primer ordenador portátil de Apple que ofrecía una pantalla en color capaz de mostrar 8 bits de profundidad (256 colores) con una resolución de 640 × 400 píxels. La segunda generación de PowerBooks, la serie 500, estaban equipados con procesadores 68040 e introdujeron el trackpad, altavoces estéreo integrados y Ethernet incorporada en todos los portátiles de fábrica en 1994.

En cuanto al Mac OS, el Sistema 7 fue reescrito en 32 bits desde Pascal a C++ e incorporó memoria virtual y mejoras en el manejo de gráficos en color, direccionamiento de memoria, redes y multitarea cooperativa. También durante este tiempo, el Macintosh comenzó a desechar las directrices del diseño "Snow White" (Blancanieves) y abandonó la consultoría de Frogdesign, por la que Apple pagaba caros honorarios, a favor de realizar ese trabajo en casa; para ello se creó el Grupo de Diseño Industrial de Apple, que se convirtió en responsable de la elaboración de una nueva imagen que acompañara al nuevo sistema operativo y todos los demás productos de Apple.

En 1994, Apple abandonó la arquitectura CISC de Motorola para introducir la nueva arquitectura RISC PowerPC desarrollada por el alianza AIM de Apple Computer, IBM y Motorola. La línea Power Macintosh fue la primera en utilizar estos nuevos chips, demostrado ser un gran éxito con más de un millón de unidades equipadas con PowerPC vendidas en nueve meses.

Este cambio de arquitectura supuso una transformación y convulsión en toda la gama de productos, ya que los nuevos procesadores PowerPC eran completamente incompatibles con la gama 68K. En un principio todo el software que existía para los Macintosh tendría que ser reescrito, pero Apple, para hacer una transición más suave, introdujo en el sistema operativo un emulador por software de las instrucciones 68k a las instrucciones PowerPC, emulador que funcionaba relativamente bien con la mayoría del software, sobre todo aquel que no hacía llamadas directas a funciones del hardware o no utilizaba ciertas características del procesador 68040, como las instrucciones de coma flotante, realmente el emulador no usaba el conjunto de instrucciones completo del 68040 sino las características e instrucciones del 68EC040.

Esta transición puedo realizarse ya que el emulador sobre PowerPC era suficientemente eficiente como para obtener un rendimiento similar a la gama 68k nativa. El cambio de arquitectura posibilitó que muchas aplicaciones multiplicaran varias veces su rendimiento al ser reescritas para la nueva arquitectura.

Con la llegada de Windows 95 y el Pentium, Microsoft e Intel mejoraron significativamente la capacidad multimedia y el rendimiento de los PC "compatibles" con IBM. Microsoft por su parte consiguió que Windows se acercara a la interfaz gráfica de los Macintosh, tanto que salieron eslóganes del tipo "Windows95 = Macintosh 84" y el rendimiento de los Pentium, aun siguiendo con la arquitectura CISC y por tanto siendo compatibles binarios con sus antecesores, consiguió acercarse en su rendimiento a la emergente línea RISC PowerPC, que era incompatible a nivel binario, que Apple estaba introduciendo. Esto llevó a que la cuota de mercado de Apple bajara rápidamente. Además en un momento dado, la oferta de Apple era muy confusa con muchas familias y subdivisiones: Classic, LC, II, Quadra, Performa, Centris, que esencialmente eran el mismo equipo con varios nombres diferentes.
A finales de 1995 también entraron en competencia Power Computing, Motorola, Umax, Tatung, Radius, MaxxBoxx y DayStar Digital con los Macintosh de Apple que ahora tenían que competir con estos recién llegados "clónicos" (hardware de terceros que corría el Sistema 7 licenciado por Apple). Esta nueva situación que nunca había ocurrido en el mercado de los Macintosh provocó un ligero aumento en la cuota de mercado global, pero un importante daño financiero a las cuentas de Apple ya que los clientes comenzaron a comprar los clónicos, más económicos.

Cuando Steve Jobs volvió a Apple en 1997, ordenó que el sistema operativo que se había mostrado previamente como la versión 7.7 fuera renombrado Mac OS 8 (en lugar de Copland OS, que nunca llegó a aparecer). Dado que Apple tenía licenciado solo la versión del Sistema 7 a terceros, esta medida puso fin a la línea de clones. La decisión causó importantes pérdidas financieras para las empresas como Motorola, con su StarMAX, Umax, que produjo el SuperMac y Power Computing Corporation, que ofrecía varias líneas de clones Mac, incluyendo PowerWave, PowerTower y PowerTower Pro. Estas empresas habían invertido importantes recursos en la creación de su propio hardware compatible con las especificaciones de los Macs. Apple compró la licencia de Power Computing, pero permitió a Umax seguir vendiendo clones Mac hasta que su licencia expirase, ya que tenía una presencia considerable en el segmento de gama baja, donde Apple no se había introducido.

Para 1997 Apple computer atravesaba serias dificultades: una cuota de mercado en mínimos históricos, una línea de productos redundante y confusa y una falta clara de visión de futuro. A pesar de que Jobs había ayudado a concebir la campaña publicitaria "Piensa Diferente" ( Think Different) como nuevo lema de la compañía, lo cierto es que la línea Macintosh se había desgastado comercialmente y creativamente, la costosa línea de productos Newton no encontraba su lugar en el mercado de PDA y Apple no tenía en su catálogo ningún producto que mostrara un camino divergente a lo que ya había en el circuito. Por aquella época Jobs había forjado un relación laboral y personal con un joven diseñador de la planilla de Apple, Jonathan Ive. Larry Ellison, amigo de Jobs y CEO de Oracle, le sugirió a este último el concepto de un terminal orientado a trabajar en red y que aprovechara el mundo en ciernes del internet, a un precio accesible. Fred Anderson, director financiero de Apple, abogó por mantener el concepto pero con una máquina más robusta incorporando un disco duro y una unidad de CD. Jon Rubinstein adaptó el procesador de la portátil Power G3 de alta gama al nuevo proyecto. Pero lo diferente en el proceso fueron las condicionantes impuestas por Steve Jobs: volver a las raíces del Macintosh original, con interés obsesivo puesto en el diseño integral del equipo, por lo que su nueva relación con Ive fue la base del proceso. Ive junto a su equipo generó diversas maquetas de las cuales Jobs eligió una ruta a seguir, creando un volumen de carcasa curvo, lúdico y compacto, con un asa en la parte posterior que difería de cualquier computador comercializado en aquel momento (casi el total de los ordenadores clónicos compatibles eran cajas color beige o blancas). Fue el primer producto desde el lanzamiento original de Mac que hacía honor al nuevo lema de la empresa: un computador visualmente descollante, con una estética ultra moderna que lo convertía en una pieza de exhibición dentro de un entorno doméstico, orientado a un acceso inmediato a internet a modo de terminal y al trabajo en redes, amigable en su uso y puesta en funcionamiento. Un año después de que Steve Jobs volviera a la compañía, Apple presentó el Macintosh todo-en-uno llamado iMac. Su caja era de plástico semi-transparente. El modelo original salió al mercado con el color azul Bondi y las siguientes versiones introdujeron otros colores. Está considerado como un hito del diseño industrial de finales de la década de 1990. El iMac se deshizo de la mayoría de los puertos "estándar" (y en su mayoría también propiedad) de Apple. Conexiones tales como SCSI y el ADB desaparecieron en favor de dos puertos USB. Además, no tenía la unidad interna de disquete y en su lugar incluía una unidad de CD-ROM para instalar software, una unidad de solo lectura (la idea era sacar o remitir información a través del correo electrónico, en ese momento algo novedoso, hoy un estándar básico). El iMac era incapaz de escribir en CD u otros medios sin hardware externo de terceros. Apple ponía su destino en manos de un solo producto.

A pesar de algunas limitaciones indicadas, este modelo demostró ser un éxito fenomenal, con 800 000 unidades vendidas en 139 días, y llevó a la compañía a un beneficio anual de 309 millones de dólares, el primer año rentable de la empresa desde que Michael Spindler asumió el cargo de CEO en 1995. La estética del iMac "azul y blanco" también fue aplicada a la línea Power Macintosh y luego a un nuevo producto, el iBook. Presentado en julio de 1999, el iBook de Apple fue el primer ordenador portátil para el mercado de consumo, que rellenaba la "esquina perdida" en marco de la estrategia de producto de cuatro áreas: Consumidor / Profesional, Portátil / Escritorio. Esta estrategia de producto fue previamente anunciada por Jobs. Se recibieron más de 140 000 pedidos anticipados antes de que comenzara a venderse en septiembre y en octubre se convirtió un éxito de ventas, tanto como el iMac.

A principios de 2001, Apple comenzó a vender ordenadores con discos CD-RW por primera vez. Apple había hecho hincapié en la capacidad de la Mac para reproducir DVD mediante la inclusión de DVD-ROM y DVD-RAM de serie. Steve Jobs admitió que Apple había llegado "tarde a la fiesta" en la tecnología del CD grabable, pero consideraba que los Mac podrían convertirse en un "centro digital" (digital hub) que unía y permitía el emergente "estilo de vida digital" (digital lifestyle). Apple, más adelante, introdujo una actualización de su software de reproducción de música iTunes que podía grabar CD, junto con una campaña de publicidad controvertida, el "Rip, Mix, Burn", que algunos medios de comunicación dijeron que alentaba la piratería. Esta campaña también acompañó al lanzamiento del iPod, en primer dispositivo portátil de Apple, con éxito. El iMac supuso un cambio radical en la forma de trabajar de Apple que imperó desde la salida forzada de Jobs: los diseñadores volvían a estar por delante de los ingenieros, los objetivos se simplificaban y las líneas productos se enfocaban a solo dos usuarios, domésticos y profesionales. La "experiencia de usuario" volvió a ser central en el diseño de los productos, con la innovación y la diferenciación como un estándar de base. En 1983, Apple fue pionera en instalar GUI en sus líneas de computadores; en 1998, al ya haberse vuelto un estándar en toda la industria por parte del Microsoft Windows y alcanzar en funcionalidad a las Mac, Apple apeló a la simplificación. En una campaña publicitaria muy cuidada, en diversos comerciales de televisión el actor Jeff Goldblum, de forma distendida, sin mencionar de forma directa al iMac, explicaba lo sencillo y económico que resultaba comprar una computadora que no fuera beige, sacarla de la caja y conectarse a internet en menos pasos que cualquier otra máquina. Finalmente el producto iMac supuso un profundo cambio en el resto de equipos en la línea Mac, incluso en la línea profesional, que tomó la impronta lúdica de la línea doméstica, y obligó a los proveedores de hardware y periféricos a diseñar líneas basadas en los principios de diseño del iMac. Otro factor fundamental era la importancia del internet, tanto su accesibilidad como la forma de visualizarlo, en el desarrollo de productos. Esto se convirtió en la base de un nuevo estilo de vida digital, donde el iMac era, sin proponérselo del todo al inicio, el puerto funcional o "hub" (en inglés) de distintos dispositivos (cámaras digitales, los futuros iPods, USB Drives, teléfonos celulares) y el enlace de aquella información descargada a través de internet, abriendo a las puertas a una revolución inédita en la industria de los contenidos (música al inicio y posteriormente películas y libros) a través de iTunes.

Apple continuó agregando nuevos productos a su línea, como el Power Mac G4 Cube, el eMac para el mercado de la educación y el PowerBook G4, un ordenador portátil para el sector profesional. El iMac original salió a la venta con un procesador G3 y en sucesivas actualizaciones se le incorporaron los G4 y G5. Las actualizaciones de los chips eran acompañadas por sucesivos nuevos diseños de la gama original; le sucedió una con variedad de colores pasando por el modelo de plástico blanco, hasta el actual iMac de aluminio. El 11 de enero de 2005, Apple anunció el lanzamiento del Mac Mini a un precio de 499 dólares, el Mac más barato hasta la fecha.

En el año 2000 el Sistema Operativo de los Macs sufría un cambio radical tanto en su diseño como en sus fundamentos. Desarrollado en un principio en Pascal y re-escrito substancialmente en C++ para la versión 7.0, este vetusto sistema con más de 14 años y 9 actualizaciones mayores se había adaptado con el tiempo a máquinas como el Mac original de 128 KiB a 8 MHz hasta los Mac equipados con procesadores G4 a 1 GHz. En la versión 8 se incorporó soporte para Multiprocesador y en la versión 9 un nuevo nanokernel, pero estas novedades se sustentaban en una arquitectura que no hacía posible el crecimiento de la plataforma. Tecnologías como multitarea preventiva o la memoria protegida no eran viables y Apple después de intentar y fallar durante varios años en generar un SO moderno y propietario como los proyectos Taligent, Copland y Gershwin, y desde 1996 año en que compró Next, centró los esfuerzos de la compañía en portar el recién adquirido NeXTSTEP (el sistema operativo de NeXT) a la plataforma PowerPC.

El NeXTSTEP se convirtió en el Mac OS X y fue presentado en septiembre del 2000 como una versión Beta para desarrolladores y el público en general y la versión 1.0 final se presentó el 24 de marzo de 2001, se trataba de un moderno SO basado en Unix con un núcleo Mach pero las APIS de este nuevo SO eran totalmente incompatibles con el Mac OS clásico y las aplicaciones existentes no funcionaban directamente así que Apple tuvo que incluir un nuevo emulador llamado BlueBox que ejecutará el MacOS 9 o "Classic" como una aplicación más para poder ejecutar las aplicaciones que no estaban portadas al nuevo SO. Además Apple fue forzada a crear una plataforma de desarrollo, llamada Carbon, que facilitara el tránsito al nuevo SO.

El Mac OS X introdujo un nuevo diseño de interfaz llamado Aqua que hace referencia a los diseños de los Macintosh. El MacOSX 1.0 salió al mercado con un precio de 29,99 dólares y con el nombre en clave de "Cheetah" (Guepardo). Las versiones posteriores de Mac OS X fueron de 10.1 "Puma" (25 de septiembre de 2001), 10.2 "Jaguar" (24 de agosto de 2002), 10.3 "Panther" (24 de octubre de 2003) y 10.4 "Tiger" (29 de abril de 2005).

En 2006 Apple abandonó el uso de los procesadores PowerPC. Seis meses antes en la WWDC del 2005, Steve Jobs había anunciado que Apple abandonaría la plataforma PowerPC para integrar en toda la gama Macintosh los procesadores X86 de Intel. El anuncio también reveló que el Mac OS X además de ser portado a PowerPC también se había mantenido el desarrollo de todas las versiones y tecnologías para la plataforma X86,así como todos los proyectos de software como iLife, iWork y todo el software profesional como FinalCutPro, Logic… Fue a partir de este año cuando se introdujo la gama Intel cambiando la nomenclatura de la mayoría de las gamas,El PowerBook pasó a denominarse MacBook Pro, el PowerMac pasó a Mac Pro, el iBook pasó a MacBook…

Esta transición como las anteriores se realizaba hacia una arquitectura diferente e incompatible con todo lo anterior, e igualmente que en las transiciones anteriores Apple introdujo un emulador por software denominado Rosseta que realizaba las tareas de cambiar las llamadas PowerPc a Llamadas Intel en tiempo real, posibilitando la ejecución del software preexistente escrito para PowerPc, aunque con limitaciones, ya que el software que accediera al hardware directamente como tarjetas gráficas, tendría que re-escribirse. También se minimizó el esfuerzo de los desarrolladores que habían optado por trabajar en Cocoa, las APIS nativas del OSX, estos no tendrían que cambiar el código fuente, y solo tendría que re-compilar para X86 mientras que los desarrolladores sobre Carbón (como Adobe con su Creative Suite), u otras plataformas sí que tendrían que re-escribir parte de software.

En esta transición Apple abandonó el soporte al emulador "Classic" que hacía viable la ejecución de aplicaciones escritas para MacOs 9.

La introducción de los chips de Intel introdujo el potencial de correr el Sistema Operativo de Microsoft; ahora los Macs podían correr Windows nativamente sin emulación por software como el Virtual PC. En marzo de 2006, un grupo de hackers anunció que habían hecho correr un Windows XP en uno de los nuevos Mac- Intel. Este grupo publicó su software como Open Source (código abierto)y fue puesto para descargarlo en su sitio web.En abril de 2006, Apple anuncio la beta pública de su propio sistema "multi SO", llamado "Boot Camp" que posibilita a los usuarios de un Mac-Intel la instalación de Windows Xp en sus máquinas. Versiones posteriores añadieron soporte para Windows Vista. Boot Camp fue incluido de manera estándar en el Mac OS 10.5. Boot Camp, aunque no oficialmente, abrió las puertas a ejecutar prácticamente cualquier SO que funcionase en la plataforma X86 como Linux, FreeBSD, Ubuntu, SUSE, Red Hat.

La introducción de esta funcionalidad dentro del SO convirtió a los Macs en los ordenadores que más Sistemas operativos podían ejecutar, cambiando el panorama de "in-compatibilidad" y plataforma cerrada de años atrás.

Si bien la fracción del mercado de venta de ordenadores ocupada por Mac no supera el 10%, en los últimos años Apple ha tenido un aumento significativo. Muchos han afirmado que esto se debe en parte, al éxito del iPod y el iPhone, ya que teóricamente un dueño satisfecho de la experiencia de un iPod o un iPhone se declinará a comprar más equipos Apple como los Mac. Este movimiento se le ha denominado "efecto halo". La inclusión de los chips de Intel es también es un factor que ha influido en el aumento de las ventas. Desde 2001 hasta la actualidad, las ventas de Mac han aumentado de forma continuada. Apple reportó ventas de 3.36 millones de Mac durante la temporada navideña del 2009. En el primer trimestre del 2011 la cuota de mercado de los Macintosh continua creciendo desde el 7.3 % en 2010 al 9.3 % en 2011.

El 24 de febrero de 2011, Apple fue la primera compañía en lanzar al mercado un equipo que utiliza e nuevo interfaz de I/O de Intel Thunderbolt (nombre en clave Light Peak). Utilizando la misma interfaz física de un puerto MiniDisplay, y compatibles hacia atrás con esa norma, Thunderbolt cuenta con dos canales de datos en paralelo con velocidades de transferencia de Gbit/s cada uno.

Apple directamente sub-contrata la producción de hardware para Asia fabricantes de equipos originales, tales como ASUS, manteniendo un alto grado de control sobre el producto final. Por el contrario, la mayoría de las empresas (incluido Microsoft) crean un software que se puede ejecutar en hardware producido por una variedad de terceros, como Dell, HP / Compaq, o Lenovo. En consecuencia, el comprador de un Macintosh tiene, comparativamente, menos opciones.

La actual familia de productos Mac usa los procesadores de la gama Intel x86-64. Apple presentó un emulador, llamado Rosetta, durante la transición de los chips PowerPC a los chips de Intel, como también lo hizo durante la transición de la arquitectura Motorola 68000 al PowerPC una década atrás. El Macintosh es la única plataforma informática que ha sido capaz de realizar con éxito la transición a una nueva arquitectura de la CPU, y lo ha hecho dos veces. Todos los Macintosh que se venden actualmente incorporan de casa 2 GB de RAM. Actualmente Apple incorpora tarjetas gráficas de ATI Radeon o nVidia GeForce. Todos los Macs actuales (excepto el MacBook Air y el más actual macbook pro retina display)
se envían con una unidad óptica que incluye una doble función DVD/grabador de CD que Apple denomina SuperDrive. Los Macs incluyen dos puertos estándar: USB y FireWire (excepto el MacBook Air y MacBook pro retina display que no incluyen el puerto FireWire). Los nuevos MacBook Pro además incorporan el puerto "Thunderbolt", desarrollado por la propia Apple junto a Intel y que según las especificaciones de Apple puede transferir datos a velocidades de 10 gigabits por segundo. El puerto USB se introdujo en el iMac G3 de 1998 y está en todos los modelos hoy en día, mientras que el puerto FireWire está principalmente reservado para los dispositivos de alto rendimiento, tales como discos duros o cámaras de video. A partir del iMac G5 lanzado en octubre de 2005, Apple comenzó a incluir las cámaras iSight en los modelos de Macintosh apropiados, y una interfaz de Media Center llamado Front Row que puede ser accionado por el mando Apple Remote o el teclado para acceder a contenidos multimedia almacenados en el equipo.

En Apple eran inicialmente reacios a aceptar ratones con varios botones y ruedas de desplazamiento. Los Macs no soportaban varios botones nativamente, incluso con hardware de terceros, hasta que llegó el Mac OS X en 2001.Apple continuó ofreciendo solo los ratones de un solo botón, algunos con cable y algunas versiones inalámbricas a través de Bluetooth, hasta agosto de 2005, cuando se presentó el Mighty Mouse. Si bien parecía un ratón tradicional con un solo botón, en realidad tenía cuatro botones y una bola de desplazamiento, con capacidad de movimientos independientes en el eje X del eje Y.Al primer modelo le siguió una versión Bluetooth en julio de 2006. En octubre de 2009, Apple presentó el Magic Mouse, que utiliza el reconocimiento de gestos multitáctil similar al iPhone en lugar de una rueda físca o bola de desplazamiento. Este ratón se encuentra disponible solo en Bluetooth, y el ratón Mighty Mouse (rebautizado como "Apple Mouse ") está disponible con cable y conexión USB. Apple también cuenta con el "Magic Trackpad" como un medio para controlar los ordenadores Macintosh de escritorio como el iMac o Mac Pro. Este se introdujo en 2010.

El Macintosh original fue el primer ordenador personal con un sistema operativo que utilizaba una interfaz gráfica de usuario desprovista de la línea de comandos. Desde el primer Mac se ha utilizado una metáfora de escritorio, representando objetos del mundo real, documentos, carpetas o un cubo de basura, como iconos que aparecen en pantalla y se comportan cómo sus homónimos reales. El software del sistema introducido en 1984 con el primer Macintosh y renombrado en 1997 cómo Mac OS, siguió evolucionando hasta la versión 9.2.2. En 2001, Apple presentó el Mac OS X, basado en Darwin y NEXTSTEP; entre sus nuevas características se incluyen el Dock y en la interfaz de usuario Aqua. Durante la transición, Apple incluyó un emulador llamado Classic que permite a los usuarios ejecutar Mac OS 9 en Mac OS X, Apple mantuvo este emulador hasta la versión 10.4 y solo funciona en máquinas PowerPC. La versión más reciente del sistema operativo es el Mac OS X v10.6 "Snow Leopard ". Además de Snow Leopard, todos los nuevos Mac se entregan con una variedad de aplicaciones de Apple, incluyendo, iLife, el navegador web Safari y el reproductor de medios iTunes. Apple desvelo la nueva versión Mac OS X v10.7 en 2010, aunque se encuentra todavía en desarrollo y se espera que esté disponible en el verano de 2011. Este sistema operativo cuenta con muchas características nuevas, tales como: Mission Control, the Mac App Store (disponible ahora desde la actualización de software), y la launchpad que es una manera parecida a como un IPAD mostraría las aplicaciones instaladas en el Mac. Apple también lanzará una característica conocida como "currículum vítae", que es similar a la función de hibernación, que se encuentran en Microsoft Windows.

Mac OS X disfruta de una situación cercana a la ausencia de diversos tipos de "malware" y "spyware" como virus, troyanos… que principalmente afectan a los usuarios del sistema operativo Windows de Microsoft. Esta situación viene dada por diversos motivos:
En febrero de 2006 se observaron gusanos, así como las posibles vulnerabilidades, que llevaron a algunos analistas de la industria y las compañías de antivirus a emitir advertencias que el Mac OS X no es inmune al "malware".

Originalmente, la arquitectura de hardware de los Macs estaba tan estrechamente ligada al Sistema Operativo Mac OS que era imposible arrancar ningún Mac con otro sistema operativo. La solución más común, incluso utilizada por la propia Apple para A/UX, era arrancar en Mac OS y luego a entregar el control a un programa que se hacía cargo del sistema y actuaba como un gestor de arranque. Esta técnica ya no fue necesaria cuando Apple introdujo el Open Firmware en los Macs con arquitectura PCI, aunque había sido utilizada previamente para mayor comodidad en muchos sistemas "Old World ROM" (ROM del viejo mundo, como referencia a los primeros Macintosh) debido a los errores en la aplicación del firmware. Ahora, el arranque del hardware se realiza directamente desde Open Firmware (la mayoría de los Macs basados en PowerPC) o EFI (todos los Macs basados en Intel), y los Macs ya no se limitan solo a la ejecución de Mac OS.

Después del lanzamiento de los Mac basados en Intel, surgieron diversos software de virtualización de otros fabricantes, como Parallels Desktop, VMware Fusion y VirtualBox. Estos programas permiten a los usuarios ejecutar Microsoft Windows y con ello todo el software que solo tiene versión para Windows velocidad muy cercana a la velocidad en una máquina nativa. Además Apple también lanzó Boot Camp con los correspondientes controladores específicos de Windows que ayudan a los usuarios instalar Windows XP o Vista de forma nativa permitiendo al usuario elegir que el arranque se efectúe con Mac OS X o con Windows. Aunque no está oficialmente soportada por Apple, es posible ejecutar el sistema operativo Linux con Boot Camp además de poder ejecutarlo con las otras soluciones de virtualización.

Debido a que Mac OS X es un sistema Unix, construido en gran medida a partir de FreeBSD, muchas aplicaciones escritas para Linux o BSD se ejecutan directamente en Mac OS X, a menudo usando X11. La cuota de mercado de Apple, más pequeña que la de Microsoft hace que existan menos aplicaciones Shareware (o versiones de bajo coste o prueba), aun así muchas aplicaciones populares de software comercial de los grandes desarrolladores como Microsoft Office y Adobe Photoshop se escriben tanto para Mac OS y Windows. Y gran parte de los software de código abierto como el navegador web Firefox y la suite ofimática OpenOffice.org son multiplataforma y funcionan de forma nativa.

Los anuncios Macintosh normalmente, han atacado al líder imperante del mercado, directa o indirectamente. Estos tienden a retratar al Mac como una alternativa a los excesivamente complejos y poco fiables clónicos PCs wintel. Apple promocionó la introducción del Mac original con el comercial 1,984 que se emitió durante el Super Bowl. Este anuncio se completaba con una serie de folletos impresos y otros anuncios de televisión que demostraban, la nueva interfaz gráfica y hacían hincapié en el uso del ratón. A estos primero folletos les siguieron muchos más para los nuevos modelos como el Macintosh Plus y Performa. En la década de 1990, Apple comenzó la campaña "What's on your PowerBook?" (¿Qué hay en tu PowerBook?) con anuncios impresos y comerciales de televisión en el que diversas celebridades describían cómo el PowerBook les ayudaba en sus negocios y en sus vidas cotidianas. En 1995, Apple respondió a la introducción de Windows 95 con varios anuncios impresos y un comercial de televisión demostrando sus desventajas y su falta de innovación. En 1997, la campaña "Think Different" (Piensa diferente) introdujo un nuevo concepto de publicidad, mostrando a Inventores, Artistas, Políticos… que no habían seguido el camino pre-establecido, sino que "pensado de manera diferente" habían cambiado el mundo, en clara referencia a que no siempre las mayorías (los PCs Wintel) son el camino a seguir. En 2002 siguió la campaña Switch (cambia). La última estrategia de publicidad de Apple es la campaña "Get a Mac" (Ven a Mac), con variantes en América del Norte, Reino Unido y Japón.

Apple siempre ha apostado por una publicidad impactante y arriesgada, por ello ha recibido diversos premios y reconocimientos así como estrepitosos fracasos como el legendario anuncio "Lemmings"

Hoy, Apple presenta nuevos productos en los "eventos especiales" en el auditorio del Salón de Apple Town, y en el discurso inaugural de la Conferencia mundial de Desarrolladores de Apple, y (ex) ferias como la Apple Expo y la MacWorld Expo. Estos eventos suelen atraer una gran número de representantes de los medios y de espectadores, y suelen estar precedidas por la especulación y rumores sobre posibles nuevos productos. Los eventos especiales se han utilizado para dar a conocer la gama de escritorio y los ordenadores portátiles, tales como el iMac y el MacBook, y otros dispositivos electrónicos como el iPod, Apple TV, iPad y el iPhone, así como proporcionar información actualizada sobre las ventas y las estadísticas de cuota de mercado. Apple ha comenzado a centrar su publicidad en sus tiendas minoristas en lugar de estas ferias. El último discurso inaugural en una feria fue en la MacWorld de 2009.

Desde la introducción de Macintosh, Apple ha tenido problemas para obtener una cuota significativa en el mercado de ordenadores personales. Al principio, el Macintosh 128K sufrió una escasez de programas para la plataforma en comparación con el PC de IBM, dando lugar a decepcionantes ventas en 1984 y 1985. En aquel entonces tardaron 74 días en vender 50 000 unidades. 

A pesar de los éxitos técnicos y comerciales en la plataforma Macinstosh, sus sistemas seguían siendo bastante caros, haciéndolos menos competitivos ante la caída de los costosos componentes que hicieron que los PC compatibles con IBM fueran más baratos y aceleraran su adopción. En 1989, Jean-Louis Gassée se había negado rotundamente a reducir los márgenes de ganancias en las computadoras Mac, por lo que hubo una escasez de componentes que sacudió a todas las industrias de las PC en expansión, obligando al jefe de Apple USA, Allan Loren, a recortar los precios, lo que redujo los márgenes de Apple. 

La iteración que lanzó Windows en 1990 se puso al alcance del rendimiento de los dispositivos Macintosh. Además, Apple había creado demasiados modelos similares que confundían a los posibles compradores.

Compaq, que anteriormente había ocupado el tercer lugar entre los fabricantes de PC durante la década de 1980 y principios/mediados de la década de los 90, inició una exitosa guerra de precios en 1994, que relegó a Apple al tercer lugar. La participación en el mercado de Apple se vio aún más afectada debido al lanzamiento de un nuevo sistema operativo Windows (Windows 95) que mejoró significativamente la capacidad multimedia y el rendimiento de las computadoras compatibles con PC de IBM, y puso las capacidades de Windows en paridad con la GUI de Mac OS. 

En 1997, tras regresar Apple como CEO interino, Steve Jobs terminó el programa de clonación de Macintosh mientras simplificaba las líneas de productos de computadoras. Para medir la participación de mercado según la base instalada, en 1997 había más de 20 millones de usuarios de Mac, en comparación con una base de alrededor de 340 millones de PC con Windows. 

En 1998, el lanzamiento del iMac G3 fue un gran éxito, se vendieron 800.000 unidades en 139 días, proporcionando un impulso muy necesario a la plataforma Macintosh en crisis. 

En el 2000, Apple lanzó el Power Mac G4 Cube, su primer escritorio desde el discontinuado Power Macintosh G3, para ubicarse entre el iMac G3 y el Power Mac G4. Incluso con su diseño innovador, inicialmente tenía un precio de 200 dólares estadounidenses más alto que el Power Mac G4. Apple vendió solo 29.000 cubos en un cuarto de trimestre del año 2000, un tercio de las expectativas. 

A partir de 2002, Apple decidió eliminar las pantallas CRT de su línea de productos como parte del diseño estético y las medidas de ahorro de espacio con el iMac G4. Sin embargo, el nuevo iMac con su flexible monitor LCD de panel plano fue considerablemente más caro en su debut que el anterior iMac G3, en gran parte debido al mayor coste de la tecnología LCD en ese momento. Con el fin de mantener el Macintosh al alcance del mercado educativo y debido a la obsolescencia del iMac G3, Apple creó el eMac en abril de 2002 como el sucesor previsto; sin embargo, el CRT del iMac lo hizo relativamente voluminoso y algo anticuado.

Los precios relativamente altos del iMac G4 se acercaban a los ordenadores que eran portátiles y tenían pantallas LCD de mayor resolución. Mientras tanto, los fabricantes de PC con Windows podrían ofrecer configuraciones de escritorio con monitores de pantalla plana LCD a precios comparables al eMac ya un costo mucho menor que el iMac G4. El fracaso del Power Mac G4 Cube, junto con el iMac G4 más caro y el pesado eMac, hizo que las ventas de computadoras Macintosh nunca alcanzaran la cuota de mercado alcanzada por el iMac G3 anterior. Durante la siguiente media década, mientras que las ventas de Macintosh se mantuvieron estables, sería el reproductor de música portátil iPod y el servicio de descarga de música de iTunes lo que impulsaría el crecimiento de las ventas de Apple.

Las estadísticas de finales de 2003 indican que Apple tenía un 2% de la cuota de escritorio en los Estados Unidos que había aumentado a un 2,88% en el cuarto trimestre de 2004. A partir de octubre de 2006, las empresas de investigación IDC y Gartner informaron que la participación de Apple en el mercado estadounidense había aumentado a alrededor del 6%. Entre 2005 y 2006, hubo un aumento de más del 30%. 

La cuota del mercado de los ordenadores personales se mide por los resultados del navegador, las ventas y la base instalada. La cuota de mercado de Mac aumentó sustancialmente en 2007. La participación de Mac OS X en el mercado de sistemas operativos aumentó de 7.31% en diciembre de 2007 y a 9.63% en diciembre de 2008, lo que representa un aumento del 32% en la participación de mercado durante 2008.

De 2001 a 2008, las ventas de Mac aumentaron continuamente en forma anual. Apple reportó ventas mundiales de 3,36 millones de Mac durante la temporada de vacaciones de 2009. A mediados de 2011, la Macintosh continúa experimentando un rápido aumento de la participación de mercado en los EE. UU., pasando del 7,3% de todos los envíos de computadoras en 2010 al 9,3% en 2011. Según el rastreador de PC trimestral de IDC, a nivel mundial, en el tercer trimestre de 2014, la participación de mercado de PC en Apple aumentó un 5,7% año tras año, con ventas récord de 5,5 millones de unidades.

Apple se posicionó en el puesto número cinco, con una cuota de mercado global de alrededor del 6% durante 2014, detrás de Lenovo, HP, Dell y Acer.

En marzo de 2011, la participación de mercado del OS X en América del Norte había aumentado a poco más del 14%. Si el tamaño de la cuota de mercado de Mac y la base instalada es relevante es un tema muy debatido. Los expertos de la industria a menudo han llamado la atención sobre la relativamente pequeña participación en el mercado de Mac para predecir la muerte inminente de Apple, especialmente a principios y mediados de los 90, cuando el futuro de la compañía parecía más sombrío. Otros argumentan que la participación en el mercado es la forma incorrecta de juzgar el éxito de Mac. Apple nombra el Mac como un ordenador personal de gama alta, por lo que puede ser engañoso compararlo con un PC económico. Debido a que el mercado general de los ordenadores personales ha creció rápidamente, el aumento en el volumen de ventas de la Mac estuvo afectado por el volumen de ventas en expansión de la industria en general. La pequeña participación de Apple en el mercado, entonces, da la impresión de que había menos personas que usaban Macs en los últimos años de la década de 2010 que diez años antes, cuando lo que ocurre es exactamente lo contrario. 

Las ventas del iPhone y el iPad crecieron por lo que las ganancias de Apple mediante Macintosh disminuyeron en 2010, cayendo a un 24%, desde un 46% los dos años anteriores. 

Apple logró convertirse en la primera empresa con un valor de mercado de USD 1 billón, cruzando un umbral numérico mundial. Su empresa tiene la economía número 16 del mundo y opera a una escala gigantesca. La combinación y creación de nuevos dispositivos permitió que aparatos electrónicos móviles como el iPad incluyeran un software incluso más potente que los modelos Macintosh de hace pocos años. 

En los meses de abril, mayo y junio de 2018, Apple ha vendido un total de 4,4 millones de unidades Mac (portátiles y de escritorio). Así, se le atribuye un 7,1% de la cuota mundial de ordenadores, aumentando un 3% desde el año pasado. 

El mundo de ventas de Apple no entra en el mismo juego que el de dispositivos de otras empresas como Windows debido a las diferencias de precios (siendo el Mac más barato de en torno a 1000 euros mientras que fácilmente se encuentra un dispositivo Windows por 500). 

Los nuevos diseños de Mac siguen sorprendiendo, la última actualización de MacBook, una novedosa línea de portátiles sin ventilador, mantiene a Apple en sus rankings de ventas.



</doc>
<doc id="41431" url="https://es.wikipedia.org/wiki?curid=41431" title="Banco de Avío">
Banco de Avío

El Banco del Avío fue fundado después de la Independencia de México, en octubre de 1830 durante la presidencia de Anastasio Bustamante por el entonces Ministro de Relaciones Exteriores, Lucas Alamán; y se convierte en el antecedente principal de la tradición bancaria mexicana, con el fin de prestar dinero para procesos de industria maquinaria.

El primer organismo de desarrollo creado en México para impulsar la industria y el crecimiento de la economía nacional fue el Banco de Avío, establecido el 16 de octubre de 1830 por Lucas Alamán. La función de esta institución consistía en otorgar préstamos a empresarios privados interesados en adquirir maquinaria para la industria manufacturera, con un cinco por ciento de interés anual sobre dichos préstamos. En sus doce años de actividad, el Banco de Avío concedió préstamos por algo más de un millón de pesos, básicamente a la industria algodonera. Aunque algunos historiadores consideran que el Banco de Avío cosechó pocos maíces y muchos fracasos, no dejan de reconocer el interés que tuvo por introducir una mejor tecnología, y por transformar el sistema productivo imperante desde el virreinato.

Fue clausurado en 1842 por un decreto del Gral. Antonio López de Santa Anna argumentando que el banco ya no podía sostenerse económicamente y que por otra parte el sector textil —primordial objetivo del banco—, se había extendido en México y que ya no necesitaba apoyo financiero.



</doc>
<doc id="41433" url="https://es.wikipedia.org/wiki?curid=41433" title="Dick Cheney">
Dick Cheney

Richard "Dick" Bruce Cheney (Lincoln, Nebraska; 30 de enero de 1941) es un político y empresario estadounidense que fue el 46°. desde el 20 de enero de 2001 hasta el 20 de enero de 2009. Ha sido citado como el vicepresidente más poderoso de la historia de Estados Unidos. Al mismo tiempo, fue uno de los políticos menos favorecidos en la historia del país: su aprobación al final de término fue de solo 13%.

Nacido en Lincoln, Nebraska, Cheney fue mayormente criado en Sumner, ciudad del mismo estado, y Casper, Wyoming. Asistió a la Universidad de Yale y más tarde a la Universidad de Wyoming, en la cual obtuvo un bachiller y una maestría en ciencias políticas. Comenzó su carrera política como becario del congresista William A. Steiger, obteniendo más tarde acceso a la Casa Blanca durante las presidencias de Richard Nixon y Gerald Ford, con el que logró ocupar el cargo de Jefe de Gabinete de la Casa Blanca, desde 1975 hasta 1977. En 1978, Cheney fue elegido como representante del distrito congresional at-large de Wyoming en la Cámara de Representantes de los Estados Unidos, rol que ocupó desde 1979 hasta 1989. Fue reelegido cinco veces, también ocupando brevemente el cargo de líder de la minoría republicana en 1989. Cheney fue elegido como Secretario de Defensa de los Estados Unidos durante la presidencia de George H. W. Bush, cargo que ocupó desde 1989 hasta 1993. Durante su desempeño como secretario de defensa, Cheney controló la Operación Desert Storm de 1991, entre otras cosas. Después de dejar el cargo cuando Bill Clinton asumió la presidencia, Cheney se convirtió en CEO de Halliburton desde 1995 hasta 2000.

En julio de 2000, Cheney fue elegido por el presunto candidato presidencial George W. Bush como candidato a vicepresidente en las elecciones presidenciales de Estados Unidos de 2000. Lograron derrotar a los candidatos demócratas, Al Gore (quien era en ese momento el vicepresidente) y el senador Joe Lieberman. En 2004, Bush y Cheney fueron reelectos, derrotando a los candidatos demócratas John Kerry y John Edwards. Durante su mandato como vicepresidente, Cheney ocupó un papel muy influyente en la respuesta de la administración a los ataques del 11 de septiembre y en la subsecuente guerra contra el terrorismo. Apoyó la Operación Iraqi Freedom y defendió el historial anti-terrorista de la administración. Se enfrentó con el presidente en la discusión del matrimonio igualitario en 2004, política que Cheney apoyaba pero Bush no. Cheney era recurrentemente criticado por las políticas de la administración de Bush con respecto a la campaña anti-terrorismo, el espionaje llevado a cabo por la NSA, y las llamadas técnicas de interrogatorio mejoradas.

Su carrera de servicio público se inició en 1969 cuando se incorporó al gobierno de Nixon y prestó servicios en un sinnúmero de cargos en el Consejo del Costo de Vida (Cost of Living Council), la Oficina de Oportunidades Económicas (Office of Economic Opportunity), y dentro de la Casa Blanca.

Cuando Gerald Ford asumió la presidencia en agosto de 1974, Cheney figuró como parte del equipo de transición y más adelante como Auxiliar Adjunto al presidente, y asistente del Jefe de Gabinete Donald Rumsfeld. Debido al estilo de "intercambiabilidad" aplicado por Rumsfeld -la práctica de dejar a su adjunto sustituirle en varias tareas de forma regular-, durante el primer año de mandato de Ford, Cheney pasó con el Presidente tanto tiempo como Rumsfeld.

En noviembre de 1975, fue nombrado Auxiliar del Presidente y Jefe de Gabinete de la Casa Blanca, un cargo que retuvo durante el resto del gobierno del presidente Ford. Había dentro de la Casa Blanca un halo de confusión y deriva. Cheney obtuvo buenas críticas por su buen hacer a la hora de hacer llegar al presidente una amplia gama de opiniones e ideas opuestas. Pero el presidente hizo poco para aliviar las tensiones entre Cheney, cuya oficina empezó a tener una creciente influencia en la elaboración de los discursos presidenciales, y Robert Hartmann, redactor de discursos y amigo personal de Ford.

David Gergen, asesor de Cheney, fue promovido al cargo de consejero especial del presidente, y Stefan Halper asignado para ayudar a Cheney y Gergen a evaluar las implicaciones políticas de todas las iniciativas de la Administración. Sin embargo, tanto Gergen como Halper eran antiguos redactores de discursos, un hecho que precipitó el conflicto entre las operaciones de Cheney y Hartmann.

Durante la campaña electoral de 1976, Cheney viajó por todo el país con el presidente, y fue uno de los integrantes de su círculo más íntimo de asesores electorales, junto a Robert Hartmann; el consultor californiano Stuart Spencer; John Marsh, hombre de total confianza del presidente; Robert Teeter, de "Market Opinion Research Corp."; y Jack Ford, uno de los hijos del presidente.

Cheney fue criticado por no explotar lo suficiente la proyección presidencial de Ford; en época de campaña, el presidente apenas concedió conferencias de prensa en la Casa Blanca, y prefería hacerlo en los pueblos y ciudades que visitaba en la ruta de campaña, lo que no le permitía aparecer más presidencial que el resto de candidatos. Pero Cheney se defendió de las críticas responsabilizando a Ford: "Es el Presidente quien establece el estilo para la Casa Blanca. Y así es como debe ser."

Tras su retorno a su estado de residencia, Wyoming, en 1978, Cheney fue elegido como republicano a la Cámara de Representantes de Estados Unidos para servir el único cargo de congresista del estado. Volvió a ser elegido cinco veces y sus colegas lo seleccionaron para que prestara servicios como Presidente del Comité Republicano de Política (Republican Policy Committee) de 1981 a 1987. Fue elegido Presidente de la Asamblea Republicana de la Cámara de Representantes (House Republican Conference) en 1987 y líder de la minoría de esa cámara en 1988.

Por lo general, Cheney, que se sentía mejor trabajando en la Cámara que en la Casa Blanca, donde se sentía un extraño, apoyaba los programas de política interna y exterior del presidente Ronald Reagan, especialmente de Defensa, aprobando los fondos para el desarrollo de la multimillonaria Iniciativa de Defensa Estratégica (SDI), más conocida como "Guerra de las Galaxias", la ayuda militar a la Contra de Nicaragua y a los insurgentes de Afganistán y Angola.

También votó contra la propuesta de exigir al presidente que notificara al Congreso en 48 horas cualquier operación secreta emprendida. En la política interna fue muy conservador, se opuso a la integración de niños blancos y negros en las escuelas públicas, al aborto y a favor de que se impartieran clases de religión en las escuelas.

En 1987 el congresista Cheney tuvo a su cargo la investigación del escándalo Irán-Contra y llegó a la conclusión de que, a pesar de posibles errores cometidos por la Administración Reagan, no existía evidencia de que el presidente tuviera conocimiento alguno de que se desviaran fondos de las armas vendidas.

El 10 de marzo de 1989, el nuevo presidente George H. W. Bush propuso a Cheney como Secretario de Defensa después de que su primera opción, el senador John Tower, fuese rechazado por el Senado. Al ser preguntado por los reporteros sobre su inexperiencia en materia de Defensa, señaló que, como jefe de gabinete del presidente Gerald Ford, había asistido a todas las juntas del Consejo de Seguridad nacional y como republicano era el de más antigüedad en el Comité de Inteligencia de la Cámara de Representantes.

A pesar de la rápida y unánime aprobación del Senado para que ocupara el puesto de Secretario de Defensa el 17 de marzo de 1989, Cheney sabía que encontraría ciertos reparos en el Pentágono por no haber servido en la guerra de Vietnam y por haber apoyado como congresista el uso del detector de mentiras en el Departamento de Defensa. Proponiéndose afirmar rápidamente su autoridad, solo permitió que dos o tres subordinados le aconsejaran en decisiones claves sobre el personal, empleó únicamente personas de absoluta confianza y nombró a David S. Addington, uno de sus más fieles ayudantes del Congreso, en el puesto de asistente especial suyo.

Para lograr el propósito del presidente Bush de alcanzar la meta de un crecimiento real del 0 % en Defensa para 1990, en 1989 Cheney tuvo que recortar 1000 millones de dólares del presupuesto de Defensa, propuso detener la producción de los nuevos helicópteros V-22 Osprey, rechazar la modernización de los aviones de caza F-14 Tomcat, cancelar la construcción de nuevos modelos de helicópteros y de un submarino de ataque, frenar el desarrollo de los bombarderos B-2 Stealth, dar de baja el portaaviones US Coral Sea y otros barcos, y reducir el personal a 17 200 efectivos, pero apoyó la investigación y desarrollo de los antimisiles Brilliant Pebbles, menos costosos que otros sistemas de antimisiles espaciales en desarrollo, que orbitarían en el espacio en prevención de ataques enemigos.

En abril de 1989 Cheney rechazó la petición de Alemania Occidental de abrir negociaciones con la URSS para reducir armas nucleares de corto alcance en Europa, por considerar dichas negociaciones una trampa peligrosa, dada la aplastante superioridad soviética en armas convencionales.

Entre 1990 y 1991 jugó un papel clave en el conflicto bélico desencadenado con Irak por la invasión de Kuwait. Dirigió todos los acuerdos y alianzas previas a la guerra del Golfo; consiguió que el rey Fahd de Arabia Saudí permitiera la instalación de bases militares estadounidenses en Daharan, y en 1992 firmó un acuerdo de seguridad entre EE. UU. y el emirato de Catar, donde se instaló el comando central de las tropas de EE. UU. estacionadas en la región del golfo Pérsico.

El Secretario Cheney recibió la Medalla Presidencial de Libertad otorgada por el presidente George Bush el 3 de julio de 1991 por su liderazgo durante la guerra del Golfo.

Abandonó el Departamento de Defensa el 20 de enero de 1993 y aterrizó en el sector privado. En 1995 fue contratado como presidente ejecutivo por Halliburton Company, firma que presta servicios de campo a la industria petrolera con sede en Dallas. En 1996 estuvo a punto de presentarse como candidato a las primarias presidenciales del Partido Republicano pero finalmente desechó la idea. Se mantuvo en el sector privado hasta que, en el año 2000, el gobernador George W. Bush lo seleccionó como compañero de fórmula en su candidatura presidencial.

Convertido en vicepresidente de los Estados Unidos en 2001, tras los atentados del 11 de septiembre fue trasladado a un lugar secreto y seguro desde el que siguió en contacto con el presidente Bush y coordinando las actividades gubernamentales, al frente de una administración de emergencia. Y la mañana del 29 de junio de 2002, mientras a Bush se le practicaba una colonoscopia, Cheney sirvió como Presidente en funciones bajo los términos de la 25.ª Enmienda de la Constitución: actuó como Presidente desde las 11:09 horas hasta las 13:24 horas. La mañana del 21 de julio de 2007, volvió a ejercer la Presidencia en funciones, mientras Bush se sometía a un procedimiento médico que requería sedación.

Consejero presidencial de primer orden en asuntos de política energética, política exterior y Seguridad Nacional, destacó como uno de los miembros más duros de la Administración Bush, defendiendo la guerra contra Irak con o sin el respaldo del Consejo de Seguridad de la ONU (2003). Y está considerado como el vicepresidente más poderoso de la historia: su poder fluía básicamente de la plena confianza que recibía del presidente Bush. Desde un principio, formó parte del círculo más íntimo del presidente, con pleno acceso al Despacho Oval, al mismo nivel del Jefe de Gabinete, sirviendo como confidente, y rompiendo con la tradición de excesiva formalidad que existía en anteriores relaciones presidente-vicepresidente.

Lejos del papel meramente ceremonial de un Vicepresidente, Cheney asumió un rol protagónico en la formulación de políticas oficiales. Síntoma de ese poder, fue la decisión del "Speaker of the House" Dennis Hastert de acondicionarle un despacho propio en la Cámara de Representantes, para tener conexión directa con la Casa Blanca. Se convirtió así en el Vicepresidente que más despachos oficiales ha tenido repartidos por los distintos centros de poder de Washington D. C.: el despacho de la Cámara de Representantes, se sumó a los que ya tenía en el Ala Oeste, en el Viejo Edificio Ejecutivo, y sus dos despachos del Senado, uno en el "Dirksen Senate Office Building", y otro pegado a la cámara del Senado.

A pesar de haber sido acusado de un posible delito de fraude cometido cuando era presidente de la compañía petrolera Halliburton Company, en las elecciones presidenciales de 2004 repitió como candidato republicano a la vicepresidencia para un segundo mandato junto a George W. Bush. Una vez ganadas las elecciones, el 20 de enero de 2005 Dick Cheney volvió a jurar como vicepresidente para otros cuatro años. Siempre ha mantenido que no tiene ambiciones presidenciales. Entregó el cargo al exsenador demócrata Joe Biden el 20 de enero de 2009.

Cheney se casó con su novia de la escuela secundaria, Lynne Ann Vincent, en 1964. La pareja tiene dos hijas adultas, Elizabeth y Mary, tres nietas y un nieto. Sufre de arritmia y ha tenido cinco infartos en su vida.



</doc>
<doc id="41435" url="https://es.wikipedia.org/wiki?curid=41435" title="Nixon">
Nixon

Nixon puede referirse a:





</doc>
<doc id="41437" url="https://es.wikipedia.org/wiki?curid=41437" title="Laura Bush">
Laura Bush

Laura Welch Bush (Midland, Texas; 4 de noviembre de 1946) es una profesora, bibliotecaria y escritora estadounidense. Es la esposa del expresidente de los Estados Unidos George W. Bush y la ex primera dama de los Estados Unidos, entre 2001 y 2009.

Única hija de Harold y Jenna Welch, nació y creció en Midland, Texas. Asistió la Southern Methodist University en Dallas, Texas, donde obtuvo un "bachelor of sciencies" en 1968.

En 1963 sufrió un accidente de tráfico en el que el otro conductor, amigo íntimo de Welch, perdió la vida. Tanto Laura, como una amiga que iba con ellos en el vehículo, resultaron ilesas, no encontrándose responsabilidad alguna por parte de Laura Welch.

Después de graduarse, trabajó como profesora de ciencias en Longfellow en el distrito escolar independiente de Dallas hasta 1969 y luego se mudó a Houston, Texas, donde enseñó en la escuela John F. Kennedy en el distrito escolar independiente de Houston, hasta 1972.

Laura Bush obtuvo un posgrado en Bibliotecología en la University of Texas, en Austin, en 1973. Posteriormente trabajó en la sucursal de Kashmere Gardens de la biblioteca pública de Houston, hasta que se volvió a mudar a Austin en 1974.

Trabajó como bibliotecaria en la escuela primaria de Dawson hasta 1977, fecha en que conoció a George Walker Bush en la casa de unos amigos comunes. Contrajeron matrimonio en noviembre de 1977 y se radicaron en Midland.

En 1981, George y Laura Bush se convirtieron en los padres de una pareja de niñas mellizas a las que llamaron Barbara y Jenna en honor a sus abuelas. En 1987, la familia se mudó a Washington, D.C., donde en aquel entonces George W. Bush trabajaba para su padre George H. W. Bush, el 41.º presidente de los Estados Unidos.

En noviembre de 1994, George Walker Bush fue elegido Gobernador de Texas y la familia se volvió a establecer en Austin, la ciudad capital del estado. En enero del 2001, George Walker Bush fue proclamado el 43º presidente de los Estados Unidos, convirtiéndose Laura en Primera Dama de los Estados Unidos.




</doc>
<doc id="41440" url="https://es.wikipedia.org/wiki?curid=41440" title="Lynne Cheney">
Lynne Cheney

Lynne Vincent Cheney (Casper, Wyoming, 14 de agosto de 1941) es la esposa del exvicepresidente de los Estados Unidos Richard B. Cheney.

Lynne Cheney es autora de "Telling the Truth" (Simon & Shuster, 1995), un libro sobre el impacto de las tendencias culturales en la sociedad y autora, con su esposo, de "Kings of the Hill" (segunda edición, 1996), una reseña de nueve de los líderes más poderosos de la Cámara de Representantes. También ha escrito novelas, columnas periodísticas y fue editora de la revista "Washingtonian". De 1996 a 1998, fue co-presentadora de "Crossfire Sunday".

Actualmente es miembro del directorio de Readers Digest Association y de Amex Mutual Funds.

Lynne Cheney obtuvo su diploma de bachillerato en letras en Colorado College, su maestría en letras en University of Colorado y su doctorado con especialización en Literatura Británica del siglo XIX en University of Wisconsin. 

Richard Cheney y Lynne Cheney contrajeron matrimonio en 1964. Tienen dos hijas adultas, Mary y Elizabeth y tres nietas, Kate, Elizabeth y Grace, y un nieto Philip.



</doc>
<doc id="41441" url="https://es.wikipedia.org/wiki?curid=41441" title="Aeropuerto Internacional Libertad de Newark">
Aeropuerto Internacional Libertad de Newark

El Aeropuerto Internacional Libertad de Newark (en inglés, Newark Liberty International Airport) (IATA: EWR, OACI: KEWR) es un aeropuerto situado entre las ciudades de Newark y Elizabeth en el norte del estado de Nueva Jersey, a 24 kilómetros de Nueva York. Este aeropuerto es uno de los más grandes de Estados Unidos con un tráfico de más de 30 millones de pasajeros por año y además fue en alguna vez el principal aeropuerto de los Estados Unidos. Es también el principal aeropuerto del área metropolitana de Nueva York. Incluye tres terminales de viajeros y otras instalaciones para carga. 

United Airlines es la aerolínea más importante en Newark, utilizando en exclusiva una terminal. Además es el tercer centro de operaciones de United Airlines (después de Houston y Chicago), y FedEx mantienen varias instalaciones importantes para sus servicios de carga aérea.

El Aeropuerto de Newark junto con los Aeropuertos de la Ciudad de Nueva York crean el sistema de aeropuerto más grande de los Estados Unidos, el segundo lugar en el mundo en términos de tráfico de pasajeros, y por primera vez en el mundo en términos de las operaciones de vuelo totales. En 2011 el aeropuerto de Newark manejo 33,8 millones de pasajeros, El Aeropuerto JFK manejo 47,8 millones, y el Aeropuerto LaGuardia poco más de 24,0 millones de pasajeros.

Un sistema de monorriel, llamado el "AirTrain" ("tren aéreo"), conecta las terminales y los aparcamientos y también proporciona conexiones con los trenes de Amtrak y NJ Transit y sus servicios a Nueva York, Trenton, Filadelfia, y a otras ciudades del noreste de los Estados Unidos. El aeropuerto es gestionado por la Autoridad Portuaria de Nueva York y Nueva Jersey.

En el pasado, las instalaciones eran conocidas como el Aeropuerto Internacional de Newark (en inglés, Newark International Airport). Tras los atentados del 11 de septiembre de 2001, el aeropuerto fue reticulado y adquirió el nombre adicional de "Libertad" para honrar a las víctimas de esa tragedia. Además este fue el origen del United 93 que fue el único vuelo de ese día que no llegó a su objetivo. Sin embargo, algunos residentes de Nueva Jersey y Nueva York no creen que esto fuese una manera apropiada de honrar a las víctimas. En la práctica este nuevo nombre es raramente utilizado. Coloquialmente se conoce simplemente como ""Newark Airport"" por la población local. 

Newark es el punto final del vuelo comercial directo más largo del mundo proveniente de Singapur por Singapore Airlines, es un vuelo de dieciocho horas el cual cesó en 2013 y reabrió en el año 2018 ahora con las nuevas aeronaves Airbus A350-900LR a diferencia del Airbus A340-500 utilizado antes de que cesara en 2013.

El Aeropuerto Internacional Libertad de Newark tiene tres terminales de pasajeros. La Terminal A y la Terminal B se completaron en 1973 y tienen cuatro niveles. Los mostradores de las entradas están en la planta superior, con excepción de la segunda planta y del primer piso operado por British Airways. Las tiendas están en el tercer piso. Una sala de llegadas internacionales (Terminal B) y cintas de equipaje (A y B) se encuentran en el segundo piso. Finalmente, el estacionamiento de corto plazo y las operaciones de rampa (áreas restringidas) se encuentran en la planta baja.

La Terminal C, fue terminada en 1988, tiene dos niveles de venta de entradas, un check-in para internacional y otro check-in para nacional. Tras su inauguración, la Terminal C tiene 41 puertas, originalmente con un nivel de salidas, una planta de llegadas, y un estacionamiento subterráneo. Del 1998 a 2003, la Terminal C fue reformada y ampliada en un programa de 1,2 mil millones dólares lo que permitió mayores salidas internacionales y una capacidad máxima de 19 aviones de fuselaje angosto (o 12 aviones de fuselaje ancho).

A partir de 2008, la Terminal B está siendo renovada para aumentar la capacidad de pasajeros de salida y su confort. Las renovaciones incluyen la ampliación y actualización de las áreas de venta de entradas, la construcción de un nivel y un nuevo punto de partida para los vuelos nacionales, y la construcción de una nueva terminal de llegadas. Los planes también están en marcha para ampliar la Terminal A mediante la adición de un nuevo estacionamiento y radicalmente ampliar el tamaño de la sala, añadir nuevas puertas de equipaje, venta de entradas, y las zonas de seguridad.


Los aeropuertos más cercanos son:




</doc>
<doc id="41444" url="https://es.wikipedia.org/wiki?curid=41444" title="Amenaza de muerte">
Amenaza de muerte

La amenaza de muerte es un delito contemplado en la mayoría de las legislaciones modernas. La conducta típica consiste en causar o infundir miedo en una o más personas, con el anuncio de la colocación del sujeto pasivo o un tercero en una situación de riesgo o contingencia de muerte.

La amenaza, para ser tal, debe cumplir varias características principales:



</doc>
<doc id="41445" url="https://es.wikipedia.org/wiki?curid=41445" title="Geografía de Irak">
Geografía de Irak

La geografía de Irak es variada y comprende cuatro regiones principales: el desierto (al oeste del río Éufrates), la Mesopotamia superior (entre la zona alta de los ríos Tigris y Éufrates), las tierras altas del norte en el Kurdistán iraquí, y la baja Mesopotamia, la planicie aluvional que se extiende desde las proximidades de Tikrit hasta el golfo pérsico. 

Irak comparte fronteras con Kuwait, Irán, Turquía, Siria, Jordania y Arabia Saudita. El país se inclina desde montañas de más de 3.000 metros sobre el nivel del mar a lo largo de la frontera con Irán y Turquía hasta llegar a los vestigios del nivel de la costa marina, las marismas en el sudeste del país. Gran parte de la tierra es desierto o páramo.

Las montañas en el noredeste son una extensión del sistema alpino que se extiende hacia el este desde los Balcanes, entra en el sur de Turquía, el norte de Irak, Irán y Afganistán, y termina en el Himalaya. El desierto se encuentra en las provincias del suroeste y centrales a lo largo de las fronteras con Arabia Saudita y Jordania y desde un punto de vista geográfico se encuentra en la península arábica. 

El promedio de temperatura varía desde 48 grados Celsius en julio y agosto hasta temperaturas bajo cero en enero. La mayoría de las precipitaciones ocurren desde el mes de diciembre hasta abril y el promedio es de entre 100 y 180 mm anuales. En la zona montañosa del norte de Irak cae bastante más lluvia que en las zonas central o del desierto del sur. 

Irak presenta un hito importante en la historia de la geografía: una tablilla de arcilla generalmente aceptada como "el mapa conocido más antiguo" fue descubierta en 1930 durante la excavación de Ga-Sur en Nuzi Yorghan Tepe, cerca de los poblados de Harran y Kirkuk, a unos 320 km al norte del sitio en que se ubicaba Babilonia. La tablilla, que mide unos 18 cm x 20 cm, ha sido fechada como de la dinastía de Sargón I de Akkad en el 2,300-2,500 a.C; el arqueólogo Leo Bagrow le ha asignado una fecha algo más antigua a esta tablilla, ubicándola en el período Agade (3,800 a.C.).



</doc>
<doc id="41448" url="https://es.wikipedia.org/wiki?curid=41448" title="Demografía de Irak">
Demografía de Irak

Casi el 75 por ciento de los habitantes de Irak vive en llanuras planas aluviales que se extienden hacia el sudeste hacia Bagdad y Basora y al Golfo Pérsico. Los ríos Tigris y Éufrates traen aproximadamente 70 millones de metros cúbicos de sedimentos al delta cada año. Conocida en tiempos antiguos como Mesopotamia, la zona es el sitio legendario del jardín del Edén. Las ruinas de Ur, Babilonia y otras ciudades antiguas se encuentran aquí.

Los dos grupos étnicos más numerosos de Irak son los árabes y los kurdos. Otros grupos notables son los turcomanos, los caldeos, los asirios, los iraníes, los luros y los armenios. El árabe es el idioma más comúnmente hablado. En el norte se habla el kurdo, y el idioma occidental más comúnmente hablado es el inglés.

Se acepta que la mayoría de los musulmanes iraquíes pertenecen a la secta chií, y que también existe una población importante de musulmanes sunníes, compuesta tanto de árabes como de kurdos. Existen también pequeñas comunidades de cristianos, judíos, baháis, mandeístas y yazidíes. La mayoría de los kurdos son musulmanes sunníes pero se diferencian de sus vecinos árabes en su idioma, su modo de vestir y sus costumbres.

Las proporciones de sunníes y chiíes dentro de Irak son un tema controvertido. Aunque suele darse por válida la idea (adoptada también por Estados Unidos en su intervención en el país) de que los chiíes son un grupo mayoritario, sometido a la preeminencia de los sunníes en las instituciones durante la época de Saddam Husein, otras fuentes afirman lo contrario. Así, algunos autores opinan que el porcentaje de chiíes fue engordado intencionadamente en los estudios demográficos iraquíes en la época en que Saddam Husein era uno de los principales aliados de Estados Unidos en la región, con el objetivo de magnificar la posible influencia del vecino y enemigo Irán en la población chií de Irak y así justificar lo autoritario del régimen y su necesidad de contar con un ejército bien armado. 
Así pues, según los datos que se suelen dar por correctos desde antes de que la composición étnico-religiosa de Irak cobrara suma importancia (Elecciones de 2005) esta sería la composición demográfica de Irak:

Etnia

Religión

Subdivisiones entre musulmanes

No obstante, es conveniente señalar que algunos expertos defienden una composición diferente. Según esa línea, un estudio del Al-Quds Press Research Center, con sede en Londres, da la siguiente composición étnica y religiosa:

Etnia

Religión

Subdivisiones entre musulmanes



</doc>
<doc id="41450" url="https://es.wikipedia.org/wiki?curid=41450" title="Historia de Irak">
Historia de Irak

Este artículo incluye una descripción de la prehistoria al presente en la región del estado actual de Irak en Mesopotamia.

Unos de los primeros asentamientos humanos más conocidos y antiguos se encuentran en lo que hoy se conoce como Irak, la zona del Oriente Próximo ubicada entre los ríos Tigris y Éufrates, si bien se extiende a las zonas fértiles contiguas a la franja entre los 2 ríos, y que coincide aproximadamente con las áreas no desérticas del actual Irak y la zona limítrofe del noreste de Siria. El punto de cruce de estos dos ejes es la cuna de la primera civilización conocida. Viviendas, templos, utensilios y cerámica encontrados en diversos sitios datan en el quinto milenio antes de Cristo. Los nombres de ciudades como Ur o Nippur, de héroes legendarios como Gilgameš, del Código Hammurabi, de los asombrosos edificios conocidos como Zigurats, provienen de la Mesopotamia Antigua. Y episodios mencionados en la Biblia o en la Torá, como en Jardín del Eden, los del diluvio o la pérdida de idiomas de la Torre de Babel, ocurrieron en esta zona.

La historia escrita (el antiguo nombre de Irak, especialmente la zona comprendida entre el Tigris y el Éufrates) se inicia con los sumerios, que para el cuarto milenio antes de Cristo se había establecido como ciudad-estado. Registros y cuentas en tablillas de arcilla demuestran que tenían una compleja organización económica antes de 3200 a. C. El reino de Sumer fue impugnado por el rey Sargón de Akkad (circa 2350 a. C.), seguida de la cultura sumerio-acadio que continuó en Erech (Tall al-Warka) y Ur (Tall al-Muqayyar) hasta que fue reemplazado por los amorreos y babilonios ( alrededor de 1900 a. C.), con su capital en Babilonia. 

La cúspide cultural de la historia de Babilonia está representada por Hammurabi (c.1750- alrededor 1792 a.C), quien compiló un célebre código de leyes. Tras el colapso de la civilización sumeria, Hammurabi venció a los príncipes rivales y fundó un reino alrededor del 1700 a. C. Durante su gobierno, Babilonia se convirtió en el principal centro de comercio de Oriente Próximo. Extendió su imperio hacia el norte a través de los valles de los ríos Tigris y Éufrates.

Entre los años 1813 y 1780 a. C., Asiria alcanzó la categoría de imperio. Fue el primer Imperio Asirio, de la mano del rey Shamshi-Adad I hasta que en el año 1760 a. C., Hammurabi de Babilonia derrotó y conquistó a los asirios. 

El siglo XVI a. C. fue un periodo de invasiones y gran confusión por toda Mesopotamia. Asiria se vio bajo el control de unos y otros invasores (los mitani y los hititas sobre todo), hasta el siglo XIV en que el rey asirio Ashur-uballit I se liberó de sus opresores e incluso llegó a agrandar los límites de sus tierras. Los sucesores de este rey ampliaron más las fronteras y supieron enfrentarse a los pueblos de alrededor.

Después de que Babilonia fue destruida por los hititas alrededor de 1550 AC, los hurritas fundaron el reino de Mitani en el norte por unos 200 años, y los casitas gobernó durante unos 400 años en el sur.

De Assur, su bastión en el norte, los asirios invadieron Mesopotamia alrededor del año 1350 a. C. y establecieron su capital en Nínive. La supremacía asiria fue interrumpida durante siglos XI y X a.C. por los arameos, cuya lengua aramea, se convirtió en el lenguaje común en la zona oriental del Mediterráneo para ese tiempo.

 
La hegemonía asiria fue finalmente aplastada por los caldeos babilonios o Neo-babilonios, quienes aliados con los medos de Persia, destruyeron Nínive en el 612 antes de Cristo. Nabucodonosor II (rc 605 - c.560 a.C.), reconstruyó la ciudad-estado de Babilonia. Nabucodonosor II (hijo de Nabopolasar) heredó el Imperio de Babilonia, al que añadió territorios, y reconstruyó la ciudad de Babilonia.

En el siglo VI a. C., Nabucodonosor II conquistó Jerusalén. El Templo de Salomón fue saqueado, el rey Joaquim y parte de la población fueron deportados a Babilonia. A Nabucodonosor II se acredita la construcción de los legendarios jardines colgantes de Babilonia, una de las Siete Maravillas del Mundo.

En el año 537 a.C., Babilonia cae ante los persas, bajo Ciro el Grande de la dinastía aqueménida. Bajo su hijo Cambises II, el Imperio Persa se extendía desde el río Oxus (Amu Darya) Río hasta el Mediterráneo, con su centro en Mesopotamia. Su poder, a su vez, fue impugnado por los griegos. Liderados por el conquistador macedonio Alejandro Magno, derrotó a los persas en 327 a.C., y penetró en tierras persas. Los seléucidas, sucesores de Alejandro en Siria, Mesopotamia y Persia, construyeron su capital, Seleucia, a orillas del Tigris, al sur de Bagdad. Tuvieron que ceder el poder a los partos, que conquistaron Mesopotamia en el año 138 a. C.

Los musulmanes conquistaron Irak en el siglo VII después de Cristo. 

En el siglo VIII, el califato abasida estableció su capital en Bagdad, ciudad que luego se convertiría en un puesto fronterizo del Imperio otomano.

A finales del siglo XIV y principios del XV, la federación Kara Koyunlu gobernó el área que hoy se conoce como Irak. En 1466, los Ak Koyunlu derrotaron a los Kara Koyunlu y toman el control. En el siglo XVI, la mayor parte del territorio estuvo bajo el control del Imperio otomano. A lo largo de la mayor parte del período de dominación otomana (1533-1918) el territorio de la actual Irak fue zona de batalla entre los imperios rivales regionales y las alianzas tribales. La dinastía Safavid de Irán afirmó brevemente su hegemonía sobre Irak en los períodos de 1508-1533 y 1622-1638. Durante los años 1747-1831 Irak era gobernado por oficiales Mamelucos de origen georgiano que obtuvieron la autonomía de la Sublime Puerta y presentaron un programa de modernización de la economía y del sistema militar. En 1831, los otomanos lograron derrocar al régimen mameluco e impusieron su control directo sobre Irak.

A finales de la Primera Guerra Mundial Irak pasó a ser un territorio bajo mandato británico, a cuya cabeza se colocó, como en Transjordania, a un miembro de la familia hachemí, en este caso a Faysal I, hermano del monarca transjordano. Inicialmente Irak contaba con las provincias otomanas de Basora y Bagdad, pero al conocerse la importancia de los yacimientos petrolíferos de la provincia de Mosul, los británicos la adjuntaron a la nueva nación.

Al acceder a la independencia en 1932 Irak siguió siendo una monarquía constitucional. Aunque el Reino de Irak fue admitida en la Sociedad de Naciones como una nación soberana, la presencia militar británica y el desventajoso acuerdo anglo-iraquí de venta de petróleo generó descontento en la población, especialmente en los nacionalistas árabes representados por Rashid Ali.

En 1941, en plena Segunda Guerra Mundial, Rashi Ali, en aquel entonces Primer Ministro, encabezó un golpe de estado con apoyo de la Alemania Nazi. El Reino Unido respondió invadiendo el país y restaurando al Regente 'Abd al-Ilah.

En 1945 Irak se unió a la Organización de las Naciones Unidas y se convirtió en miembro fundador de la Liga Árabe. En 1956 el Pacto de Bagdad convirtió a Irak, Turquía, Irán, Pakistán y el Reino Unido en aliados, y estableció su sede central en Bagdad.

El general Abdul Karim Qasim tomó el poder en un golpe de estado en julio de 1958. Durante el mismo fueron muertos el rey Faysal II y el primer ministro Nuri as-Said. El nuevo gobierno, encabezado por Qasim, pronto emprende importantes reformas; el 27 de julio de ese mismo año se promulga una nueva Constitución provisional, en la que se define a Irak como “República independiente plenamente soberana” y que considera a “árabes y kurdos compañeros en esa patria y con sus derechos tradicionales dentro de la unidad de Irak” Se emprenden importantes reformas populistas que afectan a la propiedad de la tierra y a mejoras en sanidad y educación. En el ámbito internacional, se denuncia el Pacto de Bagdad, se alcanzan importantes acuerdos estratégicos con la URSS y se define el alineamiento anti-occidental. A pesar de la aparente sintonía ideológica, las relaciones con la República Árabe Unida presidida por Gamal Abdel Nasser son malas; se producen algunos levantamientos contra Qasim e incluso un atentado contra su propia persona (en el que participa un muy joven Saddam Husein), detrás de los cuales parecía estar la mano de Nasser. Debilitado también en el interior como consecuencia del fin del apoyo del Partido Comunista (que son duramente reprimidos desde finales del 59), Qasim no tiene más remedio que desarrollar una leve apertura del régimen, con la aceptación de otros grupos políticos; sin embargo, este acercamiento será insuficiente para lograr una paz social en Irak, ya que los kurdos, molestos por la fuerte política de arabización a la que son sometidos por el gobierno de Qasim, se sublevan en septiembre de 1961, iniciándose así una guerra civil. En este contexto los baazistas van cobrando protagonismo.

En febrero de 1963 Qasim fue asesinado y el Partido Socialista Árabe Baaz (Partido Ba'ath) tomó el poder bajo el liderato del general Ahmad Hasan al-Bakr como primer ministro y el coronel Abdul Salam Arif como presidente.

Nueve meses más tarde Arif lideró un golpe de estado que derrocó al gobierno Ba'ath, iniciando una etapa de clara influencia nasserista. En abril de 1966 Arif murió al estrellarse su avión y fue reemplazado por su hermano, el general Abdul Rahman Mohammad Arif. El 17 de julio de 1968 un grupo de individuos del partido Ba'ath, junto con elementos de las fuerzas militares, derrocó el régimen de Arif. Ahmed Hassan al-Bakr reapareció como presidente de Irak y del Consejo del Comando de la Revolución (CCR).

El 2 de junio de 1972 fue nacionalizada la compañía de mayoritario capital británico Iraq Petroleum Company que hasta entonces había gestionado los ricos yacimientos petrolíferos del país. Este hecho, que sucedió poco después de que en 1971 Irak hubiera abanderado la revuelta de los países productores frente a las grandes compañías, convulsionó el mercado, propició otras nacionalizaciones, abriendo el camino al órdago de 1973

En julio de 1979 Bakr renunció, y el sucesor que eligió, Saddam Husein, asumió ambos cargos, iniciando una etapa en la que de nuevo el Partido Baaz asumiría el gobierno, esta vez ininterrumpidamente hasta la invasión de Irak en 2003. 

La guerra entre Irán e Irak duró desde 1980 hasta 1988 y devastó la economía de Irak. Con esta guerra Irak quiso recuperar unos territorios que le habían pertenecido y que fueron anexionados a Irán por el Imperio Británico, reduciendo su salida al mar. Además se puede considerar también una guerra casi impuesta por Estados Unidos y sus aliados occidentales, ante el miedo a la Revolución Islámica de Irán y la expansión de sus ideas hacia Irak y otros países árabes petroleros del Golfo Pérsico. Este miedo era compartido también por el gobierno baathista iraquí y otros países árabes de la región. El país declaró su victoria en 1988 pero, en realidad, solamente logró una fatigada vuelta al statu quo previo a la guerra. La guerra legó a Irak las instalaciones militares más importantes de la región del Golfo, pero también dejó tras sí deudas enormes y una continua rebelión de los kurdos en las montañas del norte. El gobierno reprimió supuestamente la rebelión usando armas químicas contra la población civil, inclusive un ataque masivo con armas químicas contra la ciudad kurda de Halabja que mató a varios miles de habitantes.

Irak invadió Kuwait en agosto de 1990, pero una coalición liderada por Estados Unidos, obrando de acuerdo con resoluciones de la ONU, expulsó a Irak de Kuwait en febrero de 1991, en la denominada Guerra del Golfo.

Después de la guerra las sanciones por mandato de la ONU, basadas en las resoluciones del Consejo de Seguridad, exigían que el régimen entregara sus armas de destrucción masiva y cancelara su programa de reconstrucción nuclear y se sometiera a las inspecciones de la ONU. El gobierno iraquí cooperó con los inspectores de la ONU y destruyó sus arsenales. Sin embargo, hacia 1998 el gobierno iraquí acusó a los inspectores (personal de la ONU, pero en su mayoría australianos y británicos) de ser espías norteamericanos y no permitió la continuación de su labor. Qué de hecho ya había casi destruido todo el arsenal de armamento no convencional que había en el país. 
Según el programa de la ONU, Petróleo por Alimentos, a Irak se le permitía exportar cantidades ilimitadas de petróleo para comprar alimentos, medicinas y equipos de ayuda humanitaria e infraestructura de apoyo necesarios para mantener a la población civil. La ONU, encargada de comprobar el funcionamiento del embargo, incumplió sus deberes, descubriéndose recientemente una trama de corrupción entre sus funcionarios y las empresas inspeccionadas, por el cual se recibieron sobornos y el presidente iraquí destinó supuestamente el dinero de la venta del petróleo a su enriquecimiento.

Durante los 90' Estados Unidos y el Reino Unido realizaron varias operaciones de ataque en virtud de las resoluciones de la ONU respecto a la restricción de vuelo de la aviación iraquí s en el sur y el norte de Irak para proteger a la población kurda y chií de un ataque del gobierno iraquí y veda de tránsito de vehículos en el sur de Irak para prevenir que el gobierno baathista concentrara tropas con el fin de amenazar o invadir de nuevo Kuwait.

El 19 de marzo de 2003, Estados Unidos apoyado por una coalición integrada por el Reino Unido, Australia, Polonia, Italia, España, Filipinas, El Salvador, Kazajistán y República Dominicana, inició la invasión sin apoyo de la ONU, con numerosos gobiernos de países de todo el mundo en contra, y con la opinión pública mundial también en contra (incluyendo los propios ciudadanos de Estados Unidos, España y Gran Bretaña).

Tras una rápida campaña de tres semanas, la coalición llegó a una ciudad de Bagdad, que cayó en el caos a causa del vacío de poder que se produjo con el derrocamiento del gobierno soberano de Irak. El gobierno de Saddam Husein dejó de existir el 9 de abril de 2003.

El 13 de diciembre de 2003 Saddam Husein fue capturado por soldados invasores con la ayuda de iraquíes colaboracionistas. Posteriormente se realizaron elecciones para aprobar una nueva constitución para dar legitimidad al nuevo gobierno y para elegir un nuevo presidente. Lo cierto es que desde el inicio de la invasión el país sufre un constante clima de inestabilidad política y social, sufriendo varios atentados casi diarios realizados por grupos tribales y sectarios descontrolados o, con mayor frecuencia, ataques de guerrilla convencionales, de aquellos que rechazan la permanencia de las tropas de ocupación en el territorio, así como también el constante enfrentamiento entre chiitas y sunítas, que, a partir de febrero de 2006, se recrudeció hasta estar al borde de la guerra civil. Enfrentamiento que
va acompañado también de enfrentamientos por el poder entre distintas facciones chiíes. Además de los enfrentamientos con los iraquíes reclutados por los islamistas wahabies de Al Qaeda que quieren hacerse fuertes en el país para evitar que lo controle Estados Unidos o los chiíes. Pero con ninguna o pocas probabilidades de llegar al poder. 

A pesar de que los estadounidenses decretaron el cese de las operaciones militares bélicas en mayo de 2003, pensando que ya tenían el país controlado y que ganar la guerra sería fácil, el ejército ocupante ha hecho frente a una complicada y devastadora guerra de guerrillas por parte de la Resistencia iraquí, formada por milicias y grupos guerrilleros de diferentes tipos (nacionalistas baathistas o no baathistas, comunistas marxistas, islamistas suníes con o sin relación con Al Qaeda, islamistas chiíes con o sin relación con los chiíes de Irán...) 

Varios países han comenzado la repatriación de sus tropas, al darse cuenta de la peligrosidad de la situación del país, el alto coste del mantenimiento de las tropas, la imposibilidad de ganar la guerra, y el propio sinsentido de la mayoría de sus argumentos iniciales.

Estados Unidos y Gran Bretaña, además, se han visto salpicados por el escándalo de las torturas en la cárcel de Abu Ghraib o las palizas de soldados británicos a jóvenes iraquíes. 
Y también de crímenes de guerra por la muerte de cientos de miles de civiles iraquíes atrapados en los combates y bombardeos de la guerra; en la pequeña guerra civil provocada por las divisiones étnicas, tribales y sectarias; y el empeoramiento general de las condiciones de vida desde que se inició la invasión.




</doc>
<doc id="41453" url="https://es.wikipedia.org/wiki?curid=41453" title="Economía de Irak">
Economía de Irak

La Economía de Irak se caracteriza por una fuerte dependencia de la exportación de petróleo y por acentuar un desarrollo dirigido por organismos centrales. Antes del estallido de la guerra contra Irán en septiembre de 1980, el futuro de la economía iraquí era brillante. La producción de petróleo había alcanzado el nivel de 3,5 millones de barriles por día y los ingresos del petróleo sumaban 21.000 millones de dólares en 1979 y 27.000 millones de dólares en 1980. Al comenzar la guerra, Irak había acumulado aproximadamente 35.000 millones de dólares en reservas de divisas. 

La guerra entre Irán e Irak vació las reservas de divisas de Irak, asoló su economía y dejó al país agobiado por una deuda externa superior a los 50.000 millones de dólares.

La invasión de Irak a Kuwait en agosto de 1990, las subsiguientes sanciones internacionales y el daño causado por las acciones militares de la coalición internacional a partir de enero de 1991 redujeron de modo drástico la actividad económica. Las políticas de gobierno de desviar ingresos a los partidarios claves del régimen, mientras se mantenía un gran cuerpo militar y de seguridad interna empeoraron aún más las finanzas y dejaron al ciudadano común iraquí enfrentado a graves privaciones. La puesta en práctica en diciembre de 1996 de un programa de la ONU llamado, Petróleo por Alimentos mejoró las condiciones de vida del ciudadano común iraquí. Desde 1999 se le autorizó a Irak exportar cantidades ilimitadas de petróleo para financiar sus necesidades humanitarias, entre las que se incluyen los alimentos, las medicinas y las piezas para reparar la infraestructura. Las exportaciones de petróleo varían según el régimen, alternadamente, inicia o detiene las exportaciones pero, por lo general, las exportaciones de petróleo alcanzan hoy a tres cuartos del nivel que tenían antes de la guerra del Golfo. La producción por cápita y el nivel de vida siguen estando, con mucho, por debajo de los niveles que existían antes de la guerra del Golfo.

A pesar de tener abundantes recursos de tierra y de agua Irak es un importador neto de alimentos. De acuerdo con el programa de la ONU, Petróleo por Alimentos, Irak importa grandes cantidades de cereales, carne, aves de corral y productos lácteos. El gobierno anuló su programa de colectivización de granjas en 1981, lo cual permitió que la empresa privada adquiriera una función más amplia en la industria agrícola. El Banco Cooperativo Agrícola, con capitales de casi mil millones de dólares en 1984, tiene como objetivos de sus créditos de bajo interés y bajas garantías a granjeros privados para proyectos de mecanización, cría de aves de corral y desarrollo de huertos. De momento se encuentran bajo construcción grandes y modernas granjas ganado, productos lácteos y aves de corral. Algunos de los impedimentos al desarrollo de la agricultura en Irak son la falta de mano de obra, una administración y mantenimiento inadecuados, la salinación, la migración urbana y los trastornos causados por anteriores programas de reforma agraria y colectivización.

La importación de trabajadores extranjeros y el creciente ingreso de la mujer en trabajos tradicionalmente desempeñados por hombres ayudaron a compensar la falta de mano de obra agrícola e industrial. Un desastroso intento de drenar los pantanos del sur del país para introducir en la región cultivos irrigados simplemente destruyó una zona natural de producción de alimentos, mientras que la concentración de sales y minerales en la tierra, a causa del drenaje, dejó la tierra inservible para el cultivo.

Las Naciones Unidas impusieron sanciones económicas a Irak después de la invasión de Kuwait en 1990. El rechazo del gobierno de Irak de permitir que los inspectores de armas entren al país para desmantelar su programa de armas de destrucción en masa ha resultado en la prolongación de esas sanciones. Según el programa de Petróleo por Alimentos, a Irak se le permite exportar cantidades ilimitadas de petróleo a cambio de provisiones humanitarias, entre las que se incluyen los alimentos, las medicinas, y las piezas de repuesto para la infraestructura.



</doc>
<doc id="41455" url="https://es.wikipedia.org/wiki?curid=41455" title="Política de Irak">
Política de Irak

Antes de la invasión de Irak, el Partido Ba'ath gobernaba Irak por medio de los nueve miembros que configuraban el CCR, organismo que dictaba y aprobaba leyes por decreto. El presidente del CCR (que a su vez era jefe de Estado y comandante supremo de las fuerzas armadas) era elegido por una mayoría de dos tercios del CCR. Un consejo de Ministros (el gabinete) era nombrado por el CCR y tiene responsabilidades administrativas y ciertas responsabilidades legislativas.
INFORMACION DE AÑOS PASADOS

La Asamblea Nacional estaba formada por 250 miembros de los cuales 220 eran elegidos por voto popular y desempeñaban su cargo durante un período de 4 años, y los otros 30 eran nombrados por el Presidente para representar a las tres provincias del norte. La Asamblea fue elegida por última vez en marzo del 2000. Irak se divide en 18 provincias, cada una de las cuales era dirigida por un gobernador con amplios poderes administrativos.

El sistema judicial iraquí se basaba en el modelo francés de justicia, que fue introducido en el país durante el período del régimen otomano y contaba con tres tipos de tribunales inferiores: tribunal civil, religioso y especial. Los tribunales especiales ven causas que se refieren, en forma amplia, a temas de seguridad nacional. Un sistema de tribunales de apelación y el tribunal de casación (tribunal de última instancia) completan la estructura judicial.

Principales funcionarios de gobierno: Presidente, Director del CCR, primer Ministro, secretario general del Comando Regional del Partido Ba'ath: Saddam Hussein; vicepresidente: Taha Yasin Ramadan; vicepresidente: Taha Muhyi al-Din Ma'ruf.

Ministros: viceprimer ministro: Tariq Aziz; viceprimer ministro: Abd Al-Tawab Mullah Huwaysh; viceprimer ministro: Ahmad Husayn Khudayir al-Samarrai; ministro de Información: Mohammed Saeed al-Sahhaf; ministro de Asuntos Exteriores: Naji Sabri Hadithi; ministro de Finanzas: vicepremier Hikmat al-Azzawi; Representante Permanente ante la ONU: Muhammad al-Duri; ministro de Petróleo: Amir Rashid Muhammad al-Ubaydi; ministro de Comercio: Mohammed Mahdi Salih; ministro de Estado: Arshad Mohammed al-Zibari; ministro de Salud: Omeed Midhat Mubarak; ministro de Industria y Minerales: Muyassar Raja Shalah al-Tikriti; ministro de Justicia: Mundhir Ibrahim al Shawi; ministro de Transporte y Comunicaciones: doctor Ahmed Murtadha Ahmed Khalil




</doc>
<doc id="41462" url="https://es.wikipedia.org/wiki?curid=41462" title="Ántrax (forunculosis)">
Ántrax (forunculosis)

El ántrax es una infección cutánea por estafilococos formada por una agrupación de forúnculos con extensión de la infección al tejido subcutáneo. Las lesiones presentan supuración profunda, son de lenta curación y producen cicatrices. A veces se utiliza el término ántrax para referirse al carbunco. Las dos enfermedades tienen en común las lesiones cutáneas como síntoma; sin embargo, al contrario que el ántrax de origen foruncular, las úlceras causadas por el carbunco suelen ser indoloras.

El término «ántrax» proviene del latín 'anthrax', y este a su vez, del griego 'ἄνθραξ', que significa carbón. Hace referencia al hecho de que la infección produce heridas o úlceras de un color oscuro.

Un ántrax está compuesto de varios forúnculos agrupados en una masa indiferenciada, dando el aspecto de una única lesión eritematosa llena de pus y células muertas. Se observan con más frecuencia en la espalda y la nuca. Los ántrax suelen estar causados por la bacteria "Staphylococcus aureus". La infección es contagiosa y se puede diseminar a otras áreas del cuerpo o a otras personas, por lo cual no es raro que se den en varios miembros de la misma familia al mismo tiempo.

Aunque el ántrax puede aparecer sin ninguna causa aparente, se han identificado la mala higiene, ciertas dolencias como la dermatitis o la diabetes y un sistema inmunitario debilitado como factores que aumentan el riesgo.

Los ántrax suelen ser dolorosos al tacto. La piel alrededor adquiere un color lívido, a veces con una pústula amarillenta en el centro. Pueden crecer muy rápidamente, llegando al tamaño de una pelota de golf. A menudo supuran espontáneamente, excepto cuando la infección es muy profunda. Suelen ir acompañados de malestar general y fiebre.

Una buena higiene corporal es necesaria para disminuir el riesgo de infecciones recurrentes. Es importante lavarse bien las manos después de tocar un ántrax y lavar con agua muy caliente cualquier prenda o ropa que se comparta con un enfermo, como toallas; esto disminuye el riesgo de infección por contagio.

A veces el ántrax se cura por sí solo en unas dos semanas. La aplicación de compresas húmedas calientes sobre la úlcera facilita el drenaje. El ántrax también se puede drenar quirúrgicamente. En ocasiones se recetan antibióticos tópicos u orales; en este caso se suele realizar un cultivo para determinar la cepa bacteriana responsable de la infección. Las bacterias "S. aureus" presentan a veces resistencia a antibióticos betalactámicos, lo cual puede dificultar el tratamiento.

El ántrax puede ser peligroso si las bacterias llegan a introducirse en el torrente sanguíneo; en este caso existe el peligro de choque séptico y daño a otros órganos del cuerpo. Entre las complicaciones más serias se incluyen los abcesos cerebrales y de la médula espinal, osteomielitis y endocarditis.



</doc>
<doc id="41464" url="https://es.wikipedia.org/wiki?curid=41464" title="Propelente">
Propelente

Un propelente puede referirse a:


</doc>
<doc id="41465" url="https://es.wikipedia.org/wiki?curid=41465" title="Teorema de Weierstrass">
Teorema de Weierstrass

El teorema de Weierstrass es un teorema de análisis real que establece que una función continua en
un intervalo cerrado y acotado (de números reales) alcanza sus valores máximo y mínimo en puntos del intervalo. También se puede enunciar en términos de conjuntos compactos. El teorema establece que una función continua transforma intervalos compactos en intervalos compactos, entendiéndose por intervalo compacto aquel que es cerrado (sus puntos frontera le pertenecen) y acotado.

Si una función f es continua en un intervalo compacto (cerrado y acotado) formula_1 entonces hay al menos dos puntos x,x pertenecientes a [a,b] donde f alcanza valores extremos absolutos, es decir formula_2, para cualquier formula_3.

Como formula_4 está acotada al ser "[a,b]" un compacto y "f" una función continua aplicada sobre un compacto, podemos asegurar que existe un supremo finito llamado "M". Es necesario encontrar un punto "d" en ["a","b"] que satisfaga "M" = "f"("d"). Digamos que "n" es un número natural. Como "M" es supremo, "M" – 1/"n" no lo es para "f". Entonces, existe un punto "d" en ["a","b"] tal que "M" – 1/"n" < "f"("d"). Esto genera una sucesión {"d"} según vamos dando valores naturales a "n". Como "M" es supremo por "f", tenemos que "M" – 1/"n" < "f"("d") ≤ "M" para todo "n" natural. Entonces, si hacemos tender "n" hacia infinito por el criterio de compresión tenemos que {"f"("d")} converge a "M".

Tenemos una sucesión que converge al supremo del conjunto, ahora hay que ver que precisamente el punto dónde se asume el supremo es el punto "d", incluido en el conjunto, y por lo tanto este supremo es un máximo. El Teorema de Bolzano-Weierstrass nos dice que existe una subsucesión {formula_5}, que converge a un punto "d" y, dado que ["a","b"] es cerrado, "d" está en ["a","b"]. Como "f" es continua en el conjunto (incluyendo el punto "d"), la sucesión {"f"(formula_5)} converge a "f"("d"). Pero {"f"("d")} es una subsucesión de {"f"("d")} que converge a "M", entonces "M" = "f"("d"), ya que si una sucesión es convergente a un punto cualquier sucesión parcial converge al mismo punto. Por lo tanto, "f" asume el supremo "M" en el punto "d", y como "d" es del conjunto es el máximo.

La demostración para ver que el ínfimo del conjunto "[a,b]" por f se asume dentro del conjunto y por lo tanto es mínimo es análoga a esta. ∎

El teorema de Weierstrass sigue siendo válido para funciones definidas sobre un espacio topológico con valores en los números reales. En este caso el teorema se puede enunciar como sigue:

Sea formula_7 un espacio topológico y formula_8 un conjunto compacto. 
Si formula_9 es una función continua entonces existen formula_10 tales que formula_11 para cualquier formula_12.

El hecho clave en la demostración de esta versión del teorema está en que las funciones continuas envían conjuntos compactos en conjuntos compactos.
También se puede generalizar el teorema a funciones con codominio distinto de formula_13, en este caso el teorema se enuncia de la siguiente forma:

Sea formula_14 un espacio vectorial normado, formula_7 un espacio topológico y formula_8 un conjunto compacto. 
Si formula_17 es una función continua entonces existen formula_10 tales que formula_19 para cualquier formula_12.




</doc>
<doc id="41467" url="https://es.wikipedia.org/wiki?curid=41467" title="Edward B. Lewis">
Edward B. Lewis

Edward Bok Lewis (Wilkes-Barre, Pensilvania; 20 de mayo de 1918-Pasadena, California; 21 de julio de 2004) fue un biólogo estadounidense ganador del Premio Nobel de Medicina en 1995.

Estudió biología, genética y meteorología en las universidades de Minnesota y California. Desarrolló sus principales trabajos en el campo de la genética, con descripción de la influencia de los genes en el desarrollo embrionario del feto. Junto con Christiane Nüsslein-Volhard y Eric Wieschaus obtuvo el en 1995. Nüesslein-Volhard y Wieschaus consiguieron identificar respectivamente en la "Drosophila melanogaster" una serie de genes que determinan la evolución de los distintos segmentos del animal y deciden su conversión en organismos especializados.



</doc>
<doc id="41468" url="https://es.wikipedia.org/wiki?curid=41468" title="Christiane Nüsslein-Volhard">
Christiane Nüsslein-Volhard

Christiane Nüsslein-Volhard (Alemania, 20 de octubre de 1942) es una bióloga del desarrollo alemana ganadora del Premio Nóbel de Medicina en 1995.

Christiane Nüsslein-Volhard estudió inicialmente Biología en Fráncfort, luego cambió a Física y posteriormente a Bioquímica. Desde 1985 dirige la división de genética del Instituto Max Planck de Biología del desarrollo en Tubinga, Alemania.

Recibió, junto Edward B. Lewis y Eric Wieschaus, el en el año 1995.





</doc>
<doc id="41471" url="https://es.wikipedia.org/wiki?curid=41471" title="Idioma sueco">
Idioma sueco

El sueco (en sueco: ) es una lengua germánica del norte de Europa, hablada por entre 9 y 14 millones de personas. La mayoría de sus hablantes vive en Suecia, en Finlandia y en las Islas Åland (autónomas), donde es el idioma oficial. El sueco es una lengua nórdica, al igual que el danés, el noruego, el islandés y el feroés. Las lenguas nórdicas son un subgrupo de las lenguas germánicas, las cuales forman parte de la familia lingüística conocida como lenguas indoeuropeas. El sueco, al igual que el resto de lenguas nórdicas, desciende del nórdico antiguo, hablado en Escandinavia durante la época vikinga. El sueco es en gran parte comprensible para un noruego y un danés. 

El sueco estándar ("rikssvenska") es la lengua nacional que evolucionó a partir de los dialectos del centro de Suecia durante el siglo XIX y se estableció completamente a principios del siglo XX. Si bien en la actualidad existen todavía varios dialectos rurales, el idioma hablado y escrito es uniforme y estándar con más del 99 % de habitantes adultos alfabetizados. Algunos de los dialectos genuinos difieren considerablemente del sueco estándar en gramática y vocabulario y no siempre son mutuamente comprensibles con el sueco (por ejemplo, el lenguaje del norte de Dalarna). Estos dialectos están confinados a áreas rurales con poca movilidad social. Si bien no están en peligro de desaparición, las poblaciones que hablan estos dialectos se están reduciendo a pesar de los esfuerzos de las autoridades regionales para preservar su uso.

El sueco se distingue por su prosodia, que difiere considerablemente entre variedades. Esta incluye tanto cualidades de acento como de tono. La existencia de dos tonos lingüísticos es un fenómeno compartido con el idioma noruego estándar, pero no con el danés. El idioma tiene nueve sonidos vocales que se distinguen por su longitud y sus cualidades, con lo que se forman 17 fonemas vocales. El sueco es también notable por la existencia de un fonema dorso-palatal velar fricativo sordo, un sonido que se encuentra en muchos dialectos, incluyendo las formas más prestigiosas del idioma estándar. Aunque similar a otros sonidos con distintas cualidades labiales, hasta ahora no se ha ubicado en otros idiomas; se asemeja parcialmente a una pronunciación simultánea de una "sh"-inglesa y una "j"-castellana. En el idioma escrito, este sonido se representa por combinaciones como "sk-", "skj", "stj-", "sj-" y otras.

El sueco es el idioma nacional de Suecia, es la lengua materna de unas 7 881 000 personas nativas y 1 028 000 inmigrantes (según estadísticas oficiales de 2001).

El sueco es el idioma de las islas Åland, una provincia autónoma bajo la soberanía finlandesa. En la península finlandesa, sin embargo, el sueco es la primera lengua de solamente una minoría de personas, alrededor del 5,6 %. La minoría finosueca se concentra en algunas zonas costeras y archipiélagos en el Sur y Suroeste de Finlandia, donde forman mayorías locales en algunas comunidades. En el conjunto de la población finlandesa el 46,6 % de la población conoce y utiliza la lengua sueca.

Hubo comunidades suecoparlantes en los países bálticos, especialmente en las islas a lo largo de la costa. Tras la pérdida de los países bálticos que pasaron a manos rusas a principios del siglo XVIII, muchos de los suecoparlantes fueron trasladados a Ucrania. Los supervivientes fundaron finalmente un número de pequeños pueblos suecoparlantes, que sobrevivieron hasta la revolución rusa, cuando sus habitantes fueron evacuados a Suecia. El dialecto que hablaban era conocido como "gammalsvenska" (sueco antiguo).

En Estonia, la comunidad sueca que quedó recibió muy buen trato entre la primera y la segunda guerras mundiales. Municipios de mayoría sueca, principalmente costeros, empleaban el sueco como lengua administrativa y mejoró la cultura sueco-estonia. Sin embargo, la mayor parte de los suecoparlantes emigró a Suecia tras la Segunda Guerra Mundial cuando Estonia pasó a manos de la Unión Soviética.

Hay pequeños grupos de suecoparlantes en otros países, tales como los Estados Unidos, especialmente en la ciudad de Lindsborg (Kansas), donde se concentra una colonia de inmigrantes suecos. También existen descendientes en Canadá, Australia y Nueva Zelanda.

Existe una migración considerable entre los países nórdicos, debido a las similitudes entre las lenguas y culturas, la integración se produce generalmente de manera rápida y no llegan a formarse grupos (nótese que Finlandia, a pesar de ser un país nórdico, no pertenece al grupo de países escandinavos, estrictamente hablando).

El sueco es el idioma nacional y oficial en uso en Suecia.

En Finlandia, tanto el sueco como el finés son idiomas oficiales. El sueco fue el idioma de uso gubernamental en Finlandia durante aproximadamente 700 años, hasta que en 1892 el finés, hablado por la amplia mayoría (aproximadamente tres cuartas partes de los habitantes de aquel entonces) de los habitantes de Finlandia, fue equiparado al sueco como idioma nacional según planes rusos para aislar al Gran Ducado de Finlandia de la influencia sueca. Hoy en día unas 350 000 personas, o el 5,6 % de la población total de Finlandia, tienen el sueco como lengua materna según las estadísticas oficiales de 2002. Tras una reforma educativa en los años 1970, tanto el sueco como el finés se convirtieron en idiomas obligatorios en las escuelas finlandesas como primera y segunda lengua. Al establecerse la enseñanza obligatoria del sueco en las escuelas, muchos lo consideraron un paso para evitar los movimientos nacionalistas finlandeses. Actualmente el 47% de la población finlandesa conoce la lengua sueca según el "Eurobarómetro" de 2016.

El sueco es el idioma oficial en la pequeña autonomía de las islas Åland, que pertenecen a Finlandia, protegidas por tratados internacionales y la ley finlandesa. Pero, a diferencia de la Finlandia continental, el finés no es oficial en Åland y no es obligatorio en las escuelas.

El sueco es también uno de los idiomas oficiales de la Unión Europea.

No existe ninguna institución que regule la lengua sueca, pero la Academia Sueca ("Svenska Akademien") y el Consejo de la lengua sueca ("Svenska språknämnden") desarrollan un papel importante en el seguimiento de la lengua. Su principal función es la de fomentar el uso de la lengua sueca, principalmente mediante la publicación de diccionarios. A pesar de que los diccionarios pueden ser considerados en ocasiones como una manera de definir oficialmente un idioma, su función es meramente descriptiva, no prescriptiva.

El sueco posee más de una variedad de la lengua estándar, si bien quienes hablan las variedades de Helsinki, Estocolmo/Upsala, Lund y Gotemburgo (Göteborg) no suelen considerar a las otras variedades como menos prestigiosas.

El término sueco "rikssvenska" se considera normalmente en Finlandia como el sueco hablado en Suecia, a diferencia del hablado en Finlandia (históricamente los territorios del este), pero en Suecia también puede denotar la variante hablada en Estocolmo/Upsala, empleada normalmente en los medios de comunicación. Esta última definición, equivalente a veces "sueco correcto", si bien literal significa "sueco del reino", es considerada a menudo controvertida.

Además de las variantes de la lengua estándar, se pueden distinguir numerosos dialectos de la lengua, normalmente definidos según divisiones históricas y provincias.


El sueco forma un diasistema oral (no así escrito) con el noruego. Ambos idiomas presentan una sonoridad muy característica en sus principales ciudades centrales (Upsala, Estocolmo), sonoridad que se va perdiendo según se avanza hacia el norte (Laponia) o hacia el sur (Escania).

El "sueco de Rinkeby" ("rinkebysvenska") es una jerga o variedad hablada en los barrios de inmigrantes por jóvenes en Estocolmo, Gotemburgo y Malmö, con aportes del idioma turco, bosnio y español.

Nota: Gutniska, jämska, sydsvenska ("skånska") y Dalecarlense ("dalamål") pueden ser consideradas lenguas por sí mismas. Prácticamente todos los hablantes de estas lenguas son bilingües.

El sueco es una lengua indoeuropea, que pertenece a la rama nórdica de las lenguas germánicas. Junto con el danés pertenece al subgrupo nórdico-oriental, separadas de las lenguas del subgrupo nórdico-occidental (noruego, feroés e islandés). No obstante, un análisis más reciente, basado en el grado de mutua inteligibilidad, divide las lenguas nórdicas en dos grupos: "Nórdico Continental" (danés, noruego y sueco), y "Nórdico insular" (feroés e islandés). Esta separación surge como consecuencia del aislamiento de las islas y la influencia común del bajo sajón sobre las lenguas nórdicas durante la época de la liga hanseática.

Según los criterios generales de mutua inteligibilidad, las lenguas nórdicas continentales pueden considerarse, hasta cierto punto, como variantes de una lengua común. Pero debido a los siglos de conflictos, principalmente entre Dinamarca y Suecia, que comprenden una larga serie de guerras durante los siglos XVI y XVII, y debido también a los brotes de ideas nacionalistas en los siglos XIX y XX, los tres idiomas han desarrollado diferentes ortografías, léxicos, gramáticas, y patrones de pronunciación. Por ello, el danés, el noruego y el sueco son descritos, desde una perspectiva lingüística, como un continuo dialectal del "nórdico", y muchos dialectos de las lenguas nacionales pueden considerarse como intermedios entre dos idiomas, como por ejemplo los dialectos suecos que se hablan en Dalarna y Värmland, cerca de la frontera noruega. La inteligibilidad mutua es sin embargo marcadamente "asimétrica"; algunos estudios han confirmado, por ejemplo, que un danés tiene menos dificultades para entender el sueco hablado que un sueco para entender el danés hablado.

Uno de los principales problemas que encuentran los estudiantes de sueco es la aparente falta de una pronunciación estándar. Las vocales y algunos sonidos consonánticos, especialmente los sibilantes, se realizan de formas muy distintas en diversas variantes ampliamente aceptadas. Además, el acento melódico del centro de Suecia es sorprendentemente distinto al del sur y norte, que, a su vez, difiere del de Dalecarlia y Gotland. El sueco finlandés y el sueco de algunas áreas del norte de Suecia no utilizan el acento melódico en absoluto, lugares en los que el finés domina desde hace menos de un siglo.

Un rasgo notorio del sueco es que ha desarrollado pares mínimos cuya diferencia se basa en el tono, lo que convierte al sueco en una de las pocas lenguas indoeuropeas tonales (el punyabí y otras lenguas indoarias también han desarrollado tono sobre una base independiente).

El alfabeto sueco cuenta con 29 letras: el alfabeto latino, y tres letras más "Å-å", "Ä-ä" y "Ö-ö". Estas letras se colocan con este orden al final del alfabeto tras la letra "Z", siendo la primera letra del alfabeto la "A" y la última la "Ö". Algunas palabras arcaicas y extranjerismos utilizan la letra "W", considerada como una variante de la "V". Puede encontrarse también el acento agudo en palabras de origen extranjero (ej. " kupé", " idé") y en algunos nombres (ej. "Lindén"). En estos casos sirve para marcar el acento prosódico. Se ha de mencionar también que la letra "Y" cuenta como vocal.

El alfabeto rúnico era utilizado en el nórdico antiguo y sueco antiguo antes de adoptarse el alfabeto latino, que reemplazó al antiguo sistema de escritura durante la Edad Media.

Existen en sueco dos géneros gramaticales: común y neutro, conocidos usualmente como "forma-"en"" y "forma-"ett"" respectivamente. En el sueco antiguo existían las formas masculina y femenina en lugar de la común, encontrándose aún en algunas frases antiguas y usos ceremoniales. El género de los sustantivos es generalmente arbitrario y debe ser memorizado en la mayoría de los casos. Los sustantivos forman el plural de varias maneras: añadiendo el sufijo "-r", "-er", "-or" o "-ar" en palabras de género común, en ocasiones cambiando la vocal final de la palabra (ej. "pojke" -niño, "pojkar" -niños, "by" -pueblo, "byar" -pueblos, "kvinna" -mujer, "kvinnor" -mujeres); añadiendo "-n" en palabras de género neutro terminadas en vocal (ej. "meddelande" -mensaje, "meddelanden" -mensajes); sin cambio en palabras de género neutro terminadas en consonante (ej. "tåg" -tren, trenes); o también existen plurales irregulares donde se cambia una vocal o donde se cambia una vocal y se añade un sufijo al mismo tiempo (ej. "fot" -pie, "fötter" -pies, "gås" -ganso, "gäss" -gansos). 

En una característica común con los otros idiomas escandinavos, el sueco presenta un artículo definido que es un enclítico (es decir, es un sufijo del sustantivo). Los sustantivos de "forma-"en"" toman los sufijos "-en" o "-n" en el singular y "-na" en el plural. Los sustantivos de la "forma-"ett"" toman las terminaciones "-et" o "-t" (singular) y "-na" o "-n" en plural. Ejemplos:

La mayoría de los verbos en infinitivo tienen su terminación en "-a", "-r" en el presente y "-de", "-te" o "-dde" en el pretérito imperfecto regular. Los verbos no hacen diferencia con respecto a persona o número, excepto en formas que intentan sonar arcaicas o muy solemnes (""jag" är "hemma"" ("estoy en casa") vs. ""vi" äro "hemma"" ("estamos en casa")). Otros tiempos verbales utilizan verbos auxiliares. La voz pasiva (o un verbo reflexivo) se forma también como un enclítico: rör "inte"! ("no toques"), "ej" röras! ("no se toca!", "que no sea tocado!").

Una de las características más interesantes del idioma sueco es el orden de las palabras en la oración. El orden "típico" de la oración afirmativa es SVO (sujeto-verbo-objeto) y el de la frase interrogativa es VSO. Lo peculiar del sueco es que en las oraciones afirmativas el verbo siempre tiene que ocupar la segunda posición, por eso el sueco es llamado un "idioma V2" por los lingüistas. El sujeto ocupa normalmente la primera posición, pero para dar más énfasis a otra palabra, se sitúa esta antes del verbo y el sujeto se traslada después del verbo. Veamos algunos ejemplos:

La mayoría de las palabras suecas es de origen germánico. Algunos ejemplos de palabras germánicas en sueco son "hand" (mano), "hals" (cuello) y "bok" (libro). Otras palabras han pasado a incorporarse a la lengua sueca desde el latín, el alemán (primero bajo alemán, la lengua franca de la liga hanseática y más tarde el alto alemán). A menudo, las nuevas palabras se forman por combinaciones de palabras. Los nuevos verbos pueden formarse añadiendo una "-a" o "-ra" a un sustantivo existente (p.ej., "planta" ("planta pequeña"), "plantera" ("plantar"). Muchas palabras suecas proceden también del francés: en esos casos, la ortografía sueca intenta imitar la pronunciación francesa. Así, por ejemplo, encontramos términos como "byrå" (de "bureau", buró), "adjö" (de "adieu", adiós), "fåtölj" (de "fauteuil", sillón).

El vocabulario es bastante uniforme a lo largo de Suecia, al menos el que se emplea en los medios de información y en escritos literarios. El finosueco, sin embargo, utiliza algunas palabras propias.

En los últimos tiempos, y debido a la gran influencia ejercida por el inglés sobre los medios de comunicación e Internet, ha nacido lo que viene a denominarse "Svengelska" (el equivalente sueco del spanglish). La incorporación de términos ingleses, sin embargo, pasa relativamente inadvertida por tratarse de idiomas emparentados. Además, está completamente aceptado conjugar extranjerismos y escribirlos como si fueran palabras de origen sueco. Ej. «Enviame un correo electrónico ("mail")»: «"skicka mig ett mejl"», o directamente «"mejla mig"».

Saber idiomas germánicos como el inglés o el alemán es de gran ayuda puesto que muchas normas gramaticales se pueden aplicar en el sueco. Se puede aprender también de manera autodidacta en casa con ciertos libros, aunque los más utilizados (Rivstart) son de inmersión y se recomienda usarlos con un profesor.

Hay algunos recursos como Duolingo que incluyen el sueco como idioma que se puede aprender a través del inglés. Aunque esta aplicación se recomienda, hay algunas críticas que dicen que un solo método para aprender sueco no es suficiente. Puesto que el aprendizaje de una lengua germánica como esta tiene que incluir "input" y "output", una vez se ha alcanzado un nivel básico de A2, se puede empezar a tener rutinas de inmersión como podcast, música o series de televisión.

Encontrar a compañeros de lenguas para hacer intercambios es algo difícil, puesto que el sueco está limitado a Suecia y Finlandia y no es de gran alcance, pero se pueden encontrar profesores en línea con aplicaciones como Verbling o Italki.



</doc>
<doc id="41475" url="https://es.wikipedia.org/wiki?curid=41475" title="(1034) Mozartia">
(1034) Mozartia

(1034) Mozartia es un asteroide perteneciente al cinturón de asteroides descubierto el 7 de septiembre de 1924 por Vladímir Aleksándrovich Albitski desde el observatorio de Simeiz en Crimea.
Está nombrado en honor de Wolfgang Amadeus Mozart (1756-1791), compositor alemán del siglo XVIII.


</doc>
<doc id="41476" url="https://es.wikipedia.org/wiki?curid=41476" title="(1814) Bach">
(1814) Bach

(1814) Bach es un asteroide perteneciente al cinturón de asteroides descubierto el 9 de octubre de 1931 por Karl Wilhelm Reinmuth desde el observatorio de Heidelberg-Königstuhl, Alemania.

Bach recibió al principio la designación de .
Posteriormente se nombró en honor del compositor alemán Johann Sebastian Bach (1685-1750).

Bach está situado a una distancia media de 2,226 ua del Sol, pudiendo alejarse hasta 2,516 ua y acercarse hasta 1,936 ua. Tiene una inclinación orbital de 4,346° y una excentricidad de 0,1304. Emplea en completar una órbita alrededor del Sol 1213 días.



</doc>
<doc id="41477" url="https://es.wikipedia.org/wiki?curid=41477" title="(1815) Beethoven">
(1815) Beethoven

(1815) Beethoven es un asteroide que forma parte del cinturón de asteroides y fue descubierto por Karl Wilhelm Reinmuth desde el observatorio de Heidelberg-Königstuhl, Alemania, el 27 de enero de 1932.

Beethoven se designó inicialmente como .
Más adelante fue nombrado en honor del compositor alemán Ludwig van Beethoven (1770-1827).

Beethoven orbita a una distancia media de 3,156 ua del Sol, pudiendo acercarse hasta 2,565 ua y alejarse hasta 3,747 ua. Su inclinación orbital es 2,738° y la excentricidad 0,1873. Emplea en completar una órbita alrededor del Sol 2048 días.



</doc>
<doc id="41478" url="https://es.wikipedia.org/wiki?curid=41478" title="(1818) Brahms">
(1818) Brahms

(1818) Brahms es un asteroide perteneciente al cinturón de asteroides descubierto por Karl Wilhelm Reinmuth desde el observatorio de Heidelberg-Königstuhl, Alemania, el 15 de agosto de 1939.

Brahms recibió inicialmente la designación de .
Más adelante se nombró en honor del compositor alemán Johannes Brahms (1833-1897).

Brahms está situado a una distancia media del Sol de 2,164 ua, pudiendo acercarse hasta 1,777 ua y alejarse hasta 2,55 ua. Su excentricidad es 0,1786 y la inclinación orbital 2,978°. Emplea en completar una órbita alrededor del Sol 1163 días.



</doc>
<doc id="41479" url="https://es.wikipedia.org/wiki?curid=41479" title="(2047) Smetana">
(2047) Smetana

(2047) Smetana es un asteroide que forma parte del cinturón interior de asteroides y fue descubierto por Luboš Kohoutek desde el Observatorio de Hamburgo-Bergedorf, Alemania, el 26 de octubre de 1971.

Smetana se designó al principio como .
Más adelante fue nombrado en honor del compositor checo Bedřich Smetana (1824-1884).

Smetana orbita a una distancia media de 1,872 ua del Sol, pudiendo acercarse hasta 1,866 ua y alejarse hasta 1,878 ua. Su excentricidad es 0,003401 y la inclinación orbital 25,28 grados. Emplea en completar una órbita alrededor del Sol 935,5 días.

Smetana pertenece al grupo asteroidal de Hungaria.

La magnitud absoluta de Smetana es 13,9 y el periodo de rotación de 2,497 horas.



</doc>
<doc id="41480" url="https://es.wikipedia.org/wiki?curid=41480" title="Arma química">
Arma química

Las armas químicas utilizan las propiedades tóxicas de sustancias químicas para matar, herir o incapacitar.

El armamento químico se diferencia de las armas convencionales o armas nucleares porque sus efectos destructivos no se deben principalmente a una fuerza explosiva. El uso ofensivo de organismos vivientes (como el "Bacillus anthracis", agente responsable del carbunco) es generalmente caracterizado como arma biológica, más que como arma química; los productos tóxicos producidos por organismos vivos (p. ej., toxinas como la toxina botulínica, ricina o saxitoxina) son considerados armas químicas. Según la Convención sobre Armas Químicas de 1993, se considera arma química a cualquier sustancia química tóxica, sin importar su origen, con la excepción de que sean utilizados con propósitos permitidos.

Las armas químicas están clasificadas como armas de destrucción masiva por las Organización de las Naciones Unidas y su producción y almacenamiento está proscrita por la ya mencionada convención de 1993.

Aproximadamente 70 productos químicos diferentes han sido utilizados o almacenados como agentes de armas químicas durante el siglo XX. Según la Convención, las sustancias que son suficientemente tóxicas como para ser usadas como armas químicas están divididas en tres grupos según su objetivo y tratamiento:


Las armas químicas han sido usadas en muchas partes del mundo durante cientos de años pero la «moderna» guerra química comenzó durante la Primera Guerra Mundial, aunque el primer país de la historia en usar masivamente estas armas fue España en 1925 durante la guerra del Rif, empleando masivamente en sus ataques el gas mostaza, mediante proyectiles de artillería o bombardeos aéreos. Inicialmente sólo se usaban conocidos productos químicos comerciales y sus variantes. Esto incluía el cloro y el gas fosgeno. Los métodos de dispersión de estos agentes durante el combate eran relativamente poco precisos e ineficientes.

El primer país en utilizar las armas químicas durante esta contienda fue Francia con el empleo de granadas rellenas de gas lacrimógeno (bromuro de xililo) en agosto de 1914.

Posteriormente el Imperio alemán respondió perfeccionando la técnica, iniciando ya el uso a gran escala de gases letales por ambos bandos. En los comienzos simplemente se abría los recipientes de cloro a favor del viento y se dejaba que éste la transportara hasta las filas enemigas. Poco después, los franceses modificaron su munición de artillería para contener fosfógenos, un método mucho más efectivo que se convirtió en el principal método para emplear estas armas.

Desde el desarrollo de la moderna guerra química en la Primera Guerra Mundial, las naciones han investigado y desarrollado estas armas en cuatro campos principalmente: nuevos y más mortales agentes; métodos más eficientes de lanzar estos agentes hasta el objetivo (diseminación); defensas más efectivas contra las armas químicas; y medios más precisos para detectar los agentes químicos.

Un producto químico usado para la guerra se llama «agente de guerra química» (en inglés, CWA), y habitualmente es gaseoso a temperatura ambiente, o puede ser un líquido que se evapore rápidamente. Este tipo de líquidos se llaman «volátiles», o pueden tener una alta presión de vapor. Los humos resultantes son tóxicos, y de ahí el término «gas venenoso», usado para describir un arma química usada en forma gaseosa. Muchos agentes químicos fueron diseñados en forma volátil con el fin de lograr una mejor dispersión en una gran área rápidamente.

Los primeros objetivos de la investigación sobre agentes químicos no eran la toxicidad, sino el desarrollo de agentes que pudieran afectar a la piel a través de la ropa, haciendo inútiles las máscaras de gas. En julio de 1917, los alemanes emplearon por primera vez al gas mostaza, el primer agente que, a pesar de la máscara de gas, penetraba el cuero y la tela para infligir dolorosas quemaduras en la piel.

Las armas químicas se clasifican de acuerdo a su persistencia, una medida del tiempo en que el agente químico permanece activo tras la diseminación. Los agentes químicos se dividen entre persistentes y no persistentes.

Los agentes clasificados como no persistentes pierden efectividad tras unos minutos u horas. Los agentes puramente gaseosos como el cloro son no persistentes, como tampoco los altamente volátiles como el sarín y muchos otros agentes nerviosos. Tácticamente, los agentes no persistentes son mucho más útiles contra objetivos que deben ser tomados y controlados en poco tiempo. Hablando en forma general, los agentes no persistentes presentan sólo peligro por inhalación.

En contraste con los primeros, los agentes persistentes tienden a permanecer en el entorno por periodos más largos, como una semana, complicando la descontaminación. La defensa contra los agentes persistentes requiere protección para largos períodos. Los agentes líquidos no volátiles como los agentes en ampolla y el agente nervioso oleaginoso VX no se evaporan fácilmente, y por lo tanto, presentan gran peligro al contacto.

Los agentes de la guerra química se organizan en muchas categorías de acuerdo con la forma en que afectan al cuerpo humano. Los nombres y números de las categorías varían un poco de fuente a fuente, pero los tipos generales de agentes de guerra química son los siguientes:

Existen seis tipos de agentes:


La mayoría de las armas químicas reciben una designación de una a tres letras por la OTAN, además o en lugar de su nombre común. Las armas químicas binarias, en las que las sustancias precursoras de agentes de armas químicas son mezcladas automáticamente para producir el agente justo antes de su uso, son indicados con un "-2" detrás de la designación (por ejemplo, GB-2 y VX-2).

Algunos ejemplos son los siguientes:

El factor más importante en la efectividad de las armas químicas es la eficiencia de su envío, o diseminación, al objetivo. La técnica más común incluye municiones (como bombas, proyectiles y ojivas), que permiten la diseminación a distancia, y tanques con spray, que son diseminados desde naves de baja altura. El desarrollo en las técnicas de llenado y almacenamiento de municiones ha sido siempre muy importante.

Aunque ha habido varios avances en el envío de armas químicas desde la Primera Guerra Mundial, es aún difícil lograr una dispersión efectiva. La diseminación es altamente dependiente de las condiciones atmosféricas porque muchos agentes químicos actúan en estado gaseoso. Es por esto que las observaciones del tiempo y los pronósticos son esenciales para optimizar el envío de armas y reducir el riesgo de herir a fuerzas amigas.

Las armas químicas se han usado desde hace milenios con flechas envenenadas, pero se pueden encontrar evidencias de la existencia de ingenios más avanzados en las épocas antigua y clásica.
Un buen ejemplo del temprano uso de las armas químicas fueron las sociedades de cazadores – recolectores del sur de África y de finales de la Edad de Piedra, conocidos como San. Empaparon las puntas de madera, hueso y piedra de sus flechas con venenos que obtenían en su entorno natural. Estos venenos provenían principalmente de escorpiones y serpientes, pero se cree que también utilizaron algunas plantas venenosas. Las flechas se disparaban contra el objetivo seleccionado, normalmente un antílope, y luego el cazador seguía al animal sentenciado hasta que el veneno provocaba su caída.

En el siglo V antes de Cristo algunos escritos de la secta moista en China describen el uso de fuelles para introducir el humo de las semillas de la mostaza y otros vegetales tóxicos en los túneles que excavaban los ejércitos enemigos durante los sitios. Algunos escritos chinos todavía más antiguos, datados alrededor del año 1000 a. C., contienen cientos de recetas para producir humos tóxicos o irritantes para usarlos durante la guerra, así como numerosos registros de su uso. Gracias a dichos registros sabemos del uso de la “niebla atrapa espíritus” que contenía arsénico, y el uso de cal viva pulverizada y esparcida con ayuda del viento para disolver una revuelta campesina en el año 178.

La primera noticia del uso del gas en Occidente se remonta al siglo V antes de Cristo, durante la guerra del Peloponeso entre Atenas y Esparta. Las fuerzas espartanas durante el asedio a una ciudad ateniense encendieron un fuego a los pies de las murallas hecho con madera, alquitrán y azufre, con la esperanza de que el nocivo humo incapacitara a los atenienses para resistir el asalto que siguió a continuación. Esparta no fue la única en usar estas tácticas poco convencionales durante dichas guerras: se dice que Solón de Atenas usó las raíces de eléboro para envenenar el agua de un acueducto alimentado por el río Pleistos alrededor del año 590 a. C., durante el sitio de Cirra.

Existe evidencia arqueológica que los sasánidas emplearon armas químicas contra el Ejército romano en el siglo III d.C. La investigación llevada a cabo en los túneles colapsados de Dura-Europos en Siria, sugiere que los iraníes emplearon bitumen y cristales de azufre para quemarlos. Al ser encendidos, los materiales produjeron densas nubes de gases asfixiantes que mataron a 20 soldados romanos en dos minutos. 

Las armas químicas eran conocidas en la China antigua y medieval. El historiador polaco Jan Długosz menciona el uso de gas venenoso por parte del ejército mongol durante la batalla de Legnica en el año 1241.

A fines del siglo XV, los españoles se enfrentaron a una rudimentaria arma química en la isla de La Española. Los taínos les lanzaban calabazas llenas de ceniza y ají pulverizado, a fin de crear una cortina de humo cegador antes de atacarlos.

Durante el Renacimiento se volvió a considerar el uso de la guerra química. Una de las primeras referencias proviene de Leonardo da Vinci, que propuso el uso de polvo de sulfuro de arsénico y verdín en el siglo XVI:

No se sabe si dicho polvo se usó alguna vez.

En el siglo XVIII durante los sitios, los ejércitos intentaban provocar incendios lanzando proyectiles incendiarios llenos de azufre, sebo, colofonia, aguarrás, sal de roca (nitrato de sodio o nitrato de potasio) y antimonio. Aunque no causaran incendios, los humos resultantes provocaban una considerable distracción. Pese a que su función principal nunca se abandonó, se desarrollaron nuevos productos para rellenar los proyectiles que maximizaran los efectos del humo.

En 1672, durante el sitio de la ciudad de Groninga, Christopher Bernhard van Galen (Obispo de Múnich) empleó varios explosivos y dispositivos incendiarios, algunos de los cuales incluían en su composición belladona, con la intención de producir humos tóxicos. Exactamente tres años después, el 27 de agosto de 1675, los franceses y alemanes llegaron al Acuerdo de Estrasburgo, que incluía un artículo prohibiendo el uso de los «pérfidos y odiosos» artefactos tóxicos.

En 1854, Lyon Playfar, un químico británico, propuso un proyectil de artillería antibuque cargado con cianuro de cacodilo como una forma de resolver el empate durante el sitio de Sebastopol. La propuesta fue apoyada por el Almirante Thomas Cochrane de la Marina Real británica. El primer ministro, lord Palmerston, lo consideró, pero el Departamento de Artillería británico rechazó la propuesta como «un tipo de guerra tan pernicioso como envenenar los pozos de los enemigos». La respuesta de Playfar se usó para justificar el uso de las armas químicas durante el siglo siguiente:

Después, durante la Guerra de Secesión, el profesor de escuela neoyorquino John Doughty propuso el uso ofensivo del cloro gaseoso, esparcido mediante proyectiles de 10 pulgadas (254 milímetros) llenos con cantidades variables de 2 a 3 litros de cloro líquido, que producirían varios metros cúbicos de gas de cloro. El plan de Doughty aparentemente nunca se llevó a cabo, pues probablemente fue presentado al brigadier general James W. Ripley, Jefe de Artillería, quién ha sido descrito como congénitamente inmune a las nuevas ideas.

La primera vez que se utilizaron los agentes químicos a gran escala fue durante la Primera Guerra Mundial, empezando en la Segunda Batalla de Ypres, el 22 de abril de 1915, cuando los alemanes atacaron a las tropas francesas, canadienses y argelinas con cloro. Desde entonces se utilizaron un total de 50 965 toneladas de agentes respiratorios, lacrimógenos y vesicantes por ambas partes, incluyendo cloro, fosgeno y gas mostaza. Las cifras oficiales hablan de alrededor de 1 176 500 heridos y 85 000 muertos causados directamente por los agentes químicos durante la guerra.

Incluso hoy en día es frecuente que se desentierre munición química de la Primera Guerra Mundial sin estallar cuando se excava en los antiguos campos de batalla o las áreas de almacenamiento, y continúan siendo un peligro para la población civil de Bélgica y Francia. Los gobiernos de dichos países han lanzado programas especiales para tratar la munición descubierta.

Después de la guerra, la mayoría de los agentes químicos sin usar de los alemanes fueron arrojados al mar Báltico. Con el paso del tiempo, el agua salada corroe las carcasas, y el gas mostaza derramado de dichos contenedores ocasionalmente llega a las playas como objetos sólidos con aspecto de cera, similar al ámbar. Incluso en su forma solidificada, el agente tiene actividad suficiente como para causar severas quemaduras a cualquiera que lo manipule.

Después de la Primera Guerra Mundial, los Estados Unidos y la mayoría de las potencias europeas intentaron sacar ventaja de las oportunidades que había creado la guerra, estableciendo y manteniendo colonias. Durante este periodo de entreguerras, los agentes químicos se usaron ocasionalmente para subyugar a las poblaciones y sofocar rebeliones.

Después de la derrota del Imperio otomano en 1917, el Gobierno Otomano desapareció completamente y el antiguo imperio se dividió entre las potencias victoriosas en el Tratado de Sèvres. Los británicos ocuparon Mesopotamia (actual Irak) y establecieron un gobierno colonial.

En 1920, los pueblos árabes y kurdos de Mesopotamia se rebelaron contra la ocupación británica, con grandes pérdidas por parte de los europeos. Según la resistencia mesopotámica ganaba fuerza, los británicos la reprimieron con medidas cada vez más agresivas, e incluso el propio Winston Churchill, como Secretario de las Colonias, autorizó el uso de agentes químicos, principalmente gas mostaza, contra la resistencia. Concienciado con el gasto económico de la supresión de disidentes, Churchill confiaba en que las armas químicas se pudieran utilizar de forma económica contra las tribus mesopotámicas, diciendo: «no entiendo la repugnancia sobre el uso del gas. Estoy muy a favor del uso del gas contra tribus incivilizadas». La oposición al uso del gas y las dificultades técnicas puede que impidieran su uso en Mesopotamia (los historiadores están divididos en esta materia) Las armas químicas causaron tanta miseria y repulsión en la Primera Guerra Mundial que su uso se convirtió en la peor atrocidad en la mente de la mayoría de las personas de la época. Tanto es así que en 1925 dieciséis de las mayores naciones del mundo firmaron el Protocolo de Ginebra, comprometiéndose a no usar nunca gases o armas bacteriológicas. Aunque los Estados Unidos firmaron el protocolo, el Senado no lo ratificó hasta 1975.

Durante la Guerra del Rif, en el Marruecos ocupado por España (1921-1927), empresas alemanas asesoraron y supervisaron la investigación, producción y utilización de armas químicas por parte de las fuerzas españolas en África. Al quedarles prohibida la experimentación y producción de este tipo de armamento por el Tratado de Versalles, las labores se realizaron en el protectorado español. Las fuerzas combinadas franco-españolas dispararon bombas de gas mostaza, iperita y fosgeno principalmente en un intento por sofocar la rebelión Bereber; sin embargo el éxito de la campaña química no fue tanto el empleo contra las kabilas rifeñas; sino contra sus campos de cultivo, privándoles de sus cosechas. De esta forma los gobernantes rifeños tuvieron que atacar territorio francés para, entre otros objetivos, conseguir alimentos, lo que desembocó en la alianza franco-española.

En 1935 la Italia fascista usó gas mostaza durante la invasión de Etiopía. Ignorando el Protocolo de Ginebra, firmado siete años antes, los militares italianos usaron bombas de gas mostaza, arrojadas desde aviones y lo diseminaron en forma de polvo. Se informó que hubo 15 000 bajas por armas químicas, la mayoría por gas mostaza.

Un Comité Nacional se encargó durante la Segunda República española de preparar a la ciudadanía en materia de defensa aeroquímica. Su fin fue meramente divulgativo, pues sólo las fuerzas de Asalto, en la práctica, fueron instruidas en el uso de las máscaras antigás. Había pasado tiempo más que suficiente desde la terminación de la gran guerra europea para que la concienciación en materia de gases letales se redujese a una anécdota respecto a una ciudadanía cada vez más ajena a ese género de violencias dantescas, de manera que el material instructivo consistió en un folleto de propaganda escolar que, debidamente traducido al castellano, había publicado el Servicio Químico Militar de Italia. Poco después de estallar la Guerra Civil, el bando franquista se aprestó a informar sobre posibles ataques aéreos republicanos, sirviéndose de dicho folleto, debidamente adicionado, para confeccionar ediciones de bolsillo y distribuirlas en las poblaciones que iban siendo 'liberadas'. En Huelva, una de las primeras provincias en sucumbir para la causa 'nacional', se ordenó en los primeros meses de 1936 la constitución de una Junta de Defensa Antiaérea y la impresión del folleto titulado "Instrucciones a la población civil en caso de ataque de aviones", que hoy constituye una rareza bibliográfica. En el Asedio del Alcázar de Toledo, las fuerzas republicanas emplearon «gases de guerra», según dice literalmente el informe de la Columna de Toledo, como posible solución al asedio al Alcázar (dos representantes franceses de una empresa de productos químicos las ofrecieron a las fuerzas republicanas).

Aunque el uso de armas químicas no se propagó durante la Segunda Guerra Mundial, sí existen casos documentados en los cuales las potencias del Eje utilizaron agentes químicos.

Japón utilizó gas mostaza y otro agente llamado lewisita (el cual era un agente vesicante) en algunas batallas que luchó contra China. Los trabajos de Yoshiaki Yoshimi y Seiya Matsuno muestran que Hirohito autorizó a través de órdenes específicas (rinsanmei) el uso de armas químicas contra los chinos. Por ejemplo, durante la invasión de Wuhan, de agosto a octubre de 1938, el emperador autorizó el uso de gas tóxico en 375 ocasiones distintas, a pesar de la resolución adoptada por la Sociedad de Naciones el 14 de mayo condenando el uso de gas tóxico por el ejército japonés.

Durante dichos ataques también hicieron uso de armas biológicas (Escuadrón 731) ya que intencionalmente propagaban el cólera, la disentería, el tifus, la peste bubónica y el ántrax (carbunco). Aún en el 2005, sesenta años después de la Segunda guerra sino-japonesa, se siguen encontrando contenedores de agentes químicos que fueron abandonados por los japoneses cuando emprendieron su retirada; estos contenedores han causado daños a personas y muertes.

La Alemania nazi revolucionó la guerra química al descubrir, accidentalmente, los agentes nerviosos que actualmente se conocen como tabun, sarín y soman. Los nazis desarrollaron y fabricaron grandes cantidades de estos agentes pero ninguno de los dos bandos de la guerra los uso en gran escala. Algunos documentos nazis que han sido recuperados sugieren que dentro de la Abwehr, la agencia de inteligencia alemana, se creía que los Aliados también tenían acceso a estos agentes y que el hecho que no se mencionaban en los informes científicos se debía a que era información confidencial. La realidad es que los Aliados no habían descubierto estos gases y la Abwehr interpretó la falta de información de manera errónea. Alemania finalmente optó por no utilizar estos agentes nerviosos ya que temieron que los Aliados contraatacaran utilizando sus propias armas químicas contra del Tercer Reich.

Según William L. Shirer, autor de "The Rise and Fall of the Third Reich" ("El ascenso y la caída del Tercer Reich"), los oficiales más altos del Reino Unido optaron por dejar la guerra química como la última opción en la defensa de la isla en caso que la Alemania nazi decidiera invadir las tierras británicas.

El uso de agentes químicos se dio especialmente cuando no existía miedo a un contraataque y algunas instancias en las cuales sucedieron fueron:


Después de la Segunda Guerra Mundial, los Aliados recuperaron proyectiles que contenían los tres agentes nerviosos del momento (tabun, sarin y soman), impulsando las investigaciones sobre los agentes nerviosos de todos los antiguos aliados. A pesar de que la amenaza de la aniquilación termonuclear estaba en la mente de la mayoría durante la Guerra fría, tanto los gobiernos soviéticos como los occidentales gastaron muchos recursos en el desarrollo de armas químicas y biológicas.

En 1952 el ejército de los Estados Unidos patentó un procedimiento para la "preparación de Ricino tóxico", publicando un método de producción de esta poderosa toxina.

También en 1952, investigadores de Porton Down, Inglaterra, inventaron el agente nervioso VX, pero pronto abandonaron el proyecto. En 1958 el Gobierno Británico vendió su tecnología VX a los Estados Unidos a cambio de información sobre las armas termonucleares. Su desarrollo produjo al menos tres agentes más; a los cuatro (VE, VG, VM, VX) se les conoce como el tipo de agentes nerviosos "Serie V".

Durante los años 60, los Estados Unidos exploraron el uso de agentes incapacitantes delirantes anticolinergicos. Uno de dichos agentes, con la designación BZ, se cree que se usó experimentalmente durante la guerra de Vietnam. Estas suposiciones inspiraron la película "La escalera de Jacob" de 1990.

Entre 1967 y 1968, los Estados Unidos decidieron deshacerse de las ármas químicas obsoletas en una operación llamada CHASE, acrónimo que corresponde a «Abre agujeros y húndelos» en inglés ("cut holes and sink 'em"). Las operaciones CHASE también incluían muchas cargas de munición convencional. Como su propio nombre indica, las armas se embarcaron en viejos barcos Liberty que fueron hundidos en el mar.

En 1969, 23 soldados estadounidenses y un civil estacionados en Okinawa, Japón, se expusieron a niveles bajos del agente nervioso sarin mientras repintaban los almacenes. Las armas se habían mantenido ocultas al Japón, y provocaron la ira en dicho país y un incidente internacional. Dichas municiones se trasladaron en 1971 al atolón Johnston bajo la Operación Sombrero Rojo.

Un grupo de trabajo de las Naciones Unidas empezó a trabajar en el desarme químico en 1980. El 4 de abril de 1984, el presidente de los Estados Unidos Ronald Reagan hizo un llamamiento para la prohibición internacional de las ármas químicas. El presidente George H. W. Bush y el líder de la Unión Soviética Mijaíl Gorbachov firmaron el tratado bilateral el 1 de junio de 1990 que ponía fin a la producción de ármas químicas e iniciaba la destrucción de las reservas de sus naciones. La Convención de Ármas Químicas multilateral (CWC) se firmó en 1993 y entró en vigor en 1997.

En un informe emitido por el Senado de los Estados Unidos en 1994, titulado "Is military research hazardous to veterans health? Lessons spanning a half century" ("¿Es la investigación militar un riesgo para la salud de los veteranos? Medio siglo de enseñanzas", se detalló el hecho que el Departamento de Defensa de los Estados Unidos había realizado experimentos sobre animales y humanos en diversas ocasiones y estos últimos no habrían sabido a lo que realmente se les estaba sometiendo. Algunos de estos experimentos fueron:


Debido al nivel de secreto que rodeaba al gobierno de la Unión Soviética, había disponible muy poca información sobre la dirección y el progreso de las armas químicas soviéticas, situación que ha cambiado solo recientemente. Después de la Guerra Fría (1962-1991), el químico ruso Vil Mirzayanov publicó artículos en los cuales revelaba experimentos ilegales con armas químicas en la Unión Soviética. En 1993, Mirzayanov fue encarcelado y despedido de su trabajo en el Instituto Estatal de Investigación de Química y Tecnología Orgánica, donde había trabajado 26 años. En marzo de 1994, después de una gran campaña hecha en su favor por científicos de estados unidos, Mirzayanov fue liberado.

Entre la información revelada por Mirzayanov estaba la dirección de la investigación soviética, que pretendía desarrollar agentes nerviosos aún más tóxicos, lo cual tuvo su mayor éxito a mediados de la década de los 80. Muchos agentes altamente tóxicos fueron desarrollados en este período: la única información no clasificada sobre estos agentes es que se conocen en la literatura abierta como agentes "defoliantes" (llamados así por el programa en el que se desarrollaron) y según varios nombres código como A-230 y A-232.

Según Mirzayanov, los soviéticos también desarrollaron agentes que eran más fáciles de manejar, lo cual llevó a la creación de las llamadas armas binarias, en las cuales se mezclan precursores de los agentes nerviosos dentro de una munición para producir el agente justo antes de utilizarlo. Como los precursores generalmente son significativamente menos peligrosos que los agentes mismos, esta técnica hace que tanto el manejo como el transporte de la munición sean mucho más sencillos. Además, los precursores de los agentes suelen ser mucho más fáciles de estabilizar que los agentes, lo cual permitió aumentar el tiempo de almacenamiento de los agentes. Durante las décadas de 1980 y 1990, se desarrollaron las versiones binarias de muchos agentes soviéticos que se conocen hoy en día como agentes "Novichok", "recién llegados", en ruso.

La Guerra Irán-Irak comenzó en 1980 cuando Irak intentó invadir Irán. En las primeras fases de la guerra, Irak comenzó a utilizar gas mostaza y tabun en las bombas que utilizaba en sus ataques aéreos; se ha aproximado que 5% de las muertes iraníes fueron causadas por estos agentes. Irak y los Estados Unidos anunciaron que Irán también estaba utilizando dichas armas, pero hasta la fecha esta declaración no ha sido corroborada por ninguna fuente externa.

Se dice que aproximadamente 100 000 soldados iraníes fueron víctimas de los ataques químicos de Irak. Muchos sufrieron los efectos del gas mostaza. Las cifras oficiales no incluyen a los civiles que se vieron afectados por vivir en los pueblos involucrados en el conflicto ni tampoco a los hijos ni parientes de los veteranos, muchos de los cuales han desarrollado complicaciones en su sangre, pulmones o piel (según datos de la Organización para los veteranos, Oranization for Veterans). Se dice que los agentes nerviosos mataron a aproximadamente 20 000 soldados iraníes inmediatamente (según cifras oficiales). De las 80 000 personas que sobrevivieron dichos ataques, se estima que 5000 deben de someterse a tratamientos médicos regularmente y 1000 todavía se encuentran hospitalizados debido a la gravedad de sus condiciones.

Poco después de la guerra, en 1988, la aldea iraquí de Halabja sufrió un ataque químico en el cual 5000 de sus 50 000 habitantes kurdos perecieron. Después de dicho incidente se encontraron rastros de gas mostaza, sarín, tabun y VX. Aunque el ataque pareció haber sido obra de las fuerzas del gobierno iraquí, esto todavía sigue en debate y también sigue la interrogante en cuanto a si fue un accidente o un acto premeditado.

Durante la primera Guerra del Golfo en 1991, las tropas de la Coalición iniciaron una guerra terrestre contra Irak. Aunque Irak contaba con un arsenal químico, nunca lo utilizó en contra de dicho ejército. El comandante de Coalición, Norman Schwarzkopf, declaró que este fue el caso ya que Irak temía un contraataque nuclear.

Aunque los Estados Unidos y sus aliados derrocaron al régimen de Saddam Hussein, en Irán todavía se culpa a Estados Unidos, Alemania y Francia por haber ayudado a Irak en el desarrollo de su arsenal químico. Además el hecho que Irak no haya sido sancionado por haber utilizado armas químicas también es un tema que sigue en la mente del pueblo iraní.

En 2013, varias fuentes de la oposición siria aseguraron que el gobierno sirio había usado armas químicas en contra de la población en la Guerra civil en Siria. Gran parte de la comunidad internacional había advertido que el uso de este tipo de armas traería severas consecuencias y Estados Unidos se atrevió incluso a hablar de intervención militar en Siria en caso de que probara que el uso de armas químicas. Siria ha sido uno de los pocos países que no firmaron el Tratado por la Prohibición de Armas Químicas, acuerdo que condena su uso. El 18 de marzo de 2013, hubo un supuesto ataque con armas químicas en la ciudad de Alepo, en el norte del país, donde murieron 26 personas y otras 86 quedaron heridas. Estados Unidos, junto con más países y miembros de la ONU, iniciaron una serie de operaciones para investigar si efectivamente se utilizaron armas químicas en la guerra y descubrir si fueron los rebeldes o el mismo gobierno de Siria quienes las utilizaron. Más tarde, el 13 de abril, se acusó al gobierno utilizar armas químicas en contra de los rebeldes en las afueras de la capital Damasco. El periodista Jean-Philippe Rémy y el fotógrafo Van der Stockt aseguraron haber sido víctimas de estos ataques y de los síntomas que estos provocan, además de acusar al gobierno sirio de Bashar Al-Assad ser el responsable. Otros rebeldes del Ejército Libre de Siria, también han asegurado ser víctimas de estos ataques con armas químicas. Estos incidentes provocaron que las Naciones Unidas enviaran personal al país para investigar la situación. Cada vez hay más pruebas de que los insurgentes en Siria disponen de armas químicas e incluso algunos rebeldes han admitido su responsabilidad en el uso de armas químicas.

El 21 de agosto de 2013, 1400 personas murieron y 3000 resultaron heridas en Guta, al sur de Damasco, en un ataque con gas sarín conocido como Masacre de Ghouta, el peor ataque con armas químicas en 25 años y la peor matanza humana del año 2013. La brutalidad del ataque tuvo un gran impacto en la comunidad internacional. Las naciones occidentales, lideradas por Estados Unidos, acusaron al gobierno sirio de Bashar Al-Asad de ser el responsable y Barack Obama anunció que su país podría atacar a Siria por haber perpetrado la masacre. Francia y Turquía también dieron su apoyo a un intervención militar. Debido a la escalada de tensión y ante el inminente ataque por parte de los estadounidenses, Rusia, principal aliada de Siria, ideo un plan de desarme químico a comienzos de septiembre, en el cual el gobierno sirio debería renunciar a su arsenal de armas químicas y destruirlas sistemáticamente. Estados Unidos aceptó el plan y anunció que ya no atacaría Siria si este país cumplía con el acuerdo. El Desarme químico de Siria esta vigente y se espera que se concrete a mediados de 2014. Sin embargo, más de alguna fuente denunció nuevos ataques químicos posteriores a la masacre del 21 de agosto.

Muchas organizaciones terroristas consideran a los agentes químicos como su arma predilecta al diseñar sus ataques. Usualmente dichas armas son baratas, relativamente accesibles y fáciles de transportar. Un experto en química fácilmente puede formular agentes químicos si tiene acceso a las fórmulas y los materiales.

Algunos comentaristas políticos se han puesto en contra de la noción de que las armas biológicas y químicas sean realmente las más prácticas para los terroristas. Dichos analistas han reportado que el uso de dichas armas es mucho más difícil que manejar explosivos convencionales y que las armas de destrucción masiva pueden llegar a inspirar más miedo que las armas bioquímicas.


El 20 de marzo de 1995 un grupo de terroristas japoneses, que creía en la destrucción inminente de todo el planeta, llamado Aum Shinrikyo utilizó sarín en el sistema del metro de Tokio. Dicho ataque provocó 12 muertes y más de 5000 heridos. Aum Shinrikyo ya había intentado ese tipo de ataque en diez ocasiones previas pero en cada una de ellas solo miembros del culto habían terminado afectados. En junio de 1994 el grupo lanzó un ataque químico, utilizando sarín, en contra de un edificio de apartamentos en Matsumoto.




</doc>
<doc id="41482" url="https://es.wikipedia.org/wiki?curid=41482" title="(2055) Dvořák">
(2055) Dvořák

(2055) Dvořák es un asteroide perteneciente al grupo de los asteroides que cruzan la órbita de Marte descubierto el 19 de febrero de 1974 por Luboš Kohoutek desde el Observatorio de Hamburgo-Bergedorf, Alemania.

Dvořák se designó inicialmente como .
Más tarde fue nombrado en honor del compositor checo Antonín Dvořák (1841-1904).

Dvořák está situado a una distancia media de 2,31 ua del Sol, pudiendo acercarse hasta 1,59 ua y alejarse hasta 3,029 ua. Tiene una inclinación orbital de 21,49 grados y una excentricidad de 0,3115. Emplea en completar una órbita alrededor del Sol 1282 días.

La magnitud absoluta de Dvořák es 12,7 y el periodo de rotación de 4,405 horas.



</doc>
<doc id="41483" url="https://es.wikipedia.org/wiki?curid=41483" title="(3822) Segovia">
(3822) Segovia

(3822) Segovia es un asteroide perteneciente al cinturón de asteroides descubierto por Tsutomu Seki desde el Observatorio de Geisei, Japón, el 21 de febrero de 1988.

Segovia fue designado al principio como .
Más tarde, en 1989, se nombró en honor del guitarrista español Andrés Segovia (1893-1987).

Segovia está situado a una distancia media del Sol de 2,27 ua, pudiendo acercarse hasta 2,003 ua y alejarse hasta 2,537 ua. Su excentricidad es 0,1176 y la inclinación orbital 2,56 grados. Emplea en completar una órbita alrededor del Sol 1249 días.

La magnitud absoluta de Segovia es 13,8.



</doc>
<doc id="41484" url="https://es.wikipedia.org/wiki?curid=41484" title="Arma biológica">
Arma biológica

Un arma biológica —también conocida como bioarma o arma bacteriológica— es cualquier patógeno (bacteria, virus u otro microorganismo que cause enfermedades) que se utiliza como arma de guerra. Utilizar productos tóxicos no vivientes, incluso si son producidos por organismos vivos (por ejemplo, toxinas), es considerado como un arma química bajo las provisiones de la Convención de armas químicas. Un arma biológica puede estar destinada a matar, incapacitar o impedir seriamente a un individuo como a ciudades o lugares enteros. También puede ser definida como el material o defensa contra tal empleo. La guerra biológica es una técnica militar que puede ser usada por Estados-nación o por grupos no nacionales. En el último caso, o si un Estado-Nación la usa clandestinamente, también puede ser considerado como bioterrorismo.


Son utilizadas para causar daño a las personas, de igual manera son utilizadas para matar, incapacitar e impedir seriamente a un enemigo.
Así como también pueden dañar a los animales y los alimentos que consumimos día a día.
Son la respuesta lógica a la necesidad de destruir o incapacitar a un enemigo sin acabar con sus armas o la zona en la que se encuentra que puede ser la razón de la disputa.

La mayoría de los agentes biológicos son difíciles de cultivar y mantener. Muchos se descomponen rápidamente cuando están expuestos a la luz solar y otros factores del medio ambiente, mientras que otros, tales como las esporas de "Bacillus anthracis", tienen una vida larga. Pueden dispersarse rociándolos en el aire o infectando a los animales que transmiten la enfermedad a los humanos a través de la contaminación de los alimentos y el agua. La dispersión de este tipo de armas es también compleja, dada la fragilidad de los entes vivos que la componen, y suele realizarse de las siguientes formas: 


En los Estados Unidos, a finales de 2001, esporas de "B. anthracis", fueron enviadas por correo a personas del gobierno y los medios de comunicación. Estas esporas son elaboradas en forma de un polvo blanco. Las máquinas de clasificación de la correspondencia postal y el abrir las cartas dispersó las esporas en forma de aerosoles. Ocurrieron algunas muertes como resultado de esto. El efecto era interrumpir el servicio de correos y causar pánico general entre el público con respecto al manejo de la correspondencia entregada.

La propagación de persona a persona de algunos agentes infecciosos también es posible. Los humanos han sido la fuente de infecciones de viruela, peste bubónica y los virus Lassa.

En los libros redactados por Sexto Julio Frontino (alrededor del año 90 d. C.) se mencionan acciones tales como el introducir enjambres de abejas en los túneles, lanzar contra las naves enemigas recipientes llenos de serpientes venenosas, dejar libres fieras hambrientas contra los sitiados, lanzar dentro de las murallas carroña de animales en descomposición, etc.

En Estados Unidos, la población nativa americana era diezmada después del contacto con el Viejo Mundo debido a la introducción de muchas y diferentes enfermedades letales. Hay dos casos documentados de presuntas e intentadas guerras con gérmenes. El primero, durante un parlamento a Fort Pitt el 24 de junio de 1763, Ecuyer dio al los representantes de los asediantes Delawares dos cobijas y un pañuelo que había sido expuesto a viruela, esperando que extendieran la enfermedad a los nativos en orden a terminar con el asalto. William Trent, el comandante de la milicia, dejó registros que claramente indicaban que la propuesta de darles las cobijas era “para transmitir la viruela a los indios”.

Durante la Guerra de Secesión Estadounidense, el general Sherman reportó que las fuerzas Confederadas dispararon animales de granja en estanques que la Unión dependía para tomar agua. Esto habría hecho el agua imposible de tomar, aunque los verdaderos riesgos de salud provenientes de cadáveres humanos y de animales que no murieron de enfermedad son mínimos.

Durante la Guerra Sino-Japonesa (1937-1945) y la Segunda Guerra Mundial, la Unidad 731 del Ejército Imperial Japonés condujo experimentos en humanos, la mayoría prisioneros chinos, rusos y estadounidenses. En campañas militares, el ejército japonés usó armas biológicas en soldados y civiles chinos.

Por ejemplo, en 1940, el Ejército Imperial Japonés bombardeó Ningbo con bombas de cerámica llenas de pulgas cargadas con la peste bubónica. Una película mostrando esta operación fue vista por los príncipes imperiales Tsuneyoshi Takeda y Takahito Mikasa durante una escenografía hecha por la mente maestra Shiro Ishii.

Sin embargo, algunas operaciones fueron inefectivas debido a los ineficientes sistemas de entrega, usando insectos portadores de enfermedades más que dispersando el agente como una nube de aerosol. Se estima que chinos murieron como resultado directo de las pruebas de armas biológicas en los campos japoneses.

Durante los Juicios de Crímenes de Guerra de Jabárovsk los acusados, tales como el General Mayor Kiyashi Kawashima, testificaron que desde 1941 unos 40 miembros de la Unidad 731 arrojaron desde el aire pulgas contaminadas con peste en Changde. Estas operaciones causaron brotes epidémicos de peste.

Veinticinco países firman la Conferencia de la Haya de 1899 que en una de sus cláusulas expresa que los estados firmantes se comprometen en no usar proyectiles cuyo único objetivo sea la de liberar gases asfixiantes o venenosos. 

Guerra ruso-japonesa : las naves japonesas disparan contra las rusas granadas cargadas con gases venenosos 
Segunda Convención de La Haya (18 de octubre de 1907) Se renueva la prohibición de armas químicas y la utilización de aviones en la guerra. Cinco de las potencias que luego tomaran parte en la Primera Guerra Mundial no firmaron.





Uso de bombas de fósforo blanco o cargadas con otros materiales incendiarios son lanzadas por los aliados sobre la Alemania nazi en 1943.

Estados Unidos comienza a usar napalm arrojándolo sobre Tokio, convirtiendo la ciudad en un enorme horno crematorio. Luego, meses más tarde es usado en Okinawa. 











Las características ideales de las armas biológicas que tienen como objetivos a los seres humanos son una infectividad alta, alta potencia, disponibilidad de vacunas y lanzamiento como un aerosol.

Las enfermedades que son más probables de ser consideradas para uso de armas biológicas compiten debido a su letalidad (si son lanzadas eficientemente) y robustez (al hacer factible el lanzamiento por aerosol). Por su parte, los agentes biológicos usados en armas biológicas pueden ser fabricados a menudo con rapidez y fácilmente. La dificultad principal no es la producción del agente biológico, sino el lanzamiento en una forma efectiva al objetivo vulnerable. 

Por ejemplo, el "Bacillus anthracis" es considerado un agente efectivo por varias razones. En primer lugar, forma esporas fuertes, perfectas para su dispersión en aerosoles. En segundo lugar, las infecciones neumonales (de pulmón) producidas por el carbunco o ántrax usualmente no causan infecciones secundarias en otras personas. Luego, el efecto del agente queda usualmente confinado al objetivo. Una infección neumonal causada por el ántrax empieza con los síntomas de un resfrío ordinario y rápidamente se vuelve letal, con una tasa de mortalidad del 90 % o mayor. Finalmente, el personal amigo puede ser protegido con antibióticos adecuados. Un ataque masivo que utilice esporas de esta bacteria requeriría la creación de partículas de aerosol de 1,5 a 5 micrómetros. Si fuera muy grande el aerosol sería filtrado por el sistema respiratorio. Mientras que si fuera muy pequeño, el aerosol sería inhalado y exhalado. Asimismo, a este tamaño, los polvos no conductivos tienden a aglutinarse y adherirse debido a las cargas electrostáticas, lo cual impide la dispersión. Por ello, el material debe ser tratado para aislar y descargar las cargas. El aerosol debe ser lanzado de forma que ni la lluvia ni el sol lo descomponga y el pulmón humano pueda ser infectado. Estas son solo algunas de las dificultades tecnológicas que existen.

Las enfermedades consideradas para ser usadas como armas, o conocidas por ser utilizadas como tales, incluyen el carbunco (TR), ébola, virus de Marburgo, plaga (LE), cólera (HO), tularemia (SR & JT), brucelosis (US, AB & AM), fiebre Q (OU), fiebre hemorrágica boliviana, coccidioidomicosis (OC), muermo (LA), melioidosis (HI), shigella (Y), fiebre de las Montañas Rocosas(UY), tifus (YE), psitacosis(SI), fiebre amarilla (UT), encefalitis japonesa B (AN), fiebre del valle del Rift (FA) y la viruela (ZL). Toxinas surgidas naturalmente que pueden ser usadas como armas, incluyen ricina (WA), SEB (UC), Toxina botulínica (XR), saxitoxina (TZ) y muchas micotoxinas. Los organismos que causan estas enfermedades son conocidos como agentes selectos. En Estados Unidos, su posesión, uso y transferencia son regulados por el Programa Agente Selecto del Centro de Control y Prevención de Enfermedades.

Las armas biológicas también pueden tener como objetivo plantas específicas para destruir cultivos o desfoliar vegetación. Estados Unidos y el Reino Unido descubrieron reguladores de crecimiento de las plantas (por ejemplo, herbicidas) durante la Segunda Guerra Mundial e iniciaron un programa de armas herbicidas que fue utilizado eventualmente en Malasia y Vietnam en la contrainsurgencia. Aunque los herbicidas son químicos, a menudo son agrupados con las armas biológicas como bioreguladores de manera similar a las biotoxinas.

Estados Unidos desarrolló su capacidad de destrucción de cultivos durante la Guerra Fría que usó bioherbicidas para enfermedades de plantas o micoherbicidas para destruir la agricultura del enemigo. Se creía que la destrucción de la agricultura del enemigo en una escala estratégica podía frustrar la agresión sino-soviética en una guerra general. Enfermedades tales como la roya del trigo y la roya del arroz pueden ser convertidas en armas cargando tanques para rociados aéreos y bombas de rácimo para lanzarlas a aguas enemigas que rieguen regiones agrícolas para iniciar epifitóticas (epidemias entre las plantas). Cuando Estados Unidos abandonó su programa de armas biológicas ofensivas en 1969 y 1970, la vasta mayoría de su arsenal biológico estaba compuesto de estas enfermedades de plantas.

En la década de 1980, el ministerio soviético de Agricultura desarrolló exitosamente variantes de Glosopeda y Peste bovina contra vacas, fiebre porcina africana para cerdos y Psitacosis para matar pollos. Estos agentes eran preparados para ser rociados desde tanques acoplados a aviones desde cientos de kilómetros. El programa secreto fue nombrado "Ecología." 

Atacar animales es otra área de las armas biológicas que tiene como propósito eliminar recursos animales que podrían ser utilizados como transporte o comida. En la Primera Guerra Mundial, agentes alemanes fueron arrestados en un intento por inocular animales con ántrax y se creía que eran responsables de brotes de muermo en caballos y mulas. Los británicos contaminaron pasteles con ántrax en la Segunda Guerra Mundial como un medio potencial de atacar ganado alemán, pero nunca usaron esta arma.

Sin conexión con las guerras, los seres humanos han introducido deliberadamente la enfermedad de conejos Mixomatosis, originaria de Sudamérica, a Australia y Europa, con la intención de reducir la población de conejos, lo que ha tenido resultados devastadores pero temporales, con poblaciones de conejos salvajes reducidas a una fracción de su tamaño original, pero los sobrevivientes desarrollaron inmunidad y se incrementaron nuevamente.





</doc>
<doc id="41485" url="https://es.wikipedia.org/wiki?curid=41485" title="(3910) Liszt">
(3910) Liszt

(3910) Liszt es un asteroide que forma parte del cinturón de asteroides y fue descubierto por Eric Walter Elst el 16 de septiembre de 1988 desde el Observatorio de la Alta Provenza, Francia.

Liszt se designó inicialmente como .
Más adelante, en 1989, fue nombrado en honor del compositor húngaro Franz Liszt (1811-1886).

Liszt está situado a una distancia media de 2,795 ua del Sol, pudiendo acercarse hasta 2,426 ua y alejarse hasta 3,163 ua. Su excentricidad es 0,1319 y la inclinación orbital 8,688 grados. Emplea en completar una órbita alrededor del Sol 1706 días.

La magnitud absoluta de Liszt es 11,8 y el periodo de rotación de 4,73 horas. Está asignado al tipo espectral S de la clasificación SMASSII.



</doc>
<doc id="41486" url="https://es.wikipedia.org/wiki?curid=41486" title="(3975) Verdi">
(3975) Verdi

(3975) Verdi es un asteroide que forma parte del cinturón de asteroides y fue descubierto el 19 de octubre de 1982 por Freimut Börngen desde el Observatorio Karl Schwarzschild, en Tautenburg, Alemania.

Verdi recibió inicialmente la designación de .
Más tarde, en 1989, se nombró en honor del compositor italiano Giuseppe Verdi (1813-1901).

Verdi está situado a una distancia media de 2,897 ua del Sol, pudiendo acercarse hasta 2,738 ua y alejarse hasta 3,056 ua. Tiene una inclinación orbital de 1,293 grados y una excentricidad de 0,05484. Emplea 1801 días en completar una órbita alrededor del Sol.

La magnitud absoluta de Verdi es 12,3.



</doc>
<doc id="41488" url="https://es.wikipedia.org/wiki?curid=41488" title="(3992) Wagner">
(3992) Wagner

(3992) Wagner es un asteroide perteneciente al cinturón de asteroides descubierto el 29 de septiembre de 1987 por Freimut Börngen desde el Observatorio Karl Schwarzschild, en Tautenburg, Alemania.

Wagner se designó inicialmente como .
Más adelante, en 1989, fue nombrado en honor del compositor alemán Richard Wagner (1813-1883).

Wagner orbita a una distancia media de 3,017 ua del Sol, pudiendo alejarse hasta 3,262 ua y acercarse hasta 2,772 ua. Tiene una excentricidad de 0,08119 y una inclinación orbital de 10,42 grados. Emplea en completar una órbita alrededor del Sol 1914 días.

La magnitud absoluta de Wagner es 11,4 y el periodo de rotación de 20,63 horas.



</doc>
<doc id="41489" url="https://es.wikipedia.org/wiki?curid=41489" title="(4079) Britten">
(4079) Britten

(4079) Britten es un asteroide que forma parte del cinturón de asteroides y fue descubierto el 15 de febrero de 1983 por Edward L. G. Bowell desde la Estación Anderson Mesa, en Flagstaff, Estados Unidos.

Britten fue designado inicialmente como .
Posteriormente, en 1990, se nombró en honor del compositor británico Benjamin Britten (1913-1976).

Britten está situado a una distancia media de 3,189 ua del Sol, pudiendo alejarse hasta 3,541 ua y acercarse hasta 2,837 ua. Tiene una excentricidad de 0,1103 y una inclinación orbital de 2,391 grados. Emplea en completar una órbita alrededor del Sol 2080 días.

La magnitud absoluta de Britten es 12,6.



</doc>
<doc id="41491" url="https://es.wikipedia.org/wiki?curid=41491" title="(4132) Bartók">
(4132) Bartók

(4132) Bartók es un asteroide perteneciente al cinturón de asteroides descubierto por Jeffrey Thomas Alu el 12 de marzo de 1988 desde el observatorio del Monte Palomar, Estados Unidos.

Bartók recibió inicialmente la designación de .
Más adelante, en 1989, se nombró en honor del compositor húngaro Béla Bartók (1881-1945).

Bartók está situado a una distancia media de 2,407 ua del Sol, pudiendo acercarse hasta 1,715 ua y alejarse hasta 3,099 ua. Su excentricidad es 0,2875 y la inclinación orbital 23,33 grados. Emplea 1364 días en completar una órbita alrededor del Sol.

La magnitud absoluta de Bartók es 12. Tiene 10,56 km de diámetro y un periodo de rotación de 3,297 horas. Su albedo se estima en 0,3308.



</doc>
<doc id="41492" url="https://es.wikipedia.org/wiki?curid=41492" title="Sueco">
Sueco

El término Sueco puede referirse a:


Además,

</doc>
<doc id="41493" url="https://es.wikipedia.org/wiki?curid=41493" title="(4152) Weber">
(4152) Weber

(4152) Weber es un asteroide que forma parte del cinturón de asteroides y fue descubierto por Edward L. G. Bowell desde la Estación Anderson Mesa, en Flagstaff, Estados Unidos, el 15 de mayo de 1985.

Weber fue designado al principio como .
Más adelante, en 1990,se nombró en honor del compositor alemán Carl Maria von Weber (1786-1826).

Weber orbita a una distancia media de 3,177 ua del Sol, pudiendo acercarse hasta 2,907 ua y alejarse hasta 3,447 ua. Tiene una inclinación orbital de 17,57 grados y una excentricidad de 0,08487. Emplea 2068 días en completar una órbita alrededor del Sol.

La magnitud absoluta de Weber es 11,9. Tiene un diámetro de 18,2 km y su albedo se estima en 0,0585.



</doc>
<doc id="41494" url="https://es.wikipedia.org/wiki?curid=41494" title="(4330) Vivaldi">
(4330) Vivaldi

(4330) Vivaldi es un asteroide que forma parte del cinturón de asteroides y fue descubierto por Freimut Börngen el 19 de octubre de 1982 desde el Observatorio Karl Schwarzschild, en Tautenburg, Alemania.

Vivaldi recibió inicialmente la designación de .
Más adelante, en 1990, se nombró en honor del compositor italiano Antonio Vivaldi (1680-1743).

Vivaldi orbita a una distancia media del Sol de 2,242 ua, pudiendo acercarse hasta 2,156 ua y alejarse hasta 2,327 ua. Su excentricidad es 0,03818 y la inclinación orbital 2,668 grados. Emplea 1226 días en completar una órbita alrededor del Sol.

La magnitud absoluta de Vivaldi es 13,9.



</doc>
<doc id="41495" url="https://es.wikipedia.org/wiki?curid=41495" title="(4579) Puccini">
(4579) Puccini

(4579) Puccini es un asteroide perteneciente al cinturón de asteroides descubierto por Freimut Börngen desde el Observatorio Karl Schwarzschild, en Tautenburg, Alemania, el 11 de enero de 1989.

Puccini recibió al principio la designación de .
Posteriormente, en 1991, se nombró por el compositor italiano Giacomo Puccini (1858-1924).

Puccini está situado a una distancia media de 2,4 ua del Sol, pudiendo alejarse hasta 2,769 ua y acercarse hasta 2,032 ua. Su inclinación orbital es 2,214 grados y la excentricidad 0,1535. Emplea en completar una órbita alrededor del Sol 1358 días.

La magnitud absoluta de Puccini es 13,8.



</doc>
<doc id="41496" url="https://es.wikipedia.org/wiki?curid=41496" title="(4865) Sor">
(4865) Sor

(4865) Sor es un asteroide que forma parte del cinturón de asteroides y fue descubierto por Tsutomu Seki el 18 de octubre de 1988 desde el Observatorio de Geisei, Japón.

Sor fue designado inicialmente como .
Más adelante, en 1991, se nombró en honor del guitarrista español Fernando Sor (1778-1839).

Sor orbita a una distancia media del Sol de 3,049 ua, pudiendo alejarse hasta 3,225 ua y acercarse hasta 2,873 ua. Su excentricidad es 0,05782 y la inclinación orbital 10,1 grados. Emplea 1945 días en completar una órbita alrededor del Sol.

La magnitud absoluta de Sor es 12,3.



</doc>
<doc id="41497" url="https://es.wikipedia.org/wiki?curid=41497" title="(6780) Borodin">
(6780) Borodin

(6780) Borodin es un asteroide que forma parte del cinturón de asteroides y fue descubierto el 2 de marzo de 1990 por Eric Walter Elst desde el Observatorio de La Silla, Chile.

Borodin recibió al principio la designación de .
Más tarde, en 1996, se nombró en honor del compositor y químico Aleksandr Borodín (1833-1887).

Borodin está situado a una distancia media de 2,249 ua del Sol, pudiendo alejarse hasta 2,647 ua y acercarse hasta 1,852 ua. Su inclinación orbital es 4,949 grados y la excentricidad 0,1766. Emplea 1232 días en completar una órbita alrededor del Sol. El movimiento de Borodin sobre el fondo estelar es de 0,2922 grados por día.

La magnitud absoluta de Borodin es 13,9.



</doc>
<doc id="41498" url="https://es.wikipedia.org/wiki?curid=41498" title="(10186) Albéniz">
(10186) Albéniz

(10186) Albéniz es un asteroide perteneciente al cinturón de asteroides descubierto por Eric Walter Elst desde el Observatorio de La Silla, Chile, el 20 de abril de 1996.

Albéniz se designó al principio como .
Más tarde, en 1999, fue nombrado en honor del pianista y compositor español Isaac Albéniz (1860-1909).

Albéniz orbita a una distancia media de 2,444 ua del Sol, pudiendo alejarse hasta 2,944 ua y acercarse hasta 1,944 ua. Tiene una inclinación orbital de 6,196 grados y una excentricidad de 0,2044. Emplea 1396 días en completar una órbita alrededor del Sol. El movimiento de Albéniz sobre el fondo estelar es de 0,258 grados por día.

La magnitud absoluta de Albéniz es 13,9.



</doc>
<doc id="41499" url="https://es.wikipedia.org/wiki?curid=41499" title="Trastornos del espectro autista">
Trastornos del espectro autista

Los trastornos del espectro del autismo o TEA (del inglés "autistic spectrum disorders" o ASD) abarcan un amplio espectro de trastornos que, en su manifestación fenotípica, se caracterizan por deficiencias persistentes en la comunicación social y en la interacción social en diversos contextos, unidas a patrones restrictivos y repetitivos de comportamiento, intereses o actividades. Estos síntomas han de estar presentes en las primeras fases del período de desarrollo de la persona, aunque pueden no manifestarse totalmente hasta que las demandas sociales superan sus limitaciones. También pueden permanecer enmascarados por estrategias aprendidas. La historia del estudio científico del autismo no comienza con la publicación en 1943 del artículo «Autistic disturbances of affective contact», sino en 1925, gracias a las primeras observaciones y análisis de la psiquiatra infantil soviética Grunya Efimovna Sukareva, ofreciendo la primera descripción detallada de la condición, veinte años antes de los trabajos de Asperger o Leo Kanner. («Trastornos autistas del contacto afectivo»)de Leo Kanner (1943), pero sufrirá diversos avatares que retrasarán el avance de la investigación hasta bien entrado el decenio de 1960.

Durante mucho tiempo, el autismo fue considerado un trastorno infantil. Sin embargo, hoy día se sabe que se trata de una condición permanente que acompaña a la persona a lo largo de todo su ciclo vital. Aunque aún no está clarificada su etiología, los trastornos del espectro autista parecen estar causados por la interacción entre una susceptibilidad genética heredable y factores epigenéticos y ambientales que actúan durante la embriogénesis.

Antes de la publicación del DSM-5 en 2013, el llamado «trastorno autista» (referido también como «autismo clásico o kanneriano» o simplemente «autismo») constituía según el DSM IV una subcategoría de los trastornos generalizados del desarrollo, dentro de los cuales se incluía también el trastorno de Rett, el trastorno desintegrativo infantil, el trastorno de Asperger y el trastorno generalizado del desarrollo no especificado. Este último se diagnosticaba cuando no se cumplían la totalidad de los criterios para los demás trastornos. Actualmente, esta clasificación ha cambiado. El DSM V incorpora, de acuerdo con los resultados de investigaciones posteriores, el concepto de «espectro» propuesto por primera vez por Lorna Wing a raíz de un estudio realizado junto con Judith Gould en 1979. En cuanto a la intervención, las terapias que ofrecen un mayor apoyo científico son las cognitivo-conductuales. Estas tienen como finalidad mejorar la calidad de vida de las personas TEA. 

Algunas cuestiones pendientes de investigar en los TEA son sus diversas manifestaciones en la edad adulta y en las mujeres. Se dice que su incidencia es mayor en los hombres, pero se ha sugerido que este dato no es exacto por cuanto las manifestaciones en el sexo femenino son diversas al masculino.

El DSM, en su edición de 1994 (DSM-IV), incluía el Trastorno autista en la categoría Trastornos generalizados del desarrollo, junto a otras 4 subcategorías: Trastorno de Rett, el Trastorno desintegrativo infantil, el Trastorno de Asperger y el Trastorno generalizado del desarrollo no especificado (PDD-NOS, en inglés).

En la actualidad, esta terminología ha desaparecido del DSM. El DSM-5,incluye por primera vez la categoría Trastornos del espectro autista, que sustituye a los anteriores subtipos "trastorno autista", "síndrome de Asperger", y "trastorno generalizado del desarrollo no especificado" por la categoría general “Trastornos del Espectro Autista” (TEA), quedando fuera el síndrome de Rett, pues aunque tiene similitudes en algunos de sus síntomas con el autismo, presenta una etiología genética bien definida. Por su parte, el Trastorno Desintegrativo de la Infancia deja de ser recogido por el DSM-5 ya que tiene importantes problemas de validez.

Antes de la aparición del DSM-5, este sistema de clasificación se centraba más en las clasificaciones o categorías descriptivas que en las necesidades de los pacientes. El nuevo manual, por el contrario, atiende a cuestiones de intervención, y por ello establece tres niveles de necesidad dentro de los TEA: 

De ahí que los síndromes anteriormente considerados como subtipos ahora sean considerados como manifestaciones diversas de un mismo trastorno.

Los tres criterios contemplados en el DSM-IV para el diagnóstico (disfunciones sociales, del lenguaje y comportamientos reiterativos), pasan a ser dos en el DSM-5, que reagrupa los dos primeros en uno solo.

Anteriormente, en el DSM-III (1980) se consideraba una sola categoría, la de "autismo infantil", siendo la primera edición de este manual que incluyó el trastorno; anteriormente, aunque el autismo había sido ya identificado como entidad específica, los comportamientos autistas eran desacertadamente asimilados a la esquizofrenia o, en términos generales, a las psicosis.

El CIE tiene su origen en la «Lista de causas de muerte», cuya primera edición editó el Instituto Internacional de Estadística en 1893. La OMS se hizo cargo de la misma en 1948, en la sexta edición, la primera en incluir también causas de morbilidad. A la fecha, la lista en vigor es la CIE-10 de 1992, y se usa a escala internacional para fines estadísticos relacionados con morbilidad y mortalidad, los sistemas de reintegro y soportes de decisión automática en medicina.

El CIE-10 de 1992, al igual que el DSM IV, incluía el autismo dentro de la categoría F84.Trastornos generalizados del desarrollo, junto a otras subcategorías, de acuerdo con la siguiente clasificación:

En mayo de 2019, la OMS presentó la CIE-11 que entrará en vigor en 2022. En esta edición, se actualizan los criterios de diagnóstico del autismo en la misma línea del DSM-5, recogiendo el término único de «Trastorno del Espectro del Autismo» e incluyendo en esta categoría:
En cuanto a las características nucleares del TEA, la CIE-11 —al igual que el DSM-5— las reduce a dos (dificultades para la interacción y la comunicación social; e intereses restringidos y comportamientos repetitivos; eliminando la anterior relacionada con problemas del lenguaje). Y también añade la importancia de explorar rasgos sensoriales inusuales, lo que se ha constatado como algo común entre las personas con autismo.

Las diferencias más importantes entre ambas clasificaciones son:

La palabra «autismo» fue utilizada por primera vez en 1912 por el psiquiatra suizo Eugene Bleuler, en un artículo publicado en el "American Journal of Insanity", para referirse a uno de los síntomas de la esquizofrenia o "Dementia praecox", un trastorno de muy rara ocurrencia en la infancia. Lo construyó a partir del griego αὐτὀς ("autos") que significa «uno mismo».
En 1938, el médico austríaco Hans Asperger utilizó la terminología de Bleuler usando «autístico» en el sentido moderno para describir en psicología infantil a niños que no compartían con sus pares, no comprendían los términos «cortesía» o «respeto» y presentaban además hábitos y movimientos estereotipados. Denominó el cuadro «psicopatía autística». En 1944 elaboraría un artículo para ahondar en este síndrome; sin embargo, sus trabajos pasaron desapercibidos durante décadas ante la comunidad científica internacional debido a las circunstancias históricas posteriores, ya que publicó en alemán.

El uso médico moderno del término autismo lo encontramos en el estudio de un grupo de tres niñas y ocho niños que llevó a cabo Leo Kanner, otro psiquiatra austriaco, pero que trabajaba en el Hospital Johns Hopkins de Estados Unidos, y que fue publicado en 1943, casi a la par que el de Asperger. Kanner constató que estos niños tenían «una inhabilidad innata para lograr el usual y biológicamente natural contacto afectivo con la gente» e introdujo la caracterización "autismo infantil temprano". Hans Asperger y Leo Kanner son considerados los pioneros del estudio moderno del autismo.

Las interpretaciones del comportamiento de los grupos observados por Kanner y Asperger fueron distintas. Kanner reportó que tres de los once niños no hablaban y los demás no utilizaban las capacidades lingüísticas que poseían. También notó un comportamiento auto-estimulatorio y "extraños" movimientos en aquellos niños. Por su lado, Asperger notó, más bien, sus intereses intensos e inusuales, su repetitividad de rutinas y su apego a ciertos objetos, lo cual era muy diferente al autismo de alto rendimiento, ya que en el experimento de Asperger todos hablaban. Indicó que algunos de estos niños hablaban como "pequeños profesores" acerca de su área de interés, y propuso la teoría de que para tener éxito en las ciencias y en el arte uno debía tener cierto nivel de autismo.

El trabajo de Asperger no fue reconocido hasta 1981, cuando Uta Frith —psicóloga estadounidense de origen alemán— lo redescubre y lo traduce al inglés. Aunque tanto Hans Asperger como Leo Kanner coincidían en muchas de sus apreciaciones, sus diferentes interpretaciones llevaron a Lorna Wing a usar el término síndrome de Asperger diferenciándolo del "autismo de Kanner", aunque señalaba que bien se podía tratar de dos manifestaciones distintas de una misma condición.

Debido en parte a que Leo Kanner echó mano del término autismo para describir la nueva condición que había descubierto, ésta quedó estigmatizada por la sombra de las psicosis, dificultando el avance de las investigaciones hasta bien entrada la década de los 60 en EE. UU. y aún más tarde en otros países.

De hecho, esta fue la interpretación que siguieron las corrientes principales del psicoanálisis, con la particularidad de que se pensaba que estas psicosis tenían un origen psicogenético,es decir, que estaba provocadas por la frialdad de la figura materna a la hora de interaccionar con los hijos en los primeros meses de desarrollo. Ello da lugar a uno de los episodios más oscuros de la historia del autismo y de la psiquiatría en general, pues se perpetuó la práctica de separar a los niños de sus padres y de internarlos en instituciones, además de crear serios sentimientos de culpa en los progenitores.
Estas prácticas llegaron al límite en la década de los 60de la mano del psicoanalista y psicólogo austriaco afincado en Estados Unidos, Bruno Bettelheim, quien creó el término de “madre nevera” y publicó en 1968 un libro titulado ‘‘La fortaleza vacía’’, sugiriendo que detrás de la apariencia de oposición de los niños autistas se escondía un interior muy pobre. Bettelheim adoptó una posición más doctrinaria con respecto al autismo, distanciándose de la búsqueda científica y neurobiológica de Kanner y partiendo de los postulados psicoanalíticos. Trató también de incorporar la epistemología genética de Piaget. Su objetivo era mantener la Escuela Ortogénica de Chicago, donde los niños eran separados de sus madres para emprender una terapias de dudosa efectividad. Posteriormente, se pusieron al descubierto muchas irregularidades e incluso hechos polémicos sobre los métodos utilizados y el trato dispensado a sus pacientes.

El psicólogo estadounidense de origen alemán, Eric Schopler, cuya investigación pionera en el autismo llevó a la fundación del programa TEACCH, horrorizado por las ideas y la práctica de Bettelheim, decidió dedicarse a entrenar a los padres en el proceso educativo tanto como fuera posible. De hecho, sin la participación de las asociaciones de padres y sus reivindicaciones no hubiera sido posible avanzar de modo decidido en la investigación del autismo en los años posteriores.

El psicólogo clínico Ole Ivar Lovaas (Lier, Noruega; 1932 - Lancaster, Estados Unidos 2010) fue considerado como uno de los padres de la terapia para el autismo, denominada análisis de conducta aplicada o ACA, más conocido por sus siglas en inglés como ABA (applied behavior analysis). Sin embargo, fue muy criticado por el uso de técnicas aversivas.

Las aportaciones de Charles Fester y Mirian K. DeMyer desde la perspectiva conductual de la psicología del aprendizaje hacen que a lo largo de los años 60 y posteriormente se vaya abriendo paso la perspectiva educativa en la intervención del autismo, no como métodos de supuesta cura, sino como un modo de mejorar las conductas adaptativas de los afectados.

La psiquiatra Lorna Wing, madre de una hija autista, introduce el concepto de ‘’espectro autista’’, lo que a principios de los años 80 va a significar una auténtica revolución en el modo de entender y afrontar el autismo. Esta idea fue reforzada por el trabajo de Uta Frith, pionera en gran parte de la investigación actual sobre el autismo, y a quien debemos el redescubrimiento de los trabajos de Hans Asperger.
Hoy en día está totalmente desechada la hipótesis de las "madres nevera", y aunque la etiología de los TEA no está del todo clarificada, las investigaciones señalan a una condición neurológica con concurrencia de factores genéticos, epigenéticos y ambientales que actúan durante la embriogénesis.

A partir de 1997, comienzan a publicarse las guías de buena práctica para TEA, que tienen como fin garantizar la calidad científica de las investigaciones, el rigor en el diagnóstico y la ética en las prácticas de intervención en los TEA de acuerdo con los descubrimientos científicos que fueron surgiendo a finales de la década de los 90 y principios del siglo XXI.

Según el DSM-5, el autismo se caracteriza por retraso o alteraciones del funcionamiento antes de los tres años de edad en una o más de las siguientes esferas:
El CIE-11 también agrupa en torno a estos dos rasgos nucleares las disfunciones propias del TEA.

La gravedad está basada en las deficiencias en la comunicación social y en los patrones de conducta restrictivos o repetitivos.

Es muy frecuente también la hipersensibilidad sensorial, lo que conlleva a que los estímulos considerados insignificantes podrían desatar una crisis en personas afectadas con TEA. Así se tiene:

Es importante hacer una diferenciación entre la forma en que se presentan los TEA en mujeres. Un estudio realizado en 2017 por la Journal of the American Academy of Child and Adolescent Psychiatry, ha establecido que la proporción varones/mujeres ya no sería de 4:1, sino 3:1, y esto se debe a que en los últimos años mejoraron las técnicas de detección.

Las mujeres pueden tener mayor capacidad de camuflar síntomas. Se ha evidenciado que las mujeres con TEA suelen presentar:


Cuando se habla de autismo sin otra especificación, nos estamos refiriendo al autismo idiopático, condición incluida en los TEA. En estos casos, no existe ningún marcador biológico.

Pero los síntomas de autismo pueden presentarse también como consecuencia de otra afección de etiología conocida. Es lo que se denomina ‘’autismo secundario’’ o ‘’autismo sindrómico’’.
Para hablar de autismo sindrómico (también se habla de ‘síndromes dobles’) se requiere que la enfermedad asociada con el autismo en esos pacientes haya sido descrita en su origen en otros pacientes no autistas, constituyendo éstos la mayoría de los casos.
En este caso, no se puede establecer un nexo causal entre ambos trastornos. 

Determinar esta falta de relación causal puede resultar difícil en síndromes muy infrecuentes, pero hay algunos en los que ha sido posible, como el síndrome del X frágil, síndrome deleción 22q13, síndrome de Rett, esclerosis tuberosa, fenilcetonuria no tratada, rubéola congénita, síndrome de Prader-Willi o trastorno desintegrativo de la infancia.

Aunque no se reconocen causas específicas de los trastornos del espectro autista, varios factores de riesgo han sido identificados y es posible que contribuyan al desarrollo del TEA.

La evidencia científica sugiere que en la mayoría de los casos el autismo es un desorden heredable. De hecho, es uno de los desórdenes neurológicos con mayor influencia genética. Los resultados de estudios en familias y en gemelos sugieren que los factores genéticos tienen un papel en la etiología del autismo y otros trastornos del desarrollo.

Los estudios en gemelos idénticos han encontrado que si uno de los gemelos es autista, la probabilidad de que el otro también lo sea es de un 60 %, pero de alrededor de 92 % si se considera un espectro más amplio. Incluso hay un estudio que encontró una concordancia de 95,7 % en gemelos idénticos. La probabilidad en el caso de mellizos o hermanos que no son gemelos es de un 2 a 4 % para el autismo clásico y de un 10 a 20 % para un espectro amplio. No se han encontrado diferencias significativas entre los resultados de estudios de mellizos y los de hermanos. Sin embargo, no se ha demostrado que las diferencias genéticas observadas, aunque resultan en una neurología atípica y un comportamiento considerado anormal, sean el origen patológico.

Se han encontrado dos genes relacionados con el autismo que también están relacionados con la epilepsia, el SNC1A causante del síndrome de Dravet y el PCDH19 que provoca el síndrome EFMR también llamado Juberg Hellman. También se han encontrado deleciones de pérdida de PCDH 10 que han sido relacionadas directamente con trastornos del espectro autista.

Recientemente se ha descubierto otro gen más implicado en el desarrollo del autismo y la asociación entre la epilepsia y el autismo, ya se conocen dos genes, en 2001 se encontró el SNC1A, en 2009 se descubrió la relación con el PCDH19 y en abril de 2011 se ha encontrado el SYN1 en una familia canadiense.

Otras investigaciones han descubierto que la hormona oxitocina podría jugar un papel relevante en la aparición del autismo. En el cerebro, la hormona oxitocina parece estar involucrada en el reconocimiento y establecimiento de relaciones sociales y podría estar involucrada en la formación de relaciones de confianza y generosidad.

Evidencias actuales indican que los sistemas fronto-estriado y cerebeloso motor son las principales zonas afectadas en las personas con trastornos del espectro autista, tanto a nivel anatómico como funcional.

La reducción del número de células de Purkinje (neuronas del cerebelo que controlan la función motora, el equilibrio, la marcha y el habla) y la hipoplasia cerebelosa son las alteraciones neurológicas que se han relacionado con los TEA de manera más clara. De hecho, el cerebelo es esencial en el desarrollo de la comunicación, las capacidades motrices, cognitivas y sociales básicas, y las conductas repetitivas/restrictivas, todas ellas alteradas en las personas con estos trastornos.

A pesar de que los estudios sobre gemelos indican que el autismo es sumamente heredable, parecen también indicar que el nivel de funcionamiento de las personas con autismo pueden ser afectados por algún factor ambiental, al menos en una porción de los casos. Una posibilidad es que muchas personas diagnosticadas con autismo en realidad padecen de una condición desconocida parecida al autismo causada por factores ambientales, o sea, una fenocopia. De hecho, algunos investigadores han postulado que no existe el "autismo" en sí, sino una gran cantidad de condiciones desconocidas que se manifiestan de una manera similar.

De todas formas, se han propuesto varios factores ambientales que podrían afectar el desarrollo de una persona genéticamente predispuesta al autismo:
Una serie de complicaciones prenatales y perinatales han sido reportadas como posibles factores de riesgo para autismo. Estas incluyen diabetes gestacional, edad materna y paterna mayor a 30 años, hemorragias luego del tercer trimestre, uso de medicación (ej. valproato) durante el embarazo y meconio en el líquido amniótico. Aunque las investigaciones no son concluyentes respecto de la relación de estos factores con el autismo, cada uno de estos ha sido identificado más frecuentemente en niños con autismo, en comparación con sus hermanos no autistas u otros niños con desarrollo normal.

Hay un buen número de estudios que muestran una correlación importante entre las complicaciones obstétricas y el autismo. Algunos investigadores opinan que esto podría ser indicativo de una predisposición genética nada más. Otra posibilidad es que las complicaciones obstétricas simplemente amplifiquen los síntomas del autismo.

Se sabe que las reacciones al estrés en las personas con autismo son más pronunciadas en ciertos casos. Sin embargo, factores psicogénicos como base de la etiología del autismo casi no se han estudiado desde los años 70, dado los nuevos enfoques que han surgido hacia la investigación de causas genéticas.

La suplementación con ácido fólico ha aumentado considerablemente en las últimas décadas, particularmente por parte de mujeres embarazadas. Se ha postulado que este podría ser un factor de generación de autismo, dado que el ácido fólico afecta la producción de células, incluidas las neuronas. Sin embargo, la comunidad científica todavía no ha tratado este tema.

Existe un intenso debate en la comunidad científica sobre la conexión intestino-cerebro. Varios estudios sugieren una relación entre la sensibilidad al gluten no celíaca y trastornos neuropsiquiátricos, entre los que figura el autismo.

La investigación sobre el efecto de la dieta y la nutrición en el autismo se ha incrementado en las últimas dos décadas, sobre todo en los síntomas de hiperactividad y atención. Se ha planteado la hipótesis de que algunos síntomas de los trastornos del espectro autista pueden ser causados por los péptidos opioides formados a partir de la descomposición incompleta de los alimentos que contienen gluten y caseína, que atraviesan la membrana intestinal debido a un aumento de la permeabilidad intestinal, pasan al torrente sanguíneo y cruzan la barrera hematoencefálica. El exceso resultante de los opioides se cree que conduce a los comportamientos observados en el autismo y que la eliminación de estas sustancias de la dieta podría determinar un cambio en los comportamientos autistas.

El hallazgo de anticuerpos del tipo IgG contra antígenos alimentarios se considera una evidencia indirecta del aumento de la permeabilidad intestinal. Los niños con autismo tienen, en comparación con los controles sanos, niveles significativamente más altos de anticuerpos antigliadina del tipo IgG (pero no del tipo IgA), especialmente aquellos con síntomas gastrointestinales. También se ha reportado un aumento de los anticuerpos dirigidos a varios otros alérgenos alimentarios, incluidos la caseína y la leche entera.

La eficacia de la dieta sin gluten y sin caseína en la mejora de la conducta autista aún no está definitivamente demostrada y son necesarios nuevos estudios a gran escala, aleatorizados y de buena calidad. Los estudios realizados hasta la fecha indican que sólo una parte de niños diagnosticados con trastornos del espectro autista se beneficia de la eliminación del gluten de la dieta. En general, según las observaciones de los padres, la dieta produce una mayor mejora de los comportamientos autistas, los síntomas fisiológicos y las habilidades sociales en los niños con síntomas gastrointestinales, diagnósticos de alergia alimentaria o sospecha de sensibilidad alimentaria; y en aquellos en los que la supresión del gluten y la caseína es estricta, con errores poco frecuentes tanto bajo la supervisión paterna como en el resto de situaciones. Por lo general suele acontecer que podría tratarse de una comorbilidad con enfermedad celíaca o sensibilidad al gluten no celíaca.

En 2019 Dae-Wook Kang "et al." hallaron que la flora intestinal de un grupo de niños autistas era notablemente menos rica en especies de microorganismos que la de niños neurotípicos, y consiguió mejoras en su comportamiento mediante trasplantes de heces.

La afirmación más controvertida respecto a la etiología del autismo fue sobre la vacuna SRP, y en aquellas que han utilizado el componente timerosal como conservante. Originada de un caso de fraude científico, sugería que el autismo resultaba del daño cerebral causado por componentes de la vacuna.No hay evidencia científica convincente que soporte esta hipótesis. Fue propuesta inicialmente por Andrew Wakefield, un excirujano británico e investigador médico.

Andrew Wakefield planteó la existencia de un vínculo entre la vacuna triple viral (contra el sarampión, las parotiditis y la rubéola), el autismo y la enfermedad intestinal en 1998.
Cuatro años después de la publicación del documento, los resultados de otros investigadores aún no habían podido reproducir las conclusiones de Wakefield o confirmar su hipótesis de una relación entre trastornos de la infancia gastrointestinales y el autismo. Una investigación en 2004 realizada por el periodista Brian Deer del "Sunday Times" reveló intereses financieros por parte de Wakefield y la mayoría de sus colaboradores en desacreditar la vacuna. El Consejo Médico General Británico (GMC) realizó una investigación sobre las denuncias de mala conducta contra Wakefield y dos colegas anteriores. En 2010, fue encontrado culpable de fraude y se le prohibió volver a ejercer la medicina.

En 2014 un meta análisis que incluyó a 1.25 millones de niños de diferentes partes del mundo concluyó que ninguno de los componentes de la vacuna llevaban al desarrollo de TEA.

Entre los años 50 y los 70 se creía que los hábitos de los padres eran corresponsables del autismo, en particular, debido a la falta de apego, cariño y atención por parte de madre-padre denominados "madre-padre nevera" ("refrigerator mother-father"). Estas teorías han sido refutadas por investigadores de todo el mundo en las últimas décadas. El grupo de estudio para los trastornos del espectro autista del Instituto de Salud Carlos III del Ministerio de Sanidad califica estas teorías como uno de los mayores errores en la historia de la neuropsiquiatría infantil.

Ciertas investigaciones relacionaron la privación institucional profunda en un orfanato con la aparición de un número desproporcionado de niños con algunos rasgos cuasi-autistas (aunque sin las características fisiológicas). Se postula que este fenómeno es una "fenocopia" del autismo. A diferencia de los niños con autismo, la sintomatología de estos niños, con rasgos provocados por la privación extrema, remite cuando viven en un entorno normalizado. Una privación institucional extrema, por otro lado, puede agravar el grado de afectación de un niño autista y empeorar trágicamente su pronóstico.

Las investigaciones de Spitz y toda la teoría del apego de John Bowlby, basadas en la potencia de salud mental que proporciona el apego al bebé, demuestran hasta qué punto puede beneficiar a los niños autistas un entorno de apego seguro, amor y respeto.

También durante muchos años se llegó a confundir las características de los trastornos del espectro autista con el comportamiento típico de un niño mimado y caprichoso; no obstante, en la actualidad se sabe que tal afirmación es totalmente falsa y por lo tanto esa teoría ha sido totalmente rechazada.

Muchos modelos se han propuesto para explicar qué es o qué causa el comportamiento autista.

Baron-Cohen, Leslie y Frith establecieron la hipótesis de que las personas con autismo no tienen una teoría de la mente, esto es, la capacidad de inferir los estados mentales de otras personas (sus pensamientos, creencias, deseos, intenciones) y de usar esta información para lo que dicen, encontrar sentido a sus comportamientos y predecir lo que harían a continuación.

Esta teoría explicaría la tríada de alteraciones sociales, de comunicación y de imaginación, pero no explica por qué un 20% de niños con autismo supera la tarea, ni tampoco puede explicar otros aspectos como son: repertorio restringido de intereses, deseo obsesivo de invarianza, etcétera.

Sin embargo, hay quienes ponen de manifiesto que, en caso de existir el constructo hipotético de la Teoría de la Mente, no hay datos decisivos para afirmar que los autistas no la poseen.

Esta teoría, propuesta por Uta Frith, sugiere que los niños autistas son buenos para prestar atención a los detalles, pero no para integrar información de una serie de fuentes. Se cree que esta característica puede proveer ventajas en el procesamiento rápido de información, y tal vez se deba a deficiencias en la conectividad de diferentes partes del cerebro.

Formulada por Pennington y Ozonof (1996) y Russel (1997), postula que la causa del autismo radicaría en deficiencias en las funciones ejecutivas. Explica los patrones de comportamiento, intereses y actividades restringidos y estereotipados, pero no da una explicación global del trastorno.

Baron Cohen propone en 2009 que existe una gradación de estilos cognitivos en un continuo cuyos extremos estarían definidos por la capacidad de empatización y la capacidad de sistematizar. Las personas con autismo tendrían un exceso de sistematización, por lo que fracasarían en el intento de establecer relaciones empáticas con otras personas.

A finales de los años noventa, en el laboratorio de la Universidad de California en San Diego se investigó sobre la posible conexión entre autismo y neuronas espejos, una clase recién descubierta de neuronas espejo.

La probada participación de esas neuronas en facultades como la empatía y la percepción de las intenciones ajenas sustenta una hipótesis de que algunos síntomas del autismo obedezcan a una disfunción del sistema neuronal especular. Diversas investigaciones confirman la tesis.

Las neuronas espejo realizan las mismas funciones que parecen estar desarboladas en el autismo. Si el sistema especular interviene de veras en la interpretación de intenciones complejas, una rotura de esos circuitos explicaría el déficit más llamativo del autismo: la carencia de facultades sociales. Los demás signos distintivos de la enfermedad —ausencia de empatía, lenguaje e imitación deficiente, entre otros— coinciden con los que cabría esperar en caso de disfunción de las neuronas espejo.

Las personas afectadas de autismo muestran menoscabada la actividad de sus neuronas espejo en el giro frontal inferior, una parte de la corteza premotora del cerebro; quizás ello explique su incapacidad para captar las intenciones de los demás. Las disfunciones de las neuronas espejo en la ínsula y la corteza cingulada anterior podrían responsabilizarse de síntomas afines, como ausencia de empatía, los déficit en el giro angular darían origen a dificultades en el lenguaje. Los autistas presentan también alteraciones estructurales en el cerebelo y el tronco cerebral.

Para 2010, la tasa de autismo se estimó en alrededor de 1-2 autistas por cada 1000 personas en todo el mundo y ocurrió cuatro a cinco veces más frecuente en niños que en niñas. Para 2014, cerca del 1,5 % de los niños en los Estados Unidos (uno de cada 68) le diagnosticaron TEA, un aumento del 30 % a partir de 1 de cada 88 en 2012. La tasa de autismo entre adultos mayores de 18 años en el Reino Unido es del 1,1 %. El número de personas diagnosticadas ha aumentado dramáticamente desde la década de 1980, debido en parte a cambios en la práctica diagnóstica y los incentivos financieros subsidiados por el gobierno para la identificación diagnóstica; la cuestión de si las tasas reales han aumentado sigue sin resolverse.

El diagnóstico del TEA es con frecuencia un proceso de dos etapas. La primera etapa comprende una evaluación del desarrollo general durante los controles del niño sano con un pediatra. Los niños que muestran algunos problemas de desarrollo se derivan para una evaluación adicional. La segunda etapa comprende una evaluación exhaustiva efectuada por un equipo de médicos y otros profesionales de la salud con un amplio rango de especialidades. En esta etapa, un niño puede recibir un diagnóstico de autismo o de algún otro trastorno del desarrollo.

Mucha gente—inclusive pediatras, médicos de familia, maestros y padres—pueden, al principio, ignorar los signos del TEA, al creer que los niños "alcanzarán" a sus compañeros. Aunque a usted pueda preocuparle pensar que su hijo pequeño tiene el TEA, cuanto más temprano se diagnostique el trastorno, más rápidamente pueden comenzar las intervenciones.

Un control del niño sano debería incluir una prueba para evaluar su desarrollo, con examen de detección específico del TEA a los 18 y 24 meses, como lo recomienda la Academia Americana de Pediatría. Realizar exámenes de detección del TEA no es lo mismo que diagnosticar el TEA. 

A veces, el médico interrogará a los padres acerca de los síntomas del niño a fin de detectar el TEA. Otros instrumentos de detección combinan información de los padres con observaciones del niño realizadas por el médico. Los ejemplos de instrumentos de detección para los infantes y niños en edad preescolar incluyen:

La segunda etapa de diagnóstico debe ser minuciosa a fin de encontrar si otras afecciones pueden ser las causantes de los síntomas del niño. 
Un equipo que incluye un psicólogo, un neurólogo, un psiquiatra, un logopeda u otros profesionales experimentados en el diagnóstico del TEA puede efectuar esta evaluación. La evaluación puede calificar el nivel cognitivo del niño, el nivel de lenguaje, su conducta adaptativa (habilidades adecuadas en relación con la edad necesaria para completar las actividades diarias independientemente, por ejemplo, alimentarse, vestirse y asearse), los niveles de audición, imágenes cerebrales y exámenes genéticos.
La revisión del DSM 5 modifica el enfoque anterior del DSM IV. Se pasó de categorizar a los diferentes trastornos a un enfoque dimensional para diagnosticar los trastornos que se son comprendidos dentro del espectro autista. Así, se considera que todos los trastornos antes mencionados son mejor representados en una sola categoría diagnóstica porque muestran los mismos tipos de síntomas y son diferenciados de mejor manera atendiendo variables clínicas (ej., dimensiones de severidad) y características asociadas (ej., trastornos genéticos, epilepsia, discapacidad intelectual).
Alrededor del 75% de los pacientes diagnosticados de autismo presentan algún grado de retraso mental. Se da la circunstancia de que a medida que aumenta el grado de retraso mental, aumenta también la prevalencia del autismo. Puede decirse que resulta difícil establecer los límites entre el autismo y el retraso mental en aquellos casos en los que la deficiencia mental es muy severa.. También puede ser que el retraso mental conlleve autismo de modo secundario. En cualquier caso, el problema fundamental es la fiabilidad de las pruebas que miden el cociente intelectual (CI) aplicadas a personas con una afección considerable de autismo. Esto hace que sea imposible hacer una determinación exacta y generalizada acerca de las características cognitivas del fenotipo autista. Se sabe que los niños superdotados tienen características que se asemejan a las del autismo, tales como la introversión y la propensión a las alergias. Se ha documentado también el hecho de que los niños autistas, en promedio, tienen una cantidad desproporcionada de familiares cercanos que son ingenieros o científicos. Todo esto se suma a la especulación controvertida de que figuras históricas como Albert Einstein e Isaac Newton, al igual que figuras contemporáneas como Bill Gates, tengan posiblemente síndrome de Asperger. Observaciones de esta naturaleza han llevado a la escritora autista Temple Grandin, entre otros, a especular que ser genio en sí "puede ser una anormalidad".

Los datos, pues, apuntan a que el fenotipo autista es independiente de la inteligencia. Es decir, se pueden encontrar autistas con cualquier nivel de inteligencia. Aquellos con inteligencia por debajo de lo normal serían los que tienden a ser diagnosticados. Aquellos con inteligencia normal o superior suelen escapar al diagnóstico.

El autismo infantil produce alteraciones intelectuales que a menudo son muy difíciles de diferenciar del discapacidad intelectual. Sus principales características son:

Su cociente intelectual suele ser bajo, correlacionándose en forma directa con los defectos lingüísticos. En pruebas psicométricas, el perfil de inteligencia del niño autista (al contrario del menor con retraso mental) con frecuencia muestra:

Los menores que padecen retraso mental suelen exhibir un retraso en el desarrollo lingüístico, pero siguen las mismas etapas del niño normal. El autismo infantil y el retraso mental llegan a estar relacionados y, de hecho, se ha considerado que aproximadamente tres cuartas partes de niños autistas funcionan como adultos con retraso mental.

El DSM-5 introduce una nueva etiqueta diagnóstica dentro de la categoría “deficiencias en el lenguaje”: “los trastornos de la comunicación social”. Los criterios diagnósticos de esta subcategoría solapan en parte con los del TEA; de modo que los niños diagnosticados con un trastorno de la comunicación social tienen una “deficiencia pragmática”, así como un problema de “utilización social de la comunicación verbal y no verbal”. Sin embargo, la presencia adicional de intereses obsesivos y de comportamientos repetitivos excluye la posibilidad de un diagnóstico de trastorno de la comunicación social. Por lo tanto, la presencia de comportamientos repetitivos es esencial en el establecimiento de un diagnóstico diferencial de autismo.

Aunque personas con esquizofrenia pueden mostrar comportamiento similar al autismo, sus síntomas usualmente no aparecen hasta tarde en la adolescencia o temprano en la etapa adulta. La mayoría de las personas con esquizofrenia también tienen alucinaciones y delirios, las cuales no se encuentran en el autismo.

Los objetivos de las intervenciones son disminuir el impacto de los déficits tanto en la vida personal como en la familiar y social, mejorar la calidad de vida y la independencia funcional. Ningún tratamiento se ha establecido como superior y generalmente debe ser adaptado a las necesidades del niño.

Los programas de educación especial, intensiva y sostenida, y las cognitivo-conductuales en etapas tempranas de la vida han mostrado su eficacia para ayudar a adquirir habilidades de cuidado personal, sociales y de trabajo.
Los programas de intervención temprana (de 0 a 6 años de edad) han demostrado su eficacia en la contención o eliminación de síntomas autísticos, en mejoras perceptivas, de atención, cognitivas, comunicativas o de las habilidades sociales. Es necesario, además, que la intervención se lleve a cabo con una perspectiva holística, e incidir de manera interdisciplinaria sobre todos los aspectos que ofrezcan disfunciones, bien sea en la conducta social, en el manejo de la comunicación y del lenguaje o en el comportamiento. Se trata de mejorar la situación del niño con TEA y sus habilidades, pero al mismo tiempo su bienestar, su calidad de vida y la de su familia.

Actualmente se acepta que los niños autistas pueden asistir a escuelas regulares, siempre y cuando cuenten con los apoyos necesarios que den respuesta a sus peculiaridades para poder aprender y desarrollar todas sus potencialidades en la escuela. Cada niño es único con sus fortalezas, gustos y retos, y esto se cumple de igual manera en los niños autistas. Para cada caso, resulta imprescindible diseñar un programa de intervención personalizado, partiendo de las fortalezas del niño, con la participación de los profesionales de la educación, la familia y otros especialistas externos (equipo multidisciplinar). 

Esto no siempre fue así. La propuesta del psicoanálisis, imperante durante muchos años, fue claramente segregacionista. El surgimiento posterior de la educación especial fue un hecho positivo porque significó el reconocimiento de la necesidad de una educación especializada, unos medios específicos y unos profesionales cualificados para los niños con discapacidades, pero pronto se vio que por sí sola no bastaba para dar una solución al problema, dado que tenía efectos colaterales negativos desde el punto de vista social.

Nació así el movimiento de la normalización a partir de los años 70, que buscaba la integración de las personas "deficientes" en la vida social normal. Wolfensberger (1972) definió la normalización como «la utilización de medios, culturalmente lo más normativos posible, en orden a establecer y/o mantener conductas y características personales que son tan culturalmente normativas como es posible». Dicho movimiento también tuvo su parte positiva, ya que buscaba proporcionar a las personas devaluadas socialmente la dignidad completa que les corresponde por derecho propio. Sin embargo, también se acabó revelando como insuficiente, ya que no respetaba la diversidad legítima de los individuos, cayendo muchas veces en la persecución utópica de unos modelos de persona y sociedad «normales», difíciles de definir al margen de ideologías y prejuicios.

Además, la integración escolar no produjo los efectos deseados:
De este modo, surge el nuevo paradigma de intervención unido al concepto de "inclusión" (educativa, laboral y social) de los individuos, basado más en las modificaciones ambientales que en un supuesto modelo de normalidad. Aparecen así planteamientos y programas innovadores como los de escuela inclusiva ("escuela para todos"), el "empleo con apoyo" y la "vida con apoyo", que desembocan en los paradigmas de apoyos y de calidad de vida.

La inclusión respeta la diversidad de características de los individuos y sus necesidades, promoviendo la integración cuando esta es posible, y sin desdeñar los beneficios —a menudo imprescindibles— de la educación especial, poniendo el acento en el desarrollo de potencialidades y en la calidad de vida.

Se plantean ahora nuevos retos, como son la modificación de los sistemas e instituciones educativas para dar cabida a estos planteamientos (asignatura pendiente a día de hoy), así como la sensibilización y creación de conciencia en los colegios y escuelas acerca del autismo y sus variantes (como por ejemplo el síndrome de Asperger) a fin de erradicar problemas como el acoso escolar o "bullying" a causa de la ignorancia, pues está demostrado que los niños y niñas con TEA tienen un riesgo más alto de ser víctimas, y las consecuencias que padecen son más intensas que en el resto de alumnos.

Por un lado, la Tecnología Educativa (TE) es el conjunto de conocimientos, aplicaciones y dispositivos o herramientas que otorgan la posibilidad de aplicar las TIC en el ámbito educativo, concretamente en los procesos de enseñanza y aprendizaje. Gracias a ésta, los docentes pueden diseñar y planificar el proceso de enseñanza y aprendizaje de manera óptima y eficiente.

Por otro lado, las Necesidades Específicas de Apoyo Educativo (NEAE) engloban todas las Necesidades Educativas Especiales (NEE) y Necesidades Educativas que pueda tener el alumnado. Estas últimas se derivan de Dificultades Específicas de Aprendizaje (DEA), del Trastorno por Déficit de Atención con o sin hiperactividad (TDAH), de las Altas Capacidades Intelectuales, de Especiales Condiciones Personales o de Historia Escolar (ECOPHE), o bien por Incorporación Tardía al Sistema Educativo.

La Tecnología Educativa dirigida a las personas afectadas con autismo se le conoce como "tecnología asistente" que sirve principalmente para aprender habilidades sociales y a comunicarse con los demás, marcar rutinas o bien identificar situaciones cotidianas. Para considerarlas útiles estas herramientas han de cumplir los siguientes requisitos:

El pronóstico del autismo es aparentemente impredecible. Algunos niños se desarrollan a niveles en los cuales su autismo no es comúnmente perceptible, sin razón aparente. Otros desarrollan habilidades funcionales después de un tratamiento intenso con terapia ABA. [""]Algunos padres reportan mejorías después de utilizar tratamientos biológicos (no probados). Por otro lado, muchos individuos autistas requieren ser cuidados de por vida y otros nunca desarrollan lenguaje oral. La terapia parece no tener efecto alguno en ciertos casos. Mientras que algunos autistas adultos parecen mejorar en su funcionamiento al pasar el tiempo, otros reportan que se vuelven "más autistas".

La ansiedad y la depresión se presentan con frecuencia en adolescentes y adultos autistas. Se sabe que la respuesta al estrés es más pronunciada en muchos autistas, lo cual podría ser una causa. Pero dados los déficits sociales de los autistas, también es posible que la ansiedad y depresión se deban a instancias de adversidad social.

Las diferencias cerebrales de un individuo realmente autista son tan pronunciadas que es improbable que se puedan eliminar por medio del uso de fármacos o terapia, por intensiva que sea. Aunque su comportamiento externo sea parecido al de las personas no autistas, internamente la persona autista seguirá siendo neurológicamente diferente a los demás.[""]

En una minoría de casos la persona autista puede sacar ventaja de su condición y lograr éxito en su área de interés. Un ejemplo es el caso de la Dra. Temple Grandin, considerada autista desde una temprana edad, quien ahora es Profesora Asociada de la Universidad de Colorado, experta en equipo de manejo de ganado, y autora de varios libros acerca del autismo, incluyendo el popular "Thinking in Pictures" ("Pensando en imágenes").

Para las personas con discapacidad las dimensiones de la calidad de vida son semejantes a las del resto de la población.


Un criterio común para la distinción entre autismo de alto y de bajo funcionamiento es un cociente intelectual de más de 70-80 para aquellos que se dice que son de alto funcionamiento, y de menos de 70-80 para aquellos que se dice que son de bajo funcionamiento. Este criterio tiene varios problemas:




Rimland (1978) encontró que el 10% de los autistas tienen "talentos extraordinarios" en campos específicos (comparado con un 0,5% de la población general). Brown y Pilvang (2000) han propuesto el concepto del "niño que esconde conocimiento" y han demostrado por medio de cambios en las pruebas de inteligencia que los niños autistas tienen un potencial que se esconde detrás de su comportamiento. Argumentan también que la falta de optimismo que promueve gran parte de la literatura científica sobre el tema puede empeorar la situación del individuo autista. Dawson (2005), una investigadora autista, ha realizado comparaciones cognitivas entre individuos autistas y no autistas; encontró que su rendimiento relativo en las pruebas de Wechsler y RPM son inversos. Un estudio de la Universidad Estatal de Ohio encontró que los autistas tienen mejor rendimiento en pruebas de memoria falsa. Happe (2001) hizo pruebas a hermanos y padres de niños autistas y propuso que el autismo puede incluir un "estilo cognitivo" (coherencia central débil) que confiere ventajas en el procesamiento de información.

Las experiencias sensoriales están presentes en nuestras ocupaciones y actividades diarias. En algunas poblaciones estas experiencias resultan problemáticas y afectan la participación en diferentes acciones, tareas, actividades y ocupaciones. De este modo, los problemas sensoriales en los individuos con diagnóstico de autismo están ampliamente reportados en la literatura.Las dificultades sensoriales en individuos con autismo han sido confirmadas por diferentes tipos de estudios incluyendo cuestionarios, estudios neurofisiológicos y reportes biográficos proporcionados por personas con diagnóstico de autismo.Aunque esto parece una novedad fue Kanner quien en 1943 originalmente describió no solo la fascinación y el placer que los niños con diagnóstico de autismo experimentan en relación con ciertos estímulos sensoriales como el reflejo de la luz en los espejos sino también signos asociados a estrés; como por ejemplo el cubrirse los oídos en presencia de ciertos sonidos.

Anna Jean Ayres, PhD, OTR, (January 18, 1920 – December 16, 1988) fue la Terapeuta Ocupacional que desarrolló la teoría e intervención en Integración Sensorial en los EE. UU. Los problemas sensoriales más comúnmente reportados en individuos con diagnóstico de autismo incluyen hiporrespuesta, hiperrespuesta, patrón de respuesta mixto y problemas de praxis debidos a déficits en el procesamiento e integración de diferentes sensaciones. Sin embargo, otras caracterizaciones también han sido reportadas. Por ejemplo, Baranek y sus colaboradores reportaron patrones de hiporresponsividad (hipo), hiperresponsividad (hiper), intereses, repeticiones y comportamientos de búsqueda (IRCB) y percepción superior. Las evaluaciones especializadas en integración sensorial son llevadas a cabo por Terapeutas Ocupacionales con educación de post graduación en este abordaje en más de 22 países en el mundo. Una formación internacional en esta teoría y marco de intervención ha sido desarrollada por instructores por medio de la University of Southern California (USC). Criterios de formación específica han sido establecidos y se implementan en varios países del mundo.

El DSM 5 ha reconocido la existencia de los problemas sensoriales como parte del diagnóstico de autismo. Esto ha causado un renovado interés en Integración Sensorial dentro de Terapia Ocupacional. Terapia Ocupacional con Integración Sensorial (OT/SI) ha demostrado evidencia científica de efectividad y eficacia en pruebas controladas aleatorizadas, el nivel más alto de evidencia científica. Mejorías en varias áreas incluyendo aspectos motores, del comportamiento, lenguaje, juego y actividades de la vida diaria así como también objetivos específicos establecidos con los familiares han resultado positivos cuando la intervención está implementada por Terapeutas Ocupacionales con formación adecuada en Integración Sensorial.

Aunque no haya sido incluido en el DSM-5, actualmente se diagnostica en Reino Unido un síndrome conocido como síndrome de evitación patológica de la demanda (PDA), al que se considera una condición perteneciente al espectro autista.
El "fenotipo autista ampliado" (broader autism phenotype o BAP) hace referencia a personas que, sin presentar un cuadro clínico de TEA, muestran rasgos propios del autismo tales como evitar el contacto vistual

Hay grupos que defienden que el autismo no es una enfermedad o desorden en sí, sino una forma de ser; es decir, una neurología atípica que merece respeto, creando una serie de términos para contrastar con los términos en uso común por el público (eufemismos), por ejemplo, "neurotípico" en lugar de "normal", "neurodiversidad" en lugar de enfermedad o desorden, y "neurodivergente" en lugar de "anormal".

Este movimiento ha sido criticado fuertemente por algunos padres de niños autistas, principalmente Kit Weintraub y Lenny Schaffer, quienes han acusado a los activistas autistas de no ser autistas en realidad, entre otras cosas. Algunos padres de niños autistas sí lo apoyan. Algunas de sus reivindicaciones son:


El Día del Orgullo Autista se celebra cada 18 de junio desde 2005.

Hay otros grupos que defienden que si es una enfermedad como otros autistas o científicos.

En España, el 18 de noviembre de 2014, con el respaldo de todos los grupos parlamentarios, se aprobó una proposición no de ley en la que se instaba al Gobierno a estudiar, en el ámbito de sus competencias, la elaboración y desarrollo de una Estrategia Española en Trastornos del Espectro del Autismo.





</doc>
<doc id="41500" url="https://es.wikipedia.org/wiki?curid=41500" title="Epidemia">
Epidemia

Epidemia (del griego "epi," por sobre y "demos", pueblo) es una descripción en la salud comunitaria que ocurre cuando una enfermedad infecta a un número de individuos superior al esperado en una población durante un tiempo determinado. Para evitar el sensacionalismo que conlleva esta palabra, en ocasiones se utiliza el sinónimo de el brote epidémico o brote.

En la actualidad el concepto es una relación entre una línea de base de una enfermedad, que puede ser la prevalencia o incidencia normales, y el número de casos que se detectan en un momento dado. En este sentido, si una comunidad se encuentra libre de una determinada enfermedad, un solo caso constituye una epidemia. En otras palabras, es un incremento significativamente elevado en el número de casos de una enfermedad con respecto al número de casos esperados.

En el caso de que la epidemia se difundiera por varias regiones geográficas extensas de varios continentes o incluso de todo el mundo, se trataría de una pandemia. En caso de enfermedades que afectan en una zona mantenida en el tiempo, se trataría de una endemia.

El ritmo reproductivo básico en el modelo SIR simple es

donde formula_1 es la tasa de infección (en tanto por uno) y formula_2 coincide con la extensión del período infeccioso), y la condición para el desencadenamiento de una epidemia es:
Aunque por su etimología griega ("demos" significa pueblo) la palabra se aplica originalmente a las enfermedades que afectan a los humanos, también suele emplearse en el lenguaje cotidiano para hablar de las enfermedades que afectan a grupos de animales (zoonosis).

De hecho, el término adecuado, epizootia, es menos conocido, y es común que para fenómenos médicos idénticos entre animales y humanos sean designados por las mismas palabras en el lenguaje cotidiano.

El uso del término en el lenguaje cotidiano tiende asimismo a ignorar o confundir a la incidencia y la prevalencia de la enfermedad. Así, por ejemplo, el calificativo de epidemia es a menudo erróneamente limitado exclusivamente a los casos en que la prevalencia es importante, cuando hay "numerosos individuos enfermos" sin tener en cuenta su número inicial, haciendo caso omiso de la incidencia normal. El término también es utilizado de la misma manera por la Organización Mundial de la Salud (OMS) para otros fenómenos diferentes a las enfermedades infecciosas, tales como el rápido aumento de la obesidad en el planeta.

Una endemia es la presencia habitual de una enfermedad en una zona geográfica (la prevalencia positiva incidencia (estable) puede convertirse en una epidemia si las condiciones ambientales lo permiten).

Posteriormente:

Las epidemias frecuentemente ocurren en ciclos u oleadas con una fase de auge rápido y una abrupta caída, un cierto período de actividad baja y de nuevo un auge y caída. La gripe es un ejemplo perfecto de una epidemia cíclica, con un ciclo anual o estacional. Las epidemias pueden desarrollarse en una o más olas, como fue el caso de la gripe española en 1918-1919. Los modelos epidemiológicos han demostrado que, bajo ciertas condiciones razonables, existen soluciones oscilantes, lo que explica que las epidemias ocurran en ciclos. Por ejemplo, el virus de la gripe común presenta tasas de infección diferentes en invierno y en verano. Esa variabilidad estacional de la tasa de infección da lugar a un comportamiento cíclico (si la tasa de infección fuera constante, los modelos más simples llevan a la conclusión de que el número de infectados sería una tasa constante).

Una epidemia también puede surgir sin una endemia previa, por ejemplo, a raíz de un accidente que provoca la liberación de un vector patógeno en un entorno donde anteriormente era inexistente (prevalencia e incidencia inicialmente cero). En tales circunstancias, solo algunos casos son suficientes para causar un aumento muy significativo en la incidencia de la enfermedad y darle el carácter de una epidemia.

Mark Bartholomew, investigador de la CEA, y un equipo mixto de la CEA-CNRS-Universidad de Indiana que ha modelado la propagación de las epidemias a partir de bases de datos de la IATA, concluyó en 2008 que «el avión es el factor clave de la propagación (de las epidemias) en todo el mundo (..) Las líneas en las que hay grandes flujos de pasajeros, crean caminos preferentes para la enfermedad. El SARS llegó a Francia y Canadá con vuelos procedentes de Hong Kong.» Por lo tanto, él cree que «incluso si se redujera en un 90 % el tráfico aéreo - que parece difícil de alcanzarse -, esto limitaría muy poco el número de infecciones».

En los animales, las epidemias se propagan principalmente por los animales migratorios (véase el ejemplo de la gripe aviar).

En el siglo XXI, las redes de médicos generales o médicos de los hospitales, los farmacéuticos y/o ciudades llamadas "centinelas" realizan un seguimiento epidemiológico sobre la base de protocolos estandarizados, a nivel local, regional, nacional y continental o incluso mundial en caso de enfermedades como la gripe.

Los expertos creen que las enfermedades emergentes, sobre todo las de origen animal, serán cada vez más importantes con el crecimiento de la población, el hacinamiento urbano, los cambios climáticos, el aumento del transporte de mercancías y de personas, el incremento de los factores mutagénicos y el contacto con nuevos gérmenes.

Un umbral epidemiológico, que corresponde a un número mínimo de enfermos en el momento, se establece para las principales enfermedades, a fin de comparar las tendencias epidemiológicas entre ciudades, regiones, países o continentes en diferentes momentos.

Debajo de este umbral, no se habla de una epidemia. Por encima, las autoridades sanitarias pueden adoptar o solicitar medidas de prevención y de precaución. El número de pacientes en ese tiempo describe, por lo general, una curva con forma de campana.

La epidemia corresponde al crecimiento de una enfermedad endémica o la aparición de un gran número de pacientes donde la enfermedad estaba ausente. Puede también reflejar la identificación de la mutación de un patógeno que hace visible la gravedad de los síntomas de una enfermedad que antes era asintomática.

Los investigadores están tratando de anticipar las epidemias para conseguir una mejor lucha contra ellas. Para ello están procurando desarrollar y validar modelos matemáticos.

Parece que las conurbaciones y la promiscuidad alteran la ecología de los patógenos, lo que les permite estar activos todo el año, aunque de manera discreta (se habla entonces de «ruido de fondo»).

Entre los medios de lucha contra las epidemias figuran:

Las menciones históricas de plagas y epidemias han sido frecuentes en toda la historia. El "libro del Éxodo" describe las plagas que Moisés hizo caer sobre Egipto, y muchas otras menciones bíblicas hablan de brotes epidémicos. Por ejemplo, la Biblia menciona la decisión de Senaquerib, rey de Asiria, de abandonar su intento de conquistar Jerusalén en 700 a. C. debido a que sus soldados estaban enfermos (Isaías 37, 36-38).

Los cronistas antiguos y medievales usan el término "plaga" para hablar de dichos episodios, debido a la creencia antigua de que las epidemias se debían a un castigo divino por una conducta pecaminosa. Con la aparición del sida, diversos grupos llegaron a difundir la idea de que dicha enfermedad era un castigo por una conducta inmoral. Esas visiones han supuesto históricamente un obstáculo para la prevención y el control de las epidemias.

Otras referencias históricas dejan claro el impacto histórico de brotes epidémicos y su efecto en el curso histórico de los acontecimientos. La caída de imperios ha sido atribuida directa o indirectamente a enfermedades epidémicas. En el siglo II d. C. las llamadas "plagas del reinado de Antonino" (posiblemente sarampión y viruela) se expandieron por el Imperio romano, causando estragos y una importante reducción de la población, así como dificultades económicas. Esa situación habría facilitado la intensificación de ataques de los bárbaros y finalmente el debilitamiento del imperio. El Imperio Han en China colapsó en el siglo III después de una serie de eventos similares a los registrados en el Imperio romano. La derrota a manos de Hernán Cortés de una población numéricamente muy superior del Imperio azteca, por parte de unos pocos europeos apoyados por sus aliados indígenas, estuvo influida por los efectos desastrosos que los gérmenes de origen europeo tuvieron sobre las poblaciones americanas. La viruela no solo se expandió por Mesoamérica, sino que al parecer se propagó más al sur, y pudo ser un factor importante el debilitamiento del Imperio incaico subyugado por Francisco Pizarro unos años más tarde. En México se estima que la población entre 1519 y 1530 se redujo de unos 20-30 millones a 2-3 millones.

En Europa, la peste negra procedente de Asia mató a millones de personas desde su aparición en 1346 hasta su erradiciación definitiva. Entre 1346 y 1350 murió entre un cuarto y un tercio de la población europea por culpa de la peste negra. Tras ese brote inicial, hubo brotes recurrentes durante más de 300 años. Uno de los brotes más notables fue la gran peste de Londres (1665-66) o la plaga italiana (1629-1631). Como la plaga golpeó unas regiones pero no otras, los diversos brotes de peste tuvieron un efecto notorio en el desarrollo político y económico de Europa desde la Edad Media. En la última plaga de peste bubónica en Francia (1720-22), falleció la mitad de la población de Marsella, un 60 % de la de Toulon, el 44 % de la de Arlés y un 30 % de la de Aix-en-Provence y Aviñón, aunque el brote no se había expandido fuera de la Provenza.

El historiador W. H. McNeill argumenta, en su "Plagues and Peoples", que la propagación de enfermedades infecciosas ha sido un factor importante en el desarrollo histórico. Por ejemplo, hubo un marcado incremento de la población durante el siglo XVIII, la población de China pasó de 150 millones en 1716 a 313 millones en 1794 y la población de Europa creció de 118 millones en 1700 a 187 millones en 1800. Si bien hubo muchos factores implicados en dicho incremento, incluyendo cambios en la edad de matrimonio y mejoras tecnológicas que llevaron a un aumento de los suministros, esos factores no serían importantes para explicar por sí mismos tales incrementos. Los estudios demográficos indican que una explicación satisfactoria requiere el reconocimiento de una disminución de la mortalidad debida a episodios epidémicos.




</doc>
<doc id="41501" url="https://es.wikipedia.org/wiki?curid=41501" title="Pandemia">
Pandemia

Una pandemia (del griego "πανδημία", de παν, "pan", ‘todo’, y δήμος, "demos", ‘pueblo’, expresión que significa ‘reunión de todo un pueblo’) es la afectación de una enfermedad infecciosa de los humanos a lo largo de un área geográficamente extensa.

Ha habido un número importante de pandemias en la historia humana, todas ellas generalmente zoonosis, que han llegado con la domesticación de animales, tales como la viruela, difteria, gripe y tuberculosis. Ha habido un número de epidemias particularmente importantes que merecen una mención por encima de la «mera» destrucción de ciudades:



Hay también varias enfermedades desconocidas que fueron extremamente graves pero que ahora se han desvanecido, de manera que su etiología no puede determinarse. Por ejemplo, la peste antes mencionada de Grecia en 430 a. C. y el "sudor inglés" de la Inglaterra del siglo XVI, que fulminaba a la gente en un instante y que fue mucho más temido que la peste bubónica.

La OMS indica que, para que pueda aparecer una pandemia, se necesita:




</doc>
<doc id="41502" url="https://es.wikipedia.org/wiki?curid=41502" title="Mayalde">
Mayalde

Mayalde es un municipio y localidad española de la provincia de Zamora, en la comunidad autónoma de Castilla y León.

El pleno del ayuntamiento de Mayalde, en sesión ordinaria celebrada el 24 de marzo de 1999, aprobó por unanimidad el escudo heráldico y la bandera municipal con la siguiente definición:

Vocablo procede de mayal —y este de mallal, mallar con significado de golpear—. El mayal se refiere a dos utensilios de uso habitual en las tareas agrícolas. Uno es el instrumento compuesto de dos palos, uno más largo que otro, unidos por medio de una cuerda, con el cual se desgrana el centeno dando golpes sobre él. El otro es el palo del cual tira la caballería que mueve los molinos de aceite, tahonas y malacates.

Se encuentra situada en la "calzada de El Cubo del Vino" (ZA-301), a más o menos medio camino entre El Cubo de la Tierra del Vino y Peñausende. Su término de unos 44 km², se encuentra situado a una altitud de 875 m.

La ubicación en alto de esta localidad y su topónimo de origen germánico, o de origen indoeuropeo precéltico según otros, sustentan la hipótesis de la antigüedad de este asentamiento y en cualquier caso su existencia previa a los poderes feudales. En cualquier caso, Mayalde fue una de las localidades que la monarquía leonesa integró en el contexto de las repoblaciones que llevaron a cabo durante la Edad Media. Con posterioridad se constata la primera mención documental a Mayalde, fechada en el año 1143.

Hasta la fecha no se conoce texto foral de esta localidad, sin embargo es muy factible su existencia debido a su antiguo estatus señorial, propios de una población constituida en vasallaje, con un territorio propio y legislación diferenciada y estable, de la que ha llegado noticias sin más garantía que las propias de un memorial sin fecha y de claro rango administrativo. A mediados del siglo XIII la diócesis zamorana puso en práctica la ordenación contable de todo su patrimonio territorial, para lo cual efectuó resúmenes o breves relaciones que fueron codificados en su archivo, Tumbos Negro y Blanco, como buena práctica para la gestión y dirección de sus intereses económicos.

De entre los asientos derivados de esta nueva práctica, destaca el referente a esta localidad bajo la rúbrica de ""Estos son los foros que fazen los de Mayalde al obispo"", estableciendo a continuación que cada colono daba ""una ochaba de cebada et III dineros"", mas ""VII morabetinos de servitioet martiniega"", que ""de la eglisia lieva el obispo la tercia"" y que también ""dan ela iantar"". El texto transcrito muestra una clara relación de señorío-vasallaje que estuvo vigente en esta localidad a pesar de no existir un documento en el que se contenga su carta constitutiva.

Existe una posterior cita en el Tumbo Blanco, en la que se recoge la ""Postura que hizo el obispo con el concejo de Mayalde"", recogiendo al efecto una breve ordenanza sobre el régimen del ganado de holganza y la potestad del alcalde y del casero del obispo de imponer y hacer efectiva la multa de cinco maravedíes por las infracciones tasadas. El acuerdo contempla las siguientes tres reglas:
Ya en la Edad Contemporánea, la división provincial de 1833 encuadró a Mayalde en la provincia de Zamora y la Región Leonesa, la cual, como todas las regiones españolas de la época, carecía de competencias administrativas. Un año después Mayalde fue adscrita al partido judicial de Fuentesaúco, en vez de integrarla al partido judicial de Bermillo de Sayago, como ocurrió con el resto de localidades sayaguesas. Tras la constitución de 1978, Mayalde pasó a formar parte en 1983 de la comunidad autónoma de Castilla y León, en tanto municipio adscrito a la provincia de Zamora. En 1983, tras la supresión del partido judicial de Fuentesaúco, fue integrada en el actual Partido Judicial de Zamora.


Las más importantes se celebran en San Roque, el 16 de agosto, con presencia de emigrantes y un amplio programa en el que predominan los espectáculos de toros junto a conciertos musicales, juegos infantiles, trofeos deportivos y todo tipo de actividades lúdicas.

En marzo se celebran las patronales en honor a San Benito, con procesión y actos religiosos a los que sigue un refresco popular y baile. En febrero las águedas exhiben sus trajes y celebran actividades durante tres días, y los quintos también mantienen la tradición de plantar el mayo.


</doc>
<doc id="41503" url="https://es.wikipedia.org/wiki?curid=41503" title="Helicobacter pylori">
Helicobacter pylori

Helicobacter pylori es una bacteria gramnegativa con forma de bacilo helicoidal (de esta característica morfológica deriva el nombre de la "Helicobacter") que habita en el epitelio gástrico humano. La infección por "H. pylori" puede producir inflamación de la mucosa gástrica que puede progresar llevando a la producción de gastritis, úlcera péptica y linfoma de tejido linfoide asociado a mucosa (MALT). No obstante, los individuos infectados pueden no llegar nunca a presentar ningún tipo de síntoma. Esta bacteria vive exclusivamente en el estómago humano. Por su forma de espiral puede «atornillarse» literalmente por sí misma para colonizar el epitelio gástrico, además de contar con la ureasa que le permite neutralizar su entorno ácido.

La bacteria fue llamada inicialmente Campylobacter pyloridis, después Campylobacter pylori (al corregirse la gramática latina) y en 1989, después de secuenciar su ADN, se vio que no pertenecía al género "Campylobacter", y se la reemplazó dentro del género "Helicobacter". El nombre "pylori" viene del latín "pylorus", que significa ‘guardabarrera’, y hace referencia al píloro (la apertura circular del estómago que conduce al duodeno).

En 1875, científicos alemanes descubrieron bacterias espirales en el epitelio del estómago humano. Estas bacterias no podían ser cultivadas, y por consiguiente este descubrimiento se olvidó en aquel momento. En 1892, el investigador italiano Giulio Bizzozero describió una serie de bacterias espirales que vivían en el ambiente ácido del estómago de perros.

El profesor Walery Jaworski, de la Universidad Jaguelónica en Cracovia, investigó sedimentos de lavados gástricos obtenidos de humanos en 1899. Además de unas bacterias alargadas, también encontró bacterias con una característica forma espiral, a las cuales llamó "Vibrio rugula". Este investigador fue el primero en sugerir la participación de este microorganismo en enfermedades gástricas. Aunque este trabajo fue incluido en el "Manual de enfermedades gástricas", no tuvo mucho impacto, debido a que estaba escrito en polaco.

Esta bacteria fue redescubierta en 1979 por el patólogo australiano Robin Warren, quien en investigaciones posteriores (a partir de 1981), junto a Barry Marshall, aisló este microorganismo de las mucosas de estómagos humanos y fue el primero que consiguió cultivarla. En el trabajo original, Warren y Marshall afirmaron que muchas de las úlceras gástricas y gastritis eran causadas por la colonización del estómago por esta bacteria, y no por estrés o comida picante, como se sostenía hasta entonces.

La comunidad médica reconoció muy recientemente el hecho de que esta bacteria fuese la causante tanto de úlceras gástricas como de gastritis, ya que se creía que las bacterias no podían sobrevivir por mucho tiempo en el medio ácido del estómago. La comunidad empezó a cambiar de idea basándose en estudios posteriores que reafirmaron esta idea, incluyendo uno en el que Marshall bebió un cultivo de "H. pylori", desarrollando una gastritis y recobrando la bacteria de su propio revestimiento gástrico; con esto, satisfizo 3 de los cinco postulados de Koch. La gastritis de Marshall se curó gracias a un tratamiento que ellos mismos desarrollaron, a base de bismuto con antibióticos.

Posteriormente, Marshall y Warren descubrieron que los antibióticos eran efectivos para el tratamiento de la gastritis. En 1994, los Institutos Nacionales de Salud de los Estados Unidos informaron de que las úlceras gástricas más comunes eran causadas por "H. pylori", y recomendaron el uso de antibióticos, siendo incluidos en el régimen de tratamiento. En 2005, Warren y Marshall fueron galardonados con el Premio Nobel de Medicina por sus trabajos acerca de "H. pylori".

Antes de comprobarse la implicación de "H. pylori" en enfermedades estomacales, las úlceras gástricas eran tratadas con medicamentos que neutralizaban la acidez. Esto hacía muchas veces que las úlceras reaparecieran después de dejar el tratamiento. La medicación tradicional frente a la gastritis eran las sales de bismuto (subcitrato de bismuto coloidal o subsalicilato de bismuto). Este tratamiento a menudo era efectivo, pero su efectividad disminuía con un uso prolongado, además de desconocerse el mecanismo de acción de este fármaco. Todavía no está claro si el bismuto puede actuar como antibiótico. Desde 2006 muchas úlceras son tratadas de forma efectiva utilizando antibióticos frente a "H. pylori".

Mientras "H. pylori" sigue siendo la bacteria más importante conocida que habita en el estómago humano, algunas especies más del género "Helicobacter" han sido identificadas ahora en otros mamíferos y en algunas aves. Se ha comprobado que algunas de estas pueden infectar a humanos. Existen especies de "Helicobacter" que son capaces de infectar el hígado de ciertos mamíferos, causando, por tanto, diversas enfermedades hepáticas.

Cada población humana posee una distribución característica de cepas de "H. pylori" que típicamente infectan a miembros de su población. Esta característica ha llevado a los investigadores a usar "H. pylori" para estudiar los patrones de migración en humanos. Así pues las variedades encontradas estarían relacionadas con el origen de los humanos modernos y sus migraciones prehistóricas. 

A medida que los grupos humanos originados en África se extendieron por el planeta, disgregándose y diversificando gradualmente sus características genéticas, las poblaciones de "H. pylori" que llevaban consigo hicieron lo mismo. Así pues la cepa que infecta a los habitantes de Europa, por ejemplo, es la hpEurope, mientras que las cepas hpAsia2 y hpEastasia están en la población asiática. En Oceanía hay dos tipos principales: hpSahul que se originó hace entre 31 000 y 37 000 años en el continente Sahul y es típico de nativos australianos y papúes, y la cepa hspMaori relacionada con la cepa hpEastAsia y que al parecer salió de Taiwán hace unos 5000 años y se extendió con los pueblos malayo-polinesios. 

Como se ha mencionado, "H. pylori" se encuentra presente en la mitad de la población mundial y ha sido posible relacionarla con las migraciones humanas y su distribución global, identificando las siguientes cepas: 

Se puede establecer que las cepas de "H. pylori" en nativos amazónicos tienen su origen en el este de Asia, más que en Europa, lo que sugiere que estas poblaciones llegaron a América al menos hace 11 000 años.

Un estudio realizado por expertos del Departamento de Microbiología y Parasitología de la Facultad de Medicina de la UNAM (Universidad Nacional Autónoma de México) liderados por Gonzalo Castillo, y publicado en BMC Microbiology e impulsado por el INAH (Instituto Nacional de Antropología e Historia de México), sugiere la presencia de H. pylori en las sociedades prehispánicas. Durante la investigación se usaron muestras del tejido estomacal de 6 momias (5 procedentes de la cueva La Ventana en Chihuahua y otra de Durango, México) que datan de 1350.

"H. pylori" es una bacteria Gram negativa de forma espiral, o espirilo, de alrededor de 3 micras de largo y con un diámetro aproximado de unas 0,5 micras. Tiene unos 4–6 flagelos. Es microaerófila, es decir, requiere oxígeno pero a más bajas concentraciones de las encontradas en la atmósfera. Usa hidrógeno y metanogénesis como fuente de energía. Además es oxidasa y catalasa positiva.
Con su flagelo y su forma espiral, la bacteria "taladra" literalmente la capa de mucus del estómago, y después puede quedarse suspendida en la mucosa gástrica o adherirse a células epiteliales ya que produce adhesinas, (que son proteínas fijantes). "H. pylori" produce una enzima llamada "ureasa" que transforma la urea en amoniaco y en dióxido de carbono mediante la reacción:
CO(NH) ---> 2NH + CO, y es el amoniaco que va neutralizar parcialmente la acidez gástrica (que sirve para disolver los alimentos y matar la mayor parte de bacterias digeridas). Lamentablemente el amoniaco es tóxico y va a maltratar la superficie de las células epiteliales y provocar el proceso de formación de las úlceras.

La infección por "H. pylori" puede ser sintomática o asintomática (sin efectos visibles en el enfermo); se estima que más del 70 % de las infecciones son asintomáticas. En ausencia de un tratamiento basado en antibióticos, una infección por "H. pylori" persiste aparentemente durante toda la vida. El sistema inmunitario humano es incapaz de erradicarla.

La bacteria ha sido aislada de las heces, de la saliva y de la placa dental de los pacientes infectados, lo cual sugiere una ruta oral-oral o fecal-oral como posible vía de transmisión.

Se estima que más de dos tercios de la población mundial se encuentra infectada por esta bacteria. La proporción de infección varía de nación a nación. En el mundo occidental (Europa, América y Australia) la proporción es de alrededor de un 25 por ciento de la población, siendo mucho mayor en el tercer mundo. En este último caso es común encontrar infecciones en niños, probablemente por las malas condiciones sanitarias. En los Estados Unidos la infección se da principalmente en personas de edad avanzada (más del 50 por ciento de estas ocurren en personas de más de 60 años, frente a un 20 por ciento que se presentan en personas de menos de 40) y en los sectores más pobres.

Estas discrepancias se atribuyen a una mayor higiene y al mayor uso de antibióticos en los países más ricos. De cualquier forma, en los últimos años están apareciendo cepas de "H. pylori" que presentan resistencia a antibióticos. En el Reino Unido hay incluso cepas resistentes a metronidazol.

Existen diferentes métodos para diagnosticar una infección de "H. pylori". Uno es detectando anticuerpos específicos en una muestra de sangre del paciente o de heces, utilizando antígenos. También se utiliza la prueba del aliento con urea, en la cual el paciente bebe urea marcada con C o C, produciéndose posteriormente (debido al metabolismo de la bacteria) dióxido de carbono marcado, el cual es detectado en la respiración. Otro método de diagnóstico es la biopsia, en la cual se mide la ureasa activa en la muestra extraída, el denominado "test rápido de la ureasa", el cual consiste en colocar una muestra de tejido en un medio con urea. Si la enzima ureasa producida por la bacteria está presente, descompone la urea del medio en amoníaco y CO2. El amoniaco generado provoca una elevación del pH que se evidencia a través de un cambio de coloración detectable a simple vista. Otra forma de diagnosticar una infección de "H. pylori" es por medio de una muestra histológica o de un cultivo celular. Uno de los métodos de detección más sensibles corresponde a la PCR (reacción en cadena de la polimerasa), la cual permite también identificar genes asociados a virulencia (CagA y VacA), genes asociados a adhesión (BabA) y genes de resistencia a antibióticos (Claritromicina).

Ninguno de estos métodos es completamente infalible. La prueba de anticuerpos sanguíneos, por ejemplo, tiene tan sólo entre un 76 y un 84 por ciento de sensibilidad. La medicación, por otro lado, puede afectar a la actividad de la ureasa y dar falsos positivos en los métodos basados en ella.

∗Sensibilidad de endoscópicas y no endoscópicas, todas las pruebas que identifican la infección activa por "H. pylori" es reducido por el uso reciente de PPI, bismuto o antibióticos; PPI = inhibidor de la bomba de protones; PPV = valor predictivo positivo; NPV = valor predictivo negativo; UBT = test de aliento de urea.

Un estudio publicado en 2008 en el "Diario Coreano de Microbiología y Biotecnología" reveló que el "kimchi" (platos tradicionales coreanos a base de verduras fermentadas) contiene una cepa de un tipo de bacteria "que muestra una fuerte actividad antagonista contra la "H. pylori"." Se demostró que la variedad aislada de la bacteria del "kimchi", designada "Lb. plantarum NO1", reduce la actividad de la ureasa de la "H. pylori" en un 40-60 % y anula el desarrollo de células cancerígenas en más de un 33 %.
Un estudio de 2009 desveló que el té verde puede prevenir la inflamación producida por la "H. pylori".

Inicialmente se utilizaba metronidazol, el cual presenta actualmente resistencia en más del 80 % de los casos. Posteriormente se trataba solo cuando se presentaba infección sintomática, en cuyo caso se utilizaba claritromicina, amoxicilina y tetraciclina. En la actualidad la terapia triple que consiste de un inhibidor de bomba de protones (IBP) (omeprazol, lansoprazol, pantoprazol, rabeprazol y otros), más dos antibióticos: amoxicilina y claritromicina es el tratamiento de elección, cuando las resistencias a claritromicina por Helicobacter no superan el 20 %. En caso de alergia a la penicilina, puede sustituirse la amoxicilina por el metronidazol. La erradicación se consigue en el 80-84 % de los casos.

Una vez que el "H. pylori" es detectado en pacientes con una úlcera péptica, el procedimiento normal es erradicarla y permitir que sane la úlcera. La terapia tradicional de primera línea es una semana de terapia triple consistente en los antibióticos amoxicilina y claritromicina, y un inhibidor de bomba de protones como el omeprazol. El gastroenterólogo Thomas Borody de Sídney, Australia, inventó la primera terapia triple en 1987. Al paso de los años, se han desarrollado variaciones de la triple terapia, tales como el uso de diferentes inhibidores de la bomba de protones, como el pantoprazol o el rabeprazol, o cambiando la amoxicilina por metronidazol para las personas que son alérgicas a la penicilina. Tales terapias han revolucionado el tratamiento de las úlcera pépticas y han hecho posible la cura de la enfermedad.

Se ha encontrado que cada vez más individuos infectados tienen bacterias resistentes a los antibióticos, de modo que el tratamiento inicial no resulta efectivo y se requieren rondas adicionales de terapias con antibióticos o estrategias alternativas tales como una terapia cuádruple. Los compuestos de bismuto también son efectivos en combinación con el tratamiento tradicional. Para el tratamiento de las cepas de "H. pylori" resistentes a la claritromicina se utiliza levofloxacina como parte de la terapia.

Se ha creído que, en ausencia de tratamiento, una vez que una infección de "H. pylori" se ha establecido en su nicho gástrico, persistirá de por vida. Sin embargo, en la gente anciana es posible que la infección pueda desaparecer conforme la mucosa estomacal se vuelva cada vez más atrófica e inhóspita para la colonización. La proporción de infecciones agudas que persisten no es conocida, pero varios estudios que siguieron la historia natural en diversas poblaciones han informado de la eliminación espontánea aparente.
Científicos de la Universidad de Kyoto, Japón, comprobaron que el aceite esencial de "Cymbopogon citratus" inhibía el crecimiento de la bacteria "Helicobacter pylori" en medios de cultivo, sin que dicha bacteria presentase resistencia al tratamiento con este aceite esencial. No hay pruebas de que este ni otros aceites esenciales tengan eficacia en el tratamiento de la infección gástrica en humanos.

En pacientes que presentan una infección asintomática, el tratamiento generalmente no está recomendado. Se deben atender las manifestaciones sintomáticas particulares de cada paciente.

En pacientes con úlceras gástricas en donde se detecta "H. pylori", el procedimiento habitual es erradicarlo hasta que la úlcera sane. El tratamiento más extendido es la triple terapia. El gastroenterólogo de Sídney Thomas Borody inventó la primera triple terapia en 1987. Hoy en día, la triple terapia estándar es amoxicilina, claritromicina y tetraciclina; en algunos casos se usa un inhibidor de la bomba de protones diferente. El metronidazol es utilizado en lugar de la amoxicilina en aquellos pacientes alérgicos a la penicilina. Esta terapia ha revolucionado el tratamiento de las úlceras gástricas y ha hecho posible la cura de esta enfermedad, siendo que previamente solo se controlaban los síntomas utilizando antiácidos, antagonistas de los receptores H- o inhibidores de la bomba de protones.

Desafortunadamente, se ha incrementado el número de infecciones individuales que portan cepas resistentes a este primer tratamiento con antibióticos. Esto ha hecho que el tratamiento inicial falle y se requieran aplicaciones adicionales de terapia antibiótica. Se utiliza entonces una cuádruple terapia, incorporándose el bismuto, un metal que es también efectivo en combinación con otros fármacos.

En casos de resistencia a la claritromicina, se recomienda el uso de levofloxacino como parte de la terapia.

El cáncer gástrico y el linfoma MALT (linfoma del tejido linfoide asociado a mucosa) han sido relacionados con "H. pylori", por lo que esta bacteria ha sido clasificada dentro del grupo I de carcinógenos por la Agencia Internacional de Investigación del Cáncer. Mientras que la asociación de estas enfermedades con "H. pylori" está apoyada por sospechas razonables, no está totalmente claro que haya una relación causal involucrada.

Se investigan dos mecanismos relacionados con esta supuesta capacidad de "H. pylori" de producir cáncer. El primero involucra la posibilidad de generar radicales libres asociada a una infección de "H. pylori", la cual produciría un aumento en la tasa de mutación de la célula huésped. El segundo mecanismo ha sido llamado ruta perigenética e involucra la trasformación del fenotipo de la célula huésped por medio de alteraciones en proteínas celulares tales como las proteínas de adhesión. Se ha propuesto la posibilidad de que "H. pylori" induzca inflamación y niveles localmente altos de TNF-alfa o interleucina 6. De acuerdo con el mecanismo perigenético propuesto, las moléculas señalizadoras de inflamación, tales como TNF-alfa, podrían alterar la capacidad de adhesión de las células epiteliales del estómago y conducir a la dispersión y migración de estas células epiteliales mutadas, sin necesidad de alteraciones adicionales en genes supresores de tumores (como, por ejemplo, los genes que codifican para proteínas de adhesión celular).

La tasa de infección por "H. pylori" ha ido decreciendo en países desarrollados, debido a las mejoras en la higiene y al incremento del uso de antibióticos. En consecuencia, la incidencia de cáncer de estómago en los Estados Unidos ha descendido en un 80 por ciento durante el periodo entre 1900 y 2000. No obstante, se ha visto un drástico incremento —en este mismo periodo— de ciertas enfermedades relacionadas con el reflujo gastroesofágico y el cáncer de esófago.

Se conocen varias cepas de "Helicobacter", y el genoma de dos de ellas se ha secuenciado completo. 

El genoma de la cepa "26695" consta de 1,7 millones de pares de bases nitrogenadas, con un total de aproximadamente 1550 genes. Las dos cepas secuenciadas muestran muchas diferencias genéticas, con más de un 6 por ciento de nucleótidos diferentes.

El estudio del genoma de "H. pylori" se centra en aspectos relacionados con la patogenicidad, es decir, con la habilidad de este organismo en causar enfermedades. En la base de datos del genoma de "H. pylori" existen unos 62 genes en la categoría de patogénesis.

Ambas cepas secuenciadas tienen una isla de patogenicidad (una secuencia de genes que se cree que participa en la capacidad infecciosa de la bacteria) llamada Cag: Mide 40 kilobases de tamaño y contiene unos 40 genes. Esta isla de patogenicidad está generalmente ausente en cepas de "H. pylori" aisladas de humanos con infecciones asintomáticas.

El gen "cagA" codifica para una de las proteínas de virulencia mayoritarias en "H. pylori". Las cepas bacterianas que tienen el gen "cagA", están asociadas con la habilidad de causar úlceras severas. Este gen codifica para la síntesis de una proteína relativamente larga (1186 aminoácidos). La proteína CagA ingresa a las células humanas, donde interrumpe el normal funcionamiento del citoesqueleto. La isla de patogenicidad Cag tiene unos 30 genes que codifican para un complejo de transporte (sistema de secreción tipo IV). Después de fijarse a las células epiteliales del estómago, la proteína CagA se inyecta dentro de la célula a través de este sistema de secreción. La proteína CagA es fosforilada en un residuo específico de tirosina por proteínas de membrana de la célula huésped. Se ha visto que cepas patogénicas de "H. pylori" activan el receptor del factor de crecimiento epidérmico (EGFR, en inglés), una proteína de membrana con tirosina quinasa. La activación del EGFR por "H. pylori" está asociada con alteraciones en las señales de transducción y de expresión génica en las células huéspedes, y este hecho puede contribuir a su patogenicidad. También se ha sugerido que la región C-terminal de la proteína CagA (aminoácidos 873-1002) podría regular la transcripción genética de la célula huésped, independientemente de la fosforilación. Se piensa, debido al bajo contenido GC del gen "cagA" en comparación con el resto del genoma, que este gen fue adquirido por transferencia horizontal desde otra bacteria cagA.




</doc>
<doc id="41508" url="https://es.wikipedia.org/wiki?curid=41508" title="Turba">
Turba

La turba es un material orgánico, de color pardo oscuro y rico en carbono. Está formada por una masa esponjosa y ligera en la que aún se aprecian los componentes vegetales que la originaron. Se emplea como combustible y en la obtención de abonos orgánicos.

La formación de turba constituye la primera etapa del proceso por el que la vegetación se transforma en carbón mineral. Se forma como resultado de la putrefacción y carbonificación parcial de la vegetación en el agua ácida de pantanos, marismas y humedales. La formación de una turbera es generalmente lenta como consecuencia de una escasa actividad microbiana, debida a la acidez del agua o la baja concentración de oxígeno. El paso de los años va produciendo una acumulación de turba que puede alcanzar varios metros de espesor, a un ritmo de crecimiento que se calcula de entre 10 a 50 centímetros cada cien años.

Las turberas son pantanos lacustres de origen glaciar que actualmente están repletas de material vegetal más o menos descompuesto y que conocemos como turba de agua dulce. La turba se acumula debido a que la putrefacción de la materia vegetal es muy lenta en climas fríos. La materia vegetal que se acumula por debajo del nivel del agua de un lago está en unas condiciones de continua saturación y de poca disponibilidad de oxígeno, fomentando así la actividad de los transformadores. En estas formaciones tenemos un suelo de tipo histosol.

Se pueden clasificar en dos grupos:

En estado fresco alcanza hasta un 98 % de humedad, pero una vez desecada puede usarse como combustible.

La turba también se usa en jardinería para mejorar suelos por su capacidad de retención de agua. Es más frecuente el uso de turbas rubias en cultivo sin suelo, debido a que las negras tienen una aireación deficiente y unos contenidos elevados en sales solubles. Las turbas rubias tienen un buen nivel de retención de agua y de aireación, pero son muy variables en cuanto a su composición ya que depende de su origen. La inestabilidad de su estructura y su alta capacidad de intercambio catiónico interfieren en la nutrición vegetal, al presentar un pH que oscila entre 3,5 y 8,5. Se emplea en la producción ornamental y de plántulas. 

La turba negra se utiliza en algunas zonas de Escocia para el secado de los ingredientes del whisky, al que da un aroma único. Son suelos carbonosos que se han formado como resultado de una descomposición libre de oxígeno de las plantas muertas. La turba natural es ácida y contiene mucha agua. Posee compuestos químicos que se usan para el tratamiento de la piel. Oscila entre los 5 y los 8 grados Celsius de temperatura.

Una de las mejores turberas de España se encuentra en el norte de Galicia, en la Sierra del Gistral. En el parque nacional de las Tablas de Daimiel (Castilla-La Mancha) en 2009 se declaró un incendio «subterráneo» a causa de la autocombustión de la turba de los terrenos secos, antes inundados.

Por otro lado, en el extremo sur de América, más específicamente en la Isla Grande de Tierra del Fuego, Chile y Argentina, se pueden encontrar grandes extensiones de turba. Estudios geológicos e hídricos de instituciones de Tierra del Fuego afirman que el 65 % de la superficie de la isla está cubierto por esta vegetación y que, debido a sus propiedades de oxigenación, este lugar geográfico sería de los poseedores del aire más limpio del planeta.



</doc>
<doc id="41512" url="https://es.wikipedia.org/wiki?curid=41512" title="Yoshirō Mori">
Yoshirō Mori

Nació en la prefectura de Ishikawa (Japón), de una familia de adinerados agricultores de arroz con una tradición política. Su padre, Shigeki Mori, y su abuelo se encargaron en su día de la alcaldía de Neagari, en la prefectura de Ishikawa. Su madre, Kaoru, murió cuando él tenía siete años.

Estudió en la universidad de Waseda en Tokio, y participó en el club de rugby. Posteriormente se unió al Sankei Shinbun, un periódico muy conservador en Japón. En 1962 dejó el periódico y trabajó como secretario de un miembro de la Dieta, y en 1969, con 32 años, fue elegido en la cámara baja.

Fue Ministro de Educación en 1983 y 1984, Ministro de Comercio Internacional e Industria en 1992 y 1993, y Ministro de Construcción en 1995 y 1996.

Su predecesor fue Keizō Obuchi, pero debido a un derrame cerebral el 2 de abril de 2000, Obuchi fue incapaz de proseguir su labor. Por ello, el secretario general del Partido Liberal Democrático (PDL) Yoshirō Mori pasó a ser el primer ministro. Su posición al frente del Gobierno estuvo marcada por una larga lista de errores, decisiones impopulares, errores de relaciones públicas y fallos verbales. Uno de sus mayores errores en las relaciones públicas fue el de seguir jugando al golf después de recibir la noticia de que el submarino estadounidense USS "Greeneville" había colisionado accidentalmente con el barco pesquero "Ehime Maru", hundiéndolo y causando nueve muertes entre estudiantes y profesores. Nunca fue realmente popular, y al final de su mandato su popularidad había caído por debajo del 10%. Fue descrito frecuentemente como una persona que tenía "el corazón de una pulga y el cerebro de un tiburón". El 26 de abril de 2001, Jun'ichirō Koizumi le sucedió. 

Yoshirō Mori está casado con Chieko (de apellido de soltera Maki), también estudiante y compañera de la universidad de Waseda; con la que tiene un hijo, Yuki Mori ("Mori Yūki"), y una hija, Yoko Fujimoto ("Fujimoto Yōko").




</doc>
<doc id="41513" url="https://es.wikipedia.org/wiki?curid=41513" title="Instrumento electrónico">
Instrumento electrónico

Un instrumento electrónico es un dispositivo formado por una combinación de elementos electrónicos, tales como válvulas termoiónicas, transistores o circuitos integrados, entre otros muchos, y que, combinados adecuadamente, permiten la realización de funciones tales como la medición de parámetros físicos, generación de señales de distintas frecuencias, detección de estas mismas señales y, en fin, todas aquellas funciones susceptibles de ser procesadas mediante señales eléctricas.



</doc>
<doc id="41516" url="https://es.wikipedia.org/wiki?curid=41516" title="Keizō Obuchi">
Keizō Obuchi

Nació en la prefectura de Gunma. A los 13 años fue a un instituto privado de enseñanza media en Tokio, y permaneció en la ciudad durante el resto de su vida. En 1958, se matriculó en Literatura Inglesa en la universidad de Waseda, con la esperanza de ser escritor. Cuando murió su padre ese mismo año, decidió seguir el camino de éste, así que cambió a Ciencias Políticas y se graduó en 1962.



</doc>
<doc id="41517" url="https://es.wikipedia.org/wiki?curid=41517" title="Siphonaptera">
Siphonaptera

Los sifonápteros (Siphonaptera, gr. σίφων "síphōn", «canal, tubo» y ἄπτερα "áptera", «sin alas») son un orden de pequeños insectos ápteros, conocidos popularmente como pulgas. Las pulgas son parásitos externos hematófagos de diversos animales y pueden ejecutar saltos largos en proporción a su tamaño, pudiendo así alcanzar fácilmente a nuevos huéspedes, gracias a que en sus articulaciones poseen resortes de la proteína más elástica conocida, la resilina, igual que otros insectos como saltamontes y langostas. Se conocen unas 1900 especies, varias de las cuales pueden transmitir enfermedades diversas, como el tifus, la peste negra o bubónica, o las tenias (como "Dipylidium caninum").

Las pulgas son invertebrados pequeños (de 1,5 a 3,3 mm de largo) carecen de alas, muy ágiles, de color generalmente oscuro (por ejemplo, la pulga de los gatos es de color rojizo-parduzco), que cuentan con un mecanismo bucal de tubos especialmente adaptado para poder alimentarse de la sangre de sus huéspedes. Tienen el cuerpo comprimido lateralmente, lo que les permite desplazarse con facilidad entre los pelos o plumas del huésped. Tienen las patas largas y las traseras están adaptadas para el salto, que puede ser de hasta 18 cm en dirección vertical y 33 cm en dirección horizontal. Esto representa una distancia de hasta 200 veces su propia longitud, lo que convierte a las pulgas en el mejor saltador entre los animales en relación con su tamaño corporal. El cuerpo de la pulga es duro, pulido, y está cubierto con muchos pelos y espinas cortas dirigidas hacia atrás. Esta característica les asegura un tránsito fluido entre los cabellos del huésped. La dureza de su cuerpo les permite soportar grandes presiones (probablemente como resultado de una adaptación para sobrevivir el rascado, etc.), incluso la ejercida por los dedos humanos.

Las pulgas son insectos holometábolos, es decir, tienen metamorfosis completa y pasan por un completo ciclo vital con estadios de huevo, larva, pupa y adulto. El periodo en que se completa el ciclo de huevo a adulto varía de dos semanas a ocho meses dependiendo de la temperatura, humedad, alimento y especie. Normalmente, tras alimentarse de sangre, la hembra deposita entre 15 y 20 huevos por día, hasta 600 en toda su vida, usualmente sobre el hospedador (perros, gatos, ratas, conejos, ratones, ardillas, ardillas listadas, mapaches, zarigüeyas, zorros, pollos, humanos, etc.). Los huevos depositados sueltos en el pelaje caen en su mayor parte por todos lados, especialmente donde el hospedador descansa, duerme o nidifica (alfombrillas, alfombras, muebles tapizados, cajas del perros y gatos, perreras, cajas de arena, ascensores de edificios donde se acuestan los perros, gatos, etc.)

Los huevos eclosionan entre dos y catorce días después de la puesta. De ellos salen larvas vermiformes de vida libre. Las larvas se refugian en las grietas y hendiduras del suelo, a lo largo de los rodapiés, bajo los bordes de las alfombrillas, en muebles o camas, dentro de las edificaciones. Si el desarrollo es a la intemperie tiene lugar en suelos de arena o grava (cajas de arena húmedas, bajos de las casas sucias, bajo los arbustos, etc.) donde el hospedador puede descansar o dormir. La arena y grava son muy adecuadas para el desarrollo larvario, que es la razón por la que las pulgas son llamadas erróneamente "pulgas de arena".

Las larvas son ciegas, evitan la luz, pasan por tres mudas larvarias y tardan de una semana a varios meses en desarrollarse. Su alimento consiste en sangre digerida de las heces de pulgas adultas, piel muerta, pelo, plumas y otros restos orgánicos (las larvas no chupan sangre.) Las pupas maduran al estado de adultos dentro de un capullo de seda tejido por la larva, el cual adhieren al pelo de mascotas, fibras de alfombras, polvo, trozos de hierba y otros restos. En alrededor de cinco a catorce días emergen las pulgas adultas o pueden permanecer en estado latente en el interior del capullo hasta detectar vibración (movimiento de personas o mascotas), presión (el animal hospedador apoyado sobre ellas), calor, humedad o dióxido de carbono (significando que una potencial fuente de sangre está cerca). La mayoría de las pulgas pasan el invierno en el estado de larva o pupa con mejor supervivencia y crecimiento durante inviernos cálidos y húmedos y en la primavera.

Las diferentes especies de pulgas tienen preferencia por los huéspedes, pero no especificidad.

La tierra de diatomeas es uno de los tratamientos no industriales más efectivos contra la infestación de pulgas en los hogares.

En la mayor parte de los casos las pulgas son sólo una molestia para sus huéspedes, pero algunas personas y animales sufren una reacción alérgica a la saliva de la pulga, produciéndose erupciones. Las picaduras de pulga generalmente tienen como resultado la formación de unas zonas inflamadas y ligeramente elevadas que producen picazón y que tienen un solo punto de picadura en el centro.

Sin embargo, las pulgas pueden transmitir enfermedades como el tifus y la devastadora peste bubónica, transmitida entre roedores y humanos por la pulga de la rata de alcantarilla ("Nosopsyllus fasciatus") y la pulga de la rata negra ("Xenopsylla cheopis").

La pulga común ("Pulex irritans"), la del perro ("Ctenocephalides canis") y la del gato ("Ctenocephalides felis") pueden ser huéspedes intermediarios de cestodos (tenias o solitarias) como "Dipylidium caninum" o "Hymenolepis diminuta" los cuales pueden parasitar al hombre.




</doc>
<doc id="41518" url="https://es.wikipedia.org/wiki?curid=41518" title="Partido Liberal Democrático (Japón)">
Partido Liberal Democrático (Japón)

El , también conocido por su abreviación japonesa , es un partido político conservador de Japón, siendo la principal fuerza política nipona que tradicionalmente ha gobernado el país casi ininterrumpidamente desde su fundación, en 1955. A pesar de que en Japón rige un sistema político multipartidista, para algunos autores los largos años en el poder del PLD lo han convertido más en un "sistema de partido dominante". Precisamente, la fragmentación y división de otros partidos japoneses de la oposición es una de las causas que explican la fortaleza del PLD durante varias décadas.

El Partido Liberal Democrático no debe ser confundido con el ahora difunto , que se unió en 2003 al Partido Democrático (en aquel entonces el principal partido de la oposición).

El PLD se ha mantenido la mayor parte del tiempo en el poder desde su fundación hasta la actualidad, salvo dos períodos: entre 1993 y 1994, con la formación de un gobierno de coalición que dejó fuera al PLD; un segundo período en que el PLD estuvo ausente del poder fue durante los gobiernos del Partido Democrático, entre 2009 y 2012.

El PLD fue creado en 1955 con la unión de dos formaciones políticas: el y el . Ambos, que eran dos partidos derechistas, y acordaron unirse para a su vez formar un partido unificado contra el Partido Socialista de Japón, que en aquella época gozaba de una gran popularidad entre el electorado nipón. El PLD ganó las siguientes elecciones que se celebraron, y logró formar el primer gobierno conservador de Japón con una amplia mayoría. Desde entonces lograría mantener el gobierno hasta 1993.

El PLD comenzó dando un nuevo enfoque a la política exterior de Japón, con medidas que iban desde el ingreso y la participación activa en las Naciones Unidas, al restablecimiento de contactos diplomáticos con la Unión Soviética. En la mayoría de elecciones que siguieron a 1955 el Partido Liberal Democrático se convirtió en la fuerza política más votada, encontrándose prácticamente con la única oposición que venía desde los dos principales partidos izquierdistas: el Partido Socialista de Japón (PSJ) y el Partido Comunista de Japón (PCJ). Por esta razón, entre las décadas de 1950 y 1970 , la Agencia Central de Inteligencia de los Estados Unidos gastó millones de dólares durante las campañas electorales japonesas, procurando aumentar el apoyo popular hacia el PLD y, a su vez, en contra de los partidos y movimientos de izquierda, como eran los socialistas y comunistas. 

No obstante, la larga intervención de los servicios secretos estadounidenses en favor de los conservadores japoneses no fue conocida por el público hasta la mitad de los años 1990, cuando fue revelada por el reputado diario norteamericano "New York Times".

Tras su vuelta al poder en 1996, cinco años después el partido pasó a estar bajo el liderazgo del carismático Junichiro Koizumi. Con una nueva victoria en las elecciones generales del 2005, el PLD mantuvo la mayoría absoluta en la Dieta nacional y formó un gobierno de coalición con el partido "Nuevo Kōmeitō". El período de gobierno de Koizumi estuvo marcado por su alianza con el presidente norteamericano George W. Bush, una fuerte política nacionalista y especialmente la privatización del servicio postal japonés, que en materia económica fue el principal objetivo del gobierno Koizumi.

Shinzō Abe sucedió a Junichiro Koizumi como presidente del partido el 20 de septiembre de 2006, aunque sólo ocuparía el cargo por un corto período. En ese contexto, el partido sufrió una gran derrota en las elecciones de 2007 a la Cámara de Consejeros y perdió la mayoría en esta cámara por primera vez en su historia. El 12 de septiembre de 2007, Abe dimitió como Primer Ministro y líder del partido, y le sucedió Yasuo Fukuda, que a su vez dimitiría el 1 de septiembre de 2008 tras apenas un año en el puesto. Entonces el veterano Taro Aso pasó a hacerse cargo de la dirección del gobierno y del partido, pero en las elecciones generales de 2009 el PLD sufrió un importante descalabro electoral frente al Partido Democrático al perder 177 escaños en el parlamento, lo que supuso su salida del gobierno. Esta derrota puso fin a más de medio siglo de gobiernos conservadores.

Sin embargo, después de tres años de gobierno el Partido Democrático había sufrido un fuerte desgaste y en las elecciones generales de 2012 el PLD, otra vez bajo el liderazgo de Shinzō Abe, logró una contundente victoria y su vuelta al poder. Desde esa fecha, los conservadores siguen al frente del gobierno japonés.





</doc>
<doc id="41520" url="https://es.wikipedia.org/wiki?curid=41520" title="Partido Democrático (1998-2016)">
Partido Democrático (1998-2016)

El Partido Democrático fue un partido político japonés de ideología centrista, surgido el 27 de abril de 1998 como resultado de la fusión de diversos partidos de la oposición. En las elecciones generales de 2009 el PDJ obtuvo una gran victoria, desplazando al Partido Liberal Democrático (PLD) y poniendo fin a medio siglo de gobiernos conservadores. A pesar de su importante victoria electoral, su período de gobierno estuvo salpicado de numerosos altibajos, y se vio muy criticado por la gestión posterior al tsunami que afectó a Japón en marzo de 2011, además de las numerosas disputas internas del PDJ que redujeron la acción de gobierno.

Después de obtener una arrolladora victoria en 2009, volvió a ser expulsado del gobierno por el PLD en las elecciones generales de 2012, sufriendo una importante pérdida tanto en número de votos como en escaños. A pesar de su relativa "novedad" como partido en el sistema político japonés, en comparación con la veteranía de otras formaciones políticas, el PDJ ya tiene reconocidas 6 facciones en su organización, hecho que le ha supuesto un importante lastre interno.

El 24 de febrero de 2016, el PDJ aceptó fusionarse con el Partido de la Innovación, rumbo a la elección de la Cámara de Consejeros en verano de 2016. El 14 de marzo de 2016, el nombre del partido fue anunciado como "Minshinto", tras una serie de consultas. La nueva formación política se fundó oficialmente el 27 de marzo de 2016.

El Partido Democrático surgió en la primavera de 1998 a partir de la unión de varios pequeños partidos de la oposición al gobernante Partido Liberal Democrático (PLD): el antiguo Partido Democrático, el , el y el . En el momento de su fundación, el partido disponía de unos 93 escaños en la Cámara de Representantes, que aumentaron a 127 tras concurrir a elecciones generales de 2000. De esta forma, se convirtió en el primer partido de la oposición, recuperando en cierto grado el papel que había jugado el Partido Socialista de Japón hasta comienzos de los años 1990. A lo largo de la década de los años 2000, el PDJ obtuvo unos resultados respetables en los comicios a los que el partido se presentó, aunque claramente insuficientes para imponerse al todopoderoso PLD y al entonces primer ministro conservador, Junichiro Koizumi. De hecho, en las elecciones de 2005 el PDJ sufrió una importante derrota electoral, lo que supuso la dimisión del líder demócrata, Katsuya Okada.

En las elecciones celebradas en agosto de 2009 el Partido Democrático ganó por una amplia mayoría a su gran rival el Partido Liberal Democrático (PLD), haciéndose con 308 de los 480 escaños en liza; de esta manera el Partido Democrático rompía con 55 años de gobierno prácticamente ininterrumpido del PLD. El líder en aquel momento del Partido Democrático, Yukio Hatoyama, fue nombrado primer ministro de Japón el 16 de septiembre de 2009.

Durante su período de gobierno, el PDJ se vio acosado por conflictos internos e intentó poner en práctica muchas de sus propuestas políticas, sin éxito, lo que ha sido denominado por los politólogos como la "paradoja del cambio político sin cambio de políticas". La actividad legislativa bajo el PDJ fue particularmente baja, cayendo a niveles sin precedentes en la Historia japonesa reciente. No obstante, el partido llegó a implementar una serie de medidas progresistas, tales como la provisión de educación pública gratuita hasta el bachillerato, aumentar las ayudas estatales para las familias con hijos, expandir la cobertura del seguro por desempleo, extender la duración de los subsidios a la vivienda, y regulaciones más estrictas para salvaguardar a los trabajadores temporales y a tiempo parcial.

El 2 de junio de 2010 Yukio Hatoyama renunció al cargo de primer ministro debido a las presiones de su propio partido que temía sufrir un descalabro electoral en las elecciones del 11 de julio a la Cámara de Consejeros de Japón a causa de la impopularidad de Hatoyama. Dos días después Naoto Kan fue elegido nuevo líder del Partido Democrático y poco después se convirtió en el nuevo primer ministro de Japón, tomando posesión del nuevo cargo el 8 de junio. El nuevo Primer ministro no tuvo mejor fortuna, ya que se vio afectado por la persistente crisis económica y las luchas internas del PDJ, pero su gobierno fue especialmente criticado por la gestión posterior al tsunami que afectó a Japón en marzo de 2011 y del accidente nuclear de Fukushima. Víctima de la impopularidad entre la opinión pública japonesa, la situación de Kan como líder del PDJ quedó en muy mal lugar, por lo que se comprometió a presentar su dimisión tras la aprobación en el parlamento de algunas leyes. El 29 de agosto de 2011 Yoshihiko Noda ganó la elección interna del Partido Democrático para elegir al líder de ese partido (y virtual nuevo primer ministro de Japón).

Después de la desastrosa derrota del Partido Democrático de Japón en las elecciones generales de Japón del 16 de diciembre de 2012, Yoshihiko Noda renunció a la presidencia del partido y se abrió el proceso electoral interno para elegir al nuevo líder. El 25 de diciembre de 2012 se celebró la elección interna del presidente del Partido Democrático; Banri Kaieda ganó al obtener 90 votos mientras que su principal rival, Sumio Mabuchi, obtuvo 54 votos.



</doc>
<doc id="41521" url="https://es.wikipedia.org/wiki?curid=41521" title="Mellotron">
Mellotron

El mellotron es un instrumento musical electro-mecánico polifónico que apareció a mediados de los años 1960.

Según la investigadora y cineasta estadounidense Dianna Dilworth, el "mellotron" fue inventado en Estados Unidos en la década de 1950 por Harry Chamberlin, quien tenía el objetivo de crear un instrumento hogareño que pudiera replicar los sonidos de una orquesta para cantar en reuniones familiares. Su primer instrumento se llamó "chamberlin", pero el sonido que producía no era el esperado. Asimismo, presentaba constantes problemas con el sistema de cintas grabadas y el mercado familiar no pareció interesarse en el producto, del cual apenas llegaron a fabricarse unas 500 unidades en total.

A principios de la década de 1960, un vendedor contratado por el propio Harry Chamberlin, llamado Bill Fransen, llevó la idea al Reino Unido y la presentó como propia, logrando allí interesar a una empresa llamada Mellotronics, que fabricó 2000 unidades bajo el nombre comercial "mellotron", los cuales si bien copiaban el concepto de su antecesor el "chamberlin", incorporaban algunas mejoras técnicas que lo hacían un poco más fiable.

Se trataba de un teclado capaz de reproducir en tres canales cintas pregrabadas accionadas con ambas manos: con la izquierda se podían seleccionar 18 ritmos diferentes y con la derecha otros tantos instrumentos (Mellotron MKII). Exteriormente no se diferenciaba demasiado de un piano convencional, y al principio se encajaban en un elegante mueble de madera. 

El "mellotron" fue uno de los primeros teclados eléctricos y puede considerarse como el antecedente directo del "sampler", pues utiliza "loops" (bucles) de cinta para crear sonidos. El "mellotron" permite reproducir muestras ("samples") de sonido pre-grabado en una cinta. Cada tecla está asociada a una cinta magnética de casi 1 cm de ancho que tiene una duración aproximada de 8 segundos. El músico, al presionar una tecla, hace circular su cinta correspondiente, que recorre un ingenioso sistema en forma de W, y reproduce el sonido pregrabado que contiene diversos instrumentos (de cuerda, orquestales, coro polifónico, flautas, etc). Podían agregarse nuevos sonidos.

Pese a las limitaciones de duración en las muestras, bucles o "loops" (8 segundos), el sonido del "mellotron", inconfundible, un tanto imperfecto y artificial, propició su abundante uso en los conciertos y grabaciones de rock progresivo sinfónico en los años 1970. Ni siquiera los más versátiles y compactos sintetizadores lograron hacerlo desaparecer, aunque quedó en un segundo lugar a partir de la aparición de teclados electrónicos, a mediados de los 70.

Entre las desventajas del "mellotron" se puede citar, principalmente, la imposibilidad de grabar nuevos sonidos en las cintas usando el propio aparato. Las cintas sólo podían ser grabadas utilizando un mecanismo que poseía únicamente el fabricante del instrumento. Otra desventaja radicaba en el hecho de que, con frecuencia, cada vez que se trasladaba el instrumento, había que calibrarlo, lo que exigía la intervención de una persona con los conocimientos precisos. Debido a la tendencia a desajustarse, muchos grupos que lo utilizaban salían de gira con dos unidades, para disponer de una de reserva.

El "mellotron" es famoso por su aparición en la canción «Strawberry Fields Forever» de The Beatles, cuya introducción es reproducida por el sonido de "ensamble" de flautas traveseras. Fue usado por otras bandas como The Zombies, The Moody Blues y The Rolling Stones en los años 1960. 

En los 70 se hizo común su uso entre bandas de rock progresivo, como Genesis (quienes hacen una introducción en la canción «Watcher Of The Skies» con este instrumento), King Crimson (la cual hace un amplio uso de este instrumento, siendo las apariciones más conocidas en «Epitaph» o «Starless»), Yes, la banda italiana Premiata Forneria Marconi o la banda española Triana en sus primeras grabaciones. También fue utilizado asiduamente por John Paul Jones, de Led Zeppelin en «The Rain Song», «Kashmir» y solamente la versión en vivo de «Stairway to Heaven».

Con la aparición de los modernos sintetizadores polifónicos y los sampleadores, el "mellotron" fue desplazado poco a poco. En la actualidad, algunos músicos han vuelto a recurrir al "mellotron" para darles a sus composiciones un sonido evocador de los años 1960 y 70. Hoy en día es utilizado de vez en cuando por artistas y bandas como Keane, Radiohead, Tom Waits, Tori Amos, Red Hot Chili Peppers, Incubus, Muse, Truly, Super 400, Ernesto Romeo (klauss), Fiona Apple, Oasis, Dream Theater, The Smashing Pumpkins y Maxi Trusso, aunque ahora ha vuelto su protagonismo con el resurgimiento del rock progresivo durante los años 1990, con grupos como los suecos Anekdoten, Anglagard, Opeth, The Flower Kings y Porcupine Tree.



</doc>
<doc id="41526" url="https://es.wikipedia.org/wiki?curid=41526" title="Leucemia">
Leucemia

La leucemia es un grupo de enfermedades malignas de la médula ósea (cáncer hematológico) que provoca una proliferación anormal de leucocitos en ella. Sin embargo, en algunos tipos de leucemias también pueden afectarse cualquiera de los precursores de las diferentes líneas celulares de la médula ósea, como los precursores mieloides, monocíticos, eritroides o megacariocíticos. Las células cancerígenas impiden que se produzcan glóbulos rojos, plaquetas y glóbulos blancos maduros (leucocitos) saludables. Entonces, se pueden presentar síntomas potencialmente mortales a medida que disminuyen las células sanguíneas normales. Las células cancerosas se pueden propagar al torrente sanguíneo y a los ganglios linfáticos. También pueden viajar al cerebro y a la médula espinal (el sistema nervioso central) y otras partes del cuerpo.

La leucemia puede desarrollarse rápida o lentamente. La leucemia crónica crece lentamente. En la leucemia aguda las células son muy anormales y su número aumenta rápidamente. Los adultos pueden tener cualquiera de estos tipos; los niños con leucemia, generalmente, sufren del tipo agudo. Algunas leucemias, con frecuencia, puede curarse. Otras, son difíciles de tratar, pero pueden controlarse. Los tratamientos pueden incluir quimioterapia, radioterapia y trasplante de células madre. Aun si los síntomas desaparecen, se podría necesitar tratamiento para prevenir una recaída.

Los síntomas incluyen:


Los factores de riesgo para la leucemia en niños incluyen: tener un hermano o hermana con leucemia, tener ciertos trastornos genéticos o recibir tratamiento con radiación o quimioterapia. Generalmente, la leucemia infantil se cura con el tratamiento. Las opciones de tratamiento incluyen: quimioterapia, tratamiento con otros medicamentos y radiación. En algunos casos, un trasplante de médula ósea y de células madre puede ayudar.
Existen distintos tipos de clasificación, en función del criterio que se utilice para ello.



La principal característica de las leucemias agudas es la presencia de un "cese madurativo" de las células de línea mieloide (LMA) o Linfoide (LLA) con blastosis en médula ósea (superior de 20% de celularidad no eritroide según la OMS). Dado que todavía queda hematopoyesis normal residual, puede verse en sangre periférica la existencia de un "hiato leucémico", es decir, presencia de formas inmaduras en sangre periférica y formas maduras pero con ausencia de elementos intermedios.

En las leucemias crónicas, la principal característica morfológica es la no existencia de dicho hiato leucémico, ya que no existe detenimiento madurativo, permitiendo secretar a la sangre células maduras, y su curso clínico suele ser indolente.

Se producen daños en la médula ósea, a modo de desplazamientos de las células normales de la médula ósea con un mayor número de glóbulos blancos inmaduros. Todo esto se traduce en una falta de plaquetas en la sangre, fundamentales en el proceso de coagulación sanguínea, por lo que las personas con leucemia pueden desarrollar fácilmente hematomas y un sangrado excesivo o hemorragias punteadas (petequias).

Los glóbulos blancos, implicados en la defensa del organismo, pueden ser deficientes o disfuncionales. Esto puede causar que el sistema inmune del paciente sea incapaz de luchar contra una infección simple. Debido a que la leucemia impide que el sistema inmunitario funcione con normalidad, algunos pacientes experimentan infecciones frecuentes, que van desde las amígdalas infectadas, llagas en la boca, diarrea, neumonía o infecciones oportunistas.

Por último, la deficiencia de glóbulos rojos produce anemia, que puede causar disnea y palidez.

De manera resumida, algunas de sus manifestaciones clínicas más importantes son:
Uno de los síntomas frecuentes en este padecimiento es “la fatiga”, relacionada con el cáncer, la cual tiene efectos nocivos en diferentes dimensiones del ser humano, dado que somos seres biopsicosociales, afectando entonces, la parte física, social y cognitiva, así como el funcionamiento profesional del sujeto. El National Comprehensive Cancer Network en 2014, definió la fatiga relacionada con el cáncer como una inusual sensación persistente y subjetiva de cansancio que no va de acuerdo a la actividad que realiza el paciente de manera cotidiana, interfiriendo en su funcionamiento habitual.

La palabra leucemia, que significa "sangre blanca", se deriva de la alta cantidad de glóbulos blancos que la mayoría de los pacientes de leucemia tienen antes del tratamiento. El elevado número de células blancas en la sangre es evidente cuando se observa una muestra de sangre afectada bajo el microscopio. Con frecuencia, estas células blancas extra son inmaduras o disfuncionales. El excesivo número de células también puede interferir con el nivel de otras células, causando un desequilibrio perjudicial en la proporción de la sangre.

Algunos pacientes con leucemia no tienen una alta cantidad de glóbulos blancos visibles durante un recuento sanguíneo normal. Esta enfermedad menos común se denomina "aleucemia". La médula ósea contiene las células cancerosas aún blancas de la sangre que perturban la producción normal de células sanguíneas. Sin embargo, las células leucémicas se alojan en la médula en lugar de entrar en el torrente sanguíneo, donde serían visibles en un análisis de sangre. Para un paciente aleucémico, el recuento de glóbulos blancos en la sangre puede ser normal o baja. La aleucemia puede ocurrir en cualquiera de los cuatro tipos principales de leucemia, y es particularmente común en la leucemia de células pilosas.

No hay una única causa conocida para todos los distintos tipos de leucemia que existen. Las causas conocidas, que no son factores intrínsecos de la persona, representan relativamente pocos casos.
Cada leucemia distinta puede tener varias causas diferentes.

La leucemia, al igual que otros tipos de neoplasias, son el resultado de mutaciones del ADN. Ciertas mutaciones producen la activación de oncogenes o la desactivación de los genes supresores de tumores, y con ello alteran la regulación de la muerte celular, la diferenciación o la mitosis. Estas mutaciones ocurren espontáneamente o como resultado de la exposición a la radiación o a sustancias cancerígenas, además de la probable influencia de factores genéticos.

Entre los adultos, podemos diferenciar entre los factores biológicos (virus como el virus linfotrópico T humano) y los artificiales (radiación ionizante, benceno, agentes alquilantes y quimioterapia para otras enfermedades).
El consumo de tabaco se asocia con un pequeño aumento en el riesgo de desarrollar leucemia mieloide aguda en adultos.
El uso y la exposición a algunos productos petroquímicos y tintes para el cabello se ha relacionado con el desarrollo de algunas formas de leucemia. También se han descrito casos de transmisión materno-fetal.

Algunas formas de leucemia están vinculadas a infecciones víricas. Experimentos en ratones y otros mamíferos han demostrado la relación entre los retrovirus y la leucemia y también han sido identificados retrovirus humanos. El primer retrovirus humano identificado fue el virus linfotrópico T humano o HTLV-1, el cual causa la leucemia de células T.

Algunas personas tienen una predisposición genética hacia el desarrollo de leucemias. Esta predisposición se demuestra por los antecedentes familiares y los estudios en gemelos.
Los afectados pueden tener un solo gen o genes múltiples en común. En algunos casos, las familias tienden a desarrollar el mismo tipo de leucemia que los demás miembros; en otras familias, las personas afectadas pueden desarrollar formas diferentes de leucemia o neoplasias relacionados con la sangre.

Además de estas cuestiones, las personas con anomalías cromosómicas o ciertas enfermedades genéticas tienen un mayor riesgo de padecer leucemia.
Por ejemplo, las personas con síndrome de Down tienen un riesgo significativamente mayor de desarrollar formas de leucemia aguda y la anemia de Fanconi es un factor de riesgo de desarrollar leucemia mieloide aguda.

La radiación no ionizante como causa de la leucemia ha sido estudiada durante varias décadas. Los expertos del grupo de trabajo de la Agencia Internacional de Investigaciones sobre el Cáncer realizaron una revisión detallada de todos los datos estadísticos acerca de frecuencias extremadamente bajas de energía electromagnética, que se produce de forma natural y en asociación con la generación, transmisión y uso de la energía eléctrica.
Llegaron a la conclusión de que hay muy pocas pruebas de que altos niveles de campos magnéticos de ELF (frecuencia extremadamente baja) ―pero que no sean eléctricos― podrían causar leucemia infantil. La exposición a campos magnéticos de ELF significativa podría dar lugar a un doble riesgo excesivo para la leucemia de los niños expuestos a estos altos niveles de campos magnéticos. Sin embargo, el informe también dice que las deficiencias metodológicas y sesgos en estos estudios probablemente hayan hecho que el riesgo sea exagerado.
No se ha demostrado evidencia de una relación con la leucemia u otro tipo de tumor maligno en los adultos.
Dado que la exposición a tales niveles de ELF es relativamente poco común, la Organización Mundial de la Salud concluye que la exposición de ELF, que cada año representa solo entre 100 y 2400 casos en todo el mundo, lo que representa 0,20 a 4,95 % de la incidencia total para ese año.

El diagnóstico se basa generalmente en repetidos conteos sanguíneos completos y un examen de médula ósea tras los síntomas observados. La biopsia de un ganglio linfático puede realizarse también para diagnosticar ciertos tipos de leucemia en algunas situaciones. Una vez diagnosticada la enfermedad, una analítica sanguínea puede utilizarse para determinar el grado de daño al hígado y a los riñones o los efectos de la quimioterapia en el paciente. Para observar los posibles daños visibles debidos a la leucemia, se pueden utilizar radiografías (en huesos), resonancia magnética (cerebro) o ultrasonidos (riñón, bazo e hígado). Las tomografías computarizadas rara vez se utilizan para revisar los nódulos linfáticos en el pecho.

Si bien algunas leucemias tienen carácter fulminante, otras pueden ser enfermedades indolentes, de presentación insidiosa. Ya que no existe ningún síntoma que por sí solo y de manera específica permita diagnosticar esta enfermedad, siempre debe descartarse la presencia de leucemia en presencia de manifestaciones clínicas sugerentes, tales como un hemograma alterado. El método más seguro para confirmar o descartar el diagnóstico es mediante la realización de un mielograma y, solo en casos seleccionados, puede ser necesario realizar una biopsia de la médula ósea.

El tratamiento de la leucemia puede incluir:
En un estudio realizado en el cual los pacientes fueron sometidos a un régimen de ejercicio aeróbico, el cual fue realizado en una bicicleta reclinada con una duración de 10 a 20 minutos de 3 a 4 veces por semana durante 4 a 5 semanas y un entrenamiento de resistencia como parte de un protocolo de ejercicio para atenuar la pérdida de masa muscular observada en pacientes con una estancia larga en cama, la cual es frecuente en pacientes que reciben tratamiento para leucemia, los resultados obtenidos mostraron una reducción significativa en cuanto a la fatiga y depresión, manteniendo la masa muscular de los pacientes, y dejando evidencia de que una combinación de entrenamiento aeróbico y de resistencia, crea un efecto positivo tanto físico como psicológico en pacientes que se encuentran en tratamiento.

En 2014, investigadores de Minnesota (Estados Unidos) han conseguido acabar con la leucemia de una mujer de 50 años inyectándole una dosis masiva de una cepa genéticamente modificada del virus del sarampión de tipo MV-NIS.




</doc>
<doc id="41529" url="https://es.wikipedia.org/wiki?curid=41529" title="Margueritte Laugier">
Margueritte Laugier

Margueritte Laugier (1896 - 1976), de nombre de soltera Lhomme, fue una astrónoma del Observatorio de Niza, aunque también realizó numerosas observaciones desde el Observatorio de Uccle, desde los que descubrió 21 asteroides.

Los artículos astronómicos de la época a veces la citan como "Madame" Laugier.

Entre 1932 y 1955 descubrió 21 asteroides. El Minor Planet Center acredita sus descubrimientos como M. Laugier.

Además descubrió desde Niza y de forma independiente el asteroide (1564) Srbija descubierto el que fue comunicado el mismo día pero con anterioridad por M. B. Protitch, a quien se le atribuye.


El asteroide (1597) Laugier descubierto en 1949 por Louis Boyer fue nombrado en su honor.



</doc>
<doc id="41530" url="https://es.wikipedia.org/wiki?curid=41530" title="Mandíbula">
Mandíbula

La mandíbula (antiguamente denominada también maxilar inferior) es un hueso impar, plano, central y simétrico, en forma de herradura, situado en la parte anterior, posterior e inferior de la cara.Durante el desarrollo está compuesta por dos mitades, una de cada lado, llamadas hemimandíbulas. En los seres humanos, las hemimandíbulas se encuentran completamente fusionadas alrededor de los cinco años de edad, pero en muchos mamíferos permanecen independientes toda la vida.

La cirugía oral y maxilo-facial, especialidad de la odontología, es la encargada de estudiar su anatomía, así como su estructura y los procesos patológicos que allí pueden asentar.

Presenta para su estudio una parte media o cuerpo y dos extremos laterales o ramas ascendentes, situadas a ambos lados del cuerpo. Es el hueso más denso y prominente de la cara.

Tiene forma de herradura cuya concavidad está dirigida hacia atrás. Presenta para su estudio dos caras (anterior y posterior) y dos bordes (superior e inferior):

En el plano sagital medio y visible solo desde su cara anterior se encuentra la sínfisis mentoniana, que no es más que un vestigio de la unión ósea. A lo largo de esta línea hay varias crestas de osificación que forman una eminencia piramidal denominada eminencia mentoniana. Desde esta eminencia y a ambos lados se desprende una línea saliente denominada línea oblicua externa, la cual cruza diagonalmente la cara anterior del hueso y va a terminar al borde anterior de la rama. En esta línea se insertan los músculos triangular de los labios, cuadrado de la barba y algunas veces el cutáneo del cuello. A la altura del ápice del segundo premolar se encuentra el orificio mentoniano, el cual es atravesado por el paquete vasculonervioso mentoniano.

En la cara posterior, a ambos lados del plano medio sagital del cuerpo, se encuentran unas rugosidades denominadas apófisis geni. Dos apófisis geni superiores y dos inferiores, dando las superiores inserción al músculo geniogloso y las inferiores al músculo genihioideo. Al igual que en la cara externa, aquí se encuentra otra línea que atraviesa la cara interna del cuerpo diagonalmente en busca del borde anterior de la rama: la línea oblicua interna o milohioidea, en la cual se inserta el músculo milohioideo. Por arriba de la línea oblicua interna y a cada lado de las apófisis geni se encuentra una depresión más palpable que visible denominada fosita sublingual, en la cual se aloja la glándula sublingual. Por debajo de la línea milohioidea a nivel de los molares se encuentra otra depresión mucho más pronunciada denominada fosita submaxilar la cual brinda asentamiento a la glándula submaxilar.

El Borde superior, también denominado borde alveolar, recibe este nombre puesto que aquí es donde se encuentran los alvéolos dentarios en los cuales articulan las raíces de las piezas dentarias.
El borde inferior, romo, presenta a cada lado del plano medio sagital dos pequeñas depresiones para la inserción de los vientres anteriores del músculo digástrico. No es raro encontrar en algunos casos, en el extremo posterior de este borde, la escotadura facial, producida por la actividad pulsátil de la arteria facial que por allí abandona el cuello para llegar al territorio de la cara.

Parten de las extremidades posteriores del cuerpo hacia la zona superior, formando un ángulo de unos 15º, denominado ángulo mandibular o gonion. Cada rama, en su parte superior, presenta dos estructuras óseas: una anterior denominada apófisis coronoides, que sirve de inserción para el músculo temporal y otra posterior denominada cóndilo mandibular. Entre ambas se encuentra la escotadura sigmoides. El cóndilo en estado fresco se encuentra recubierto por fibrocartílago y se articula con la fosa mandibular (o cavidad glenoidea) del hueso temporal, constituyendo la articulación temporomandibular o ATM, situada por delante del conducto auditivo externo.

Para su estudio se dividen en 2 caras (externa e interna) y 4 bordes (superior, inferior, anterior y posterior):
Presenta numerosas rugosidades, sobre todo en su parte inferior que es donde se inserta el músculo masetero.
En la parte media de dicha cara, encontramos un orificio, que es el agujero mandibular u orificio de entrada al conducto dentario inferior, por donde ingresa al hueso el paquete vasculonervioso dentario inferior. Delante de este agujero encontramos una laminilla triangular llamada "Espina de Spix", que es donde se inserta el "ligamento esfenomandibular". De la parte inferior y posterior de esta cara encontramos un canal muy marcado denominado "canal milohiodeo" por donde recorren los nervios y vasos del mismo nombre. Igualmente encontramos en su parte inferior, diversas líneas de rugosidades donde se inserta el músculo pterigoideo interno.
Oblicuo de arriba abajo, representa un canal cuyos dos bordes se separan uno del otro a medida que descienden y se continúan respectivamente a nivel del cuerpo del hueso con las líneas oblicuas externa e interna respectivamente.
También denominado borde parotídeo (por su relación con la glándula parótida) tiene forma de S itálica, es redondeado y liso.
Continua con el borde inferior de la rama, en la unión de este borde con el borde posterior constituye el "ángulo de la mandíbula", importante para otros tipos de estudios.
Se compone de 2 eminencias, una anterior denominada apófisis coronoides (dónde se inserta el músculo temporal) y una posterior llamada cóndilo mandibular (que se articula con la cavidad glenoidea y forma la articulación temporomandibular) separados por la escotadura sigmoidea (por donde pasan los nervios maseterinos).




</doc>
<doc id="41534" url="https://es.wikipedia.org/wiki?curid=41534" title="Malinche">
Malinche

El término Malinche (también Malintzin) puede referirse a:


</doc>
<doc id="41537" url="https://es.wikipedia.org/wiki?curid=41537" title="Casa de Habsburgo">
Casa de Habsburgo

La Casa de Habsburgo (también llamada Casa de Austria) fue una de las más influyentes y poderosas casas reales de Europa. Los Habsburgo ocuparon el trono del Sacro Imperio Romano Germánico de forma continua entre 1438 y 1740. También ocuparon en distintos momentos los tronos de los reinos de España, Portugal, Bohemia, Inglaterra, Hungría y Croacia y el Segundo Imperio Mexicano.

Los Habsburgo eran originarios de un modesto castillo en Argovia, actual Suiza. El fundador de la dinastía, el conde Radbot de Habsburgo pasó a controlar el castillo como vasallo del duque de Suabia a principios siglo XI. Por medio de una ambiciosa política matrimonial y de alianzas, los Habsburgo comenzaron a extender su influencia hasta que en el siglo XIII Rodolfo I de Habsburgo, por entonces controlando partes de Alsacia y Argovia, logró ser nombrado rey de romanos en 1273 durante el interregno subsiguiente a la muerte de Federico II de Hohenstaufen. El conflicto entre Rodolfo I y el heredero de los Hohenstaufen, el rey Ottokar II de Bohemia, resultó en que los Habsburgo adquirieran los territorios de Austria, Estiria, Carniola y Carintia, que formarían parte de los territorios centrales de la familia durante el resto de su historia. En 1452 el influyente duque de Austria Federico III fue elegido emperador del Sacro Imperio Romano Germánico, y desde ese momento hasta su disolución los Habsburgo controlaron el trono del Sacro Imperio.

A partir del reinado del emperador Carlos V (1519-1556), la Casa de Habsburgo se dividió en dos ramas. La rama principal de los Austrias españoles, que gobernó el Imperio español, se extinguió en 1700 a la muerte de Carlos II de España sin descendencia. La rama cadete de los Habsburgo austríacos, que gobernó el Sacro Imperio Romano y los territorios de Austria-Hungría (y pasó a ser la primogénita tras la desaparición de la española), se extinguió en 1780 a la muerte de la emperatriz María Teresa I de Austria.

Los descendientes de la emperatriz María Teresa, pertenecientes a la rama Vaudemont de la Casa de Lorena, continuaron gobernando Austria y el Imperio Austro-Húngaro hasta 1918 bajo el nombre formal de Casa de Habsburgo-Lorena ("Habsburg-Lothringen"). Por ello, pese a que dinásticamente los Habsburgo-Lorena no pertenecen a la casa de Habsburgo, es habitual referirse a ellos como miembros de la casa de Habsburgo. En tiempos actuales, el cabeza de la casa de Habsburgo-Lorena fue el príncipe Otón de Habsburgo (1912-2011), y en la actualidad el jefe de dicha casa es el archiduque y príncipe Carlos de Habsburgo Lorena (desde 2007).

Los Habsburgo fueron:

Otras coronas controladas brevemente por la Casa fueron:

El nombre proviene del castillo suizo "Habichtsburg" (Castillo del halcón), la residencia familiar de los Habsburgo durante los siglos XI, XII y XIII, en lo que hoy es el Cantón de Argovia, en el antiguo ducado de Suabia, en la actual Suiza (Suiza no existía entonces en su forma actual, y las tierras suizas formaban parte principalmente del Sacro Imperio Romano Germánico). Desde el sudoeste de Alemania (principalmente Alsacia, hoy Francia, Brisgovia, Argovia y Turgovia) la familia extendió su influencia y asentamientos a los extremos del sudeste del Sacro Imperio Romano Germánico, aproximadamente lo que es hoy en día Austria (1278-1382). En solo dos o tres generaciones, los Habsburgo habían logrado obtener un alcance inicialmente intermitente en el trono imperial que duraría siglos (1273-1291, 1298-1308, 1438-1740 y 1745-1806).

Después del matrimonio de Maximiliano I con María, heredera de Borgoña (que controlaba los Países Bajos) y el matrimonio de su hijo Felipe el Hermoso con Juana, heredera de España y su recién fundado imperio, Carlos V heredó España, Italia del sur, Austria y los Países Bajos. En 1580 su hijo Felipe II heredó Portugal y sus colonias.

Bajo Maximiliano II, los Habsburgo adquirieron primero la tierra donde después construirían el "Palacio de Schönbrunn": el palacio de verano de los Habsburgo en Viena y uno de los símbolos más duraderos de la dinastía.

Después de la asignación el 21 de abril de 1521 de las tierras austríacas a Fernando I, por su hermano, el Emperador Carlos V (también rey Carlos I de España) (1516-1556), la dinastía se dividió en una rama austríaca y una española. Los Habsburgo austríacos llevaron (después de 1556) el título de Emperador del Sacro Imperio Romano Germánico, así como de las tierras hereditarias de los Habsburgo y los Reinos de Bohemia y Hungría, mientras los Habsburgo españoles gobernaban sobre los reinos españoles, los Países Bajos, las posesiones españolas en Italia provenientes de la Corona de Aragón, y durante un tiempo, Portugal. Hungría, nominalmente bajo el reinado de los Habsburgo desde 1526 pero en su mayor parte bajo ocupación turca otomana durante 150 años, fue reconquistada en 1683-1699.

Los Habsburgo españoles desaparecieron en 1700 por causa de un rey completamente disminuido e incapaz de gobernar, Carlos II de España aunque la dinastía de los Habsburgo españoles continuó con la descendencia de Don Juan José de Austria (único hijo ilegítimo reconocido por Felipe IV) y de Don Carlos Fernando de Austria y Manrique. Tras la muerte de Carlos II se produjo la Guerra de Sucesión Española, cosa que se repetiría con los Habsburgo austríacos en 1740, originando la Guerra de Sucesión Austriaca. Sin embargo, la heredera del último Habsburgo austríaco (María Teresa) se casó con Francisco Esteban, Duque de Lorena, ambos bisnietos del Emperador Habsburgo Fernando III, pero desde diferentes emperatrices. Sus descendientes continuaron la tradición de los Habsburgo de Viena bajo el nombre dinástico Habsburgo-Lorena. Se especula a menudo que los matrimonios consanguíneos entre ambas líneas contribuyeron a su extinción, pero hubo pocos matrimonios de este tipo en la línea austríaca. La muerte de las jóvenes herederas por viruela fue la causa.

El 6 de agosto de 1806 se disolvió el Sacro Imperio Romano Germánico bajo la reorganización de Alemania del emperador francés Napoleón Bonaparte. Sin embargo, como anticipación de la pérdida de su título de Emperador del Sacro Imperio Romano Germánico, Francisco II se declaró a sí mismo Emperador de Austria hereditario (como Francisco I) el 11 de agosto de 1804, tres meses después de que Napoleón se hubiera declarado a sí mismo Emperador de los franceses el 18 de mayo de 1804.

El emperador Francisco I de Austria usó el gran título oficial: «Francisco I, por la gracia de Dios Emperador de Austria; Rey de Jerusalén, Hungría, Bohemia, Dalmacia, Croacia, Eslavonia, Galitzia y Lodomeria; Archiduque de Austria; Duque de Lorena, Salzburgo, Wurzburgo, Franconia, Estiria, Carintia y Carniola; Gran Duque de Cracovia; Gran Príncipe de Transilvania; Margrave de Moravia; Duque de Sandomir, Masovia, Lublin, Alta y Baja Silesia, Auschwitz y Zator, Teschen y Friuli; Príncipe de Berchtesgaden y Mergentheim; Conde de Habsburgo, Gorizia y Gradisca y del Tirol; y Margrave de Alta y Baja Lusacia e Istria».

En 1867 se dio autonomía efectiva a Hungría bajo los términos del "Ausgleich" o 'compromiso' ("véase" Imperio austrohúngaro) hasta la deposición de los Habsburgo de Austria y Hungría en 1918 después de la derrota en la Primera Guerra Mundial.

El miembro actual de la familia de los Habsburgo es Carlos de Habsburgo-Lorena, el hijo mayor del Archiduque Otto.


Antes de la ascensión de Rodolfo a rey de Alemania, los Habsburgo fueron Condes en lo que es hoy el sudoeste de Alemania y Suiza.



Después de la muerte Rodolfo III en 1229 y de su hijo Godofredo I en 1271, los Habsburgo-Laufenburg se dividieron en los señores de Laufenburg y los condes de Klekgau. La primera rama de Laufenburg deriva de Rodolfo IV (†1315), hijo de Godofredo, y se extinguió en 1408 al morir Juan IV. La segunda rama de Klekgau deriva de Eberhard I, hijo de Godofredo, y se extinguió en 1415 al fallecer Eggon II, reuniéndose sus territorios con la línea principal de los Habsburgo.

En la Baja Edad Media, cuando los Habsburgo expandieron sus territorios hacia el este, a menudo gobernaron como duques del Ducado de Austria que cubría solo lo que hoy es Baja Austria y la parte oriental de Alta Austria. Las posesiones de los Habsburgo también incluían Estiria y entonces se expandieron hacia el oeste para incluir Carintia y Carniola en 1335 y Tirol en 1363. Sus dispersas posesiones originales en el sur de Alsacia, sudoeste de Alemania y Vorarlberg eran conocidas colectivamente como Austria Anterior. Los duques de Habsburgo perdieron gradualmente sus tierras originales al sur del Rin y el Lago de Constanza por la expansión de la Antigua Confederación Suiza. Los duques de Austria también gobernaron Austria Anterior hasta 1379; a partir de ese año, Austria Anterior fue gobernada por el conde de Tirol.


Debido a que ambos murieron a edad temprana, sus derechos revirtieron a su tío Alberto II.

A la muerte de Alberto II (1358), le heredaron sus hijos Rodolfo IV, Federico III (duque de Carintia, †1362), Alberto III y Leopoldo III; pero dado que los tres últimos eran menores, el gobierno efectivo lo ejerció Rodolfo IV.

A la muerte de Rodolfo IV, sus hermanos Alberto III y Leopoldo III gobernaron juntos las posesiones de los Habsburgo de 1365 a 1379, cuando dividieron los territorios en el Tratado de Neuberg, quedándose Alberto el Ducado de Austria y gobernando Leopoldo Estiria, Carintia, Carniola, Tirol y Austria Anterior.




Segismundo no tuvo hijos y adoptó a Maximiliano I, hijo del duque Federico V (emperador Federico III). Con Maximiliano, las posesiones de los Habsburgo se unirían de nuevo bajo un único gobernante, después de que reconquistara el Ducado de Austria tras la muerte de Matías Corvino, quien residió en Viena y se nombró duque de Austria (1485-1490).

Los Habsburgo reinaron en España constituyendo la estirpe denominada "Casa de Austria". En consecuencia, cada uno de aquellos monarcas y todo su linaje ostentaron el "de Austria", y nunca el "de Habsburgo" como apellido familiar. Así figura en todo tipo de documentos, inscripciones, lápidas, creaciones artísticas contemporáneas a ellos, etc. Solo desde finales del s.XX se empieza a generalizar de forma imprecisa el uso del apellido Habsburgo para referirse a los de la dinastía que rigió España durante los siglos XVI y XVII.

Con Carlos II se extinguió la línea de los Habsburgo españoles. Tras la Guerra de Sucesión se impidió el acceso al trono español a los Habsburgo austríacos en beneficio de la Casa de Borbón.





La descendencia del hermano del emperador Fernando II (archiduque Fernando III) fueron archiduques en Austria Anterior y Condes de Tirol desde que el emperador le cediera estos territorios en 1623.


Con María Teresa acaba la casa de Habsburgo, que derivó en sus sucesores en la Casa de Habsburgo-Lorena. La nueva Casa real surge cuando el emperador Carlos VI del Sacro Imperio Romano Germánico en sus últimos años de vida emite la Pragmática Sanción de 1713, donde se establece que su hija María Teresa I de Austria, sería la emperatriz germánica y su esposo solo su consorte. Así, los hijos de María Teresa y Francisco de Lorena serían los miembros de la nueva dinastía.


La Casa de Habsburgo-Lorena conservó Austria y las posesiones adjuntas después de la disolución del Sacro Imperio Romano Germánico.

Un hijo de Leopoldo II fue Rainiero de Austria, cuya mujer era de la Casa de Saboya; una hija Adelaida, Reina de Cerdeña, fue la mujer del rey Víctor Manuel II de Piamonte, Saboya y Cerdeña y rey de Italia. Sus hijos se casaron en las Casas Reales de Bonaparte; Sajonia-Coburgo-Gotha (Braganza, Portugal); Saboya (España), y los ducados de Montferrat y Chablis.


Casa de Habsburgo-Lorena, línea principal: Jefes de la Casa de Habsburgo (post-monarquía). Carlos I fue expulsado de sus dominios después de la Primera Guerra Mundial y el imperio fue abolido.


Francisco II asignó el Gran Ducado de Toscana a su segundo hijo Pedro Leopoldo, que a su vez lo asignó a su segundo hijo después de su ascensión como Emperador del Sacro Imperio Romano Germánico. Toscana permaneció como dominio de esta rama de cadetes de la familia hasta la unificación de Italia.

Casa de Austria-Toscana: línea de Toscana, posmonarquía

El ducado de Módena fue asignado a una rama menor de la familia por el Congreso de Viena. Se perdió con la unificación de Italia.

Casa de Austria-Este: línea de Módena, posmonarquía

El ducado de Parma fue asimismo asignado a un Habsburgo, pero no estuvo en la Casa mucho tiempo antes de sucumbir a la unificación de Italia. Fue otorgado a la segunda mujer de Napoleón Bonaparte, María Luisa Duquesa de Parma, una hija de Francisco II, Emperador del Sacro Imperio Romano Germánico, que era la madre de Napoleón II de Francia. Napoleón se había divorciado de su estéril esposa Marie Josèphe Rose Tascher de la Pagerie (más conocida en la historia como Josefina de Beauharnais) en favor de María Luisa.

Maximiliano, hermano del emperador de Austria Francisco José, fue convencido por una delegación mexicana del partido conservador, la Iglesia católica y Napoleón III para asumir el trono de México. El Imperio Mexicano terminó en una derrota militar por el partido liberal, ya que el apoyo militar francés fue retirado antes de tiempo por problemas internos que tuvo el emperador de Francia, aunado a las derrotas que los mexicanos liberales (apoyándose con armamento norteamericano) le causaban al ejército francés. Maximiliano fue fusilado por las tropas republicanas de Benito Juárez en el «Cerro de las Campanas» en Querétaro en 1867.

El ducado de Teschen fue sucedido por Carlos de Austria-Teschen, hijo de Leopoldo II y de María Luisa de Borbón, comandante en jefe del ejército austriaco durante las guerras napoleónicas.
Casa de Austria-Teschen: posmonarquía

La Casa Ducal de Hohenberg desciende de la condesa Sophie Chotek (1868-1914), que en 1900 se casó con el archiduque Francisco Fernando de Austria-Este (1863-1914), el heredero al trono de Austria-Hungría. Como su matrimonio era morganático, ninguno de sus hijos estaba en la línea de sucesión al trono austro-húngaro por los que a su muerte el archiduque Carlos, luego Carlos I, heredaría el trono. Representan la línea agnática superior de la Casa de Habsburgo-Lorena.
Casa de Hohenberg: posmonarquía

El reino de Hungría permaneció en la familia de los Habsburgo durante siglos; pero como el reino no fue estrictamente heredado (Hungría era una monarquía electiva hasta 1687) y fue a veces usado como un terreno de entrenamiento para los jóvenes Habsburgo, las fechas de gobierno no siempre encajan con las de las posesiones principales de los Habsburgo. Por eso, los reyes de Hungría están listados de forma separada.




El reino de Bohemia fue durante siglos una posición elegida por sus nobles. Como resultado, no era una posición automáticamente heredada. El rey de Bohemia tendía a ser un Habsburgo, pero no siempre. Así, los reyes de Bohemia y sus fechas de gobierno están listadas de forma separada.



Desde la ascensión de María Teresa, el reino de Bohemia se unió con las posesiones austríacas.

Dado que desde épocas modernas chocaron los intereses de los territorios regidos por los Habsburgo (Austria y España) y los regidos por las dinastías sucesoras de los Capeto (Valois y Borbón), se intentó suavizar las tensiones conviniendo matrimonios con reyes de Francia.




Durante el siglo XVI, la fortuna de los Habsburgo hízose en gran medida a partir de los ingresos procedentes de Castilla, América y los Países Bajos. Los banqueros Welser y Fugger adelantaban dinero a un imperio siempre necesitado de ingresos para costear sus innumerables conflictos militares en Europa.
La Paz de Westfalia de 1648 supuso la pérdida de poder real del emperador y una mayor autonomía de los trescientos cincuenta Estados resultantes. A todos los efectos, el Sacro Imperio Romano Germánico pasó a ser una confederación de Estados. Después se creó en 1658 la Liga del Rin que interfirió aún más en el poder de la Casa de Austria.





</doc>
<doc id="41539" url="https://es.wikipedia.org/wiki?curid=41539" title="Vómer">
Vómer

El vómer es un hueso de la cara, de forma laminar, cuadrangular, irregular, compacta, impar y mediano. Constituye la parte posterior del tabique nasal, que divide la nariz en fosas nasales izquierda y derecha.

Bastante irregularmente planas, las dos caras del vómer están directamente cubiertas por la membrana pituitaria. Presentan algunos surcos, más o menos marcados según los sujetos, en los cuales se alojan vasos y nervios. Uno de estos surcos más largo y ordinariamente más pronunciado que los otros llamado surco vomeriano, se dirige oblicuamente de arriba abajo y de atrás a delante y en él se aloja el nervio nasopalatino.

Está formado, en su totalidad, por hueso compacto. Es frágil , puede fracturarse y desplazarse hacia un lado, provocando desviaciones del tabique nasal.

Resulta este hueso de la fusión de dos láminas paralelas, las cuales son apreciables entre el sexto y séptimo mes de la vida fetal.

El vómer se articula con seis huesos:
En estado fresco se articula también con el cartílago del tabique nasal por su parte anterior.


</doc>
<doc id="41540" url="https://es.wikipedia.org/wiki?curid=41540" title="Hueso etmoides">
Hueso etmoides

El hueso etmoides () es un hueso del cráneo, corto y compacto, central, impar y simétrico, con forma de T; constituido por 4 partes: por una "lámina vertical" y media, una "lámina horizontal" perpendicular a la primera y dos masas laterales suspendidas en los extremos laterales de la lámina horizontal. Es un hueso de superficies muy anfractuosas y con numerosas cavidades (celdillas etmoidales).Está diseñado especialmente para contener las raíces nerviosas del nervio olfatorio y contribuir con la formación y protección de los espacios aéreos.

Se encuentra situado por debajo de la parte horizontal y llenando la "escotadura etmoidal" del hueso frontal y anterior al esfenoides. Se articula con estos y con los palatinos por detrás, con el hueso propio de la nariz por delante, con el maxilar superior y unguis por fuera y con el vómer y el cornete inferior por debajo.

Forma parte del suelo de la fosa craneal anterior y participa en el macizo facial (cavidad nasal y órbitas).

También denominada lámina cribosa del etmoides. Es una lámina cuadrangular con múltiples orificios para los nervios olfatorios, más prolongada en sentido anteroposterior, que se extiende de una masa lateral a otra. Su cara inferior forma parte de las fosas nasales; está dividida por la "cresta galli" en dos mitades, derecha e izquierda. Cada una de estas en el llamado canal etmoidal del olfatorio, que está acribillado de agujeros, por lo que recibe el nombre de lámina cribosa del etmoides.

También conocida como cresta de gallo por su forma, consta de dos bordes, una base y dos caras. Apófisis triangular
Está situada por encima de la lámina horizontal. Su base descansa en la lámina horizontal. Su vértice da inserción a la hoz del cerebro. Su borde anterior completa el agujero ciego.

Sale de la lámina cribosa hacia el inferior, formando la parte superior del tabique nasal.

También se conocen como "'laberintos del etmoides" o masas esponjosas. Contienen las celdas aéreas etmoidales que, según su localización, se denominan anteriores, medias o posteriores. Forman parte de la pared medial de las órbitas y de las paredes laterales (externas) de las fosas nasales. Se articulan con el hueso unguis, con el hueso frontal, con la apófisis orbitaria del hueso palatino y con el cuerpo del esfenoides.

El etmoides se articula con 13 huesos




</doc>
<doc id="41541" url="https://es.wikipedia.org/wiki?curid=41541" title="Hueso frontal">
Hueso frontal

El hueso frontal () es un hueso del cráneo, en el ser humano, es impar, central, simétrico y plano, con dos caras (endocraneal y exocraneal) y un borde circunferencial.

El hueso frontal se encuentra en la parte anterosuperior del cráneo por delante de los huesos parietales y un poco por arriba del esfenoides, y montado sobre el etmoides, y el macizo facial. El hueso frontal ocupa la superficie de la cara que se corresponde con la frente y la prominencia cubierta por las cejas.

El hueso frontal presenta dos porciones:

Visto en conjunto, el hueso frontal presenta dos caras y un borde. Una cara es posterior y cóncava: la cara endocraneal o cerebral; la otra cara es anterior, convexa hacia adelante: es la superficie exocraneal o cara cutánea. Ambas caras están separadas por un borde circunferencial o borde supraorbital.

La cara exocraneal del hueso frontal también se conoce como "cara cutánea" por ser la parte del hueso sobre la que se apoya parte de la piel facial.




Accidentes óseos sobre la cara endocraneal del hueso frontal:


El borde circunferencial es la línea ósea límite entre las caras exocraneal y endocraneal del frontal. Se lo puede dividir en dos segmentos:

El hueso frontal es un hueso plano, diseñado para conformar una cavidad ósea de protección y contención: el cráneo. La parte vertical del frontal está constituida (como la de todos los huesos craneales) por una doble lámina de hueso compacto: las tablas externa e interna, que encierran una lámina media de tejido esponjoso, llamado "diploe". Es el hueso más duro del cráneo.



El hueso frontal se articula con 12 huesos: huesos parietales (2), etmoides (1), esfenoides (1), maxilar (2), cigomático o malar (2), lacrimales (2), y nasales (2).


En la mayoría de los vertebrados, el hueso frontal está emparejado, en lugar de presentar una estructura única, fundida que se encuentra en los seres humanos . Por lo general se encuentra en la parte superior de la cabeza, entre los ojos, pero en muchos animales no mamíferos no forma parte de la cavidad orbital. En cambio, en los reptiles, peces óseos y anfibios que a menudo se separa de las órbitas por uno o dos huesos adicionales que no se encuentran en los mamíferos. Estos huesos, los prefrontales y postfrontales, juntos forman el margen superior de las cuencas de los ojos, y se encuentran a ambos lados de los huesos frontales.

El hueso frontal es una de las principales huesos emparejado por la línea media en los cráneos de los dinosaurios. Este hueso es parte del techo del cráneo, que es un conjunto de huesos que cubren el cerebro, los ojos y la nariz. El frontal se pone en contacto con varios huesos en el cráneo. La parte anterior del hueso se articula con el hueso nasal y el hueso prefrontal. La parte posterior del hueso se articula con el hueso postorbital y el hueso parietal. Este hueso define todos parte del borde superior de la órbita.



</doc>
<doc id="41543" url="https://es.wikipedia.org/wiki?curid=41543" title="Hueso parietal">
Hueso parietal

El hueso parietal es un hueso del cráneo, plano, par, de forma cuadrilátera, con dos caras, interna (endocraneal) y externa (exocraneal), y cuatro bordes con sus respectivos ángulos. El término "parietal" significa "de la pared".

Se encuentra cubriendo la porción superior y lateral del cráneo, por detrás del frontal, por delante del occipital y montado sobre el temporal y el esfenoides. Ambos huesos parietales se articulan, a través de una línea media: la sutura sagital.

Para el estudio del hueso parietal, se le reconocen dos caras, cuatro bordes y sus respectivos ángulos. Se detallan para cada parte, los accidentes óseos con más importancia.

Se corresponde con la cara lateral del cráneo, y en ella se distinguen:

Se corresponde con la cara medial del cráneo, y en ella se distinguen:

Se describen 5 bordes:

Se describen 4 ángulos:

Es un hueso plano y comparte la estructura de los huesos de la bóveda craneal. Dos tablas de tejido óseo compacto que cubren una región media de tejido esponjoso. Dos tablas de tejido óseo compacto, una externa y otra interna, cubren una región media de tejido óseo esponjoso denominada diploe

Cada hueso parietal está formado por la osificación endomembranosa de un único primordio mesenquimal fibroso que se desarrolla hacia la mitad del segundo mes de vida fetal.

El hueso parietal se articula mediante sinfibrosis con otros huesos craneales:


En otras especies de vertebrados , los huesos parietales forman típicamente la parte posterior o central del techo del cráneo, situada detrás de los huesos frontales. En muchos tetrápodos, están bordeando en la parte posterio por un par de huesos postparietales que pueden estar únicamente en el techo del cráneo, o hacia abajo de pendiente para contribuir a la parte posterior del cráneo, dependiendo de la especie. En los tuatara y muchas especies fósiles del mismo género, hay una pequeña abertura, el foramen parietal, se encuentra entre los dos huesos parietales. Esta apertura es la ubicación de un tercer ojo en la línea media del cráneo, que es mucho menor que los dos ojos principales

El hueso parietal está normalmente presente en el extremo posterior del cráneo y está cerca de la línea media. Este hueso es parte del techo del cráneo, que es un conjunto de huesos que cubren el cerebro, los ojos y la nariz. Los huesos parietales se ponen en contacto con varios otros huesos en el cráneo. La parte anterior del hueso se articula con el hueso frontal y el hueso postorbital. La parte posterior del hueso se articula con el hueso escamoso, y con menos frecuencia el hueso supraoccipital. los huesos de soporte de los de los volantes de la nuca de los ceratópsidos se formaron por las extensiones del hueso parietal. Estos lujos, que sobresalen por encima del cuello y se extienden más allá del resto del cráneo es un rasgo de diagnóstico de ceratópsidos. Las cúpulas del cráneo reconocibles presentes en paquicefalosaurios se formaron por la fusión de los huesos frontal y parietal y la adición de gruesos depósitos de hueso para esa unidad.


</doc>
<doc id="41544" url="https://es.wikipedia.org/wiki?curid=41544" title="Emiliano de la Cogolla">
Emiliano de la Cogolla

Emiliano de la Cogolla (en latín figura en ocasiones como "Æmilianus", también conocido como Millán, Berceo, 473 - Monasterio de San Millán de Suso, 574) fue un ermitaño, discípulo de Felices de Bilibio, considerado santo. Las reliquias de ambos santos se conservan en el Monasterio de San Millán de Yuso.

Hijo de un pastor, Millán ejerció ese oficio hasta la edad de veinte años. Desde finales del siglo IV se dio con cierta frecuencia entre los cristianos de Occidente la conversión ascética. Millán fue uno de estos que eligió ser un eremita ascético en un lugar retirado. El sitio escogido fue en medio de una exuberante vegetación, en la vertiente oriental de la sierra de la Demanda, que separa la meseta del valle del Ebro. En la roca del monte excavó su propia celda. Fue ordenado sacerdote por el Dídimo del pueblo de Berceo en el año 560. Tras haber experimentado la vida eremítica y clerical, decidió volver a la soledad a la vida monástica al monasterio de Suso, donde murió a los 101 años.

La iniciación como ermitaño la comenzó con otro eremita llamado Félix, del que se dice fue "varón santísimo" y con quien estaría tres años en los Riscos de Bilibio próximos a Haro. Después marchó a refugiarse en los montes Distercios o Cogollanos, rincón escondido en el que levantaría altares y donde vivió 40 años en soledad.

Sabedor el obispo Dídimo de Tarazona de sus virtudes, le nombró sacerdote de su villa natal, Berceo, cargo que ejerció durante tres años. Otros clérigos le acusaron de malgastar los bienes eclesiásticos, dada su generosidad con los menesterosos, por lo que se apartó a las cuevas de Aidillo, lugar donde se construiría más adelante el Monasterio de Suso. Rápidamente se le unieron otros clérigos: Aselo, Cotonato, Geroncio, Sofronio, etc., incluso una mujer llamada Potamia, venida de Narbona. Este grupo iría incrementándose en lo sucesivo.

Cerca del año 550, siendo rey Agila I, excavaron nuevas cuevas, colocadas en dos pisos que estaban unidos por un pozo, donde habitaba Millán. Allí falleció y fue enterrado a la edad de 101 años.

Con la llegada de los árabes no cambió nada en aquel lugar y sus alrededores. Las tierras de las cuencas del Ebro y del Duero eran tierras de nadie, habitadas únicamente por ermitaños como Millán.

Su sepulcro se convirtió en lugar de peregrinaje al que acudían condes y reyes castellanos para encomendar sus batallas contra los musulmanes.

Varias fuentes, entre ellas la "Historia de la traslación" y el "Libro de los milagros de san Millán", narran cómo el rey navarro García Sánchez, tras inaugurar en 1052 el Monasterio de Santa María la Real de Nájera, quiso enriquecerlo trayendo los cuerpos de Santos de la comarca. Así, el 29 de mayo de 1053 intentó llevar a dicho monasterio los restos de Millán sin conseguirlo, por el milagro de los bueyes que no querían continuar con el traslado. Por este milagro decidió construir un nuevo monasterio para albergar su cuerpo en el lugar donde los bueyes habían quedado parados, este sería el Monasterio de Yuso.

La vida de Millán la contó, en latín y por primera vez, Braulio de Zaragoza, obispo, cincuenta años después de su fallecimiento, indicando que su lugar de nacimiento fue Berdeio en una famosa "Vita Aemiliani".

Más tarde, en el siglo XIII, contó su vida Gonzalo de Berceo en versos alejandrinos con el título "Estoria del Sennor Sant Millan", aunque esta obra es más conocida como "Vida de San Millán de la Cogolla".

Finalmente, en la misma época en que vivió el primer poeta conocido en lengua castellana, un monje del mismo Monasterio de San Millán de la Cogolla escribió dos obras en latín dando noticia de la traslación de las reliquias del santo riojano y los milagros que sucedieron post mortem. Estas obras, cuyos títulos son "Liber Translatinis Sancti Emiliani" y "Miracula Beati Emiliani", fueron traducidas en el siglo XV como "Istoria de la traslación del glorioso cuerpo del bienaventurado sant Millán" y "Libro de los miraglos del bienaventurado señor sant Millán".

En el siglo X, en tiempos de García Sánchez I de Navarra y el conde Fernán González de Castilla, se multiplicaron los milagros de san Millán junto a su tumba y el cenobio pasó a ser lugar sagrado.

En la batalla de Simancas, año 939, el rey Ramiro II de León , el conde Fernán González de Castilla y García Sánchez I de Pamplona del reino de Pamplona-Nájera se enfrentan al califa cordobés Abd al-Rahman III. Según la tradición, Santiago Apóstol acompaña a San Millán y ambos se aparecen en mitad del combate en defensa de los cristianos. San Millán fue elevado a patrono de castellanos y navarros comprometiéndose a pagar tributos; son los llamados "Votos de San Millán". Fernán González favorecerá enormemente al monasterio de San Millán con privilegios y donaciones.

Gonzalo de Berceo en su "Vida de San Millán" cuenta la promesa de los votos legendarios, de una parte Ramiro II de León a Santiago y de la otra, Fernán González a San Millán. Luego refiere la maravillosa aparición de ambos patronos en la batalla de Hacinas, en la que elogia la intervención a favor de los vasallos con estos versos:

Pese a la "imposición" del patronazgo de Santiago tras la unificación de Castilla y León, los castellanos continuaron reclamando que San Millán era su patrono y así en tiempos de Enrique II de Castilla en 1373, la Universidad de Ciudad y Tierra de Ávila llegó a negarse a pagar el voto a Santiago y sus procuradores llevaron el asunto a las Cortes. Los castellanos pagaban el Voto a San Millán.

En el siglo XVII, al desarrollarse un amplio debate sobre patronos, San Millán volverá a ser reclamado como patrón de Castilla y por lo mismo copatrón de España junto a Santiago, patronazgo que se mantuvo en los misales hasta la reforma litúrgica del concilio Vaticano II.

La iconografía de San Millán es muy rica, ya que puede aparecer como pastor, ermitaño, monje benedictino o sacerdote diocesano. Al ser patrón de Castilla y Navarra, en muchas iglesias de pueblos aparece de distintas maneras.

En los marfiles del arca relicario aparece como pastor o como venerable sacerdote curando enfermos o enfrentándose a Satán, al que vence una y otra vez.
En la portada del Monasterio de San Millán de Yuso y en el altar mayor, lo mismo que en la iglesia de san Millán y san Cayetano de Madrid, aparece representado san Millán luchando contra los moros, al igual que el apóstol Santiago. Es la iconografía que se pone de moda en el Barroco.

Aparte la montaña de mayor altitud de la provincia de Burgos (2131m) ubicada en la sierra de la demanda tiene su nombre Pico San Millán.

La Cruz de San Millán está en el epitafio [corrección: ¿cenotafio?] románico del siglo XII del Monasterio de Suso y es el emblema desde 1980 de la Asociación Amigos de San Millán. Es una cruz visigótica de plata, de ocho cabos, fileteada de gules y diapedrada en sinople. Cargada con cruz de plata de cuatro cabos iguales rematados en tres salientes trebolados, el central más alto, fileteada de gules y cargada en abismo con florón de plata fileteado de gules.





</doc>
<doc id="41546" url="https://es.wikipedia.org/wiki?curid=41546" title="Idioma danés">
Idioma danés

El danés "(dansk)" es una lengua nórdica, correspondiente a un grupo de las lenguas germánicas, a su vez, de la familia indoeuropea. Es hablado por cerca de seis millones de personas, y es el idioma oficial de Dinamarca y cooficial en las Islas Feroe (territorio danés). Aunque se habla también en Groenlandia (también territorio danés), no es oficial.

Por motivos históricos de dominación danesa y de intercomunicación entre los países acogidos al Consejo Nórdico, en Islandia se enseña esta lengua en las escuelas, lo que permite comunicarse a los islandeses en danés con cierta competencia.

La mayor parte del léxico danés procede del antiguo nórdico, del cual se han formado nuevas palabras mediante composición. Sin embargo, un porcentaje considerable de vocabulario procede del bajo alemán, (por ejemplo, "betale" = 'pagar', "måske" = 'a lo mejor'. La relación genética e histórica entre el inglés y el danés hace que numerosas palabras de ambas lenguas se parezcan; por ejemplo, "næse" ("nose"), "blå" ("blue"). Sin embargo, la pronunciación de estas palabras en ambas lenguas varía considerablemente. Esta relación está marcada especialmente por la invasión vikinga de parte del norte de Inglaterra.

Algunos ejemplos de escritores famosos de obras en danés son el filósofo existencial Søren Kierkegaard, el prolífico escritor de fábulas Hans Christian Andersen y el dramaturgo Ludvig Holberg. Tres autores daneses del siglo XX han recibido el Premio Nobel de literatura: Karl Adolph Gjellerup y Henrik Pontoppidan, que lo recibieron en 1917; y Johannes Vilhelm Jensen, galardonado en 1944.

La primera traducción de la Biblia al danés se publicó en 1550.

Los parientes más cercanos del danés son las demás lenguas germánicas del norte de Escandinavia: noruego y sueco. En particular, las formas escritas del danés y del noruego bokmål son muy cercanas, aunque la pronunciación de los tres difiere significativamente. Los hablantes de cualquiera de las tres lenguas, con algo de práctica, pueden entender las otras dos. Las similitudes son tantas que algunos lingüistas los clasifican como tres dialectos de un mismo idioma.

El danés se habla en Dinamarca, y tiene presencia en Groenlandia, las Islas Feroe (Ambos territorios daneses), en las Islas Vírgenes de los Estados Unidos y la región alemana de Schleswig-Holstein.

El danés es el idioma oficial de Dinamarca, y es uno de los dos idiomas oficiales del territorio danés de las Islas Feroe (cooficial con el feroés). Además, existe una pequeña, pero importante, comunidad de hablantes del danés en la parte de Alemania que colinda con Dinamarca.


En los colegios se enseña el danés oficial, llamado Rigsdansk (danés del reino), una forma "neutral" que es la que se habla en las grandes ciudades.

Dinamarca es un país compuesto por cerca de 400 islas. Existen innumerables dialectos porque las pequeñas islas o comunidades desarrollan los suyos. En las zonas rurales se hablan dialectos prácticamente incomprensibles para los ciudadanos que hablan Rigsdansk, aunque las nuevas generaciones hablan dialectos adaptados que son entendidos en toda Dinamarca.

El danés es plano y monótono comparado con los tonales noruego y sueco. La 'r' es uvular, muy profunda y producida detrás de la garganta, contrario a lo que pasa en los idiomas eslavos o romances, como el español.

Dos características importantes son, además:

El danés moderno estándar tiene 14 fonemas vocálicos. Todas menos dos de estas vocales pueden ser largas o cortas, siendo las excepciones schwa y . Las realizaciones largas y cortas a menudo difieren en calidad y hay varios alófonos que difieren si ocurren junto a una . Por ejemplo, es más baja cuando se da antes o después y es pronunciada cuando es larga.

 son ensordecidas en todos los contextos. a menudo tienen una pequeña fricación, pero se pronuncian normalmente como aproximantes. La distinción entre , / y solo se hace al comienzo de la palabra o al comienzo de una sílaba tónica. De ahí que "lappe" y "labbe" se pronuncien . La combinación de se realiza como alveolo-palatal fricativa, , haciendo posible postular un fonema tentativo en danés. se puede describir como "tautosilábica", es decir, que toma la forma de una consonante fonética o una vocal. Al comienzo de una palabra o después de una consonante, se pronuncia como una fricativa uvular, , pero en la mayoría de las otras posiciones como una vocal central baja, (que es casi idéntica a como se pronuncia /r/ a menudo en alemán) o simplemente se combina con la vocal precedente. El fenómeno es comparable también con las pronunciaciones no róticas del inglés.

Existen dos géneros gramaticales para los sustantivos daneses: "común" y "neutro". Aunque la mayor parte de los sustantivos (75 %) son "comunes", el género no es fácilmente predecible y, en general, debe memorizarse. 

Una característica distintiva de los idiomas escandinavos, entre ellos el danés, es el artículo definido enclítico. Un ejemplo: "un hombre" (indefinido) es: "en mand", pero "el hombre" (definido) es "manden". En ambos casos, el artículo es "en". 

El artículo indefinido toma las siguientes formas:

El definido es como sigue:

Sin embargo, cuando el sustantivo es precedido por un adjetivo, este declina y el artículo definido enclítico (al final) se pierde y se reemplaza por una partícula:

La declinación normal del adjetivo es como sigue, para el adjetivo "smuk" - bonito.

La mayoría de adjetivos permanecen inalterados cuando preceden a un sustantivo común, pero añaden -t antes de sustantivos neutros y -e antes de plurales y en construcciones con artículos definidos.

Sin embargo, existen muchas excepciones. Principalmente:


Los comparativos siguen un patrón semejante al del inglés, con declinaciones en el caso de adjetivos cortos y con las palabras "mere" (más) y "mest" (el más) para los largos:


Hay excepciones notables:


El infinitivo de los verbos daneses termina en vocal, que casi siempre es la letra "e". Los verbos se conjugan según el tiempo verbal, pero no varían según la persona ni el número. Por ejemplo, el presente de "comer" ("spise") es "spiser", que es la misma forma verbal sin importar que el sujeto esté en primera, segunda o tercera persona, ni que esté en singular o plural.

Se clasifica a los verbos daneses en tres grupos. El primero constituye la mayor parte, y sus desinencias son "-ede" para el pasado y "-et" para el participio:


El segundo grupo forma los pasados con "-te" y los participios pasados con "-t":


El último grupo está constituido por los verbos irregulares:

El tiempo futuro, como tal, no existe. El tiempo presente con alguna indicación de futuro sirve como sustituto: "jeg kommer i morgen", "vendré mañana". Igualmente, se pueden usar algunos verbos auxiliares:

ville - jeg vil komme (vendré)
skulle - jeg skal komme (vendré)

Como las demás lenguas de su familia, el danés tiene palabras compuestas, aunque su formación no es tan libre como la del alemán. 

Un ejemplo de tales combinaciones es el siguiente:


Para formar "kvindehåndboldlandsholdet", equipo nacional femenino de balonmano.

Los números del uno al veinte en danés son: "en", "to", "tre", "fire", "fem", "seks", "syv", "otte", "ni", "ti", "elleve", "tolv", "tretten", "fjorten", "femten", "seksten", "sytten", "atten", "nitten" y "tyve". Para contar más allá del 40, se emplea un sistema parcialmente vigesimal (base 20):


También existe el sistema familiar en sueco, terminado en -ti(o), pero no se usa con frecuencia. Este sistema fraccionario de conteo se utiliza también para dar la hora; por ejemplo: ""halvtre"" (literalmente, "media para las tres", o "mitad de la tercera hora") es la expresión que se usa para "dos y media".

El danés se escribe con el alfabeto latino, con las tres letras adicionales ya mencionadas (æ, ø, å). Antes de la reforma ortográfica de 1948, se empleaba "aa" en lugar de "å", y el uso antiguo se sigue dando en documentos antiguos y nombres. Actualmente, en el orden alfabético, "aa" y "å" son equivalentes, a pesar de que "aa", a primera vista, son dos letras. Lo mismo sucede con "ø", que viene de "oe", y con "æ", que viene de "ae".

Los ejemplos más antiguos conservados de la escritura danesa (de la edad de Hierro y la era Vikinga) están en alfabeto rúnico. La introducción del cristianismo también trajo la escritura latina a Dinamarca, y al final de la Alta Edad Media, las Runas habían sido más o menos sustituidas por las letras latinas.

Al igual que en Alemania, las letras Fraktur (góticas) aún eran de uso común en el (hasta 1875, a los niños daneses se les enseñó a leer las letras Fraktur en la escuela), y muchos libros fueron impresos con el tipo de letra Fraktur incluso al comienzo del siglo XX, sobre todo por los conservadores. Sin embargo, el alfabeto latino fue utilizado por los modernistas, por ejemplo, la Real Academia Danesa de Ciencias y Letras cambió de estilo en 1799. Los sustantivos se escribían utilizando mayúsculas, como en alemán, hasta la reforma ortográfica de 1948.

El teclado danés tiene teclas para Æ, Ø, y Å. En los teclados noruegos, en su lugar, tienen Æ y Ø.

El alfabeto danés moderno es similar al inglés, con tres letras adicionales: æ, ø y å, que se encuentran al final del alfabeto, en ese orden. 

Una reforma de la ortografía en 1948 introdujo la letra å, aún en uso en Noruega y Suecia, en el alfabeto danés para sustituir el dígrafo aa; el antiguo uso persiste en algunos nombres personales y geográficos (por ejemplo, el nombre de la ciudad de Aalborg se escribe con Aa siguiendo una decisión por parte del Ayuntamiento en los años 70). Al representar el sonido å, aa es tratada como å en la ordenación alfabética, incluso aunque parezcan ser dos letras. Cuando las letras no están disponibles debido a limitaciones técnicas (por ejemplo, en las direcciones URL), a menudo son reemplazadas por ae (Æ, æ), œ o 'o' (Ø, ø), y aa (Å, å), respectivamente.

El danés y el noruego moderno usan el mismo alfabeto, aunque la ortografía es ligeramente diferente.

Véase: 

Braille danés




</doc>
<doc id="41549" url="https://es.wikipedia.org/wiki?curid=41549" title="Accidente cerebrovascular">
Accidente cerebrovascular

El accidente cerebrovascular (ACV) es una afección médica en la que el flujo sanguíneo deficiente al cerebro produce muerte celular. Se consideran sinónimos ictus, infarto cerebral, derrame cerebral o, menos frecuentemente, apoplejía o ataque cerebrovascular.

Según la Organización Mundial de la Salud los ACV son, junto a la enfermedad de las arterias coronarias, las principales enfermedades cardiovasculares. El mismo organismo internacional estima que en 2015 murieron 17.7 millones de personas a causa de las enfermedades cardiovasculares. Del total de estas muertes, 6.7 millones corresponde a los ACV.
La principal causa es la presión arterial elevada, a la que sigue el sedentarismo (poca movilidad corporal, en especial de las extremidades inferiores: falta de caminatas que duren al menos media hora al día), el alto consumo de radicales provenientes, entre otros, del tabaco, frituras o grasas hidrogenadas, a lo que puede sumarse el consumo excesivo de alcohol, de tabaco o de drogas, así como padecer problemas cardíacos como la fibrilación auricular u otras afecciones inicialmente no cardíacas ni vasculares como diabetes o estrés; cualquiera de estos factores, o más de uno al mismo tiempo, predisponen a sufrir ACV. La enfermedad celíaca puede provocar, si no se trata, este tipo de ataques, especialmente en personas jóvenes y niños; no obstante, suele pasarse por alto, sin reconocer ni diagnosticar, principalmente por cursar sin síntomas digestivos evidentes. Los retrasos en el diagnóstico pueden hacer que los daños sean irreversibles.

El ataque cerebrovascular tiene dos formas bien diferenciadas:

Las enfermedades cerebrovasculares constituyen, en la actualidad, uno de los problemas de salud pública más importantes. Son la tercera causa de muerte en el mundo occidental, la primera causa de invalidez permanente entre las personas adultas y una de las principales causas de déficit neurológico en el anciano. No obstante, se ha demostrado que los ataques cerebrovasculares (ACV) en niños de 0 a 14 años son los que tienen mayor tasa de recuperación, debido a que tienen un cerebro flexible y joven.

El daño cerebral supone una ruptura en la trayectoria vital del paciente y, por su elevado coste sociosanitario, condiciona las situaciones familiares, sociales e institucionales.

Prevalencia:

Incidencia por edad y sexo:

Mortalidad:

Morbilidad:

Otros problemas que presentan derivados del ictus se refieren a: epilepsia, espasticidad, incontinencia urinaria, problemas intestinales, úlceras de decúbito, etc.

El número de personas afectadas, la duración, gravedad y variedad de las secuelas, su repercusión en la calidad de vida de los afectados y sus familias, sus consecuencias económicas y productivas convierten al daño cerebral adquirido en un problema sociosanitario de primera magnitud.

Según su etiología, un ataque cerebrovascular (ACV) tiene dos variantes: isquémicos (embólico y trombótico) y hemorrágicos. El cuadro clínico es variado y depende del área encefálica afectada.

Un ataque cerebrovascular isquémico o ataque cerebrovascular oclusivo, también llamado infarto cerebral, se presenta cuando la estructura pierde la irrigación sanguínea debido a la interrupción súbita e inmediata del flujo sanguíneo, lo que genera la aparición de una zona infartada y es en ese momento en el cual ocurre el verdadero «infarto cerebral», y se debe solo a la oclusión de alguna de las arterias que irrigan la masa encefálica, ya sea por acumulación de fibrina o de calcio o por alguna anormalidad en los eritrocitos, pero generalmente es por arterioesclerosis (también ateroesclerosis, de ateroma) o bien por un émbolo (embolia cerebral) que procede de otra localización, fundamentalmente el corazón u otras arterias (como la bifurcación de la carótidas o del arco aórtico). La isquemia de las arterias cerebrales puede producirse por los siguientes mecanismos y procesos:

Estenosis de las arterias (vasoconstricción) reactiva a multitud de procesos (vasoespasmo cerebral). Con frecuencia se debe a una disminución del gasto cardíaco o de la tensión arterial grave y mantenida, lo que genera una estenosis y el consecuente bajo flujo cerebral. 

Es muy importante controlar la fibrilación auricular cardíaca, ya que las fibrilaciones del corazón forman trombos que pueden llegar al cerebro, provocándole ACVs.

Se forma un coágulo (trombo) en una de las arterias que irrigan el cerebro, lo que provoca la isquemia; este fenómeno se ve favorecido por la presencia de placas de aterosclerosis en las arterias cerebrales.
Es consecuencia de un coágulo formado en una vena de otra parte del cuerpo (émbolo) y que, tras desprenderse total o parcialmente, viaja hacia el cerebro a través del torrente sanguíneo. También puede deberse a otro material llegado al torrente circulatorio por diferentes motivos. Habitualmente es un coágulo formado en el corazón, o también una fractura (embolismo graso), un tumor (embolismo metastásico), un fármaco o incluso una burbuja de aire. Al llegar a las pequeñas arterias cerebrales, el émbolo queda encallado cuando su tamaño supera el calibre de estas, dando lugar al fenómeno isquémico.

Estenosis por fenómenos compresivos sobre la pared vascular: abscesos, quistes, tumores y otros.

Se deben a la rotura de un vaso sanguíneo encefálico debido a un pico hipertensivo o a un aneurisma congénito. Pueden clasificarse en: intraparenquimatosos y hemorragia subaracnoidea.

La hemorragia conduce al ataque cerebrovascular (ACV) por dos mecanismos. Por una parte, priva de riego al área cerebral dependiente de esa arteria, pero por otra parte la sangre extravasada ejerce compresión sobre las estructuras cerebrales, incluidos otros vasos sanguíneos, lo que aumenta el área afectada. Ulteriormente, debido a las diferencias de presión osmótica, el hematoma producido atrae líquido plasmático, con lo que aumenta nuevamente el efecto compresivo local. Es por este mecanismo por lo que la valoración de la gravedad y el pronóstico médico de una hemorragia cerebral se demora 24 a 48 horas hasta la total definición del área afectada. Las causas más frecuentes de hemorragia cerebral son la hipertensión arterial y los aneurismas cerebrales.

Los síntomas de un ataque cerebrovascular son muy variados, en función del área cerebral afectada: pueden ser síntomas puramente sensoriales o puramente motores o una combinación de ambos (sensitivomotores). Los más frecuentemente diagnosticados son:


No obstante, numerosos cuadros de ataque cerebrovascular (ACV) de baja intensidad y duración pasan inadvertidos, debido a lo anodino de la sintomatología: parestesias, debilidad de un grupo muscular poco específico (su actividad es suplida por otros grupos musculares), episodios amnésicos breves, pequeña desorientación y otros. Son estos síntomas menores los más frecuentes, y tienen una gran importancia, porque dan un aviso prematuro acerca de la patología subyacente.

En realidad los primeros auxilios que corresponden a un ataque cerebrovascular (ACV), una hemorragia cerebral o ictus debe llevarlos a cabo, lo más pronto posible, el personal médico, y deberá mantenerse, mientras tanto, a la persona afectada en la mayor calma e inmovilidad posibles (sin esfuerzos ni violencia) hasta la llegada del personal médico (sin administrar al afectado ningún fármaco no prescrito por autoridad médica). Las cuatro primeras horas son cruciales para la atención de quien sufre un ACV, y durante ese lapso es necesaria la participación del personal médico.

Para considerar la existencia de un ACV, por leve que este sea ―es necesario recordar que un ataque cerebrovascular (ACV) leve puede transformarse en uno grave―, se debe tener en cuenta el siguiente cuadro sintomático, llamado en inglés "FAST" (‘rápido’, en la traducción al español, ya que ante estos síntomas la atención médica debe ser urgente), que en inglés son las iniciales de "Face Arm Speech Time", ‘prueba de rostro ("face"), brazos ("arms") y habla ("speech")’, que consiste en lo siguiente: 


Se requiere de un programa de rehabilitación interdisciplinaria que provea una asistencia integrada para las personas que han sobrevivido a un ataque cerebral. Esta debe atender tanto los aspectos motores como los relacionados con el habla, los trastornos visuales, las actividades de la vida diaria y las secuelas incapacitantes como la espasticidad, para que el sobreviviente del ACV puedan alcanzar un grado de independencia suficiente como para retomar, al menos parcialmente, sus actividades habituales. Este equipo interdisciplinario debe estar formado por fisioterapeutas, neuropsicólogos, fonoaudiólogos (logopedas) , terapeutas ocupacionales, y los relacionados con la medicina, como el médico rehabilitador o fisiatra, el psiquiatra y el neurólogo. 

Otro grupo que se ve afectado luego de un ACV son los familiares y amigos de la persona quienes requieren de orientación sobre la mejor manera de acompañar a la persona que se está recuperando de su ataque cerebral. Esto fundamentalmente porque, ante la incertidumbre y angustia en la que se encuentran, pueden actuar obstaculizando el proceso de rehabilitación.

Una revisión sistemática de 15 estudios, la mayoría realizados en Asia, particularmente China, y uno en Sudáfrica, encontró evidencia de que la rehabilitación comunitaria genera un impacto positivo en personas con discapacidades. De seis estudios centrados en personas con discapacidades físicas, tres mostraron efectos beneficiosos para casos de apoplejía. Resulta necesaria una evaluación costo-efectividad de las rehabilitaciones que permita evaluar la asignación de los recursos. 


El 29 de octubre es el Día Mundial de Lucha contra el ACV.

En 2016, el programa La Marató de TV3 hizo una campaña para conseguir dinero para investigar sobre el Ictus y las lesiones medulares y cerebrales traumáticas.




</doc>
<doc id="41550" url="https://es.wikipedia.org/wiki?curid=41550" title="Acetona">
Acetona

La acetona o propanona es un compuesto químico de fórmula CH(CO)CH del grupo de las cetonas que se encuentra naturalmente en el medio ambiente. A temperatura ambiente se presenta como un líquido incoloro de olor característico. Se evapora fácilmente, es inflamable y es soluble en agua. La acetona sintetizada se usa en la fabricación de plásticos, fibras, medicamentos y otros productos químicos, así como disolvente de otras sustancias químicas.

Este compuesto se disuelve fácilmente en el agua, ya que es un compuesto orgánico polar, interacciona fácilmente con otros compuestos polares como los del agua, generando relaciones de momentos dipolo-dipolo, atrayendo las cargas parciales contrarias del otro compuesto.

La síntesis a escala industrial de la acetona se realiza mayoritariamente (90% de la capacidad en los EE.UU.) según el proceso catalítico de hidrólisis en medio ácido del hidroperóxido de cumeno, que permite también la obtención de fenol como coproducto, en una relación en peso de 0,61:1

Un segundo método de obtención (6% de la capacidad de los Estados Unidos en 1995) es la deshidrogenación catalítica del alcohol isopropílico.

Otras vías de síntesis de acetona:


En los EE.UU, el 90% de la producción de acetona manufacturada se realiza mediante el proceso de peroxidación de cumeno con una capacidad que alcanzó en el 2002 los 1,839 millones de toneladas. En el 2002 la capacidad se repartía en 11 plantas de fabricación pertenecientes a 8 compañías.

Miles de toneladas:

Total 1,839 millones de toneladas

Según el directorio de la Federación empresarial de la industria química española (FEIQUE) las siguientes compañías se dedican en España a la fabricación industrial de acetona:


La acetona es uno de los disolventes generales que más empleo tienen en la técnica industrial y profesional, debido a sus excelentes propiedades disolventes. Es un eficaz quitamanchas y es muy utilizado para quitar el esmalte de las uñas. También es un excelente disolvente de los barnices y pinturas oleosas (esmaltes sintéticos o "pinturas de aceite"). 

La repartición de las aplicaciones del uso de acetona en los EE.UU. se encontraba en el 2002 en los siguientes segmentos:


La aplicación más importante de la acetona se encuentra en la fabricación de Metil metacrilato (MMA), mercado que experimenta una demanda creciente (3 % anual) desde el 2002 por el incremento en los usos del Polimetilmetacrilato (PMMA), un material antifragmentación alternativo al vidrio en la industria de la construcción.

La demanda de Bisfenol-A y de resinas de policarbonato se ha duplicado en la década de los 1990, convirtiéndose en la segunda aplicación importante de la acetona (7 % incremento anual), demandada por la industria del automóvil y de microelectrónica (fabricación de discos CD y DVD).

La demanda de acetona es un indicador del crecimiento económico de cada región ya que depende directamente de la marcha de las industrias del automóvil, construcción y microelectrónica. Así entre el 2000-2001 la demanda decreció un 9 % mientras que en el 2002 apuntó una recuperación como resultado del resurgimiento económico estadounidense.

En los EE.UU. la demanda interna en el 2002 fue de 1,188 millones de toneladas, con un crecimiento medio en el periodo 1997-2002 del 0,9 %. En el 2006 la demanda prevista era de 1,313 millones de toneladas.

Se encuentra en forma natural en plantas, árboles y en las emisiones de gases volcánicos o de incendios forestales, y como producto de degradación de las grasas corporales. También se encuentra presente en los gases de tubos de escape de automóviles, en humo de tabaco y en vertederos. Los procesos industriales aportan una mayor cantidad de acetona al medio ambiente que los procesos naturales.

La acetona se forma en la sangre cuando el organismo utiliza grasa en vez de glucosa como fuente de energía. Si se forma acetona, esto usualmente indica que las células carecen de suficiente insulina o que no pueden utilizar la presente en la sangre para convertir glucosa en energía. La acetona sigue su curso corporal hasta llegar a la orina. El aliento de personas que tienen gran cantidad de acetona en el organismo exhala olor a fruta y a veces se le denomina "aliento de acetona". 

Si una persona se expone a la acetona, ésta pasa a la sangre y es transportada a todos los órganos en el cuerpo. Si la cantidad es pequeña, el hígado la degrada a compuestos que no son perjudiciales que se usan para producir energía para las funciones del organismo. Sin embargo, respirar niveles moderados o altos de acetona por períodos breves puede causar irritación de la nariz, la garganta, los pulmones y los ojos; dolores de cabeza; mareo; confusión; aceleración del pulso; efectos en la sangre; náusea; vómitos; pérdida del conocimiento y posiblemente coma. Además, puede causar acortamiento del ciclo menstrual en mujeres.

Tragar niveles muy altos de acetona puede producir pérdida del conocimiento y daño a la mucosa bucal. El contacto con la piel puede causar irritación y daño a la piel.

El aroma de la acetona y la irritación respiratoria o la sensación en los ojos que ocurren al estar expuesto a niveles moderados de acetona son excelentes señales de advertencia que pueden ayudarlo a evitar respirar niveles perjudiciales de acetona.

Los efectos de exposiciones prolongadas sobre la salud se conocen principalmente debido a estudios en animales. Las exposiciones prolongadas en animales produjeron daño del riñón, el hígado y el sistema nervioso, aumento en la tasa de defectos de nacimiento, y reducción de la capacidad de animales machos para reproducirse. No se sabe si estos mismos efectos pueden ocurrir en seres humanos.

Según el INSHT (documento se recogen los Límites de Exposición Profesional para Agentes Químicos adoptados por el Instituto Nacional de Seguridad e Higiene en el Trabajo (INSHT) para el año 2009), la acetona tiene un Valor Límite Admitido de 500 ppm ó de 1210 mg/m3.




</doc>
<doc id="41551" url="https://es.wikipedia.org/wiki?curid=41551" title="Acidosis láctica">
Acidosis láctica

Se conoce como la acumulación excesiva de lactato en condiciones anaerobias, debido a que el cuerpo trata de transformar energía en condiciones aerobias, pero cuando este se ve insuficiente, el cuerpo repone energía a través de la formación de ácido láctico.
Acidosis láctica es un tipo de acidosis metabólica, caracterizada por el aumento de la producción de ácido láctico como respuesta a la dificultad de utilización del oxígeno a nivel tisular, con aumento del "hiato aniónico" y disminución del bicarbonato.

Se presenta cuando el ácido láctico, que es producido cuando los niveles de oxígeno en el cuerpo caen, se acumula en el torrente sanguíneo más rápido de lo que puede ser eliminado.

El ácido láctico es el producto final del metabolismo anaerobio de la glucosa (glucólisis anaérobica).
El paso de piruvato + NADH a láctico + NAD es reversible, catalizado por la deshidrogenasa láctica, lo que se acentúa en anerobiosis.

En condiciones aeróbicas, los hidrogeniones llegan a formar agua, dejando el piruvato para su descarboxilación oxidativa u otras reacciones.

Si acelera la glucólisis anaeróbica (por una hipoxia tisular), se acentúa la tasa de lactato sobre piruvato, situación que se asocia con acidosis metabólica.

Puede ser una manifestación de la enfermedad de Wernicke.

Algunas de las causas de la acidosis láctica incluyen:


Exámenes de sangre para verificar los niveles de electrolitos

La remoción directa del lactato del organismo (por ejemplo con hemofiltración) es dificultosa, con evidencias limitadas de beneficios. En acidosis láctica de tipo A el tratamiento consiste en un efectivo manejo de la causa subyacente, y evidencia limitada acredita el uso de soluciones de bicarbonato de sodio para mejorar los valores de pH (lo cual está asociado con un incremento de la generación de dióxido de carbono y podría reducir los niveles de calcio).

En la acidosis láctica producida por medicación la retirada de ésta podría ser necesaria.

La acidosis láctica en contexto de las enfermedades mitocondriales (tipo B3) podrían ser tratadas con una dieta cetogénica y posiblemente con dicloroacetato (DCA), aunque esto podría ser complicado por una neuropatía periférica y tiene una base de evidencia débil.




</doc>
<doc id="41552" url="https://es.wikipedia.org/wiki?curid=41552" title="Afta">
Afta

Un afta (del griego antiguo ἄφθαι, "aphtai", "quemaduras") es una úlcera superficial, pequeña, redondeada, blanquecina y con borde rojo bien delimitado; de origen desconocido, que aparece durante el curso de ciertas enfermedades. Suele ser recurrente. Se forma en la mucosa de la boca o de otras partes del tubo digestivo, o en la mucosa genital; como la presentación más habitual es la orofaríngea, se usa con frecuencia en un sentido más restringido, referido tan solo al afta bucal.

El afta bucal u oral o estomatitis aftosa o úlcera bucal es una lesión o erosión mucosa, como una pequeña herida o llaga, que se localiza generalmente en la mucosa oral de bordes planos y regulares y rodeada de una zona de eritema.

Las aftas son una de las lesiones más frecuentes de la cavidad bucal con una prevalencia entre el 5 y 80% de la población. Se presenta con gran frecuencia entre niños y adolescentes, especialmente entre los 10 y 19 años de edad.

Las causas que provocan el desarrollo de las aftas bucales no están completamente claras, si bien se sabe que tienen un origen multifactorial. Diversas alteraciones en el funcionamiento del sistema inmunitario, tanto de origen genético (congénitas o innatas) como otras que se desarrollan a lo largo de la vida (adquiridas), juegan un importante papel.

Entre los principales factores que modifican la respuesta inmunitaria se incluyen: deficiencias de microelementos y vitaminas, alergias a alimentos, enfermedades sistémicas (tales como la enfermedad celíaca, la enfermedad de Crohn, la colitis ulcerosa y el sida), trastornos hormonales, algunas infecciones virales y bacterianas, el aumento del estrés oxidativo, lesiones mecánicas, la ansiedad y el tabaco.

Se ha demostrado en numerosos estudios que las aftas orales recurrentes aparecen principalmente en pacientes con trastornos gastrointestinales, especialmente en aquellos que padecen enfermedad inflamatoria intestinal (colitis ulcerosa, enfermedad de Crohn) y enfermedad celíaca. Las razones que explican esta correlación son, por un lado, las deficiencias de nutrientes y micronutrientes, y por otro, la aparición de reacciones autoinmunes, que son características típicas en estas enfermedades. Las aftas recurrentes u otras lesiones bucales pueden constituir los primeros signos que indican la presencia de estos trastornos. No en vano, la palabra "esprúe", utilizada en Gastroenterología para denominar ciertas dolencias (esprúe celíaco, esprúe tropical) deriva de la palabra holandesa "spruw", que significa "aftoso". En el caso de la enfermedad celíaca, pueden tratarse de su única manifestación y constituyen una importante ayuda para orientar el diagnóstico, ya que esta enfermedad cursa con frecuencia sin síntomas digestivos o completamente asintomática, por lo que el diagnóstico se demora durante años, con importantes consecuencias negativas sobre la salud.

La estomatitis aftosa se presenta con una o más lesiones recubiertas de una capa amarillenta sobre una base roja, las cuales tienden a recurrir. No suele acompañarse con fiebre aunque a menudo son dolorosas. Por lo general comienzan con una sensación de ardor en el sitio de la futura úlcera. Al cabo de varios días progresan a una tumefacción que se vuelve ulcerosa. El área de color gris, blanca o amarillenta se debe a la formación de fibrina, una proteína asociada con la coagulación de la sangre.

Por las manifestaciones clínicas, se establecen tres tipos de aftas: menores o leves, mayores o graves y ulceraciones herpetiformes recidivantes o estomatitis aftosa recidivante.

Se puede confundir con el herpes simple, causado por el virus "herpes hominis", pero no tienen relación. Los herpes orales presentan ampollas pequeñas y dolorosas principalmente en los labios y son contagiosas, sin embargo las aftas bucales aparecen en la zona interior de los labios, las mejillas, las encías, o la lengua, y no son una enfermedad contagiosa.

El primer paso, antes de iniciar un tratamiento, es identificar las posibles causas que provocan la aparición de las aftas, tales como deficiencias nutricionales o alergias. 

Los médicos de cabecera pueden evaluar y diagnosticar ciertos factores ambientales, pero la posible presencia de una enfermedad sistémica causante de las aftas, como la enfermedad celíaca, la enfermedad de Behçet o el sida, debe ser estudiada por los especialistas adecuados, ya que su reconocimiento, diagnóstico y tratamiento es muy difícil para los médicos de cabecera.

En el caso de la enfermedad celíaca, el tratamiento con la dieta sin gluten resuelve las aftas aproximadamente en el 90% de los pacientes. La persistencia de las lesiones en una minoría de celíacos se considera un indicativo de posible falta de cumplimiento estricto de la dieta, pues la exposición a mínimas cantidades de gluten puede provocar el desarrollo o reaparición de lesiones en la boca.

Antes de plantear el tratamiento de aftas conviene valorar la conveniencia de tratarlas, ponderando los beneficios y los riesgos. Deben considerarse cuatro parámetros para la indicación terapéutica: 
Los lavados bucales suaves con enjuague bucal antimicrobiano y las pomadas de antibióticos-hidrocortisona disminuyen el dolor y contribuyen a la curación. Es de especial utilidad la hidrocortisona en una base adhesiva. Indirectamente pueden ser de utilidad los sedantes, analgésicos y vitaminas. Los corticoesteroides por vía general en dosis elevadas por un corto periodo pueden ser de mucha utilidad para los ataques graves y debilitantes de afta.



</doc>
<doc id="41555" url="https://es.wikipedia.org/wiki?curid=41555" title="Cetoacidosis diabética">
Cetoacidosis diabética

La cetoacidosis diabética (CAD) (en inglés "Diabetic Ketoacidosis", DKA) hace referencia a una descompensación de la diabetes mellitus tipo I que, junto con el estado hiperosmolar y el coma hipoglucémico, son las tres principales complicaciones agudas de la diabetes mellitus. A diferencia de las complicaciones crónicas, esta complicación se desarrolla en cuestión de horas y pone en peligro la vida del paciente, por lo que se considera una emergencia médica.

Las personas con diabetes tipo 1 presentan un déficit absoluto de insulina por destrucción de las células beta pancreaticas. La insulina es una hormona que el cuerpo utiliza para la captación de la glucosa por los tejidos para obtener energía. Cuando la glucosa no está disponible, se metaboliza la grasa en su lugar.

A medida que las grasas se descomponen, se forman las moléculas llamadas cuerpos cetónicos, que son cetoácidos (cetonas y ácidos carboxílicos) que se acumulan en la sangre y la orina. En niveles altos, los cuerpos cetónicos son tóxicos.

En 1956 Dreschfeld proporcionó la primera descripción de cetoacidosis diabética (CAD) en la literatura moderna y antes del descubrimiento de la insulina por el Dr. Frederick Banting en 1921 su mortalidad era prácticamente del 100%. La CAD y el Estado Hiperosmolar Hiperglucémico (EHH) son las complicaciones más graves y potencialmente mortales de la Diabetes Mellitus (DM). En la actualidad se sabe que tanto la CAD como el EHH se pueden presentar en DM tipo 1 (DM1) y en DM tipo 2 (DM2). La tasa de mortalidad en CAD es menor de 5% en centros con experiencia, mientras que la tasa de mortalidad en el EHH varía de 10 a 50%. La muerte en ambas condiciones usualmente es causada por el factor desencadenante, pero en ocasiones también puede ser el resultado de la instauración de una terapia inadecuada y las complicaciones que se pueden presentar durante la misma. Es por eso que en el presente artículo, se brindan cada una de estas entidades a fin de garantizar la adecuada evolución del paciente desde su captación y disminuir al máximo el riesgo de complicaciones.

En la CAD influyenː la reducción de la acción efectiva neta de la insulina circulante como resultado de la disminución en su secreción, la elevación de los niveles de hormonas contrareguladoras (glucagón, catecolaminas, cortisol y hormona del crecimiento), y como resultado incapacidad de la glucosa para entrar a los tejidos sensibles a la insulina (hígado, músculo y adipocitos). 

Para que se desarrolle una CAD es especialmente necesaria la combinación de déficit de insulina y exceso de glucagón, lo que da origen a un acelerado catabolismo, gluconeogénesis, glucogenólisis y lipólisis que incrementa la producción de glucosa, ácido láctico, formación de cuerpos cetónicos en el hígado, además de aumentar el suministro al hígado de sustratos procedentes de la grasa y el músculo (ácidos grasos libres y aminoácidos). 

La cetosis es el resultado de un incremento notable de los ácidos grasos libres procedentes de los adipocitos, con un desplazamiento hacia la síntesis hepática de los cuerpos cetónicos. Además de su papel en la regulación del metabolismo de la glucosa, la insulina estimula la lipogénesis que permite que los triglicéridos sean incorporados a las células grasas e inhibe la liberación de estos de dichas células y bloquea la lipólisis. El descenso de los valores de insulina, combinado con elevaciones de catecolaminas y hormona del crecimiento, bloquea la lipogénesis y aumenta la lipólisis y la liberación de ácidos grasos libres por acción de las hormonas contrareguladoras. Normalmente, estos ácidos grasos libres son convertidos en triglicéridos y lipoproteínas de muy baja densidad (VLDL) en el hígado, pero en la CAD la hiperglucagonemia altera el metabolismo hepático favoreciendo la formación de cuerpos cetónicos, a través de la activación de la enzima palmitoiltransferasa de carnitina I. Esta enzima es crucial para la regulación del transporte de ácidos grasos al interior de las mitocondrias, donde ocurre la oxidación beta y la conversión en cuerpos cetónicos.

En resumen, al no haber insulina, la célula no capta hidratos de carbono, así que para obtener energía recurre a vías metabólicas alternativas. Las células empiezan a consumir ácidos grasos que se degradan parcialmente por la falta de glucosa intracelular, dando lugar a cuerpos cetónicos. Por otro lado hay un aumento de glucagón que activa la carnitina, la cual favorece el paso de ácidos grasos a cuerpos cetónicos.

Los cuerpos cetónicos provocan:

A medida que las grasas se descomponen, los ácidos llamados cetonas se acumulan en la sangre y orina. En niveles altos, las cetonas son tóxicas, Esta afección se denomina cetoacidosis. La CAD algunas veces es el primer signo de diabetes tipo 1 en personas que aún no han recibido el diagnóstico.
Los dos factores desencadenantes más comunes en el desarrollo de la CAD y la EHH sonː 
Los pacientes con DM2 propensos a desarrollar CAD generalmente son personas de mediana edad, obesos, con diagnóstico reciente de DM, expuestas a situaciones de estrés; tales como, infecciones severas y enfermedades cardiovasculares, y hasta un 50% de estos pacientes son afroamericanos e hispanos. Tanto en la CAD como en el EHH el pronóstico empeora sustancialmente con la edad avanzada y en presencia de coma e hipotensión.

Otros factores precipitantes son: 

Inicialmente los síntomas son los propios de una diabetes mellitus descontrolada: poliuria, polifagia y polidipsia, a los que se le añade malestar general, cefalea, debilidad, astenia y adinamia. Conforme la deshidratación y la alteración hidroelectrolítica se acentúan, se añade desorientación y estupor (especialmente en pacientes debilitados o ya enfermos), así como náuseas, vómito y dolor abdominal.

La exploración física muestra datos típicos de deshidratación (boca y conjuntivas secas, ojos hundidos, piel seca, pulso débil, hipotensión, respiración superficial), junto con datos de gravedad (hipotensión severa, pulso no detectable, falta de reacción a estímulos) y algunos más específicos como la respiración de Kussmaul (respiración rápida, profunda, irregular) provocada por la acidosis metabólica (pH entre 7.0 y 7.2) y el "aliento cetónico" o aliento con olor a frutas ácidas, provocado por la salida de acetona a través del aliento.

El diagnóstico se confirma sobre la base de la historia clínica y el examen físico. Los síntomas reflejan la hiperglicemia y el aumento de los ácidos grasos libres.

La CAD se caracteriza por hiperglucemia, cetosis y acidosis metabólica (con aumento del anion gap). Es frecuente que el bicarbonato sérico sea <10 mmol/L, y el pH arterial oscile entre 6.8 y 7.3, dependiendo de la gravedad de la acidosis.

El ascenso del nitrógeno de la urea sanguínea (BUN) y de la creatinina sérica refleja el decremento de volumen intravascular. Las determinaciones de creatinina sérica pueden estar falsamente elevadas por el acetoacetato (un cuerpo cetónico).

Se encuentra a menudo leucocitosis, hipertrigliceridemia e hiperlipoproteinemia.

El sodio sérico está disminuido a consecuencia de la hiperglicemia, ya que ésta provoca que salga agua de las células y termina diluyendo el sodio sérico. Habrá depleción del potasio.

La determinación de cetonas séricas es fundamental para el diagnóstico. También hay cetonuria aumentada, elevación del hematocrito, leucocitos, hiperuricemia e hiperosmolaridad.

formula_1

El diagnóstico diferencial de la CAD incluye cetoacidosis por inanición, cetoacidosis alcohólica y otras acidosis con aumento del Anion GAP (o aniones no medibles en sangre).

El tratamiento es urgente y debe instalarse de inmediato, y tiene dos principales objetivos: la corrección de la deshidratación y la de la hiperglucemia. Adicionalmente se debe vigilar y tratar el déficit de electrolitos y la eliminación o tratamiento de la causa de base o factores predisponentes. Por lo general, la meta es sacar al paciente de la acidosis en un máximo de 6 horas.

Antes de iniciar el tratamiento se debe valorar el grado de descompensación metabólica, la repercusión hemodinámica y respiratoria y si el paciente presente acidosis grave o acidosis moderada pero con alteración del nivel de conciencia o intolerancia oral.

La corrección de la deshidratación puede intentarse por vía oral, si el paciente se encuentra orientado y consciente. Desafortunadamente, la gran mayoría de los pacientes tiene algún grado de alteración en el nivel de alerta (desorientación, somnolencia, estupor, coma), y en tal caso está formalmente contraindicada la administración oral de líquidos, debido al riesgo de broncoaspiración.

La reposición de líquidos se inicia generalmente con una solución hipotónica de NaCl al 0.45% de concentración, con el fin de administrar esencialmente agua "libre", restituyendo el volumen intravascular y corrigiendo la deshidratación. Paulatinamente y de acuerdo a la mejoría del paciente, se pueden alternar soluciones isotónicas con NaCl al 0.9%, para continuar la hidratación y reponer el sodio perdido por diuresis, sin provocar un desequilibrio electrolítico.

La corrección de la hiperglucemia se realiza con la administración de insulina por vía parenteral. La dosis depende de la vía y métodos utilizados. Por ejemplo: si se utiliza la vía intramuscular (IM) o la subcutánea (SC). Por lo general se indican 0.1 UI/kg de peso corporal como dosis inicial, dividida la mitad por vía intravenosa (IV) y la otra mitad por la vía elegida (IM o SC). Si se usa solo la vía IV (más práctica) se da una dosis inicial de 0.15 UI/kg. En ambos casos se continúa el tratamiento con un infusión intravenosa de insulina a dosis de 0.1 UI/kg/h. La insulina intramuscular es una alternativa cuando no se dispone de una bomba de infusión continua o cuando se dificulta el acceso endovenoso, como en el caso de niños pequeños.

Es esencial recordar que la vigilancia de los niveles sanguíneos de glucosa, electrólitos séricos, pH sanguíneo y osmolaridad sérica debe realizarse con frecuencia —cada hora— y una vez iniciada la terapia, para evitar errores de dosificación, deben ajustarse las dosis de acuerdo a la evolución y prevenir trastornos como la hiponatremia dilucional —exceso de restitución hídrica—, edema cerebral —por alteración de la osmolaridad— o hipoglucemia —por sobredosis de insulina—.

Además de lo anterior, debe considerarse la reposición de potasio si la concentración es menor de 3.3 mEq/L, y de bicarbonato si el pH es menor de 7.0, aunque ninguna de estas medidas debe ser prioritaria a la restitución de líquido y al tratamiento de la hiperglucemia. Aunque la concentración de potasio en sangre parezca ser fisiológica, todo paciente con cetoacidosis diabética tiene una disminución del potasio corporal, que puede resultar ser grave. Solo se administra potasio si el paciente tiene buena función renal, y no se indica en las primeras horas si el enfermo está recibiendo una rehidratación veloz. Si el potasio en el plasma sanguíneo es menor de 3.3 mEg/L se suele administrar 40 mEq de potasio en 24 horas. Si el potasio sérico está entre 3.3 y 5 mEq/L se administran entre 20 y 30 mEq en 24 horas.

Las medidas anteriores deben mantenerse hasta alcanzar una concentración de glucosa igual o inferior a 200 a 250 mg/dL. Una vez logrado este objetivo, el resto de la terapia deberá ser acorde a las características individuales del paciente y su evolución.

Los problemas de salud que pueden presentarse a consecuencia de la CAD incluyen cualquiera de los siguientes:



</doc>
<doc id="41556" url="https://es.wikipedia.org/wiki?curid=41556" title="Cuerpo cetónico">
Cuerpo cetónico

Los cuerpos cetónicos son compuestos químicos producidos por cetogénesis en las mitocondrias de las células del hígado. Su función es suministrar energía al cerebro. 

La utilización de cuerpos cetónicos como energía es una situación fisiológica, natural. Se conoce como cetosis nutricional. 

En la diabetes mellitus tipo 1, se puede acumular una cantidad excesiva de cuerpos cetónicos en la sangre, produciendo cetoacidosis diabética. 

Los compuestos químicos son el ácido acetoacético (acetoacetato) y el ácido betahidroxibutírico (β-hidroxibutirato); una parte del acetoacetato sufre descarboxilación no enzimática dando acetona (una cantidad insignificante en condiciones normales); los dos primeros son ácidos y el tercero, una cetona.

El lugar primario de formación de los cuerpos cetónicos es el hígado y, en menor proporción, también el riñón. El proceso tiene lugar en la matriz mitocondrial y ocurre en diferentes pasos:


Una parte del acetoacetato se reduce a β-hidroxibutirato en la propia mitocondria, lo que consume un equivalente de ATP (una molécula de NADH); esta reacción la cataliza la β-hidroxibutirato deshidrogenasa que se encuentra estrechamente asociada a la membrana mitocondrial interna.

La acetona se forma por una lenta descarboxilación espontánea, no enzimática del ácido acetoacético. Así pues, los niveles de acetona son mucho menores que los de los otros dos tipos de cuerpos cetónicos. Dado que no puede volver a transformarse en acetil-CoA, se excreta a través de la orina o bien mediante exhalación. La exhalación de la acetona es la responsable de un olor "afrutado" característico en el aliento.

Hay cierta discusión y no está claro, si la utilización de los cuerpos cetónicos como energía por el organismo puede presentar unos efectos positivos para la salud humana [1]. Por ejemplo, se cree pero no está demostrado que la inflamación celular y la actividad de radicales libres se reducen. También hay menor daño por la glicación.
La glicación de las proteínas debida a los altos niveles de glucosa y la actividad de los radicales libres contribuyen al desarrollo y progresión de una larga lista de complicaciones vasculares en los pacientes con diabetes como son el daño renal, la ceguera, el daño de los nervios periféricos y la enfermedad cardiovascular.

Los posibles efectos antioxidantes y antiinflamatorios de la cetosis nutricional son la razón por la que las dietas cetogénicas y los ayunos están siendo promocionadas actualmente, no obstante, se desconoce si las dietas cetogénicas realmente presentan beneficios en sujetos sanos. No obstante, la cetosis nutricional y los cuerpos cetónicos han sido estudiados de forma extensa para el tratamiento de muchas enfermedades.

Un creciente número de trabajos de investigación [2] han sido publicados sobre el efecto anti-inflamatorio de los cuerpos cetónicos en enfermedades como la epilepsia, la esclerosis múltiple, la Esclerosis Lateral Amiotrófica, la enfermedad de Parkinson, la enfermedad de Alzheimer, el traumatismo craneoencefálico, el cáncer, la enfermedad cardiovascular, el síndrome de ovario poliquístico, el autismo, las migrañas, los ictus, la depresión, el acné y, especialmente, la diabetes.

Los cuerpos cetónicos se elevan de forma patológica en la cetoacidosis diabética y en la cetoacidosis alcohólica. Se trata de una emergencia médica que puede amenazar la vida del paciente. 

Tanto el acetoacetato como el betahidroxibutirato son ácidos, y si hay altos niveles patológicos de alguno de estos cuerpos cetónicos se produce una disminución en el pH de la sangre. Esto se da en la cetoacidosis diabética y en la cetoacidosis alcohólica.

Por tanto no debe confundirse la cetoacidosis (estado patológico) con la cetosis nutricional(estado fisiológico), en la que los cuerpos cetónicos constituyen las moléculas con función energética predominantes y no se generan alteraciones del pH sanguíneo.



</doc>
<doc id="41560" url="https://es.wikipedia.org/wiki?curid=41560" title="Glucagón">
Glucagón

El glucagón es una hormona peptídica de 29 aminoácidos producida por las células alfa del Páncreas, y cuya principal función es estimular la producción de glucosa, aumentando así la glucemia. Esta hormona tiene un peso molecular de 3485 dalton y es sintetizada por las células alfa del páncreas (en la estructura anatómica denominada islotes de Langerhans).

Una de las consecuencias de la secreción de glucagón es la disminución de la fructosa-2,6-bisfosfato y el aumento de la gluconeogénesis.

Se usa glucagón recombinante inyectable en los casos de hipoglucemia por "choque insulínico". La inyección de glucagón eleva el nivel de glucosa en la sangre. El glucagón también se utiliza como antídoto para las intoxicaciones por betabloqueadores.

La existencia de glucagón postuló por primera vez en 1923 por John Charles Kimball y Murlin, de la Universidad de Rochester, quienes lo identificaron como un «contaminante» hiperglucémico de los extractos de páncreas.

Lo nombraron "glucagón"puesto que creían que era un agonista de la glucosa.

Pero no fue sino hasta treinta años después, entre 1953 y 1957, que fue aislado y descrito con su secuencia de aminoácidos por Bromer, Staub, Sinn y Behrens.

Todavía hubo que esperar hasta la década de 1970 para que quedase perfectamente establecido el papel del glucagón en términos fisiológicos y patológicos.
Cuando el glucagón sintetizado en el páncreas llega al hígado se une a receptores específicos en la membrana celular de los hepatocitos. El receptor a su vez sufren un cambio conformacional de su dominio citoplasmático. La nueva conformación hará posible que se una a una proteína G (una fosfatasa). Entonces la proteína G reemplazará la molécula de GDP que lleva por una de GTP. Esto modificará la proteína liberando su subunidad alpha que desencadena una cascada de reacciones que acabarán con la formación de glucosa. La subunidad alpha activará la adenilato ciclasa a partir de ATP, que lo convertirá en AMPc (AMP cíclico) [El AMPc es una molécula común para todas las rutas que señalizan la falta de metabolitos].

El AMPc a su vez se unirá al enzima cinasa A (también proteína quinasa A, o PKA). PKA está involucrada en el metabolismo de lípidos, además del metabolismo de glucógeno y glucosa del que estamos hablando. PKA al unir AMPc se disociará en dos subunidades la R (reguladora, que mantiene la enzima no funcional) y la C (catalítica). Una vez la subunidad C liberada fosforilizará para activar a la fosforilasaquinasa. A su vez ésta fosforilará a la fosforilasa b del glucógeno. La fosforilación activará el enzima (denominada fosforilasa a) la cual ya degrada el polímero de glucógeno liberando moléculas individuales de glucosa (glucosa-1-fosfato) que podrán entrar en la glucólisis para la obtención de energía.

Es una hormona de estrés. Estimula los procesos catabólicos e inhibe los procesos anabólicos.
Tiene, en el hígado, un efecto hiperglucemiante debido a su potente efecto glucogenolítico (activación del glucógeno fosforilasa e inactivación del glucógeno sintasa).
Desactiva a la piruvato kinasa y estimula la conversión del piruvato en fosfoenolpiruvato (inhibiendo así la glucólisis).
Estimula la captación de aminoácidos por el hígado para incrementar la producción de glucosa. Estimula la gluconeogénesis. También tiene un efecto cetogénico.




El principal factor regulador es el nivel de glucosa en sangre. Los bajos niveles de glucosa estimulan de forma directa a las células α, acción que se ve inhibida de forma paracrina por la presencia de insulina.
Los aminoácidos también elevan el glucagón, lo cual es importante para evitar una hipoglucemia provocada por una comida rica en proteínas. En presencia de glucosa este efecto es menor.
Los ácidos grasos libres, en humanos, ejercen un efecto inhibidor sobre la secreción de glucagón.
Los péptidos intestinales secretados en respuesta a la ingesta, provocan liberación de glucagón (CCC y gastrina).
Las catecolaminas, la hormona del crecimiento y los glucocorticoides estimulan su secreción, estos últimos de forma directa y de forma indirecta por su acción sobre el incremento de aa en plasma.
La estimulación simpática a través de receptores alfa adrenérgicos estimulan la liberación de glucagón, siendo ésta una de las vías de actuación del estrés. La estimulación vagal y ACh también tienen un efecto estimulador.



</doc>
<doc id="41565" url="https://es.wikipedia.org/wiki?curid=41565" title="Becquerel">
Becquerel

El becquerel (símbolo: Bq) es una unidad derivada del Sistema Internacional de Unidades que mide la actividad radiactiva. Un becquerel se define como la actividad de una cantidad de material radiactivo con decaimiento de un núcleo por segundo. Equivale a una desintegración nuclear por segundo. La unidad de Bq es por consiguiente inversa al segundo. 

Se puede calcular derivando "'N" respecto al tiempo ("t"):

formula_1

siendo N el número de núcleos radiactivos sin desintegrarse, formula_2 la constante radiactiva, característica de cada isótopo, y formula_3 la actividad en el instante inicial.

Toma su nombre en honor del físico francés Henri Becquerel.

Por otra parte, 3,7·10 Bq equivalen a 1 curio.

La Comisión Electrotécnica Internacional prefiere "becquerelio". La DRAE sólo registra "becquerel".


</doc>
<doc id="41570" url="https://es.wikipedia.org/wiki?curid=41570" title="Plutonio">
Plutonio

El plutonio es un elemento transuránico radiactivo con el símbolo químico Pu y el número atómico 94. Es un metal actínido con apariencia gris plateada que se oscurece cuando es expuesto al aire, formando una capa opaca cuando se oxida. El elemento normalmente exhibe seis estados alotrópicos y cuatro de oxidación. Reacciona con el carbono, los halógenos, nitrógeno y silicio. Cuando se expone al aire húmedo forma óxidos e hidruros que expanden hasta un 70% su volumen, que a su vez, se desprende en forma de polvo que puede inflamarse de forma espontánea. También es un elemento radiactivo y se puede acumular en los huesos. Estas propiedades hacen que manipular plutonio sea peligroso.

El plutonio es el elemento primordial más pesado en virtud a su isótopo más estable, el plutonio-244, con una semivida aproximada de 80 millones de años es tiempo suficiente para que el elemento se encuentre en pequeñas cantidades en la naturaleza. El plutonio es principalmente un subproducto de la fisión nuclear en los reactores, donde algunos de los neutrones liberados por el proceso de fisión convierten núcleos de uranio-238 en plutonio.

Uno de los isótopos del plutonio utilizados es el plutonio-239, que tiene una semivida de 24 100 años. El plutonio-239, junto con el plutonio-241 son elementos fisibles, esto quiere decir que el núcleo de sus átomos se puede dividir cuando es bombardeado con neutrones térmicos, liberando energía, radiación gamma y más neutrones. Esos neutrones pueden mantener una reacción nuclear en cadena, dando lugar a aplicaciones en armas y reactores nucleares.
El plutonio-238 tiene una semivida de 88 años y emite partículas alfa. Es una fuente de calor en los generadores termoeléctricos de radioisótopos, que son utilizados para proporcionar energía a algunas sondas espaciales. El plutonio-240 tiene una tasa elevada de fisión espontánea, aumentando el flujo de neutrones de cualquier muestra en la que se encuentre. La presencia de plutonio-240 limita el uso de muestras para armas o combustible nuclear y determina su grado. Los isótopos del plutonio son caros y difíciles de separar, por esto suelen fabricarse en reactores especializados.

El plutonio fue sintetizado por primera vez en 1940 por un equipo dirigido por Glenn T. Seaborg y Edwin McMillan en el laboratorio de la Universidad de California, Berkeley bombardeando uranio-238 con deuterio. Posteriormente se encontraron trazas de plutonio en la naturaleza. La producción de plutonio en cantidades útiles por primera vez fue una parte importante del Proyecto Manhattan durante la Segunda Guerra Mundial, que desarrolló las primeras bombas atómicas. La primera prueba nuclear ("Trinity", en julio de 1945), y la segunda bomba atómica usada para destruir una ciudad ("Fat Man" en Nagasaki, Japón en agosto de 1945) tenían núcleos de plutonio-239. Durante y después de la guerra, se realizaron experimentos con humanos sin consentimiento informado que estudiaban la radiación del plutonio y tuvieron lugar varios accidentes críticos, algunos de ellos letales. La eliminación de los residuos de plutonio de las centrales nucleares y el desmantelamiento de las armas nucleares construidas durante la Guerra Fría son preocupaciones sobre la proliferación nuclear y el medio ambiente. Otras fuentes de plutonio en el medio ambiente son consecuencia de las numerosas pruebas nucleares en la superficie (ahora prohibidas).

Enrico Fermi y un equipo de científicos de la Universidad de Roma informaron que habían descubierto el elemento 94 en 1934. Fermi llamo al nuevo elemento "hesperio" y lo mencionó en su discurso del Nobel en 1938. La muestra era en realidad una mezcla de bario, kriptón y otros elementos, pero esto no se conocía en ese momento porque la fisión nuclear todavía no se había descubierto.

El plutonio (específicamente, plutonio-238) fue producido y aislado por primera vez el 14 de diciembre de 1940 y fue identificado químicamente el 23 de febrero de 1941 por el Dr. Glenn T. Seaborg, Edwin M. McMillan, J. W. Kennedy y A. C. Wahl bombardeando uranio con deuterio en el ciclotrón de 150 cm de diámetro de la Universidad de California, Berkeley. En el experimento de 1940, se produjo neptunio-238 en el bombardeo pero se desintegró por emisión beta con una semivida corta de unos dos días, que indicaba la formación del elemento 94.

Un documento científico del descubrimiento fue preparado por el equipo y enviado a la revista "Physical Review" en marzo de 1941. El documento fue retirado antes de la publicación debido a que descubrieron que un isótopo de este nuevo elemento (plutonio-239) podría experimentar la fisión nuclear de forma que podría ser útil para la bomba atómica. La publicación fue retrasada hasta un año después del fin de la Segunda Guerra Mundial debido a las preocupaciones sobre la seguridad.

Edwin McMillan había nombrado recientemente el primer elemento transuránico debido al planeta Neptuno y sugirió que el elemento 94, siendo el siguiente elemento de la serie, fuera nombrado como el que en ese momento era el siguiente planeta, Plutón. Seaborg originalmente consideró el nombre "plutio", pero después pensó que no sonaba tan bien como "plutonio". Él eligió las letras "Pu" como una broma, que fue aprobada sin previo aviso en la tabla periódica. Otros nombres alternativos considerados por Seaborg y otros fueron "ultimio" o "extremio" debido a la creencia errónea de que habían encontrado el último elemento posible en la tabla periódica.

Después de unos pocos meses de estudio inicial se encontró que la química básica del plutonio era parecida a la del uranio. Las primeras investigaciones continuaron en el Laboratorio Metalúrgico de la Universidad de Chicago. El 18 de agosto de 1942, una cantidad muy pequeña fue aislada y medida por primera vez. Fueron producidos unos 50 mg de plutonio-239 junto con uranio y productos de la fisión y solo se aisló 1 mg aproximadamente. Este procedimiento permitió a los químicos determinar la masa atómica del nuevo elemento.

En noviembre de 1943 algunos trifluoruros de plutonio fueron reducidos para crear la primera muestra de plutonio metálico: unos pocos microgramos de perlas metálicas. Se produjo suficiente plutonio para que fuera el primer elemento sintético visible a simple vista.

Las propiedades nucleares del plutonio-239 también fueron estudiadas; los investigadores encontraron que, cuando un átomo es golpeado por un neutrón, se rompe (fisión), liberando más neutrones y energía. Esos neutrones pueden golpear otros átomos de plutonio-239 y así sucesivamente en una rápida reacción nuclear en cadena. Esto puede originar una explosión suficientemente grande como para destruir una ciudad si se concentra suficiente plutonio-239 para alcanzar la masa crítica.

Durante la Segunda Guerra Mundial, el Gobierno federal de los Estados Unidos creó el Proyecto Manhattan, que fue el encargado de desarrollar la bomba atómica. Los tres principales sitios de investigación y producción del proyecto fueron: la instalación de producción de plutonio en lo que ahora es Hanford Site, las instalaciones de enriquecimiento de uranio en Oak Ridge, Tennessee y el laboratorio de investigación y diseño de armas, ahora conocido como Laboratorio Nacional de Los Álamos

El primer reactor de producción que producía plutonio-239 fue el reactor de Grafito X-10. Empezó a funcionar en 1943 y fue construido en una instalación en Oak Ridge que después se convirtió en el Laboratorio Nacional Oak Ridge.

El 5 de abril de 1944, Emilio Segrè, en Los Álamos, recibió la primera muestra de plutonio producido por reactor de Oak Ridge.

La primera prueba de una bomba atómica, denominada "Trinity" y detonada el 16 de julio de 1945 cerca de Alamogordo, Nuevo México, contenía plutonio como su material de fisión. En el diseño de la implosión del "dispositivo" se usó lentes explosivos convencionales para comprimir una esfera de plutonio en una masa supercrítica, que era bombardeado simultáneamente con neutrones desde el «"Urchin"», un iniciador hecho de polonio y berilio. En conjunto, estos aseguraron una reacción en cadena y una explosión. El arma en su totalidad pesaba más de 4 toneladas, a pesar de que solo habían sido utilizados 6,2 kilogramos de plutonio en su núcleo. Aproximadamente el 20% del plutonio utilizado en el arma Trinity se sometió a fisión, lo que resultó en una explosión con una energía equivalente a aproximadamente 20 000 toneladas de TNT.

Un diseño idéntico fue utilizado en la bomba atómica «Fat Man», lanzada sobre Nagasaki, Japón, el 9 de agosto de 1945, matando a 70 000 personas e hiriendo a otras 100 000. La bomba «Little Boy» lanzada sobre Hiroshima tres días antes, usó uranio-235 y no plutonio. Fue hecha pública la existencia del plutonio solamente luego del anuncio de las primeras bombas atómicas.

Durante la guerra fría fueron construidas grandes reservas de plutonio para armas nucleares, tanto por la Unión Soviética como por Estados Unidos. Los reactores estadounidenses de Hanford y Savannah River Site en Carolina del Sur producían 103 toneladas, y se estimaba que se producían 170 toneladas de plutonio de grado militar en la Unión Soviética. Cada año aún son producidas alrededor de 20 toneladas del elemento como un subproducto de la industria de energía nuclear. Aproximadamente 1000 toneladas de plutonio pueden estar almacenadas junto con más de 200 toneladas de plutonio extraído desde armas nucleares. El Instituto Internacional de Estudios para la Paz de Estocolmo estimaba que las reservas mundiales de plutonio en 2007 eran de 500 toneladas, divididas en partes iguales entre reservas civiles y armamentísticas.

Durante y después del final de la Segunda Guerra Mundial, los científicos que trabajaban en el Proyecto Manhattan y en otros proyectos de investigación de armas nucleares, llevaron a cabo estudios de los efectos del plutonio en animales de laboratorio y en seres humanos. Los estudios en animales revelaron que unos pocos miligramos de plutonio por kilogramo de tejido representan una dosis letal.

En el caso de los seres humanos, dichos experimentos implicaban inyectar soluciones que contenían (por lo general) cinco microgramos de plutonio en pacientes hospitalarios que se creía que sufrían de una enfermedad terminal, o que tuvieran una esperanza de vida menor a diez años ya sea debido a la avanzada edad o por la condición de una enfermedad crónica. Esta cantidad fue reducida a un microgramo en julio de 1945 después de que en los estudios en animales se constatara que la forma en la que el plutonio se distribuía en los huesos era más peligrosa que la del radio. Muchos de estos experimentos dieron como resultado fuertes mutaciones. La mayoría de los sujetos de prueba —de acuerdo a lo dicho por Eileen Welsome— eran pobres, impotentes y enfermos.

El plutonio, como la mayoría de los metales, tiene una apariencia plateada brillante al principio, muy parecida a la del níquel, pero se oxida rápidamente a un gris opaco, aunque también se reportan amarillo y verde oliva. A temperatura ambiente, el plutonio esta en su forma α ("alfa"). Esta, la forma estructural más común del elemento (alótropo), es casi tan dura y quebradiza como el hierro fundido gris a menos que se alee con otros metales para hacerlo blando y dúctil. A diferencia de la mayoría de los metales, no es un buen conductor de calor o electricidad. Tiene un punto de fusión bajo. (640 °C) y un inusualmente alto punto de ebullición (3,228 °C).

La desintegración alfa, la liberación de un núcleo de helio de alta energía, es la forma más común de desintegración radioactiva para el plutonio. Una masa de 5 kg de Pu contiene alrededor de 12.5 × 10 átomos. Con una vida media de 24.100 años, aproximadamente 11.5 × 10 de sus átomos se descomponen cada segundo emitiendo una partícula alfa de 5.157 MeV. Esto equivale a 9,68 vatios de potencia. El calor producido por la deceleración de estas partículas alfa las hace calientes al tacto.

La resistividad es una medida de la fuerza con la que un material se opone al flujo de corriente eléctrica. La resistividad del plutonio a temperatura ambiente es muy alta para un metal, y se hace aún más alta con temperaturas más bajas, lo que es inusual para los metales. Esta tendencia continúa hasta 100 K, por debajo de la cual la resistividad disminuye rápidamente para las muestras frescas. La resistividad comienza a aumentar con el tiempo alrededor de los 20 K debido al daño por radiación, con la tasa dictada por la composición isotópica de la muestra.

Debido a la autoirradiación, una muestra de plutonio se fatiga en toda su estructura cristalina, lo que significa que la disposición ordenada de sus átomos se ve interrumpida por la radiación con el tiempo.. La autoirradiación también puede conducir a recocido que contrarresta algunos de los efectos de la fatiga a medida que la temperatura aumenta por encima de los 100 K.

A diferencia de la mayoría de los materiales, el plutonio aumenta en densidad cuando se funde, en un 2,5%, pero el metal líquido exhibe una disminución lineal en densidad con la temperatura. Cerca del punto de fusión, el plutonio líquido tiene una viscosidad y tensión superficial muy alta en comparación con otros 
metales.

El plutonio normalmente tiene seis alótropos y forma un séptimo (zeta, ζ) a alta temperatura dentro de un rango de presión limitado. Estos alótropos, que son diferentes modificaciones estructurales o formas de un elemento, tienen energías internas muy similares pero densidades y estructuras cristalinas significativamente variables. Esto hace que el plutonio sea muy sensible a los cambios de temperatura, presión o química, y permite cambios drásticos de volumen después de transiciones de fase de una forma alotrópica a otra. 
Las densidades de los diferentes alótropos varían de 16,00 g/cm a 19,86 g/cm.

La presencia de estos muchos alótropos hace muy difícil el mecanizado del plutonio, ya que cambia de estado muy fácilmente. Por ejemplo, la forma α existe a temperatura ambiente en plutonio no aleado. Tiene características de mecanizado similares al hierro fundido pero cambia a la forma plástica y maleable β ("beta") a temperaturas ligeramente más altas. Las razones del complicado diagrama de fase no se entienden del todo. La forma α tiene una estructura monoclínica de baja simetría, de ahí su fragilidad, resistencia, compresibilidad y baja conductividad térmica.

El plutonio en la forma δ ("delta") normalmente existe en el rango de 310 °C a 452 °C pero es estable a temperatura ambiente cuando se alea con un pequeño porcentaje de galio, aluminio, o cerio, lo que mejora la trabajabilidad y permite su soldadura. La forma δ tiene un carácter metálico más típico, y es más o menos tan fuerte y maleable como el aluminio. En las armas de fisión, las ondas de choque explosivas utilizadas para comprimir un núcleo de plutonio también causarán una transición de la forma habitual de plutonio de fase δ a la forma más densa de α, ayudando significativamente a lograr la supercrítica. La fase ε, el alótropo sólido de más alta temperatura, exhibe una autodifusión atómica anómicamente alta en comparación con otros elementos

El plutonio es un metal radiactivo actínido cuyo isótopo, plutonio-239, es uno de los tres isótopos primarios fisibles (uranio-233 y uranio-235 son los otros dos); plutonio-241 también es altamente fisible. Para ser considerado fisionable, el núcleo atómico de un isótopo debe ser capaz de romperse o fisionarse cuando es golpeado por un neutrón de movimiento lento y liberar suficientes neutrones adicionales para sostener la reacción en cadena nuclear mediante la división de más núcleos.

El plutonio-239 puro puede tener un factor de multiplicación (k) mayor que uno, lo que significa que si el metal está presente en cantidad suficiente y con una geometría apropiada (por ejemplo, una esfera de tamaño suficiente), puede formar una masa crítica. Durante la fisión, una fracción de la energía de unión nuclear, que mantiene un núcleo unido, se libera como una gran cantidad de energía electromagnética y cinética (gran parte de esta última se convierte rápidamente en energía térmica). La fisión de un kilogramo de plutonio-239 puede producir una explosión equivalente a 21,000 tons de TNT. Es esta energía la que hace que el plutonio-239 sea útil en armas nucleares y reactores.

La presencia del isótopo plutonio-240 en una muestra limita su potencial de bomba nuclear, ya que el plutonio-240 tiene una tasa relativamente alta de fisión espontánea (~440 fisuras por segundo por gramo—más de 1.000 neutrones por segundo por gramo), elevando los niveles de neutrones de fondo e incrementando así el riesgo de predetonación. El plutonio se identifica como grado de armamentístico, de grado combustible o de grado reactor según el porcentaje de plutonio 240 que contiene. El plutonio apto para armas contiene menos del 7% de plutonio 240. El plutonio apto para uso en reactores contiene de 7% a menos de 19%, y el de grado de reactor de potencia contiene 19% o más de plutonio-240. El Plutonio súper-grado, con menos del 4% de plutonio-240, se utiliza en armas de la Armada de los Estados Unidos almacenadas cerca de las tripulaciones de barcos y submarinos, debido a su menor radiactividad. El isótopo plutonio-238 no es fisil pero puede sufrir fisión nuclear fácilmente con neutrones rápidos así como también desintegración alfa.

Se han caracterizado 20 isótopos radioactivos de plutonio. Los más longevos son el plutonio-244, con una vida media de 80,8 millones de años, el plutonio-242, con una vida media de 373.300 años, y el plutonio-239, con una vida media de 24.110 años. Todos los isótopos radioactivos restantes tienen una vida media inferior a 7.000 años. Este elemento también tiene ocho estados meta-estables, aunque todos tienen vida media inferior a un segundo.

Los isótopos conocidos del rango del plutonio en número de masa de 228 a 247. Los modos de desintegración primaria de los isótopos con unos números de masa inferior al del isótopo más estable, el plutonio-244, son la fisión espontánea y emisión alfa, que en su mayor parte forman uranio (92 protones) y neptunio (93 protones) como producto de desintegración (descuidando el amplio rango de núcleos hijos creados por procesos de fisión). El modo de desintegración primaria para los isótopos con un número de masas superior al del plutonio-244 es emisión beta, que en su mayor parte forman americio (95 protones) como productos de desintegración. El plutonio-241 es el isótopo padre de la serie de desintegración del neptunio, que se descompone a americio-241 mediante la emisión beta.

El plutonio-238 y 239 son los isótopos más ampliamente sintetizados. El plutonio-239 se sintetiza a través de la siguiente reacción utilizando uranio (U) y neutrones (n) a través de la desintegración beta (β) con el neptunio (Np) como sustancia intermedia:

</chem>

Los neutrones de la fisión del uranio-235 son capturados por núcleos de uranio-238 para formar uranio-239; un desintegración beta convierte un neutrón en un protón para formar neptunio-239 (vida media de 2,36 días) y otra descomposición beta forma plutonio-239. Egon Bretscher trabajando en el proyecto británico Tube Alloys predijo esta reacción teóricamente en 1940. 

El plutonio-238 se sintetiza bombardeando el uranio-238 con deuteróns (D, el núcleo de hidrógeno) en la siguiente reacción: 

\ce


</doc>
<doc id="41574" url="https://es.wikipedia.org/wiki?curid=41574" title="Idioma checo">
Idioma checo

El idioma checo (autoglotónimos "čeština, český jazyk") es una lengua eslava occidental hablada principalmente en la República Checa, que se representa en la escritura utilizando el alfabeto latino junto con diversos signos diacríticos.

La lengua checa puede parecer compleja para quienes la aprenden como lengua extranjera; dicha complejidad se debe, entre otros aspectos, a su morfología. Otra peculiaridad de esta lengua es la aparente libertad en la sintaxis; a menudo, cualquier posibilidad organizativa de los términos es gramaticalmente aceptable. El ruso es una de las lenguas eslavas que comparte estos rasgos. A su vez, su similitud con el eslovaco provoca que sea un idioma inteligible con este.

Es hablada por la mayor parte de la población de la República Checa, además hay hablantes de este idioma distribuidos por los seis continentes (cerca de unos 13 millones en total).

El checo es una de las lenguas eslavas occidentales, junto con el polaco, el eslovaco, el casubio, el pomeranio y el sorabo.

El idioma checo como otras lenguas indoeuropeas es una lengua flexiva, en el nombre y el adjetivo se distinguen hasta siete casos gramaticales (ver más adelante), tres géneros gramaticales (masculino, femenino y neutro, además de particularidades en los animados frente a inanimados) y dos números (singular y plural).

El caso gramatical, al igual que sucede en otras lenguas indoeuropeas, se distribuye por declinaciones temáticas (es decir, el sonido final de la raíz o la vocal temática, junto con el género y el tipo de animacidad, e indica qué paradigma morfológico presenta la palabra). En el nombre y el adjetivo se distinguen hasta siete casos diferentes que, comúnmente en checo, se numeran del 1 al 7 de acuerdo a la siguiente tabla:

En el verbo existe un sistema aspectual donde existe una forma de la raíz para las formas de perfecto y otra forma de la raíz para las formas de imperfecto.

Una característica del checo es que admite grupos consonánticos complicados, y fonéticamente el núcleo silábico puede estar ocupado por una líquida cualquiera, debido a que algunas palabras carecen de vocales propiamente dichas; así, "ztvrdl, scvrkl, prst (dedo)"; sin embargo, la sonoridad de las sonantes /l/, y /r/ cubre la función vocálica en esas palabras, pudiendo actuar como núcleo silábico. Por otro lado, la consonante /ř/ tiene una realización fonética que parece ser única del checo y realmente complicada de pronunciar para hablantes no nativos. Es una líquida vibrante palatal, es decir, se articula como las vibrantes del castellano, con la lengua en el paladar. Otra característica del checo es que todas las palabras tienen acento tónico en la primera sílaba.

El checo tiene diez fonemas vocálicos, cinco vocales breves y sus contrapartidas largas, la siguiente tabla engloba los alófonos principales de las vocales:

Los grafemas <"y, ý"> representan a las desaparecidas vocales /ɨ, ɨː/ (estas vocales acabaron confundiéndose en checo moderno con /i, iː/). El grafema <"ů"> representa un antiguo diptongo /ʊo/ que con el tiempo se confundió con /uː/. El inventario de consonantes viene dado por:

El alfabeto checo se basa en el latino y contiene las siguientes 34 letras, ordenadas alfabéticamente:
El sistema fonético del idioma checo es bastante complejo. Las vocales siempre son de dos calidades – larga o corta (las primeras se marcan con un acento agudo y las segundas no llevan dicho acento agudo). Además de la tilde para marcar las vocales largas, el acento en el alfabeto usa el circunflejo invertido o háček para indicar calidad palatal de las consonantes.
Letras usadas en el idioma ausentes en el alfabeto
La diferencia entre la ú y la ů es únicamente etimológica, la pronunciación es igual.
La "y" y la "i" siempre tienen el mismo sonido, tanto de calidad larga o corta.

A la "y" se le dice “la y dura” ("tvrdé y") y a la "i" se le dice “la i suave” ("měkké i"). Debe notarse que hay ciertas consonantes que siempre se escriben con la "i" y hay otras que siempre les sigue una "y". Mas también hay consonantes que les puede seguir cualquiera de estas. Véase más las consonantes “suaves”, “duras” y “ambiguas”. Todo lo mencionado causa problemas a la gente que no se sabe bien la ortografía. Intercambian la "y" y la "i". 

Grupos de letras y sus pronunciaciones. Las siguientes sílabas tienen una pronunciación diferente del español.

Nota: Fíjese bien que “la i suave” palataliza la consonante anterior, mientras “la y dura” la conserva.

Con la ě

dě – la pronunciación es /ď + e/. 

tě – la pronunciación es /ť + e/. 

ně – la pronunciación es /ñe/.
bě – la pronunciación es /bie/.

pě – la pronunciación es /pie/.

vě – la pronunciación es /vie/.
mě – la pronunciación es /mñe/.

Nota: Los grupos bě, pě, vě se pronuncian más bien con la j checa como /bje/, /pje/, /vje/. La diferencia es que, al pronunciarse con la i, se forman dos sílabas, mientras, cuando se pronuncia con la j, hay una sola sílaba y así es correcto. Pronunciar la ě en este caso como la ie de “cielo” puede causar incomprensibilidad porque “ci-e-lo” tiene tres sílabas mientras tanto “cělo” tendría sólo dos. Hay que tener cuidado con esta diferencia.

Los casos del checo son nominativo, acusativo, genitivo, dativo, instrumental, locativo y vocativo. Los géneros son masculino animado, masculino inanimado, femenino y neutro.

Abreviaturas ("m". masculino, "f". femenino, "n". neutro, "adj". adjetivo):



</doc>
<doc id="41576" url="https://es.wikipedia.org/wiki?curid=41576" title="Reforma protestante">
Reforma protestante

Se conoce como Reforma protestante —o, simplemente, la Reforma— al movimiento religioso cristiano iniciado en Alemania en el sigloXVI por Martín Lutero, que llevó a un cisma de la Iglesia católica para dar origen a numerosas iglesias agrupadas bajo la denominación de protestantismo.

Otra denominación usada para este movimiento por algunos historiadores como Ricardo García-Villoslada es el de Revolución Protestante.

La Reforma tuvo su origen en las críticas y propuestas con las que diversos religiosos, pensadores y políticos europeos buscaron provocar un cambio profundo y generalizado en los usos y costumbres de la Iglesia católica, además de negar la jurisdicción del papa sobre toda la cristiandad. El movimiento recibirá posteriormente el nombre de Reforma protestante, por su intención inicial de "reformar" el catolicismo con el fin de retornar a un cristianismo primitivo, y debido a la importancia que tuvo la "Protesta" de Espira, presentada por algunos príncipes y ciudades alemanas en 1529 contra un edicto del emperador CarlosV tendiente a derogar la tolerancia religiosa que había sido anteriormente concedida a los principados alemanes.

Este movimiento hundía sus raíces en elementos de la tradición católica medieval, como el de los Alumbrados y la reforma del Cardenal Cisneros en España, y también el movimiento de la Devoción moderna en Alemania y los Países Bajos, que era una piedad laica antieclesiástica y centrada en Cristo. Además, la segunda generación del humanismo la siguió en gran medida. Comenzó con la predicación del sacerdote agustino Martín Lutero, que revisó la doctrina de la Iglesia católica según el criterio de su conformidad a las Sagradas Escrituras. En particular, rechazó la teología sacramental católica que, según Lutero, permitía y justificaba prácticas como la «venta de indulgencias», consideradas un secuestro del Evangelio, el cual debía ser predicado libremente y no vendido.

La Reforma protestante dependió del apoyo político de algunos príncipes y monarcas para poder formar iglesias cristianas de ámbito estatal (posteriormente iglesias nacionales). Los principales exponentes de la Reforma protestante fueron Martín Lutero y Juan Calvino.

El protestantismo ha llegado a constituir la segunda gran rama del cristianismo, con un grupo de fieles que actualmente supera los 900 millones.

En el siglo XVI se produjo una gran crisis en la Iglesia católica en Europa Occidental, debido a numerosas acusaciones de corrupción eclesiástica y falta de piedad religiosa. Fue la venta de indulgencias para financiar la construcción de la Basílica de San Pedro en Roma lo que dio inicio a la Reforma protestante, la cual provocaría finalmente que la cristiandad occidental se dividiese en dos: una liderada por la Iglesia católica, que tras el Concilio de Trento se reivindicó a sí misma como la verdadera heredera de la cristiandad occidental, expulsando cualquier disidencia y sujetándose a la jurisdicción del papa, y otra mitad que fundó varias comunidades eclesiales propias, generalmente de carácter nacional para, en su mayoría, rechazar la herencia cristiana medieval y buscar la restauración de un cristianismo primitivo idealizado. Esto dio lugar a que Europa quedara dividida entre una serie de países que reconocían al papa, como máximo pontífice de la Iglesia católica, y los países que rechazaban la teología católica y la autoridad de Roma y que recibieron el nombre común de protestantes. Dicha división provocó una serie de guerras religiosas en Europa.

La Reforma protestante se inició en Alemania y se explica en gran parte por las condiciones económicas y sociales que tenía el Sacro Imperio Romano Germánico. Numerosas ciudades eran muy ricas gracias al comercio, además los burgueses eran partidarios del humanismo y de reformar la corrupción de la Iglesia católica. Pero el grupo más importante en Alemania era la alta nobleza; los grandes nobles eran casi independientes y señores de numerosas tierras y vasallos campesinos, siempre estaban conspirando contra la autoridad del emperador germánico, que apenas tenía poder sobre ellos. Pero junto a la alta nobleza existía una pequeña nobleza formada por los nobles más pobres y los segundones de las grandes casas nobiliarias. A principios del sigloXV, esta pequeña nobleza estaba completamente arruinada y para recuperar sus ingresos, los pequeños nobles buscaban una oportunidad para apoderarse de los bienes y las improductivas tierras de la Iglesia católica. La pequeña nobleza aprovechó las ideas de los humanistas, que criticaban las excesivas riquezas, pompas y boatos de la Iglesia católica, para proclamar que ella no tenía necesidad de propiedades e intentar apropiarse de sus cuantiosas riquezas. Por esta razón, la pequeña nobleza será la primera en apoyar y aprovechar las convulsiones reformadoras.

Además, existía la figura del emperador del Sacro Imperio, uno de los poderes universales forjados en mutua competencia durante la Edad Media (el otro era el papa), cuyo poder efectivo dependía de su capacidad de hacerse obedecer en cada uno de los territorios, prácticamente independientes, y antes de eso de ser elegido por los príncipes electores, unos laicos y otros eclesiásticos. También disponía de unas funciones de dimensión religiosa indudable, que le permitía incluso convocar Dietas con contenido organizativo e incluso doctrinal, como CarlosI de España hizo de hecho durante todo el proceso de la Reforma protestante. Para algunos autores, la postura recelosa de los pueblos germánicos desde la alta Edad Media (Concilio de Frankfurt, 794, frente al Concilio de NiceaII, 787) se había expresado también en esas luchas entre pontificado e imperio, de una forma incluso protonacionalista, en la que Roma era vista como

El fundador de la Reforma protestante fue el monje católico agustino alemán Martín Lutero, quien ingresa en 1507 en la orden religiosa de los agustinos.

En el convento católico, Lutero prosiguió sus estudios y se convirtió en un experto en la Biblia y en los autores cristianos medievales; llegó a ser un doctor universitario y se le contrató para dar clases en la nueva universidad de Wittenberg, que entonces era la capital del ducado de Sajonia. A partir de la revitalización que vivió el Sacro Imperio Romano Germánico desde que OtónI el Grande se convirtiera en emperador germánico en el 962, los papas y emperadores se vieron involucrados en una continua contienda por la supremacía en los asuntos espirituales y temporales.

Este conflicto concluyó, a grandes rasgos, con la victoria del papado, pero creó profundos antagonismos entre Roma y el Imperio Germánico, que aumentaron durante los siglos XIV y XV. La animosidad provocada por los impuestos papales y por la sumisión a los delegados pontificios se extendió a otras zonas de Europa. En Inglaterra, el principio del movimiento para lograr una independencia absoluta de la jurisdicción papal empezó con la promulgación de los estatutos de Mortmain (1279), Provisors (1351) y Praemunire (1393), que redujeron, en gran medida, el poder de la Iglesia católica en el control del gobierno civil sobre las tierras, en el nombramiento de cargos eclesiásticos y en el ejercicio de la autoridad.

En este tiempo estalló un gran escándalo en Alemania a causa de la cuestión de las indulgencias, concepto de la teología católica consistente en que ciertas consecuencias del pecado, como la pena temporal del mismo, pueden ser objeto de una remisión o indulgencia concedida por determinados representantes de la Iglesia y bajo ciertas condiciones. Esta institución se remonta al cristianismo antiguo y tanto su práctica como su formulación han evolucionado a lo largo del tiempo.

Muchos consideraron esta práctica como un abuso escandaloso y la culminación de una serie de prácticas anticristianas fomentadas por el clero católico, pero será Lutero el primero que expondrá públicamente su opinión contraria a la doctrina de las indulgencias.

Para Lutero, las indulgencias eran una estafa y un engaño a los creyentes con respecto a la salvación de sus almas. En 1517, Lutero clavó en la puerta de la iglesia de Wittenberg sus noventa y cinco tesis, en las que atacaba las indulgencias y esbozaba lo que sería su doctrina sobre la salvación solo por la fe. Este documento es conocido como las noventa y cinco tesis de Wittenberg y se consideró el comienzo de la Reforma Protestante.

Las noventa y cinco tesis se difundieron rápidamente por toda Alemania gracias a la imprenta, y Lutero se convirtió en un héroe para todos los que deseaban una reforma de la Iglesia católica. En algunos lugares hasta se iniciaron asaltos a edificios y propiedades de la misma Iglesia católica. Por sus noventa y cinco tesis, Lutero se había convertido en el símbolo de la rebelión de Alemania contra lo que ellos consideraban prepotencia de la Iglesia católica. Lutero arriesgaba además su vida, ya que podía ser declarado hereje por la jerarquía eclesiástica y ser condenado a la hoguera.

Al principio, la Iglesia católica no dio demasiada importancia a las ideas de Lutero, ni a sus ataques contra la doctrina de salvación por las obras, pero muy pronto tuvo que reaccionar ante las noticias que llegaban de Alemania, de que gran parte de la gente estaba desafiando la autoridad del papa.

Lutero continuó atacando las indulgencias y la doctrina que sustentaba tal práctica mediante escritos que la imprenta difundía por toda Alemania. Lutero hacía un llamado a la nobleza alemana para que negase obediencia al papa y apoyase la formación de una Iglesia alemana; afirmaba también que, de acuerdo a la Biblia, todos los cristianos eran sacerdotes sin necesidad de ninguna ordenación especial y negaba la jurisdicción suprema del papa sobre la cristiandad universal. Lutero criticaba así mismo los sacramentos de la Iglesia católica, reduciéndolos a solo dos, que él pensaba eran bíblicamente fundamentados y afirmaba también que los poderes civiles debían tener plena autoridad política sobre la Iglesia católica. Esto iba más allá de la doctrina de la salvación por la fe y suponía una auténtica amenaza para el catolicismo. Finalmente, el papa declaró a Lutero un hereje y lo excomulgó, es decir, lo declaró separado de la comunidad de la Iglesia católica.
En 1521, el recién elegido emperador CarlosV "de Alemania" (Sacro Imperio Romano) (rey de España como CarlosI) convocó una Dieta (asamblea de todas las autoridades del imperio) en la ciudad de Worms e invitó a Lutero a que asistiera a la misma para explicar su postura.

Muchos advirtieron a Lutero que se trataría de una trampa, pero Lutero estaba decidido a acudir pese a todos los peligros. La Dieta se celebró y Lutero expuso su doctrina ante el mismo CarlosV, pero este no quedó convencido por Lutero y, en cambio, hizo una declaración de lealtad y fidelidad a los principios de la Iglesia católica. A partir de entonces, la dinastía de los Habsburgo se convertirá en la primera defensora de la Iglesia católica contra los protestantes. Como los Habsburgo eran también reyes de España, la defensa del catolicismo se convertiría en una de las bases de la identidad española, durante siglos.

La Dieta terminó y Lutero se dispuso a regresar a Wittenberg, pero en el camino de vuelta, fue secuestrado por agentes de FedericoIII de Sajonia, que quería protegerle y que lo escondió con nombre falso en el castillo de Wartburg.

El duque quería salvar a Lutero de posibles maniobras de la Iglesia católica, por lo que Lutero tuvo que quedarse en el castillo y aprovechó ese tiempo para realizar su primera traducción al alemán de la Biblia. Mientras Lutero estaba escondido, sus partidarios empezaron a interpretar sus doctrinas, en un sentido que Lutero no había previsto, como producto de la doctrina de Lutero de la interpretación libre de las Escrituras.

Varios seguidores de Lutero (pronto serían rechazados por el propio Lutero y denominados «reformadores radicales») comenzaron a decir que se debían destruir todas las pinturas, estatuas e imágenes religiosas, que los sacerdotes tenían el deber de casarse, y no solo afirmaban que la Iglesia cristiana no debía tener propiedades, sino que, según la Biblia, todos los cristianos debían tener las mismas propiedades y que, por lo tanto, se debía abolir la propiedad privada y repartir todos los bienes entre los integrantes de la comunidad cristiana. De esta manera, corrientes radicales que apoyaban todo esto, como el anabaptismo, fueron criticadas por Lutero y posteriormente combatidas por católicos y protestantes por igual.

La alta nobleza reunió un gran ejército que derrotó brutalmente a estos protestantes radicales sublevados en una sola batalla. La represión fue durísima y miles de protestantes fueron ejecutados con extrema crueldad; entre los ejecutados se encontraba el dirigente más importante de esta reforma radical, Thomas Müntzer.

Lutero apoyó desde un primer momento a la nobleza, ya que pensaba que su autoridad era legítima y que su apoyo era indispensable para el triunfo de la reforma de la Iglesia cristiana. Durante estos años, CarlosV no pudo intervenir en Alemania, pues debió proseguir sus guerras contra Francia y sus campañas contra los turcos, pero en 1529 consiguió un periodo de paz con Francia que le permitió ocuparse de la situación religiosa en Alemania.

En 1529, CarlosV convoca una Dieta en la ciudad de Espira y en ella intenta convencer a los nobles que se han convertido al luteranismo, para que se sometan a la jurisdicción del papa, pero los príncipes y señores luteranos se niegan y protestan en la convocatoria de la Dieta, y a causa de esta protesta los católicos comenzarán a llamarlos con el nombre de protestantes.

En 1530, CarlosV convocó otra Dieta en la ciudad de Augsburgo y en ella intentó conseguir que los luteranos y los católicos se pusieran de acuerdo para aceptar una doctrina cristiana común que superase la división religiosa. Lutero fue invitado de nuevo a asistir, pero se negó y envió en su lugar a su discípulo Philipp Melanchthon. Los esfuerzos de CarlosV en la Dieta fueron inútiles, pues Melanchthon se negó a cualquier acuerdo y en su lugar los protestantes redactaron la llamada Confesión de Augsburgo, en la que exponían sistemáticamente los principios de su doctrina. Los católicos seguirían pronto su ejemplo, redactando también su compendio doctrinal, de modo que la cristiandad occidental se había dividido irremediablemente.
Lutero muere en 1546 mientras CarlosV preparaba en Alemania una campaña contra la liga de Esmalcalda, defensora del protestantismo.
CarlosV presentó su campaña no como una guerra contra los protestantes, sino como un castigo contra los nobles que se habían rebelado contra su emperador; en su ejército había, sobre todo, tropas españolas, pero también nobles protestantes que no se habían unido a la liga y que permanecían fieles a CarlosV. El ejército de CarlosV derrotó a la liga de Esmalcalda en 1547 en la gran batalla de Mühlberg. Parecía que el triunfo de CarlosV era total y toda Sajonia fue ocupada por las tropas del emperador germánico.

CarlosV se proponía ahora encontrar una solución a la división religiosa de Alemania, pero su triunfo había asustado a todos los nobles de Alemania, tanto a los católicos como a los protestantes, que temían que el emperador se volviera demasiado poderoso. Todos estos nobles van a formar posteriormente en secreto una alianza contra CarlosV anulando las ventajas conseguidas por la victoria de Mühlberg.

En un momento en que CarlosV se encontraba en Alemania sin tropas españolas, los nobles alemanes se rebelan contra él y el emperador tuvo que escapar hacia Italia, mientras su poder y autoridad se derrumbaban en Alemania.

CarlosV se vio obligado a aceptar las condiciones de los nobles rebeldes y en 1555 firmó la paz de Augsburgo. Según esa paz, cada príncipe alemán podía profesar la religión que quisiera sin que el emperador lo pudiese impedir ("cuius regio eius religio"); sin embargo, todos los vasallos de un noble tenían que tener la misma religión. Finalizaba así el anhelo de CarlosV de mantener la unidad religiosa en sus dominios.

Durante aproximadamente 20 años, la Iglesia católica había visto cómo gran parte de sus fieles se peleaban entre ellos en Europa, y obispos que dejaban de reconocer al papa como Primus inter pares o como máximo pontífice de la Iglesia católica, y se separaban de Roma incluso algunos cardenales, en consecuencia, hubo muchos católicos que requerían una reacción de la Iglesia que mejorase las costumbres, corrigiendo los abusos que habían alimentado la Reforma protestante. A esta reacción de la Iglesia católica ante el protestantismo se le conoce generalmente con el nombre de Contrarreforma católica, aunque algunos historiadores consideran más preciso el término «Reforma católica».

Aunque muchos creían que era necesario reformarse, no sabían el modo de hacerlo. Pronto, se llegó a la idea de que la mejor solución era convocar a un Concilio donde se pudiesen discutir las posibles reformas. CarlosV presionaba también a los papas para que se convocase ese concilio con la esperanza de que la Iglesia católica volviese a existir unificada, pero los papas desconfiaban de las pretensiones políticas de CarlosV en Italia y no convocaron este concilio sino hasta 1545, reunión que sería conocida como Concilio de Trento.

Las sesiones del Concilio de Trento duraron casi 17 años, ya que fueron interrumpidas muchas veces. Varios papas se sucedieron en Roma en ese lapso y cuando dicho concilio finalizó, en 1562, ya había muerto CarlosV.

El concilio se desarrolló sin la participación de los adherentes al emergente protestantismo (aunque fue Lutero quien primero propuso la necesidad de un concilio, en 1518), debido a que ellos mismos se negaron a participar, pues ya habían creado nuevas iglesias separadas del catolicismo.

En el Concilio de Trento se reformaron los abusos anteriores: se cuidó la formación de los obispos, se establecieron medidas de disciplina para los sacerdotes y se crearon seminarios para que los nuevos sacerdotes tuvieran una preparación religiosa adecuada para poder enseñar la fe católica.

Se reafirmaron todos los puntos de la doctrina milenaria católica frente a las protestantes:

La Contrarreforma alimentó un renacer en el catolicismo, impulso que se manifestó en el reavivamiento de antiguas órdenes religiosas, como la Orden de los carmelitas descalzos, reformada en España por Santa Teresa de Jesús y San Juan de la Cruz, los dos grandes escritores místicos de la península ibérica.

Pero la orden religiosa que más ayuda prestó a la Contrarreforma católica fue la Compañía de Jesús, fundada por San Ignacio de Loyola, de la que se distinguieron varios teólogos participantes en el Concilio de Trento.

Comenzó con la difusión en la isla de los primeros escritos de Martín Lutero, Ulrico Zwinglio y otros reformadores continentales. Además, la tradición de John Wyclif, reformador medieval, probablemente aún ejercía influjo en ciertos sectores de la Iglesia de Inglaterra.

EnriqueVIII ascendió al trono de Inglaterra siendo muy joven y al principio no se interesó por los problemas de gobierno, que dejó en manos de su favorito, el cardenal Thomas Wolsey, a quién nombró canciller de Inglaterra.. EnriqueVIII siempre fue un católico convencido, y un ardiente partidario de la primacía de Roma sobre la cristiandad, por ello fue declarado «Defensor de la Fe» ("Fidei Defensor") por el papa León X tras publicar «La Defensa de los Siete Sacramentos» (1521), donde argumentaba con vehemencia a favor de las prerrogativas del papado. Por ello resulta curioso el hecho de que la Iglesia de Inglaterra se haya separado de la Iglesia católica a mediados del siglo XVI, no por aceptar o compartir las ideas reformadoras de Lutero u otros protestantes, sino que por iniciativa del rey EnriqueVIII.

EnriqueVIII se opuso sin embargo a la reforma de la Iglesia de Inglaterra tras decretar el Acta de supremacía en 1534, por la que el mismo rey se convertía en jefe de la Iglesia de Inglaterra, no se realizó ninguna modificación doctrinal o litúrgica sustantiva bajo su gobierno, solo se prohibió a obispos y sacerdotes ingleses tener relación con la Curia Romana y se expropiaron los bienes excedentes de la Iglesia católica en beneficio de la Corona Real. A partir de esto emergieron, en Inglaterra, muchísimas sectas.

Al sucederle su hijo Eduardo con el nombre de EduardoVI, con apenas 9 años de edad, se produjeron los primeros avances efectivos de la reforma de la Iglesia de Inglaterra, pues se redactó el primer Libro de Oración Común, que introdujo, gracias al trabajo del arzobispo de Canterbury Thomas Cranmer, ciertos cambios menores en la doctrina y sobre todo en la forma de celebrar la misa. Este libro fue la primera expresión concreta de la reforma de la Iglesia de Inglaterra.

En 1553, EduardoVI muere a la edad de 15 años, dejando como sucesora a Jane Grey (coronada el 10 de julio de 1553), quien gobernó solo unos días. Se produjo una breve guerra de sucesión hasta que se impuso como reina (con el apoyo de la mayoría) MaríaI de Inglaterra, quien rápidamente abrogó las reformas religiosas introducidas durante el reinado de EduardoVI y sometió nuevamente a la jurisdicción papal a la Iglesia de Inglaterra, en noviembre de 1554.

Restablecido el catolicismo, el Acta de Supremacía y el Libro de Oración Común fueron suprimidos y se nombraron nuevos obispos, se persiguió a los partidarios de la independencia de la Iglesia de Inglaterra (ya conocidos como anglicanos), y algunos de ellos acabaron en la hoguera (no todos eran favorables a la reforma religiosa).

María murió en 1558 a los 42 años de edad y sin hijos, por lo que su media hermana, IsabelI de Inglaterra fue proclamada reina. Isabel asumió el trono de Inglaterra tratando de mantener la unidad nacional por sobre las diferencias religiosas, por lo que no mostró inicial apoyo a ninguno de los bandos en disputa (protestantes y católicos), sin embargo, la política internacional y especialmente las conspiraciones y rebeliones, la hicieron dar cada vez más apoyo al bando protestante.

Isabel restauró el Acta de Supremacía, por lo que los obispos partidarios de la supremacía católica fueron depuestos y sustituidos, proclamó luego el Acta de Uniformidad que obligaba a todas las parroquias de la Iglesia de Inglaterra a utilizar el Libro de Oración Común (con aquellos pequeños cambios introducidos por Cranmer) con su texto en inglés y no en latín. Todo ello dio espacio para la difusión de las ideas de la Reforma protestante en Inglaterra, no obstante la moderación que en general siguió teniendo la Iglesia de Inglaterra al conservar casi intacta su tradición medieval.

En Suiza también se van a separar algunos territorios de la Iglesia católica; las ideas de Lutero llegaron muy pronto a Suiza y aparecieron una serie de predicadores que criticaban la corrupción de la Iglesia católica y defendían la creación de una «Iglesia» distinta. Uno de los primeros fue Zwinglio. Aunque compartía muchas de las ideas de Lutero, Zwinglio quería dar una mayor libertad a su nueva «Iglesia» cristiana y rechazaba el sometimiento de los cristianos a la nobleza como defendía Lutero. Al final el mismo Lutero criticó a Zwinglio y se alegró públicamente de su muerte en un combate contra los suizos católicos.

Pero el principal foco de la Reforma protestante en Suiza va a ser la ciudad de Ginebra, gracias a la actuación de Juan Calvino que con Lutero es la mayor figura de la Reforma protestante.

En Ginebra una serie de reformadores habían asaltado las iglesias y conventos expulsando a los sacerdotes católicos, pero estos reformadores no sabían cómo organizar la nueva «Iglesia» que pretendían crear ni tampoco tenían claro qué nueva doctrina querían establecer, por lo que llamaron a una figura de prestigio dentro del campo protestante, que supiera cómo organizar la nueva Iglesia y diera un contenido religioso claro, y llamaron a la ciudad a Juan Calvino.

Este era un francés que había estudiado teología en varias universidades, entre ellas la de París; aunque al principio acepta algunas de las ideas luteranas, muy pronto piensa que Lutero ha conservado demasiadas cosas de la Iglesia católica que debían ser suprimidas. Calvino también opina que el hombre debe acceder a la fe por medio de la lectura de la Biblia, pero considera que se debían de eliminar todos los sacramentos de la Iglesia católica, incluyendo los tres que había conservado Lutero. Para él todas las imágenes debían ser eliminadas de los templos religiosos.

Calvino también pensaba que no debían existir ni sacerdotes ni obispos y que los jefes religiosos debían ser pastores elegidos por la congregación; pero la teoría religiosa más importante que Calvino predicó como producto de su libre interpretación de la Biblia es la predestinación: según esta teoría el hombre por sí mismo no puede hacer nada para alcanzar la salvación, ni por la fe ni por las obras, sino que antes de nacer Dios ya ha elegido a un hombre para la condenación o la salvación y el hombre no puede hacer nada para cambiar el designio divino. En la sociedad humana se puede distinguir a los hombres elegidos para su salvación en los que llevan una vida virtuosa y sin pecado y en los que tienen riquezas y éxito material en la vida, pues eso es signo de la protección de Dios.

Calvino empezó a exponer sus ideas en París, pero como Francia era católica tuvo que huir del Reino y refugiarse en el extranjero. Ya empezaba a ser conocido entre los protestantes europeos como un hombre firme y enérgico, un gran teólogo y un buen organizador que sabía dirigir a los hombres, y por esta razón fue llamado por los protestantes de Ginebra.

Cuando Calvino llega a Ginebra, toma la decisión de que si quiere imponer una nueva «iglesia» que adopte sus puntos de vista religiosos tiene que controlar el gobierno de la ciudad; intenta dar órdenes al consejo municipal, que termina por expulsarle de Ginebra.

Sin embargo, la situación en Ginebra continuaba sin aclararse, las autoridades de la ciudad eran incapaces de organizar una «Iglesia» nueva y Calvino seguía teniendo partidarios en la ciudad; estos partidarios convencen a las autoridades de Ginebra para que permitan el regreso de Calvino a Ginebra con la promesa de que no se entrometerá en el gobierno político de la ciudad. Y las autoridades autorizan el regreso de Calvino a Ginebra en 1541.

Calvino ha aprendido la lección y ha comprendido que no puede manifestar abiertamente su deseo de controlar políticamente la ciudad; sin embargo, no renuncia a hacerse con el poder de Ginebra, que para él era indispensable para fundar su nueva Iglesia. Durante doce años Calvino va a llevar a cabo una paciente labor para ganarse partidarios en el gobierno de la ciudad, aumentar su influencia en Ginebra hasta que llegase el día en que el gobierno y todas sus instituciones estuvieran bajo su control. Cuando ya Calvino está a punto de controlar el gobierno se produce la ejecución en la hoguera del español Miguel Servet.

Miguel Servet era un humanista español típico de la época del Renacimiento, tenía curiosidad por todas las materias desde la ciencia a la medicina pasando por la filosofía y la religión. Como muchos hombres de su tiempo estaba descontento con la Iglesia católica y rechazaba la doctrina católica milenaria. Servet desarrolló sus propias ideas religiosas y llegó a creer que Jesucristo no había sido hijo de Dios, que sólo tenía naturaleza humana y no divina; esto era adoptar una corriente de los primeros siglos del cristianismo, que la Iglesia católica había condenado por herética en el sigloIV y que todos los protestantes rechazaban con escándalo. Servet fue a estudiar a las universidades francesas y también en la de París, donde conoció a Calvino. Allí Calvino comenzó a tener un profundo odio hacia aquel español al que consideraba un peligroso hereje.

A causa de sus opiniones, Servet tuvo que escapar de París, cambió de nombre y se instaló como médico en Vienne de Isère, localidad cerca de la frontera con Suiza; tuvo éxito como médico y llegó a adquirir una respetable situación económica y fue en esos años cuando descubrió la circulación de la sangre.

Pero Servet seguía con sus inquietudes religiosas y escribió un libro sobre sus doctrinas acerca de Jesucristo, que hizo imprimir clandestinamente en una imprenta secreta.

Pero Servet cometió el error de escribir a Calvino en Ginebra enviándole ejemplares de su libro, y Calvino en una carta lo denuncia a la Inquisición francesa católica. Sin embargo, Servet tenía amigos que le protegieron y ayudaron a ocultar su imprenta, y la Inquisición católica renunció a investigar. Pero Calvino envió las cartas que el propio Servet había escrito; las cartas eran una prueba irrefutable de que aquel médico era el español Miguel Servet.

La Inquisición católica condena a Servet a la hoguera pero la noche antes de la ejecución sus amigos ayudan a Servet a escapar. Servet no sabe la influencia política que Calvino tiene en Ginebra y comete el tremendo error de intentar refugiarse en esa ciudad, creyendo que allí estaría seguro. En Ginebra, Calvino lo reconoce y consigue que las autoridades de la ciudad lo detengan como hereje. Calvino quiere que se juzgue a Servet y se le queme en la hoguera, pero todavía no controla del todo el gobierno de la ciudad y el juicio de Servet se va a convertir en un pulso entre Calvino y los gobernantes de la ciudad que se oponen a él, pero finalmente Calvino se impone y Servet es condenado a la muerte en la hoguera.

La muerte de Servet, alejó de Calvino a una serie de protestantes europeos que se habían refugiado en Ginebra. Estos protestantes también tenían sus propias ideas religiosas, sintieron sus vidas amenazadas y escaparon de la ciudad; el más famoso de estos refugiados fue Sebastián Castellion que desde el extranjero denunció a Calvino por la muerte de Servet defendiendo la tolerancia religiosa y el derecho del hombre a tener sus propias opiniones; Castellion es considerado el padre de la libertad de pensamiento en Europa.

Pero la muerte y el juicio de Servet le sirvieron a Calvino para hacerse definitivamente con el gobierno de la ciudad, los adversarios de Calvino fueron expulsados del gobierno municipal y algunos de ellos ejecutados. Ahora toda Ginebra obedecía las órdenes de Calvino.

Calvino quiso hacer de Ginebra la capital religiosa de un nuevo cristianismo y quiso obligar a sus habitantes a la fuerza a llevar una vida virtuosa y cristiana: se suprimieron todos los bailes, se prohibieron todas las canciones, se prohibieron todos los espectáculos y representaciones teatrales, se cerraron las tabernas y se prohibieron las bebidas y las borracheras, todos debían ser buenos cristianos a la fuerza.

Toda Ginebra se convirtió en una ciudad calvinista dedicada solo al trabajo y a la oración. Pero Calvino quería extender toda su comunidad cristiana por toda Europa y en Ginebra se fundaron escuelas calvinistas para todos los protestantes extranjeros que visitaban la ciudad; estos extranjeros debían regresar a sus países de origen y enseñar allí la doctrina calvinista. 

El más importante de estos extranjeros fue el escocés John Knox, que consiguió que toda Escocia se convirtiera al calvinismo; en Escocia los calvinistas recibieron el nombre de presbiterianos. Escocia fue el único país donde el calvinismo se convirtió en religión oficial, pero también llegó a ser mayoritario en Holanda y hubo importantes minorías calvinistas en Alemania, Inglaterra y en Francia; en Inglaterra los calvinistas recibieron el nombre de puritanos y en Francia se les dio el nombre de hugonotes.

La reforma se extendió rápidamente por toda Europa, y en particular en el reino de Hungría, donde adquirió connotaciones políticas muy serias. Los húngaros enfrentaron a los turcos otomanos durante varios siglos, hasta que finalmente en la batalla de Mohács en 1526, los musulmanes derrotaron a los húngaros y murió el rey LuisII de Hungría (ya para la época del rey LuisII, el poder real se había enfrentado a protestantes húngaros que pretendían contraponer al catolicismo).

Pronto el Reino de Hungría tras la derrota se dividió en tres partes: una al oeste controlada por los germánicos, donde FernandoI de Habsburgo, hermano del emperador del Sacro Imperio Romano Germánico fue coronado como rey húngaro; una región central controlada por los otomanos, y una región oriental independiente en la forma del Principado de Transilvania, donde el conde húngaro Juan Segismundo Szapolyai fungió de Príncipe. Szapolyai y toda Transilvania, si bien eran independientes, el Estado era vasallo del Imperio otomano y el sultán decidía qué noble húngaro ocuparía el trono del Principado y cuáles serían sus movimientos políticos principales.

De esta forma el nuevo mapa húngaro tomó forma, y Transilvania y sus Príncipes se convirtieron en las figuras representantes de Hungría, contraponiendo a la dinastía de los Habsburgo que por otra parte portaba la corona del reino. Juan Segismundo Szapolyai se convirtió al protestantismo y comenzó a albergar a todos los filósofos y religiosos checos y germánicos que huían del Imperio Germánico y de los Habsburgo. La estrategia de Szapolyai y de los posteriores fue utilizar al protestantismo como instrumento contra los Habsburgo fielmente católicos, de los cuales querían deshacerse para así poder reunificar el reino húngaro dividido bajo la figura húngara de mayor relevancia, el líder transilvano.

En el año de 1541 se publicó la primera traducción al húngaro del Nuevo Testamento, obra del monje "Juan Sylvester" y luego en 1590 el pastor protestante Gáspár Károli publicó la primera Biblia completa en húngaro, conocida como la "Biblia de Károli". Los príncipes transilvanos promovieron las escuelas protestantes, las cuales eran cada vez más populares en ciudades como Bratislava, Sopron, Szárlőrinc, Sárvár, de igual forma como los asentamientos sajones en el Principado húngaro independiente. Al mismo tiempo, el nuevo movimiento religioso protestante de Juan Calvino, conocido como calvinismo continuó con la misión del luteranismo y pronto la región Nor-Este del Reino húngaro se sumergió a tal punto en la nueva confesión religiosa, que la ciudad de Debrecen era denominada en aquella época «La Roma Calvinista».

Uno de los pensadores protestantes más significativos fue el pastor húngaro Ferenc Dávid (1510-1579), quien primero profesó el luteranismo y luego el calvinismo, posteriormente se volvió un gran defensor del antitrinitarismo, es decir, no aceptaba la existencia de la Santísima Trinidad, y de esta manera, pronto se convirtió en el fundador del unitarismo en Transilvania, agregando aún una confesión religiosa más al gran mosaico existente en aquel tiempo. La diversidad religiosa en el Principado alcanzó tales niveles, que el príncipe Juan Segismundo Szapolyai, de confesión protestante, aconsejado por sus religiosos, convocó a la Gran Asamblea transilvana en la cual se sancionó el Edicto de Turda en 1568. Este documento rezaba que todas las confesiones religiosas cristianas eran aceptadas por igual en el Principado húngaro. Así, éste sería el primer Estado en el mundo en reconocer la diversidad de culto cristiano: catolicismo, luteranismo, calvinismo y unitarismo.

Posteriores príncipes como el barón húngaro Esteban Bocskai (1605-1606) y el conde húngaro Gabriel Bethlen (1613-1629) fueron fuertes defensores del protestantismo en Transilvania y Hungría, catapultando a los húngaros a un nivel cultural, socio-político y económico de desarrollo a la par con Francia, el Imperio Germánico e Inglaterra. Ellos condujeron guerras de independencia contra los Habsburgo, e inglusive Gabriel Bethlen participó en la guerra de los Treinta Años (1618-1648) apoyando la confederación protestante.

La situación religiosa en la región central del reino húngaro se inclinaba igualmente hacia el protestantismo, pues ahí eran raramente vistos los sacerdotes católicos. Los predicadores protestantes contaban con la protección de los otomanos, quienes en realidad no se entrometían en los conflictos religiosos de los cristianos, sino que lo único que les importaba era que estos pagasen los impuestos a los turcos. Como era de suponerse, en la región del reino húngaro bajo control germánico, el catolicismo se mantuvo con gran fortaleza, y si bien los predicadores protestantes eran comunes, la influencia del rey Habsburgo no permitía su predominancia.

Posteriormente los Habsburgo introdujeron en Hungría la contrarreforma católica, y la ardua labor de los jesuitas como el cardenal Pedro Pázmány lograron la paulatina conversión de gran parte de la población al catolicismo (sin embargo, Transilvania permanecerá, a grandes rasgos, protestante hasta la época actual).

1454: Johannes Gutenberg publica la primera Biblia impresa con caracteres móviles.

1483: 10 de noviembre; nace Martín Lutero en Eisleben (Alemania).

1484: 1 de enero; nace Ulrico Zuinglio en Wildhaus (Suiza).

1505: 17 de junio; Lutero entra en el convento agustino de Erfurt (Alemania).

1509: 10 de julio; nace Juan Calvino en Noyon (Francia).

1512: Lutero se convierte en profesor de enseñanza de la Biblia en la Universidad de Wittenberg.

1517: 31 de octubre; Lutero clava las noventa y cinco tesis en el portal de la Iglesia del Castillo de Wittenberg.

1518: En Heidelberg Lutero expone la defensa de 28 de sus 95 tesis en la disputa de Heidelberg.

1519: Zuinglio comienza a predicar en Zúrich.

1520: 15 de junio; León X emite la Exsurge Domine bula en la que intima a Lutero a que se retracte de 41 de sus 95 tesis.

1521: En enero, Lutero es excomulgado.

1523: En Zúrich, disputa acerca de la predicación según las Escrituras, la misa y las imágenes.

1524: El Consejo de Zúrich permite la retirada de las imágenes de las iglesias.

1525: Los Doce Artículos de Memmingen expresan las demandas de los campesinos alemanes.

1529: 21 de febrero; la Segunda Dieta de Espira pone fin a la tolerancia de los luteranos en los territorios católicos. Seis príncipes y catorce ciudades protestan contra la decisión, dando lugar al término «protestante».

1530: El teólogo luterano Philipp Melanchthon elabora las Confesiones de Augsburgo, la primera exposición de la fe cristiana luterana presentada al emperador CarlosV en la Dieta de Augsburgo (de allí el nombre) como un intento de reconciliación.

1531: 11 de octubre; Zuinglio muere en la batalla de Kappel.

1532: En el Sínodo de Chanforan (en los Valles Valdenses de Piamonte), los valdenses se adhieren a la Reforma.

1534: El Parlamento inglés promulga el acto de supremacía que libera a la Iglesia inglesa de cualquier subordinación al papado.

1536: Juan Calvino publica la primera edición de la institución de la religión cristiana.

1538: Calvino es expulsado de Ginebra por decisión del Consejo de la ciudad.

1539: La Compañía de Jesús, constituida en 1534, recibe la aprobación papal.

1541: Llaman a Calvino de regreso a Ginebra.

1542: El papa PabloIII con la bula "Licet ab initio" instruye a la Inquisición romana.

1545: 13 de diciembre; se abre el Concilio de Trento.

1546: 18 de febrero; muere Lutero en Eisleben.

1549: Pier Paolo Vergerio, obispo de Capodistria, se retira a los Grigiones para escapar de un proceso inquisitivo con la acusación de herejía protestante.

1553: Condena por herejía y ejecución del antitrinitario Miguel Servet en Ginebra.

1555: Paz de Augsburgo. Las divisiones territoriales existentes en el Sacro Imperio Romano se reconocen con el principio "«Cuius regio, eius religio»".

1559: El teólogo John Knox, después de pasar algunos años en el exilio en Ginebra, regresa a Escocia para dirigir la Reforma, e instituye la Iglesia Presbiteriana (calvinista).

1561: El 5 de junio, con los Acuerdos de Cavour, los Saboya garantizan a los valdenses la oportunidad de profesar públicamente su fe en los territorios de sus valles.

1562: Estallan las guerras de religión en Francia, que se extenderán hasta el Edicto de Nantes (1598).

1563: Bajo el reino de Elisabeth I, se publican los «39 Artículos de Religión», una confesión de fe fundamental de la Iglesia Anglicana.

1564: 27 de mayo; Juan Calvino muere en Ginebra.

1572: 24 de agosto; durante la Cuarta Guerra de Religión, tendrá lugar la masacre de 20,000 hugonotes en la noche de San Bartolomé.

1598: El Edicto de Nantes pone fin a las guerras de religión en Francia, declarando la religión del Estado el catolicismo y, al mismo tiempo, concediendo una serie de derechos los hugonotes, incluida la libertad de culto (con la excepción de París y algunas otras ciudades).




</doc>
<doc id="41577" url="https://es.wikipedia.org/wiki?curid=41577" title="Somatostatina">
Somatostatina

La somatostatina (o GHIH, del inglés "Growth Hormone Inhibiting Hormone") es una hormona peptídica con 14 aminoácidos producida por el hipotálamo y por las células delta de los islotes de Langerhans en el páncreas. Inhibe la producción de hormona del crecimiento por la hipófisis, tiene también efectos sobre el páncreas, donde inhibe la secreción de insulina y glucagón. Fue la primera proteína recombinante producida en "E. coli". Esto supuso un éxito científico al conseguir obtener por primera vez una proteína recombinante. Fórmula global: CHNOS

Inhibe la síntesis y/o secreción de la hormona del crecimiento (GH, STH o somatotropina) por parte de la adenohipófisis o hipófisis anterior, por lo que es una hormona de anti-crecimiento. También inhibe el eje hipotálamo-hipófisis-tiroides, bloqueando la respuesta de la hormona estimulante de la tiroides (TSH o tirotropina) a la hormona liberadora de tirotropina o TRH. Además, los tumores carcinoides pueden expresar receptores para la somatostatina y, por otra parte, se ha descubierto que tiene funciones como neurotransmisor en el sistema nervioso central.

Otros efectos fisiológicos de la somatostatina pancreática son:


La secreción de la somatostatina está estimulada a nivel gastrointestinal (por la mucosa gastrointestinal) y regulada por los altos niveles de glucosa, aminoácidos, glucagón, ácidos grasos libres y de diversas hormonas gastrointestinales. Su déficit o su exceso provocan indirectamente trastornos en el metabolismo de los carbohidratos. También es secretada por el hipotálamo y por otras zonas del sistema nervioso central (región paraventricular anterior, capa externa de la eminencia media, órgano subcomisural, glándula pineal). 



</doc>
<doc id="41579" url="https://es.wikipedia.org/wiki?curid=41579" title="Conservatorio Nacional de Música (Argentina)">
Conservatorio Nacional de Música (Argentina)

El Conservatorio Nacional de Música fue un organismo federal argentino cuya sede se ubicaba en el cruce de las avenidas Callao y Las Heras, en la ciudad de Buenos Aires. Fue fundado por el compositor Carlos López Buchardo en 1924 y en 1982 se trasladó a su sede en Avenida Córdoba 2445. 

El origen del “Conservatorio de Música de la Ciudad de Buenos Aires” se remonta a 1989, cuando el Ministerio de Educación y Justicia da la Resolución 643/89 dividiendo el "Conservatorio Nacional de Música Carlos López Buchardo” en dos establecimientos: 

Con la Ley 24.049, promulgada el 2 de enero de 1992, se transfiere el Ciclo Medio de dicho Conservatorio a la Municipalidad de la Ciudad de Buenos Aires. . El Ciclo Superior de la Institución permaneció en la órbita del Ministerio Nacional de Cultura y Educación.

En los últimos años, administrativa y jerárquicamente, estuvo inscrito en el sistema universitario argentino (IUNA). Su sede se encuentra en la Avenida Córdoba 2445, capital federal.
Luego el Ex.-Conservatorio Nacional de Música y Arte Escénico Carlos López Buchardo, que se dividió en dos por Decreto del Ejecutivo pasando a la Ciudad de Buenos Aires una parte y tomando el nombre del Conservatorio Superior de Música de la Ciudad de Buenos Aires "Astor Piazzolla” (Sarmiento 3401, Almagro, CABA).- Este último toma nivel inicial -denominado Iniciación Musical y formado por dos años de estudios- y el llamado Primer Ciclo o Ciclo Elemental -constituido por los siguientes cuatro años de estudios y el primero se queda con los ciclos Superiores, de Arte, ubicado en la Av.Córdoba 2445.-



</doc>
<doc id="41580" url="https://es.wikipedia.org/wiki?curid=41580" title="Placa dental">
Placa dental

Se llama placa dental a una acumulación heterogénea de una comunidad microbiana variada, aerobia y anaerobia, rodeada por una matriz intercelular de polímeros de origen salival y microbiano. Estos microorganismos pueden adherirse o depositarse sobre las paredes de las piezas dentarias. Su presencia puede estar asociada a la salud, pero si los microorganismos consiguen los sustratos necesarios para sobrevivir y persisten mucho tiempo sobre la superficie dental, pueden organizarse y causar caries, gingivitis (enfermedades de las encías) o Periodontitis.

Las encías enrojecidas, inflamadas o que sangren pueden ser las primeras señales de una gingivitis. Si la enfermedad es ignorada, los tejidos que mantienen a los dientes en su lugar pueden comenzar a destruirse y eventualmente se pierden los dientes.

La placa dental se forma en la superficie de dientes, encía y restauraciones, y difícilmente puede observarse, a menos que esté teñida. Su consistencia es blanda, mate, color blanco-amarillo. Se forma en pocas horas y se elimina principalmente con el arrastre mecánico del cepillo dental. Además, puede eliminarse con chorros de agua a presión como los que ofrecen los irrigadores dentales. Lo más importante para su eliminación es el arrastre mecánico de la placa bacteriana. Varía de un individuo a otro, y también varía su localización anatómica. Si la placa dental se calcifica, puede dar lugar a la aparición de cálculos o sarro tártaro.

La matriz intracelular + las bacterias+las células grupales

La matriz intracelular es un entramado orgánico con origen de bacterias, formado por restos de la destrucción de bacterias y polisacáridos de cadena muy largas sintetizados por las propias bacterias a partir de las sales de la dieta. Tiene tres funciones: sujeción, sostén y protección de las bacterias de la placa.

Las bacterias de la placa dental son muy variadas: hay unos 200-300 tipos. Las características bacterianas de cariogenicidad son las siguientes:

Se trata de un revestimiento insoluble que se forma de manera natural y espontánea en la superficie dentaria. Es una película orgánica de origen salival, libre de elementos celulares, que se forma por depósito selectivo de glucoproteínas salivales en la superficie de la hidroxiapatita del esmalte dental. 
Tiene dos funciones principales: 

Formada la película, es colonizada por microorganismos de la cavidad bucal (residentes); el primer microorganismo que establece esta unión es el "Streptococcus sanguis", posteriormente seguirán coagregándose muchos más microorganismos, sobre todo del género Gram + y aerobios.




El término "placa bacteriana" ya no se utiliza, porque se han aislado de la placa virus, micoplasmas, hongos, protozoarios y rickettsias y arqueas o archaeas. Debido a esto, se usa el término "biofilm oral", ya que hoy existen evidencias que la masa microbiana no se limita a formarse solo en los dientes, y el complejo microbiano ofrece la posibilidad de englobar millones de microorganismos en comunidades bien organizadas.



</doc>
<doc id="41582" url="https://es.wikipedia.org/wiki?curid=41582" title="Enfermedad de Batten">
Enfermedad de Batten

La enfermedad de Batten es una enfermedad hereditaria mortal que afecta al sistema nervioso y que comienza en la niñez. Los primeros síntomas de este trastorno aparecen generalmente entre las edades de 5 y 10 años, cuando los padres o los médicos advierten que un niño previamente normal ha comenzado a presentar convulsiones o problemas de visión. En algunos casos los primeros signos son sutiles, manifestándose en cambios de personalidad y del comportamiento, lentitud en el aprendizaje, torpeza o tropiezos al caminar. 

Al pasar del tiempo, los niños afectados padecen incapacidades mentales, convulsiones más graves y la pérdida progresiva de la vista y de las capacidades motrices. Los niños que padecen la enfermedad de Batten terminan quedando ciegos, postrados en una cama y hasta dementes. La enfermedad de Batten a menudo es mortal al llegar a los últimos años de la adolescencia o al llegar a la edad de 20 años.

La enfermedad de Batten recibe su nombre del pediatra británico Frederick Batten que la describió por primera vez en 1903. También conocida como enfermedad de Spielmeyer-Vogt-Sjogren-Batten, es la forma más común de un grupo de trastornos llamados ceroidolipofuscinosis neuronales (NCL por su sigla en inglés). Aunque la enfermedad de Batten se considera generalmente como la forma juvenil de NCL, algunos médicos suelen utilizar el término de enfermedad de Batten para describir todas las formas de ceroidolipofuscinosis. Se incluye dentro de las lipidosis o enfermedades por almacenamiento de lípidos.

Existen otros tres tipos principales de ceroidolipofuscinosis neuronales (NCL), incluyendo dos formas que empiezan al principio de la niñez y una forma muy poco común que afecta a los adultos. Los síntomas de estos tres tipos son similares a los causados por la enfermedad de Batten, pero llegan a ser evidentes en diversas edades y progresan en modos distintos.


La enfermedad de Batten y otras formas de ceroidolipofuscinosis neuronales son relativamente raras y se dan entre 2 a 4 de cada 100 mil nacimientos en los Estados Unidos. Estos trastornos parecen ser más comunes en Finlandia, Suecia, otras partes de Europa del Norte y en la provincia de Terranova en Canadá. Aunque las ceroidolipofuscinosis neuronales se clasifican como enfermedades raras, atacan a menudo a más de una persona en las familias que presentan los genes defectuosos.

Las ceroidolipofuscinosis neuronales infantiles son trastornos autosómicos recesivos; es decir, ocurren solamente cuando un niño hereda dos copias del gen defectuoso, uno de cada padre. Cuando ambos padres poseen un gen defectuoso, cada uno de sus niños tiene una posibilidad en cuatro de padecer una ceroidolipofuscinosis neuronal. Al mismo tiempo, cada niño también tiene una posibilidad en dos de heredar solo una copia del gen defectuoso. A los individuos que poseen solamente un gen defectuoso se les conoce como portadores, lo que significa que ellos no desarrollan la enfermedad pero que pueden trasmitir el gen a sus propios niños. Debido a que se conocen cuáles son los genes mutados que están involucrados en ciertas formas de la enfermedad de Batten, la detección del portador es posible en algunos casos.

La ceroidolipofuscinosis neuronal del adulto puede ser heredada como un trastorno autosómico recesivo y, con menos frecuencia, como un trastorno autosómico dominante. En casos de herencia autosómica dominante, los individuos que heredan una sola copia del gen de la enfermedad la desarrollan. Por lo tanto, no hay portadores del gen que no estén afectados por la enfermedad.

Los síntomas de la enfermedad de Batten y de otras ceroidolipofuscinosis neuronales se asocian a una acumulación de sustancias llamadas lipofucinos (lipopigmentos) en los tejidos del cuerpo. Estos lipopigmentos se componen de grasas y de proteínas. Su nombre viene de la palabra técnica lipo, que es una abreviación de la palabra "lípido" o grasa y del término pigmento, denominado así porque las sustancias adquieren un color amarillo verdoso cuando se les visualiza con un microscopio de luz ultravioleta. Los lipopigmentos se acumulan en las células del cerebro y del ojo, así como en la piel, los músculos y muchos otros tejidos. En las células, estos pigmentos forman depósitos con formas particulares que pueden visualizarse con un microscopio electrónico. Algunos parecen medialunas, otros parecen huellas digitales. Los médicos buscan estos depósitos cuando examinan una muestra de piel para diagnosticar la enfermedad de Batten.

Los defectos bioquímicos que constituyen la base de varias ceroidolipofuscinosis neuronales se han descubierto recientemente. Se ha demostrado que una enzima llamada tioesterasa de proteína palmitóilica tiene una actividad insuficiente en la enfermedad de Batten infantil (o CLN1). En la forma preadolescente de esta enfermedad (CLN2), se ha detectado que su causa es una deficiencia de una proteasa ácida, una enzima que hidroliza las proteínas. Se ha identificado un gen mutado en la enfermedad de Batten juvenil (CLN3), pero no se ha identificado la proteína correspondiente a este gen.

Debido a que la pérdida de la vista es a menudo un síntoma precoz, la enfermedad de Batten se puede detectar primero durante un examen de la vista. Un oftalmólogo puede detectar una pérdida de células en el ojo que ocurre en las tres formas infantiles de ceroidolipofuscinosis neuronales. Sin embargo, debido a que esta pérdida de células ocurre en otras enfermedades oftalmológicas, el trastorno no puede ser diagnosticado únicamente por este síntoma. A menudo, el oftalmólogo u otro médico especialista puede sospechar la presencia de una ceroidolipofuscinosis neuronal puede referir al niño a un neurólogo, un médico que se especializa en las enfermedades del cerebro y del sistema nervioso.

Para diagnosticar una ceroidolipofuscinosis neuronal, el neurólogo necesita el historial médico y varias pruebas de laboratorio del paciente. Las pruebas de diagnóstico utilizadas para detectar las ceroidolipofuscinosis neuronal incluyen:


Hasta ahora, no se conoce ningún tratamiento específico que pueda detener o revertir los síntomas de la enfermedad de Batten o de otras ceroidolipofuscinosis neuronales. Sin embargo, en algunos casos se pueden reducir o controlar las convulsiones con medicamentos anticonvulsivos y se pueden tratar otros problemas médicos apropiadamente a medida que se presentan. Al mismo tiempo, la terapia física y ocupacional puede ayudar a los pacientes a conservar el funcionamiento de su organismo por el mayor tiempo posible.

Algunos informes han señalado retardos de la enfermedad en niños que padecen de la enfermedad de Batten que fueron tratados con las vitaminas C y E y con dietas bajas en vitamina A. Sin embargo, estos tratamientos no previnieron que los pacientes murieran a causa de la enfermedad.

Ayudar y estimular a los pacientes puede contribuir a que ellos y sus familias puedan hacer frente a las situaciones graves de incapacidad y demencia causadas por las ceroidolipofuscinosis neuronales. A menudo, los grupos de ayuda permiten a niños, adultos y a las familias afectadas compartir preocupaciones y experiencias comunes.

Mientras tanto, los científicos prosiguen con la investigación médica que podría dar como resultado un tratamiento eficaz en el futuro.

En el gobierno federal de los Estados Unidos, el punto focal que lleva a cabo la investigación sobre la enfermedad de Batten y otros trastornos neurogenéticos es el Instituto Nacional de Trastornos Neurológicos y Accidentes Cerebrovasculares (NINDS por su sigla en inglés). El NINDS, que forma parte de los Institutos Nacionales de la Salud, es responsable de apoyar y llevar a cabo las investigaciones sobre el cerebro y el sistema nervioso central. Gracias al trabajo de varios equipos científicos, la búsqueda de las causas genéticas de las ceroidolipofuscinosis neuronales está cobrando velocidad.

Otros investigadores también están trabajando en identificar qué sustancias constituyen los lipopigmentos. Aunque los científicos saben que los depósitos de los lipopigmentos contienen grasas y proteínas, no ha podido determinarse con exactitud la identidad de muchas de las moléculas de dichos depósitos. Los científicos han descubierto pistas potencialmente importantes. Por ejemplo, un científico patrocinado por el NINDS ha utilizado modelos animales de ceroidolipofuscinosis neuronales y ha descubierto que gran parte de este material acumulado es una proteína llamada sub-unidad c. Esta proteína se encuentra normalmente dentro de las mitocondrias de las células, estructuras pequeñas que producen la energía que las células necesitan para hacer su trabajo. Actualmente, los científicos están trabajando en entender qué papel puede desempeñar esta proteína en las ceroidolipofuscinosis neuronales, incluyendo cómo esta proteína termina encontrándose en la ubicación incorrecta y acumula células enfermas interiores. Otros investigadores también están examinando estos depósitos para identificar qué otras moléculas contienen.

Además, los científicos investigadores están trabajando con modelos animales de ceroidolipofuscinosis neuronales para entender y tratar mejor estos trastornos. Por ejemplo, un equipo de investigación está probando la utilidad del trasplante de médula en un modelo de ovejas, mientras que otros investigadores están trabajando en desarrollar modelos en ratas. Los modelos en ratas facilitarán que los científicos estudien la genética de estas enfermedades, puesto que las ratas se reproducen rápidamente y con gran frecuencia.





</doc>
<doc id="41583" url="https://es.wikipedia.org/wiki?curid=41583" title="Hidrocefalia">
Hidrocefalia

La hidrocefalia (término que deriva de las palabras griegas «hidro» que significa agua y «céfalo» que significa cabeza) es un trastorno cuya principal característica es la acumulación excesiva de líquido cefalorraquídeo dentro del cráneo.

Esta se conocía antiguamente como "agua en el cerebro", aunque este líquido no es agua sino que es líquido cefalorraquídeo (LCR), un líquido transparente que rodea el cerebro y la médula espinal.

La acumulación excesiva de líquido cefalorraquídeo tiene como consecuencia una dilatación anormal de los espacios en el cerebro llamados ventrículos. Esta dilatación ocasiona una presión potencialmente perjudicial en los tejidos del cerebro, generalmente el aumento del líquido en los ventrículos cerebrales es producido por la obstrucción de los conductos situados por debajo del cerebelo.

El sistema ventricular está constituido por cuatro ventrículos conectados por vías estrechas. Normalmente, el líquido cefalorraquídeo fluye a través de los ventrículos, sale a cisternas (espacios cerrados que sirven de reservorios) en la base del cerebro, baña la superficie del cerebro y la médula espinal y, luego, es absorbido en la corriente sanguínea.

El líquido cefalorraquídeo tiene tres funciones vitales importantes:



Esta enfermedad puede ser heredada o adquirida en una fase adulta, donde se dan más casos a partir de los 60 años. Las causas más frecuentes de la hidrocefalia son:


Estudiando el caso y los síntomas que producen en el paciente y gracias a una resonancia magnética cerebral se puede diagnosticar la enfermedad.

El tratamiento consiste en liberar la causa que produce la obstrucción del líquido cefalorraquídeo. También puede usarse una sonda para desviar a su flujo normal el LCR hacia el abdomen en donde puede reabsorberse, esta sonda se implantaría quirúrgicamente.

La hidrocefalia puede ser congénita o adquirida. La hidrocefalia congénita se halla presente al nacer y puede ser ocasionada por influencias ambientales durante el desarrollo del feto o por predisposición genética. La hidrocefalia adquirida se desarrolla en algún momento después del nacimiento. Este tipo de hidrocefalia puede afectar a personas de todas las edades y puede ser ocasionado por una lesión o una enfermedad que causa agrandamiento de los ventrículos a consecuencia de un aumento del volumen del líquido cefalorraquídeo causando, por lo general, una obstrucción. La hidrocefalia también puede ser comunicante o no comunicante.

La hidrocefalia comunicante ocurre cuando el flujo del líquido cefalorraquídeo se ve bloqueado después de salir de los ventrículos al espacio subaracnoideo. Esta forma se denomina comunicante porque el líquido cefalorraquídeo aún puede fluir entre los ventrículos, que permanecen abiertos. La reabsorción de este líquido está alterada en las vellosidades aracnoideas por infecciones o hemorragias. Se caracteriza por una dilatación de las cavidades ventriculares del cerebro por delante del sitio de la obstrucción. Dependiendo de la velocidad de insaturación y la edad del paciente, puede ser una hidrocefalia aguda (caracterizada por herniación cerebral y muerte súbita) e hidrocefalia crónica, con signos y síntomas de aparición lenta e hipertensión endocraneana. Cursa con retraso mental en los niños y demencia en los adultos.

La hidrocefalia no comunicante, llamada también hidrocefalia obstructiva, ocurre cuando el flujo del líquido cefalorraquídeo se ve bloqueado a lo largo de una o más de las vías estrechas que conectan los ventrículos. Una de las causas más comunes de hidrocefalia es la estenosis acuaductal. La causa más frecuente es la hidrocefalia congénita, que afecta a 11 000 nacimientos, con obstrucción del acueducto de Silvio, un pequeño conducto entre el tercero y cuarto ventrículo en la mitad del cerebro. Otra causa es la malformación de Arnold-Chiari, asociada o heredada como rasgo ligado al cromosoma X. Puede también estar causada por tumores localizados en el tronco del encéfalo, cerebelo y región pineal o por hemorragias cerebrales y subaracnoideas o cicatrices derivadas de una meningitis.

La hidrocefalia ex vacuo también conocida como, agrandamiento compensatorio de espacios de líquido cefalorraquídeo, no es realmente una hidrocefalia pura sino una condición compensatoria a la pérdida de masa encefálica. Es una condición usualmente asintomática en personas de la tercera edad, que se da como resultado de la perdida relativa de volumen cerebral. Puede verse en condiciones patológicas como:

1.Degeneración generalizada de la masa encefálica ( Enfermedad de Alzheimer, Leucodistrofias, demencia vascular) 

2.Encefalomalacia debido a daño focal ( Por accidente cerebrovascular y lesiones traumáticas) 

La hidrocefalia de presión normal, o hidrocefalia normotensiva, ocurre comúnmente en las personas ancianas y está caracterizada por muchos de los mismos síntomas asociados con otras condiciones que ocurren más a menudo en los ancianos, tales como pérdida de memoria, demencia, trastorno patológico al andar, incontinencia urinaria y una reducción general de la actividad normal del diario vivir. Esta enfermedad como la anterior (ex vacuo) afecta precisamente a los adultos.



</doc>
<doc id="41584" url="https://es.wikipedia.org/wiki?curid=41584" title="Miastenia gravis">
Miastenia gravis

La miastenia gravis (MG) es una enfermedad neuromuscular autoinmune y crónica caracterizada por grados variables de debilidad de los músculos esqueléticos (los voluntarios) del cuerpo. La denominación proviene del latín y el griego, y significa «debilidad muscular grave». 

Empieza con un cuadro insidioso de pérdida de fuerzas, que rápidamente se recuperan con el descanso pero que reaparece al reiniciar el ejercicio. Suele iniciarse en los músculos perioculares. En pocas ocasiones el inicio es agudo. 

La característica principal de la miastenia gravis es una debilidad muscular que aumenta durante los períodos de actividad y disminuye después de períodos de descanso. Ciertos músculos —como los que controlan el movimiento de los ojos y los párpados, la expresión facial, la masticación, el habla y la deglución (tragar)—, a menudo se ven afectados por este trastorno. Los músculos que controlan la respiración y los movimientos del cuello y de las extremidades también pueden verse afectados, pero, por fortuna, con un control médico se puede controlar tal enfermedad.

La miastenia gravis es causada por un defecto en la transmisión de los impulsos nerviosos a los músculos. Ocurre cuando la comunicación normal entre el nervio y el músculo se interrumpe en la unión neuromuscular, el lugar en donde las células nerviosas se conectan con los músculos que controlan.

Normalmente, cuando los impulsos recorren el nervio, las terminaciones nerviosas secretan una sustancia neurotransmisora llamada acetilcolina. La acetilcolina se difunde a través del espacio sináptico en la unión neuromuscular, y se une a los receptores de acetilcolina, en la membrana post-sináptica. Los receptores se activan y generan una contracción muscular.

En la miastenia gravis, los anticuerpos bloquean, alteran, o destruyen los receptores de acetilcolina en la unión neuromuscular, lo cual evita que ocurra la contracción muscular. Estos anticuerpos son producidos por el propio sistema inmunitario del cuerpo. Por ende, la miastenia gravis es una enfermedad autoinmune, porque el sistema inmunitario, que normalmente protege al cuerpo de organismos externos, se ataca a sí mismo por error. Además, se ha demostrado que los pliegues post-sinápticos están aplanados o "simplificados", disminuyendo la eficacia de la transmisión. La acetilcolina es liberada normalmente, pero los potenciales generados son de intensidad inferior a la necesaria.

Los mecanismos por los cuales los anticuerpos disminuyen el número de receptores son tres:
Los anticuerpos son del tipo IgG dependientes de linfocitos T, por lo que el tratamiento inmunosupresor constituye una diana terapéutica.

Por otro lado, la actividad repetida acaba disminuyendo la cantidad de acetilcolina liberada (lo que se conoce como "agotamiento presináptico"). También conlleva una activación cada vez menor de fibras musculares por impulso sucesivo ("fatiga miasténica"). Estos mecanismos explican el aumento de la fatiga tras el ejercicio, y la estimulación decreciente en el electromiograma.

El timo, es un órgano situado en el área superior del pecho debajo del esternón (hueso en el centro del pecho) exactamente en el mediastino anterior, desempeña un papel importante en el desarrollo del sistema inmunitario durante la etapa fetal. Sus células forman una parte del sistema inmunitario normal del cuerpo. La glándula es bastante grande en los niños, crece gradualmente hasta la pubertad y luego se reduce en tamaño hasta ser reemplazada por tejido graso con la edad. En los adultos que padecen de miastenia gravis, la glándula del timo es anormal. Contiene ciertos racimos de células inmunitarias características de una hiperplasia linfoide, una condición que generalmente se presenta solamente en el bazo y los ganglios linfáticos durante una respuesta inmunitaria activa. Un 10 % de los pacientes que padecen de miastenia gravis desarrollan timomas. Los timomas generalmente son benignos, pero pueden llegar a ser malignos. Suelen ser por la aparición de células mioides (similares a miocitos), que también pueden actuar como productoras del autoantígeno, y generar una reacción autoinmune contra el timo.

No se ha entendido completamente la relación entre la glándula del timo y la miastenia gravis. Los científicos creen que es posible que la glándula del timo genere instrucciones incorrectas sobre la producción de anticuerpos receptores de acetilcolina, creando así el ambiente perfecto para un trastorno en la transmisión neuromuscular. Sin embargo, sí se ha demostrado que el 65 % de los pacientes miasténicos tienen un timo anómalo, y el 35 % lo tienen hiperplásico.

Clínica: con base en los signos y síntomas y según la gravedad se encuentran:

Prueba farmacológica: Principalmente se utiliza la prueba con bromuro de edrofonio (prueba de Tensilon) endovenoso la cual tiene una sensibilidad del 80 a 95 % en pacientes con miastenia gravis presentando mejoría inmediata sobre todo ante su facies característica. La dosis inicial es de 1 mg, seguida de 2,3 y 5 mg, en intervalos de 3 a 5 minutos. Para poder controlar síntomas gastrointestinales (o en casos menos frecuentes bradicardia e hipotensión) es necesario tener a mano una jeringa con atropina. Un 20 % de los pacientes con MGO tiene una prueba falsa negativa.

Pruebas electrofisiológicas: Se basa en la estimulación eléctrica repetitiva de distintos nervios para detectar una alteración de la transmisión neuromuscular. El estudio se realiza aplicando de cuatro a seis estímulos a una frecuencia de 2 Hz, antes y después de l30 segundos de ejercicio. Se repiten estos estímulos en intervalos de 1 minuto hasta 5 minutos después de finalizado el ejercicio. La prueba se considera positiva cuando existe una diferencia de amplitud de más del 10 % entre el primer y quinto potencial evocado. Esta prueba no es específica de la miastenia, ya que puede ser positiva en otras enfermedades neuromusculares. Otro método diagnóstico es la electromiografía de fibra aislada.

Anticuerpos contra receptores de acetilcolina: Esta prueba es la más fidedigna donde se va a identificar la presencia de ACRA en pacientes compatibles con miastenia gravis. Existen tres tipos de anticuerpos: bloqueantes, moduladores y de ligadura. Los ACRA están presentes en un 75-85 % de los pacientes con miastenia gravis generalizada. En entre un 10 y 50 % de estos enfermos se detectan otros anticuerpos frente a una enzima muscular (MuSK, siglas en inglés de "muscular specific kinase").Aunque los falsos positivos son raros, se han observado en pacientes con lupus eritematoso y con enfermedades hepáticas autoinmunes.

Estudios radiológicos: Un 10 % de los pacientes de miastenia gravis padecen timomas. La mayoría de estos tumores son normalmente benignos pero pueden ser localmente invasores. Por esta razón, se recomienda obtener una tomografía computarizada o una resonancia magnética de tórax en todo paciente diagnosticado.

Hoy día, casi todos los pacientes pueden reanudar una vida normal con un tratamiento adecuado. Los más importantes son los siguientes:

Se define como crisis miasténica a la exacerbación de la debilidad muscular capaz de poner en riesgo la vida del paciente. Generalmente ocurre por insuficiencia respiratoria causada por debilidad de la musculatura intercostal y el diafragma, y sólo suele ocurrir en pacientes mal controlados. 

Tratamiento de la crisis Miasténica

El tratamiento debe ser en una Unidad de Cuidados Intensivos, por un equipo con experiencia en el tratamiento de este tipo de crisis, de la insuficiencia respiratoria y de las complicaciones infecciosas y la terapia hidroelectrolítica.

Se debe excluir la posibilidad de que la crisis sea causada por un tratamiento excesivo de la medicación anticolinérgica (crisis colinérgica), suprimiendo el tratamiento temporalmente. Sin embargo, la causa más frecuente de la crisis es la infección recurrente, que altera la inmunidad del paciente. El enfermo miasténico con fiebre e infección debe ser tratado como cualquier paciente inmunodeprimido. Las bases del tratamiento consisten en la antibioterapia rápida y eficaz, la ventilación asistida y la fisioterapia respiratoria. La plasmaféresis y la IVIg suelen ser útiles para acelerar la recuperación del paciente.

Si bien es cierto la miastenia gravis es una enfermedad autoinmune que disminuye la fuerza física del paciente, es importante revisar la debilidad-fragilidad emocional provocada por esta enfermedad. Es muy importante que los familiares del paciente entiendan lo difícil que es vivir con una enfermedad que "no permite realizar las actividades diarias de manera común".

Además del tratamiento médico, es imprescindible brindar al paciente con miastenia gravis apoyo psicológico para que le oriente y ayude a entender estos cambios violentos en su conducta, entender los mecanismos de la enfermedad y la aceptación de la misma. Básicamente es poder disponer de un espacio de contención y de ayuda que aporten a la mejoría. 

El paciente miasténico no sólo debe realizar terapia psicológica sino que además debe evitar situaciones estresantes que puedan empeorar su miastenia. Por ejemplo: discusiones, peleas, disgustos, situaciones límites, etc.

También es importante hacer notar que el rendimiento en el trabajo o en los estudios se puede ver disminuido considerablemente, y que laboralmente requiere apoyo y evitar cargas de trabajo y/o estrés, pues de lo contrario seria un motivo para crear crisis miasténica. Por eso es importante incorporar todos los recursos mencionados en este apartado para lograr una mejoría constante y una mejor calidad de vida.

A diferencia de otras enfermedades como el cáncer, epilepsia, migraña (por mencionar alguna) la miastenia gravis es una enfermedad poco común que por él mismo desconocimiento, un paciente puede tardar años en tener un diagnóstico certero, ya que normalmente los síntomas son confundidos. Esta enfermedad tiene mayor prevalencia en el género femenino. El 60 % de ellas tiene entre 20 y 60 años.

El magnate griego Aristóteles Onassis tenía miastenia gravis y llegó a aparecer en público con cintas adhesivas pegadas en los párpados con el fin de poder mantenerlos abiertos.

La pianista venezolana de fama mundial Teresa Carreño (1853-1917) muere en Nueva York luego de que le fuera diagnosticada una miastenia grave la cual "causó la pérdida del control de los nervios y músculos de la cabeza, cara y garganta, y progresivamente se propagó hacia abajo hasta que alcanzó los órganos respiratorios y finalmente el corazón, el tan funesto día 12 de junio de 1917" (carta de su último esposo, Arturo Tagliapietra). 

Static Major entró al Baptist Hospital East en Louisville el 25 de febrero de 2008, quejándose de una tos persistente, ronquera, dificultad para hablar, mareos, dolores musculares y dificultad para respirar. Se le diagnosticó con miastenia gravis.

El ajedrecista brasileño Henrique Mecking a partir del año 1978 se le agravó una miastenia.




</doc>
<doc id="41586" url="https://es.wikipedia.org/wiki?curid=41586" title="Esclerosis múltiple">
Esclerosis múltiple

La esclerosis múltiple (EM) o esclerosis de placas es una enfermedad neurológica crónica de naturaleza inflamatoria y autoinmune caracterizada por el desarrollo de lesiones desmielinizantes, y de daño axonal en el sistema nervioso central. Es una de las principales causas de discapacidad neurológica de origen no traumático en adultos jóvenes, principalmente mujeres, afectando a entre 2.3 y 2.5 millones de personas en el mundo.

La causa de la enfermedad es desconocida, aunque se sabe que su desarrollo se asocia a una combinación de factores genéticos, que predisponen a padecer la enfermedad, y factores ambientales e infecciosos (infecciones virales y parasitosis), que desencadenan la aparición de Esclerosis Múltiple. Entre los factores ambientales se incluyen la deficiencia de vitamina D, toxinas ambientales, la dieta y hábitos nocivos como el tabaquismo. 

Asimismo, en su desarrollo o agravamiento se ha implicado la existencia de una permeabilidad intestinal aumentada o "intestino permeable", que permite el paso incontrolado de sustancias a la sangre (virus, bacterias, toxinas, alimentos no digeridos...), lo que puede provocar la respuesta anormal del sistema inmunitario. Los dos factores más potentes que provocan esta excesiva permeabilidad son ciertas bacterias intestinales y el gluten, en cualquier persona que lo come.

La inflamación en la EM se caracteriza por una infiltración linfocítica que produce daño en la mielina y en los axones. Inicialmente la inflamación es transitoria y es seguida de una remielinización variable, de forma tal que el comienzo de la enfermedad suele caracterizarse por períodos de disfunción neurológica seguidos de una recuperación total. Sin embargo, a medida que avanza en el tiempo, los cambios patológicos predominantes consisten en un aumento importante de la actividad de microglias, proliferación de astrocitos y gliosis junto a una neurodegeneración extensa y crónica, correlacionándose con una acumulación progresiva de discapacidad.

El cuadro clínico de la EM es muy hetérogeneo, pues incluye un amplio espectro de síntomas que varían entre pacientes e, incluso, en el mismo individuo. Así, se puede encontrar pacientes con manifestaciones neurológicas leves hasta casos con una enfermedad rápidamente progresiva y discapacitante. Dada la variedad de síntomas que produce, la Esclerosis Múltiple es conocida como <nowiki>"la enfermedad de las mil caras"</nowiki>. Se distinguen varios fenotipos de EM, principalmente formas recurrentes (con brotes) y poca discapacidad y formas progresivas con discapacidad acumulativa.

La EM se diagnostica cuando en la evaluación neurológica se encuentran síntomas y signos compatibles, acompañado de la demostración por resonancia magnética (IRM) de lesiones desmielinizantes en el sistema nervioso central. Los criterios de McDonald actualizados en 2017 son los admitidos internacionalmente para el diagnóstico de EM. El tratamiento consiste en fármacos que retrasan la acumulación de lesiones (tratamiento modificador de la enfermedad), tratamiento para las recaídas (brotes), tratamiento sintomático y de rehabilitación. Se recomienda un diagnóstico y tratamiento oportunos con el fin de prevenir la progresión de la discapacidad.

La esclerosis múltiple se caracteriza por dos fenómenos:
Como resultado, las neuronas del cerebro pierden parcial o totalmente su capacidad de transmisión, causando los síntomas típicos de adormecimiento, cosquilleo, espasmos, parálisis, fatiga y alteraciones en la vista.

En la variante remitente-recurrente también se ha detectado inflamación en el tejido nervioso y transección axonal, o corte de los axones de las neuronas, lo que hace que las secuelas sean permanentes.

Se han definido cuatro subtipos de esclerosis múltiple, no obstante, la primera manifestación de la enfermedad suele considerarse como un subtipo aparte denominado síndrome clínico aislado. 

A menudo, la neuromielitis óptica se diagnostica de forma incorrecta como esclerosis múltiple o se percibe como un tipo de esta, aunque se trata de una enfermedad distinta. De igual forma, la encefalomielitis aguda diseminada también es diagnosticada con frecuencia como un primer ataque grave de esclerosis múltiple, puesto que algunos síntomas pueden ser similares. Sin embargo, la EM no suele causar síntomas de encefalitis. 






La actividad de la enfermedad se establece considerando:
La enfermedad se considera «no activa», cuando los tres parámetros mencionados no han avanzado.

La descripción más antigua de una probable esclerosis múltiple se encuentra en la Saga de Thorlak, de Torlak de Islandia (1133-1193) donde se menciona la ceguera, sordera y los disturbios del lenguaje de Halla, una mujer vikinga, que se habría recuperado milagrosamente. Así mismo en un texto relacionado se describe el caso de otra mujer llamada Halldora que padecía de parálisis entre 1193 y 1198 cuyo cuadro clínico se corresponde con una EM remitente-recurrente.

Robert Hooper (1773-1835), un patólogo británico y médico en ejercicio, Robert Carswell (1793-1857), profesor británico de patología, y Jean Cruveilhier (1791-1873), un profesor de anatomía patológica francés, fueron los primeros en describir la enfermedad con ilustraciones médicas y detalles clínicos, pero no lo identificaron como una enfermedad distinta. Para ser más exactos, Carswell describió lesiones que eran "remarcables lesiones de la médula espinal acompañadas de atrofia".

También usando el microscopio, el patólogo sueco  (1836–1908) anotó en 1863 que las lesiones asociadas estaban distribuidas alrededor de los vasos sanguíneos.

Basándose en esto, Jean-Martin Charcot (1825-1893), un neurólogo francés, resumió los datos anteriores y realizó importantes contribuciones con las observaciones clínicas y patológicas propias. Reconoció la esclerosis múltiple (a la que denominó "sclérose en plaques disséminées" como una enfermedad distinta y separada).

Tras los trabajos de Charcot, varios investigadores como Eugène Devic (1858-1930), Jozsef Balo (1895-1979), Paul Ferdinand Schilder (1886-1940) y Otto Marburg (1874-1948) encontraron casos especiales de la enfermedad que ahora se conocen como formas limítrofes de la esclerosis múltiple, ya que algunos autores los clasifican como esclerosis múltiple y otros no. Estos casos son un caso clínico especial, la enfermedad de Devic, también conocida como esclerosis múltiple óptico-espinal o neuromielitis óptica, NMO), tres formas patológicas (esclerosis concéntrica de Balo, esclerosis de Schilder (o esclerosis difusa) y Esclerosis de Marburg, también llamada esclerosis aguda o esclerosis maligna), una forma asociada (neuropatía periférica autoinmune) y una enfermedad asociada (encefalomielitis diseminada aguda o ADEM).

Desde la casi completa erradicación de la poliomielitis, la esclerosis múltiple es ―tras la epilepsia― la enfermedad neurológica más frecuente entre los adultos jóvenes y la causa más frecuente de parálisis en los países occidentales. Afecta aproximadamente a 1 de cada 1000 personas, en particular a las mujeres.

Los primeros síntomas suelen aparecer en personas entre los 20 y los 40 años, pero también se puede presentar en niños o sobre los 50 años. En niños y personas jóvenes la incidencia en mujeres es el doble que la de los hombres; por otro lado, en paciente sobre 50 años, la incidencia es igual en ambos sexos. Otra excepción a la mayor incidencia en el sexo femenino es la forma primaria progresiva de la enfermedad (EMPP), ya que se desarrolla por igual en hombres y mujeres.

La EM es casi inexistente en los africanos negros y en las poblaciones indígenas de América y Oceanía, mientras que es muy prevalente en zonas alejadas del Ecuador, como Canadá y Nueva Zelanda.

En general, uno de cada 25 hermanos de un individuo con la enfermedad también se verá afectado. Si un gemelo univitelino se ve afectado, existe hasta un 50 % de probabilidad que el otro gemelo también enferme. Pero solo uno de cada 20 gemelos bivitelinos se verá afectado si su hermano ha enfermado. Si uno de los padres está afectado por la enfermedad, cada uno de los hijos tendrá una probabilidad de 1 entre 40 de desarrollarla de adulto.

Dos estudios realizados en Canadá y Gran Bretaña muestran la siguiente tabla de probabilidades de enfermar según el grado de parentesco:

La esclerosis múltiple está ampliamente extendida en Europa septentrional donde su mayor prevalencia (200 por cada 100 000 habitantes) se ha relacionado con la falta de vitamina D. En Europa Central su prevalencia es de 80 por cada 100 000 habitantes; en Estados Unidos 8,3 por cada 100 000; en el Sudeste asiático 2,8 por cada 100 000; en África es de menos de 0,5 por cada 100 000.

En los migrantes, si la migración ocurre antes de los 15 años, el inmigrante adquiere la susceptibilidad a la esclerosis de la región a la que se ha desplazado. Si el desplazamiento ocurre después de los 15 años, la persona mantiene la susceptibilidad de su país de origen.

En cambio, en la península arábiga, Asia, América Central y América del Sur continental la frecuencia es mucho menor. En el África Subsahariana es extremadamente rara. Con excepciones importantes, hay un gradiente norte-sur en el hemisferio norte y sur-norte en el hemisferio sur, con las menores frecuencias en las zonas ecuatoriales.
En España este gradiente norte-sur se mantiene, la prevalencia media de la península ibérica se puede cifrar en 45 casos por cada 100 000 habitantes.

La esclerosis múltiple, salvo notables excepciones, es más frecuente en latitudes externas, incluyendo América del Norte, Europa, Australia y Argentina. Sin embargo, la frecuencia de la EM está aumentando en otras regiones y grupos que antes se consideraban de bajo riesgo, tales como hispanos, asiáticos, afroamericanos, y en otros países de América Latina. En México, la prevalencia se incrementó de 1.6 a 12 pacientes por cada 100 000 habitantes los últimos años.

La probabilidad de enfermar es tanto menor cuanto más tiempo se haya convivido con los hermanos. Más de cinco años de contacto reduce el riesgo. La explicación sería que la exposición a temprana edad a infecciones regula hacia abajo la respuesta inmune.



La causa de la esclerosis múltiple es desconocida. Se cree que se produce por una combinación de factores predisponentes genéticos e influencias medioambientales. Estas últimas incluyen los niveles de vitamina D, la exposición solar, el tabaquismo y algunas infecciones virales.

La epigenética de la esclerosis múltiple relaciona ciertas infecciones virales, como la del virus Epstein-Barr, la deficiencia de vitamina D, factores dietéticos (catequinas, isoflavonas o arctigenina), microbiota y exposición a elementos ambientales (tabaco, metales, solventes) con modificaciones en la incidencia de la enfermedad. El aumento de la permeabilidad intestinal puede provocar o empeorar la enfermedad, por su relación con el paso incontrolado de sustancias a la sangre, lo que puede desencadenar una respuesta anormal del sistema inmunitario.

La prevalencia de la esclerosis múltiple presenta una agregación familiar; comparado con el 0.1% a 0.3% de la prevalencia en la población general, los hermanos de individuos afectados por la enfermedad presentan un riesgo de presentarla entre 10 a 20 veces mayor (2% a 4%); padres e hijos de afectados por la enfermedad tienen riesgo de un 2%, y parientes de segundo y tercer grado un 1%. Los gemelos monocigóticos presentan un riesgo aun mayor (25% a 30%).

Otro factor que apoya la heredabilidad de la EM es la distribución mundial de su prevalencia. Las personas que viven en Europa del Norte y Norteamérica presentan una mayor incidencia al compararlos con los europeos del sur. Además, la EM es rara en ciertos grupos étnicos tales como uzbecos, saamis, turcos, kirguís, kazajos, nativos de Siberia, indígenas de América, japoneses, chinos, negros africanos, y maoríes de Nueva Zelanda. Aunque parte de esta situación puede ser explicada por factores ambientales, la existencia de grupos raciales ancestrales resistentes a la EM sugiere que la arquitectura del ADN influye en el riesgo de padecer la enfermedad. 

La esclerosis múltiple (EM) aparece principalmente en caucásicos. Es 20 veces menos frecuente entre los inuit de Canadá que entre los demás canadienses que viven en la misma región. Estos ejemplos señalan que la genética tiene un papel importante en el desarrollo de la enfermedad.

Las observaciones epidemiológicas, especialmente la relación no linear entre la distancia genética de un probando y el riesgo de por vida de desarrollar la EM, sugieren una herencia poligénica de la enfermedad, siguiendo el paradigma «enfermedad común-variante común» de la genética y la herencia. De acuerdo con este modelo, el riesgo total de padecer la EM es el resultado de la contribución de múltiples genes polimórficos con alelos de riesgo comunes, cada uno determinando una porción del riesgo total. Este patrón de transmisión no medeliano no es exclusivo de la EM sino que se puede observar en otras enfermedades autoimunes y crónicas tales como la diabetes tipo II y la obesidad, denominadas en conjunto «enfermedades genéticas complejas» que se caracterizan por el riesgo poligénico y la interacción gen-ambiente multifacético.

La asociación entre el complejo mayor de histocompatibilidad y los antígenos leucocitarios humanos con la esclerosis múltiple se ha observado desde hace décadas. La asociación más fuerte se da en el locus HLA-DRB1.

Genes específicos que se han relacionado con EM incluyen diferencias en el sistema HLA (Human Leukocyte Antigen) -un grupo de genes del cromosoma 6 equivalentes al Sistema Mayor de Histocompatibilidad (MHC). El descubrimiento más importante en la asociación entre EM y alelos del sistema MHC fueron DR15 y DQ6; existen otros loci a los que se les ha asociado un efecto protector, tales como HLA-C554 y HLA-DRB1*11. Nuevos métodos genéticos han demostrado que al menos existen doce genes fuera del sistema HLA capaces de aumentar la probabilidad de padecer EM.

Se considera que la esclerosis múltiple aparece cuando se da una combinación de factores ambientales en personas genéticamente predispuestas a adquirirla. El clima, la dieta, toxinas, la luz solar y/o enfermedades infecciosas son algunos de los factores planteados como desencadenantes de la enfermedad.

Diversos virus han sido propuestos como agentes generadores de la EM, como el virus de Epstein-Barr, de la varicela o del herpes zóster. 

La presencia de rasgos de autoinmunidad y defectos en la producción de enzimas antioxidantes son factores de riesgo para padecer la enfermedad.

Entre los factores ambientales, diversos componentes dietéticos parecen jugar un papel en el desarrollo y progreso de la esclerosis múltiple y otras enfermedades autoinmunes, tales como la carencia de vitamina D, la presencia del gluten y las proteínas de la leche, entre otros.

La esclerosis múltiple es una de las diversas enfermedades que pueden desarrollarse o agravarse como consecuencia de una permeabilidad intestinal aumentada o "intestino permeable". Cuando hay un aumento excesivo de la permeabilidad intestinal, se produce un paso incontrolado de todo tipo de sustancias a la sangre (virus, bacterias, toxinas, alimentos no digeridos...), lo que puede provocar respuestas inmunitarias locales y sistémicas (en cualquier parte del organismo). Los dos factores más potentes que causan el aumento de la permeabilidad intestinal son ciertas bacterias intestinales y la gliadina, que es la principal fracción tóxica del gluten, en cualquier persona que lo come e independientemente de la predisposición genética, es decir, tanto en celíacos como en no celíacos. El descubrimiento del papel de la permeabilidad intestinal en el desarrollo de estas enfermedades desbarata las teorías tradicionales y sugiere que estos procesos pueden ser detenidos si se impide la interacción entre los genes y los factores ambientales desencadenantes, a través del restablecimiento de la función de la barrera intestinal.

Los individuos que migran a regiones de alta prevalencia antes de los 15 años tiene mayor riesgo de desarrollar la enfermedad; este fenómeno ha sido atribuido al grado de exposición a los rayos UV, ya que en latitudes de 40° al norte o sur, donde es más frecuente la enfermedad, la exposición a los rayos solares y el índice de radiación UV es menor. Por otro lado, la forma activa de la vitamina D o 1,25-hydroxyvitamin D3, se sintetiza en la piel gracias a la exposición de luz solar. Los niveles bajos en sangre de esta hormona esteroidea ha sido asociada al incremento en el riesgo de padecer EM atribuido por un efecto en la inmunidad.

El tabaquismo, la exposición a metales (mercurio, plata, oro, solventes orgánicos, pegamento), están estrechamente asociados a la aparición o empeoramiento de las lesiones desmielinizantes. Además de todo esto también se han propuesto otras causas como el estrés, aunque hay pruebas que sustentan que es débilmente relacionado. La vacunación fue estudiada también como un factor causal, sin embargo, no se encontró ninguna relación.
También se ha determinado la hipótesis de la higiene que propone que la exposición a ciertos agentes infecciosos en las primeras etapas de la vida es protectora para EM, y si existen condiciones higiénicas en extremo, como los países desarrollados, donde se evita este contacto, entonces no se cuenta con esta profilaxis y se desarrolla la enfermedad. Este supuesto no se ha demostrado, por lo que no existe una recomendación para someterse a una exposición innecesaria a agentes infecciosos. 

En pacientes con Esclerosis Múltiple la gota aparece con menos frecuencia y los niveles de ácido úrico son inferiores de lo que cabría esperar. Esto ha llevado a la teoría de que el ácido úrico es protector, aunque su importancia exacta sigue siendo desconocida.

En 2009 se propuso la relación con una enfermedad vascular conocida como insuficiencia venosa cerebroespinal crónica. No obstante, actualmente las evidencias demuestran que se trata de una hipótesis equivocada.

Las principales características de la EM son la formación de lesiones en el sistema nervioso central (también denominadas placas), en las cuales hay inflamación y la destrucción de las vainas de mielina (formadas por los oligodendrocitos) que recubren los axones (prolongaciones neuronales). Las lesiones interactúan de una manera compleja dañando el tejido nervioso , lo cual provoca los signos y síntomas de la enfermedad. El daño está causado al menos en parte, por el ataque del sistema nervioso de una persona mediado por su propio sistema inmune. 

Los tejidos del sistema nervioso y de la médula espinal están protegidos por un sistema de vasos capilares, llamado barrera hematoencefálica, que en los pacientes de esclerosis múltiple disfunciona. Por causas desconocidas, macrófagos y linfocitos pueden cruzar la BHE y comenzar un ataque autoinmune.

Se ha conseguido reconstruir el proceso del ataque del sistema inmunitario a la mielina a partir de observaciones en los tejidos dañados y el estudio de la encefalomielitis experimental autoinmune (o EAE, de sus siglas en inglés), que es una enfermedad similar a la EM que puede ser inducida en los roedores para fines de investigación.

En la EM hay inflamación, desmielinización, gliosis reactiva (generación de cicatrices) y daño axonal. El daño al tejido en la EM es el resultado de una interacción compleja y dinámica entre el sistema inmune, la glía (oligodendrocitos que producen mielina y sus precursores, la microglía y los astrocitos) y las neuronas . 

La barrera hematoencefálica (BHE) normalmente protege al sistema nervioso central. En la EM hay disfunción de la BHE, lo cual permite la infiltración de células del sistema inmune hacia el tejido del sistema nervioso central. Cuando se realizan estudios de resonancia magnética (IRM) se puede detectar el daño en la BHE al observar lesiones que captan el medio de contraste (gadolinio). La permeabilidad vascular anormal precede a la desmielinización inflamatoria, en la cual hay daño a los oligodendrocitos que recubren los axones. 

Los linfocitos T, llamados células CD4-Th1-T, tienen una función clave en el desarrollo de la enfermedad. Bajo circunstancias normales, estos linfocitos pueden distinguir entre células propias y ajenas. En una persona con esclerosis múltiple, sin embargo, las células reconocen partes sanas del sistema central como ajenas y las atacan como lo harían con un virus. En la EM, la parte atacada es la mielina, una sustancia grasa que cubre los axones de las células nerviosas y que es importante para una transmisión nerviosa adecuada.

En estado normal, existe una barrera entre el sistema nervioso central y la sangre llamada barrera hematoencefálica, que está formada por células endoteliales tapizando las paredes de los vasos sanguíneos. Por causas desconocidas, en los pacientes de EM esta barrera no funciona bien, y las células T autorreactivas la cruzan. A partir de este momento, estas células T van a atacar la mielina del sistema nervioso, produciendo desmielinización. La inflamación es facilitada por otras células inmunitarias y elementos solubles, como la citocinas y los anticuerpos. A causa de este comportamiento anormal del sistema inmunitario, la esclerosis múltiple es considerada una enfermedad autoinmunitaria.

La inflamación finalmente lleva al daño de la barrera hematoencefálica, lo que puede acarrear problemas como inflamación (edema). También causa la activación de macrófagos, de metaloproteinasas y otras proteasas y citocinas. Finalmente llevará a la destrucción de la mielina, proceso llamado desmielinización.

Las células involucradas en el daño inflamatorio de origen auto inmune en EM son principalmente linfocitos (linfocitos T y linfocitos B), macrófagos y microglia. Se han descrito linfocitos T auxiliares (CD4 +) y citotóxicos (CD8 +) en lesiones de esclerosis múltiple: las células T CD4 + están más concentradas en el manguito perivascular, mientras que las células T CD8 + están ampliamente distribuidas dentro del parénquima. A medida que avanza la enfermedad, aumenta la proporción relativa de linfocitos B y las células plasmáticas. El papel de las linfocitos B en EM también se demuestra al detectar anticuerpos de producción intratecal (bandas oligoclonales). Otras funciones de los linfocitos B son: la presentación de antígenos a los linfocitos T y la producción de citocinas. La microglia y los macrófagos mantienen un estado crónico de activación en el transcurso de la enfermedad, formando placas que lesionan la mielina y los oligodendrocitos. También los oligodendrocitos pueden volverse disfuncionales y morir, causando daño tisular por pérdida del soporte metabólico a los axones. Entonces, de manera secundaria, hay muerte neuronal y pérdida de tejido (atrofia cerebral). Dicho proceso se autoperpetúa llevando a discapacidad irreversible (Jarius 2017). En las lesiones crónicamente desmielinizadas, los axones desnudos permanecen vulnerables y pueden degenerarse lentamente; posibles mecanismos incluyen el transporte axonal alterado, la disfunción mitocondrial y el aumento de la demanda de energía relacionada con los canales iónicos.

La EM en su evolución conduce a la pérdida gradual neuroaxonal que se correlaciona con la discapacidad del paciente, atrofia cerebral y múltiples cicatrices gliales escleróticas en la sustancia blanca. La desmielinización también afecta sustancia gris de la corteza, los núcleos de la base y la médula espinal. La neuroplasticidad se encarga de remielinización aunque generalmente de manera incompleta.

La inflamación está presente en todas las etapas de la EM, pero es más pronunciada en las fases agudas que en las crónicas. A medida que la enfermedad progresa, la inflamación se focaliza dentro del SNC. Se pueden formar estructuras linfáticas terciarias en las meninges, y hay evidencia creciente de que la inflamación meníngea desempeña un papel importante en la patología de la corteza en etapas progresivas.

La National Multiple Sclerosis Society (de Estados Unidos) ha lanzado un proyecto llamado The Lesion Project para catalogar todos los tipos de lesiones posibles y desarrollar un modelo más preciso de cómo ocurren las cosas. Se han encontrado cuatro familias de lesiones diferentes pero no hay un consenso en cuanto al significado de este hecho. Unos piensan que esto significa que la esclerosis múltiple es realmente una familia de enfermedades. Otros piensan que las lesiones pueden cambiar de un tipo a otro con el tiempo o según el individuo. Las cuatro familias o "patrones" son los siguientes:


Los dos primeros patrones se consideran ataques autoinmunes contra la mielina y los dos últimos contra los oligodendrocitos. Los dos primeros patrones son además similares a los producidos en EAE (encefalomielitis alérgica experimental).

Las formas limítrofes de la esclerosis múltiple también presentan lesiones incluidas en estos patrones. Así la neuromielitis óptica tendría lesiones mediadas por activación de complemento (patrón II). La esclerosis concéntrica de Baló mostraría pérdida de MAG según el patrón III y la esclerosis múltiple progresiva primaria se correlacionaría con el patrón IV.

Las lesiones de EM pueden ser clasificadas en cuatro inmunopatrones sobre la base de la pérdida específica de proteína de la mielina, la extensión de la placa y la topografía, la destrucción de oligodendrocitos, la deposición de inmunoglobulinas y la activación del complemento.

Patrón I: se encuentran en el 15% de los pacientes con EM, se asocian principalmente con inflamación de células T, desmielinización activa, con muchos macrófagos activados, ausencia de inmunoglobulina y deposición del complemento. Estas características sugieren que la desmielinización y la lesión tisular pueden estar mediadas por productos tóxicos producidos por macrófagos activados. Estas lesiones tienen bordes claramente demarcados y la típica distribución perivenosa que, por confluencia, da como resultado grandes placas desmielinizadas.

Patrón II: encontradas en el 58% de pacientes con EM, muestran no solo infiltración densa con linfocitos T y macrófagos cargados de mielina, sino también deposición de inmunoglobulina y activación del complemento sobre mielina y dentro de macrófagos en sitios desmielinizantes activos sitios, con axones conservados. Este hallazgo sugiere que los mecanismos mediados por anticuerpos y el complemento pueden inducir la desmielinización y la lesión tisular. Estas lesiones también tienen bordes demarcados, típica distribución perivenosa que por confluencia dan grandes placas desmielinizadas, y exhiben pérdida variable de oligodendrocitos en el borde lesional activo, con la reaparición de numerosos oligodendrocitos en el centro de la placa inactiva.

Patrón III: encontradas en el 26% de pacientes con EM se definen por apoptosis de oligodendrocitos, inflamación de linfocitos T, activación de macrófagos y microglías y pérdida preferencial temprana de glicoproteína asociada a la mielina. Hay una pérdida pronunciada de oligodendrocitos en el borde de la placa activa. El centro inactivo está desprovisto de oligodendrocitos. La deposición de inmunoglobulina y complemento está ausente. Las lesiones tienen bordes mal definidos y no están centradas en venas o vénulas.

La neurodegeneración es una parte temprana de EM que se autoperpetúa y debe considerarse una segunda fase de la EM, sino un proceso concomitante que ya está en marcha en el momento del diagnóstico clínico. El daño axonal puede detectarse incluso en lesiones de la EM donde los axones aún están mielinizados y la pérdida de la reserva funcional se correlaciona con la discapacidad.

La EM es también una enfermedad de la sustancia gris, la desmielinización cortical es más extensa de lo que se creía previamente. La corteza se afecta con placas clásicas desmielinizadas o como pérdida neuronal y atrofia después de la degeneración retrógrada de las lesiones de la sustancia blanca.

Debido a su ubicación, se han descrito tres tipos de lesiones corticales.

Las lesiones subpiales se extienden desde la superficie pial hacia las capas corticales más profundas, alcanzando generalmente la capa cortical tres o cuatro, y son más comunes en la EM crónica. También pueden extenderse a todo el ancho de la corteza, con o sin afección de la sustancia blanca subcortical, y pueden afectar varias circunvoluciones.

Las lesiones intracorticales son pequeñas perivasculares confinadas a la corteza y que evitan tanto la corteza superficial como la sustancia blanca.

Las lesiones leucocorticales afectan tanto a la sustancia gris como a la blanca en su unión y evitan las capas corticales superficiales.

La desmielinización cortical más extensa se ha detectado en la circunvolución cingulada y las cortezas temporales, frontales, insulares y cerebelosas. También se ha detectado una desmielinización extensa en el hipocampo de pacientes con EM progresiva. La desmielinización cortical es más prominente en la EM progresivas, y puede tener un importante correlato patológico de discapacidad irreversible y deterioro cognitivo.

En estado normal, existe una barrera entre el sistema nervioso central y la sangre llamada barrera hematoencefálica, que está formada por células endoteliales tapizando las paredes de los vasos sanguíneos. Por causas desconocidas, en los pacientes de esclerosis esta barrera no funciona bien, y las células T autorreactivas la cruzan. A partir de este momento, estas células T van a atacar la mielina del sistema nervioso, produciendo una desmielinización.

A la vez aparece un proceso inflamatorio. La inflamación es facilitada por otras células inmunitarias y elementos solubles, como la citocina y los anticuerpos. A causa de este comportamiento anormal del sistema inmunitario, la esclerosis múltiple es considerada una enfermedad autoinmunitaria.

Ampliamente aceptado es que un subtipo especial de linfocitos, llamados células CD4-Th1-T, tienen una función clave en el desarrollo de la enfermedad. Bajo circunstancias normales, estos linfocitos pueden distinguir entre células propias y ajenas. En una persona con esclerosis múltiple, sin embargo, las células reconocen partes sanas del sistema central como ajenas y las atacan como lo harían con un virus. En la esclerosis, la parte atacada es la mielina, una sustancia grasa que cubre los axones de las células nerviosas y que es importante para una transmisión nerviosa adecuada.

La inflamación finalmente lleva a la apertura de la barrera hematoencefálica, lo que puede acarrear problemas como edemas. También causa la activación de macrófagos, de metaloproteinasas y otras proteasas y citocinas. Finalmente llevará a la destrucción de la mielina, proceso llamado desmielinización.

Los pacientes con EM con una mayor capacidad de remielinización tienen menor discapacidad. Más del 40% de las lesiones crónicas de EM muestran evidencia de remielinización y en aproximadamente el 20% de los pacientes con EM, la remielinización es extensa.

La sustancia gris remieliniza más rápidamente que la sustancia blanca en las lesiones leucocorticales, y puede estar relacionada con niveles más bajos de inhibidores de la remielinización dentro de las lesiones corticales o diferencias en la actividad de las células inmunes innatas.

La remielinización es una de las razones por las que, especialmente en las primeras fases de la enfermedad, los síntomas tienden a disminuir o desaparecer después de días a meses. La remielinización en EM también debe considerarse en el contexto del envejecimiento, ya que, independientemente del curso de la enfermedad, los pacientes más jóvenes tienen una mayor probabilidad de recuperación. Aún se desconoce por qué la remielinización es extensa en algunos pacientes cuando falla en otros y por qué las áreas remielinizadas son más vulnerables que la sustancia blanca normal a la desmielinización secundaria. Una hipótesis sería que una lesión inmunomediada inicial también afectaría a las células precursoras de oligodendrocitos y a las nuevas vainas de mielina. Los insultos desmielinizantes repetidos podrían agotar la fuente de las células precursoras de oligodendrocitos, que sumado a la lesión axonal alteran las interacciones apropiadas para la remielinización, ya que los axones proporcionan señales críticas a los oligodendrocitos en el proceso de mielinización . La cicatriz glia densa que se forma dentro de las placas y en sus bordes funciona como una barrera entre el tejido dañado inflamado y el cerebro normal, lo que impide que las células precursoras de oligodendrocitos migren hacia las lesiones.

A través de la resonancia magnética y otros sistemas, se ha demostrado que el daño a los axones es una de los principales causas del desarrollo de discapacidad y pérdida del volumen cerebral (atrofia). Se ha demostrado que esta lesión interviene no solo en formas crónicas o en estadios tardíos de la enfermedad, sino que está presente formas tempranas de la enfermedad. 

Los síntomas de la EM son causados por lesiones múltiples en el sistema nervioso central y pueden variar mucho entre individuos, dependiendo de dónde ocurran las lesiones. La plasticidad del cerebro a menudo puede compensar una parte del daño, sobre todo en las primeras etapas de la enfermedad.

Un brote o una recaída de la esclerosis múltiple consiste en la aparición de nuevos síntomas o un empeoramiento de los existentes. La frecuencia de los brotes en etapas tempranas de la esclerosis múltiple está relacionada con el grado de discapacidad en el futuro, por lo que evitarlos es clave para mejorar el pronóstico del paciente. Al igual que los síntomas son muy variados, sus desencadenantes también lo son, por lo que los pacientes evitan aquellas circunstancias que consideran que empeoran su estado.

¿Qué factores evitables pueden desencadenar un brote de la esclerosis múltiple?


Los síntomas de la esclerosis múltiple se desarrollan a lo largo de varios días, expresándose de acuerdo a la zona afectada y en un tiempo variable de forma máxima tras transcurrir una a dos semanas de enfermedad, para luego en la mayoría de los casos ir disminuyendo y eventualmente resolverse en semanas o meses. Los síntomas usualmente son múltiples. Pueden quedar síntomas remanentes crónicos, especialmente los sensitivos. Las manifestaciones dependen de la localización del foco de desmielinización, pero típicamente se presenta como un trastorno sensitivo y/o motor en una o más extremidades (50% de los pacientes), neuritis óptica (síntoma inicial en el 25% de los pacientes), diplopia, ataxia, vejiga neurogénica, fatiga, disartria, neuralgia del trigémino (en menos del 10%), nistagmo, vértigo. En forma más rara puede presentarse afasia, apraxia, convulsiones, demencia, corea y rigidez. 

El signo de Lhermitte es un rasgo clínico frecuente de la esclerosis múltiple; se trata de una sensación de tipo eléctrica que recorre la espalda en sentido caudal hasta las piernas, cuando el cuello es flexionado. Dicha sensación se da en el 30% o más de los pacientes. 

La fatiga es otro de los síntomas comunes de la esclerosis múltiple, que suele en muchas ocasiones interferir su vida diaria imposibilitando o entorpeciendo su desempeño académico, laboral e incluso sus relaciones interpersonales.

Las lesiones del sistema nervioso central que causan la esclerosis múltiple no siempre se manifiestan directamente como síntomas clínicos detectables y claramente atribuibles a la enfermedad, por lo que en ocasiones se tiende a restar importancia a los primeros signos.

Aunque en algunas ocasiones al principio de la esclerosis múltiple se acumula poca discapacidad y la calidad de vida no se ve demasiado afectada, la realidad es que el sustrato de la enfermedad ya se está desarrollando. De lo que ocurra en las fases iniciales de la esclerosis múltiple, depende en gran medida su evolución posterior. En otras palabras, las lesiones de hoy en el sistema nervioso central, son la causa de la discapacidad de mañana.

Las alteraciones motoras suelen presentarse como diversos grados de debilidad, rara vez presentando pérdida total de la fuerza de algún lado del cuerpo (hemiplejía). Los reflejos tendinosos profundos (p. ej., el rotuliano y el aquileano) suelen estar aumentados y a menudo se presenta una respuesta plantar extensora (signo de Babinski) y clonus. Los diversos grados de debilidad, pueden ir desde una leve paresia hasta grados de debilidad avanzada que puede dejar al paciente confinado a una silla de ruedas y/o cama. Más tarde pueden aparecer espasmos flexores dolorosos en respuesta a los estímulos sensitivos. Las lesiones cerebrales o de la médula espinal con más frecuencia cervical pueden producir una hemiparesia, que a veces es el síntoma de presentación.

Se manifiesta típicamente con las siguientes características;


El vértigo es una sensación en las que las personas tienen la percepción de que el entorno o ellas mismas se mueven. Se calcula que alrededor del 20% de los afectados de EM pueden sufrirlo en alguna ocasión, cuando la enfermedad provoca algún problema en la zona del cerebro que controla el equilibrio o como cualquier otra persona, cuando se afecta alguno de los sistemas que regulan la sensación del equilibrio.

Si bien antiguamente se consideraba que no existían déficits cognitivos en la esclerosis múltiple, sino que era una enfermedad fundamentalmente física; actualmente se sabe que es común un patrón de déficit cognitivo específico.

El patrón que siguen los déficits cognitivos es un patrón fronto-subcortical; afectando por tanto a las funciones localizadas en el lóbulo frontal y en las zonas subcorticales.

El déficit cognitivo más acentuado y observado en la esclerosis múltiple es la disminución de la velocidad de procesamiento. Esto es explicado por la desmielinización de los axones de las neuronas, que aparte de proteger el axón de las neuronas, hacen que el impulso nervioso se desplace más rápido. Seguidamente se observan otros déficits cognitivos, pero que pueden deberse al déficit de base anteriormente mencionado. Entre estas otras alteraciones estaría la atención, principalmente atención sostenida, selectiva y alternante. La atención sostenida puede ser difícilmente disociable de la fatiga que estos pacientes muestran. Las alteraciones en la atención conllevan a déficits en la memoria, especialmente en la memoria de trabajo (funciones ejecutivas), y déficits en la codificación y recuperación.

Respecto a los déficits frontales puede haber cambios conductuales (apatía, falta de motivación, pseudodepresión; o por el contrario desinhibición, impulsividad, agresividad e irascibilidad, infantilismo...) y en las funciones ejecutivas (capacidad para planificar, flexibilidad mental, razonamiento...) además de las anteriormente mencionadas.

Los déficits cognitivos que muestran los pacientes son evaluados neuropsicológicamente, y la terapia de rehabilitación suele ayudar a ralentizar el curso de la enfermedad.

Dado que la esclerosis múltiple afecta el cerebro y la espina dorsal, es de naturaleza degenerativa y a menudo lleva a una discapacidad progresiva, no es extraño que las personas afectadas sufran dificultades emocionales. Depresión, ansiedad, ira, miedo, junto con otras emociones se diagnostican a menudo entre las personas que sufren esclerosis múltiple o los allegados.

A pesar de que estos sentimientos son comprensibles, tienen implicaciones sociales, especialmente cuando interfieren con el trabajo, la escuela y la vida familiar. En particular, la depresión lleva asociada el peligro de suicidio.

Todos estos problemas pueden ser solucionados con tratamientos psicológicos y medicación adecuada.

La depresión en estos casos puede originarse tanto en las implicaciones del diagnóstico como en los efectos que produce la enfermedad en el cerebro. El cerebro es un órgano altamente especializado y es el origen de todas las emociones. Cuando un brote afecta la parte del cerebro en que se procesan las emociones, el resultado puede incluir depresiones.

También se ha informado de casos de depresión en algunos pacientes que toman interferón beta (Avonex® o Rebif®) para tratar la enfermedad.

La ansiedad también es un problema frecuentemente asociado con la esclerosis múltiple. Al igual que la depresión, puede estar originada por la enfermedad o por las implicaciones del diagnóstico. Las consecuencias cognitivas de la esclerosis múltiple también suelen estar asociadas a ella. No es raro que personas que sufren la enfermedad desarrollen falta de memoria, dificultades en explicar conceptos abstractos, organizar, planificar u olviden determinadas palabras. Estos síntomas pueden generar ansiedad y resultar en una retirada de la vida social.

La ira es otro sentimiento que puede ir asociado a la esclerosis múltiple y, de hecho, con todas las enfermedades crónicas graves. También se ha informado de la aparición de "risa patológica" y "llanto patológico". Esencialmente se refiere a una reacción inapropiada a la situación, tal como risas en un funeral o llanto al oír buenas noticias. A menudo es consecuencia de la desmielinización de las áreas del cerebro encargadas del juicio.

El diagnóstico de la esclerosis múltiple depende de la documentación de múltiples episodios neurológicos de exacerbación y remisión de la enfermedad a través del tiempo y en diversos sitios del sistema nervioso central. No existe ninguna prueba o marcador biológico que permita en forma aislada establecer el diagnóstico definitivo de EM, se requiere en su conjunto de una evaluación clínica que se apoye con estudios paraclínicos como la Resonancia Magnética (IRM), Bandas Oligoclonales en líquido cefalorraquídeo y potenciales evocados.

El Panel Internacional para el diagnóstico de la Esclerosis Múltiple, estableció las últimas revisiones a los criterios diagnósticos de McDonald en 2017, estos criterios se basan en el número de brotes y la evidencia objetiva de daño neurológico mediante signos clínicos así como en dos pilares fundamentales que son la demostración de la diseminación de las lesiones desmielinizantes en espacio y en el tiempo.
Brote, recaída o exacerbación-Síntomas de instauración aguda con signos típicos de una afección inflamatoria desmielinizante en SNC, con una duración al menos de 24 horas en ausencia de fiebre o infección.

Signos clínicos-Evidencia Clínica de una lesión desmielinizante que se documenta de forma objetiva mediante la exploración neurológica.

Lesiones desmielinizantes en diferentes regiones del sistema nervioso central.
Se requiere al menos una lesión en Resonancia Magnética en secuencia T2 en por lo menos dos de las siguientes 4 áreas:

Afectación en diferentes momentos de la evolución de la enfermedad. La demostración de los cambios temporales de las lesiones desmielinizantes (progresión en número y tamaño de las lesiones), se realiza mediante estudios de seguimiento en IRM cerebral que deben encontrar:

1.-Una nueva lesión en IRM en la secuencia T2 

2.-Una lesión que capta gadolineo en el seguimiento por IRM, con referencia a una IRM basal 

3.-Presencia de lesiones asintomáticas gadolineo positivas y no gadolineo positivas en cualquier momento.

4.-Nuevo brote.

5.-Bandas Oligoclonales positivas en LCR

Resonancia Magnética 

La IRM es la técnica más sensible en la identificación de las placas desmielinizantes, detecta en más del 97%  pacientes con EM clínicamente definida. Como consecuencia de esta alta sensibilidad, la RM se ha convertido en una técnica esencial, no solo en el diagnóstico de la EM sino también como marcador pronóstico.

Las lesiones desmielinizantes son múltiples y de pequeño tamaño (menores de 25 mm) de morfología ovoidea o nodular.

Detección de Bandas Oligoclonales (BOC) de IgG

El análisis del LCR sirve para obtener evidencia de la naturaleza inmunitaria e inflamatoria de la enfermedad, en el 50% de los pacientes hay un incremento discreto de linfocitos en LCR (pleocitosis linfocítica).

El 87-95% de los pacientes con EM tiene BOC positivas.  Se considera que hay síntesis intratecal de inmunoglobulinas si hay más de dos bandas  oligoclonales en el LCR que no se encuentren en suero. 

El isoelectroenfoque(IEF) y  una tinción inmuno específica para las moléculas de IgG, se ha propuesto como el método más adecuado para la detección de las BOC. 

Detección de bandas Oligoclonales de IgM

La presencia de BOC de IgM  principalmente, las BOC de IgM se han asociado a un pronóstico más agresivo de la enfermedad.

Diagnóstico Diferencial

Dentro de los diagnósticos diferenciales de la Esclerosis Múltiple hay que descartar principalmente neuromielitis óptica que es más prevalente en la población latinoamericana, encefalomielitis diseminada aguda(EMDA) principalmente en la población pediátrica, manifestaciones neurológicas del VIH, enfermedad de Behcet, sarcoidosis, síndrome de Sjögren, lupus eritematoso sistémico, neuropatía aguda óptica isquémica, síndrome de Susac, síndrome antifosfolípidos, neurosífilis, neurocisticercosis, enfermedad de Lyme, deficiencia de cobre o zinc, enfermedad celíaca, deficiencia de vitamina E, enfermedad de Wilson, porfiria, arteriopatía cerebral autosómica dominante con infartos subcorticales y leucoencefalopatia, linfoma de sistema nervioso central, síndrome paraneoplásico, malformación vascular medular, lesiones desmielinizantes por tóxicos, entre otros.

Criterios de Diagnóstico en EM Primaria Progresiva 

Al menos un año de progresión continua de la enfermedad y al menos 2 de los 3 siguientes hallazgos: IRM de Cráneo con 9 lesiones en T2, IRM medular con 2 lesiones desmielinizantes y un LCR positivo (que demuestre BOC o elevación del índice de IgG).

Hasta ahora, no existe un tratamiento curativo disponible para la EM, de manera que la estrategia se enfoca a reducir la actividad tanto clínica como radiológica y la progresión de discapacidad.

El objetivo primario en el tratamiento de la esclerosis múltiple, especialmente en la variante recurrente, es la reducción de la actividad de la enfermedad para optimizar la reserva neurológica, la cognición y la función física. Esto se consigue mediante el uso de «fármacos modificadores de la enfermedad» tales como interferón, acetato de glatiramer, anticuerpos monoclonales y moduladores del receptor esfingosina-1-fosfato.

Los fármacos modificadores de la enfermedad aprobados para el tratamiento de la esclerosis múltiple son:

En general, podemos dividir el tratamiento para Esclerosis Múltiple en 3 apartados: 


Las metas de las TME son: Modificar la historia natural de la enfermedad, reducir la actividad inflamatoria en el Sistema Nervioso Central (SNC) y prevenir o enlentecer la progresión de la discapacidad. Según lo anterior, la recomendación actual es “la optimización” del tratamiento , lo cual significa hacer una selección de un fármaco que tenga un perfil de riesgo/beneficio favorable para el paciente de acuerdo a múltiples factores pronósticos que pueden considerarse al iniciar el tratamiento y en el monitoreo subsecuente para lograr definir si la respuesta es adecuada o se requiere un cambio oportuno de fármaco

En 1993, la Food and Drug Administration (FDA) autorizó el primer interferón beta para el tratamiento de la EM; a partir de entonces, múltiples tratamientos se han introducido para su manejo.

Para finales de 2017 se contaban con 11 fármacos aprobados por FDA para el tratamiento de la Esclerosis Múltiple en su forma Recurrente-Remitente, lo cual permite una mejor oportunidad de individualizar el manejo según factores pronósticos de cada paciente considerando eficacia, efectos adversos, mecanismos de acción y aspectos de conveniencia 

Podemos dividirlos como:

1 Terapias inyectables

"Interferones" 

El Interferón B1b estuvo disponible por primera vez en Australia en 1995. Su mecanismo de acción no es del todo claro, pero se sabe que regula las células y sustancias proinflamatorias, así como su paso al SNC. Se han generado nuevas formulaciones a fin de disminuir la frecuencia de su aplicación y con ello mejorar el apego terapéutico.

"Acetato de Glatiramer"

Su primer estudio pivotal se publicó en 1995; su mecanismo de acción no es completamente claro pero se sabe que genera u cambio en la diferenciación de células pro-inflamatorias a anti-inflamatorias y recientemente ha cambiado su presentación de 20mg a 40mg con lo cual su forma de aplicación cambió de ser diaria a ser en días alternos. 

2 Terapias orales

La Cladribina Tabletas fue aprobada por parte de la Agencia Europea de Medicamentos fue agosto de 2017 y a partir de ahí se ha registrado en más de 38 países en todo el mundo. Las Tabletas de Cladribina son una terapia oral de cursos cortos que se cree que actúa selectivamente sobre los linfocitos. Las tabletas de Cladribina se encuentran actualmente bajo investigación clínica y no están aprobadas para el tratamiento de ningún uso en los Estados Unidos. Actualmente ha recibido aprobaciones para pacientes con EMR de alta actividad según lo definido por las características clínicas o de imagen en la Unión Europea (UE), Israel, Argentina, Emiratos Árabes Unidos, Chile y Líbano. En diciembre de 2017, Canadá y Australia aprobaron Tabletas de Cladribina para el tratamiento de EM recurrente-remitente (RRMS).”

El fingolimod, aprobado en más de 60 países, es el primer tratamiento oral modificador de la enfermedad aprobado en primera línea en EE. UU. para la esclerosis múltiple recidivante, la forma más frecuente. En la Unión Europea se aprobó en marzo de 2011 para los pacientes con esclerosis múltiple recidivante-remitente grave de evolución rápida o muy activa.

La esclerosis múltiple progresiva primaria es muy difícil de tratar. Los corticoesteroides a altas dosis cada tres meses pueden tener algún efecto. En principio no existe un tratamiento preventivo efectivo para la esclerosis múltiple progresiva primaria. El tratamiento de los síntomas, y la rehabilitación llevada a cabo principalmente desde terapia ocupacional, fisioterapia y logopedia, tienen un papel importante. Es muy importante, igualmente, la evaluación por parte de un neuropsicólogo para poder abordar cualquier déficit cognitivo que pudiera instaurarse.

Entre las intervenciones dietéticas, en ocasiones se ha empleado la dieta sin gluten, debido a la existencia de casos documentados que confirman la remisión de la esclerosis múltiple simplemente tras la retirada estricta y mantenida de los alimentos que contienen gluten en la dieta. Otras intervenciones dietéticas en estudio incluyen la suplementación con vitamina D, ácidos grasos poliinsaturados, antioxidantes (vitaminas A, C y E) y probióticos.

Medicamentos experimentales y terapias alternativas se describen en el artículo Tratamiento de la esclerosis múltiple. El estado de las terapias experimentales, llamadas "pipeline", pueden ser consultadas en sitios especializados.

"Teriflunomide" 

Es el segundo fármaco oral aprobado (2012) para las formas recurrentes de EM. Funciona disminuyendo la capacidad de replicación de los linfocitos responsables de la inflamación en el SNC con lo cual se busca una reducción de actividad clínica y radiológica de la enfermedad según los estudios clínicos. Es el único fármaco oral que ha mostrado eficacia en fases tempranas de la enfermedad (Síndrome Clínico Aislado).

"Dimetil-Fumarato"

Está disponible en nuestro país para formas recurrentes de EM. La dosis es una toma cada 12h. Su mecanismo de acción no está bien entendido pero sus efectos son antiinflamatorios e inmunomoduladores a través de la regulación de la función del sistema inmunológico al estrés oxidativo, lo cual ha demostrado reducir la cantidad de recaídas y las lesiones vistas en la resonancia magnética.           

3 Anticuerpos Monoclonales

"Natalizumab" 

Primer anticuerpo monoclonal aprobado en 2004 en Estados Unidos y en 2006 por la Unión Europea  para el  tratamiento de la EM. Su indicación es como monoterapia en pacientes con formas recurrentes que han presentado respuesta inadecuada a otros fármacos o como primera opción en casos de enfermedad agresiva.

Su mecanismo de acción es evitar el paso de las células del sistema inmunológico (linfocitos) al Sistema Nervioso Central (SNC), limitando así el proceso inflamatorio con lo que disminuye el número de recaídas clínicas, la progresión de discapacidad y el número de lesiones por Resonancia Magnética, siendo considerado uno de los medicamentos con mayor eficacia. Se aplica de manera mensual con pocas reacciones asociadas a la infusión.

"Alemtuzumab"

Es un anticuerpo monoclonal aprobado para pacientes con EM Recurrente-Remitente en la Unión Europea desde 2013 y más recientemente en Estados Unidos. Su indicación es como monoterapia en pacientes con formas recurrentes y activas de EM.

Su mecanismo de acción consiste en reducir el número de linfocitos en la sangre de manera significativa y después de algunas semanas generar una repoblación de estas células  generando un nuevo balance del sistema inmunológico, con lo cual que ha demostrado una reducción significativa en el número de recaídas, la progresión de discapacidad, las lesiones vistas en resonancia magnética incluyendo el nivel de atrofia cerebral, etc. Su forma de administración es a través de una infusión que dura alrededor de 4 horas en dos ciclos de tratamiento, el primero de ellos administrando una dosis diaria por cinco días y el segundo doce meses después aplicando una dosis diaria por tres días. 

El efecto clínico de estas aplicaciones se mantiene a largo plazo en los estudios clínicos, aunque de ser necesario, los pacientes pueden recibir más ciclos de tratamiento de acuerdo a su evolución.

4 Trasplante Autólogo de Células Madre 

Con la finalidad de reconstituir el sistema inmunológico de los pacientes. Sin embargo, no se cuenta con un protocolo definido y tiene muchas limitantes, por lo que se considera aún como un tratamiento experimental en desarrollo.

Es importante que los paciente que cursan con la enfermedad reconozcan cuales son los signos y síntomas de un brote, que traduce actividad clínica de la enfermedad, y así puedan acudir a la brevedad posible a su neurólogo, quien deberá identificar el brote o bien descartar procesos infecciosos o condiciones que simulan un brote "pseudobrotes", instaurando el tratamiento médico en agudo. 

En pacientes con EM tipo RR en brote, con síntomas neurológicos que condicionan discapacidad o involucro en la visión, la fuerza o la función cerebelosa, se recomienda el tratamiento con glucocorticoides. La metilprednisolona vía intravenosa es de elección, sin embargo estudios también sugieren que el régimen oral es igual de efectivo. Los brotes que condicionen síntomas sensitivos moderados no deben ser tratados.

La inyección con corticotropina en gel es una alternativa para pacientes en brote, los cuales no pueden tolerar altas dosis de glucocorticoides vía intravenosa. Se deben descartar procesos infecciosos. 

En pacientes que cursan con déficit neurológico agudo, severo, con poca respuesta al tratamiento con altas dosis de glucocorticoides, se sugiere el tratamiento con plasmaferesis. 

En los pacientes con EM tipo remitente recurrente con actividad manifestada por síntomas clínicos o por lesiones que realzan al contraste, se debe ofrecer terapia modificadora de la enfermedad.

Entre los problemas sintomáticos que padecen los pacientes con EM se encuentran la disfunción cognitiva, la depresión, la fatiga y las alteraciones en la marcha, las cuales se presentan con mayor frecuencia con la progresión de la enfermedad. La espasticidad, el temblor, las crisis epilépticas, la disfunción esfintérica y vesical pueden también complicar la progresión de la enfermedad. 

Es importante categorizar al paciente en el tipo de disfunción: falla del detrusor por hiperactividad, falla en el vaciamiento por hipoactividad de la vejiga, o una combinación de ambas fallas.

Las metas para el tratamiento del paciente con disfunción vesical neurogenica inluyen: preservar la función renal, minimizar las complicaciones del tracto urinario y lograr continencia.

El tratamiento de la disfunción vesical en pacientes con EM involucra suprimir la urgencia y asegurar un adecuado drenaje urinario.

La terapia inicial para el ajuste de la disfunción esfintérica incluye restricción de líquidos ≤2 L al día y coordinar los tiempos para realizar la micción.

En pacientes con hiperctivdad del detrusor, los medicamentos anticolinérgicos y antimuscarinicos representan la primera opción. La Oxibutinina es el medicamento de primera elección. Otros medicamentos anticolinérgicos incluyen tolterodina, propantelina, propiverina, fesoterodina y solifenacina.

Una terapia adjunta con cateterización intermitente puede ser de ayuda en el vaciado eficaz vesical y la continencia.

La terapia dual es efectiva y bien tolerada en algunos pacientes, se pueden incluir medicamentos con diferentes mecanismos de acción como anticolinérgicos, agonistas colinérgicos, bloqueadores alfa adrenérgicos, antidepresivos tricíclicos y agentes simpaticomiméticos.

Se debe tener cuidado con los efectos adversos de los anticolinergicos que incluyen confusión, sobre todo en pacientes mayores.

La inyección con toxina botulínica es una opción en pacientes con disfunción vesical debido a hiperactividad del detrusor que son resistentes a tratamiento o intolerantes a terapia con anticolinérgicos/ antimuscarínicos.

La hiperactividad del detrusor que no responde al tratamiento con toxina botulínica puede beneficiarse con la neuromodulación sacra y estimulación eléctrica a nivel de raíces S3, así como de la estimulación nerviosa del nervio periférico de del nervio dorsal del pene, clítoris o nervio tibial posterior. Dicho tratamiento inhibe el reflejo de micción.

En pacientes con disinergia del esfínter del detrusor, los alfa antagonistas como prazocina, terazocina, doxacocina y tamosulocina pueden ser beneficiosos.

La desmopresina es una buena opción para tratar la nicturia.

Los pacientes con infecciones recurrentes del tracto urinario deben ser investigados con ultrasonido y cistoscopia para descartar anormalidades que puedan estar subyacentes, de ser así el caso, se recomienda iniciar con dosis bajas de antibióticos de forma preventiva. 

Los problemas comunes incluyen constipación, falta de evacuación, e incontinencia.

Otras causas de disfunción intestinal son consecuencia de la disminución en la actividad física y motilidad, lo cual puede impactar en la frecuencia del movimiento intestinal, de manera secundaria otras condiciones médicas relacionadas con la EM y los efectos adversos de los medicamentos relacionados con el tratamiento de la espasticidad, dolor y disfunción vesical.

El estreñimiento se trata mediante una dieta rica en fibra y una ingesta hídrica adecuada. Si esto no es suficiente, se pueden asociar laxantes.

Las terapias de biofeedback han demostrado también su utilidad.

El tratamiento de la incontinencia es sintomático, se basa en asegurar una dieta rica en fibra para producir heces blandas pero formadas que resultan más fáciles de controlar que las heces líquidas, en terapias de biofeedback y, en casos extremos, en el uso de obturadores anales.

Las alteraciones más comunes son la falta de atención, en la función ejecutiva, conceptualización abstracta, memoria a corto plazo, recordar palabras, lenguaje y procesamiento de la información. Es importante evaluar la depresión, las alteraciones en la higiene del sueño, síndromes dolorosos y la fatiga en todos los pacientes con déficit cognitivo, ya que dichos problemas pueden tener impacto en las experiencias de los pacientes. El régimen de medicación deberá ser analizado para medicamentos asociados con efectos sedantes, hipnóticos y efectos psicomotores.

No hay terapias que prueben eficacia para el tratamiento del déficit cognitivo relacionado con EM, sin embargo los efectos de algunos agentes modificadores de la enfermedad en EM e inhibidores de la colinestersada se encuentran en estudio en relación de su beneficio e impacto en la función cognitiva.

Los inhibidores de la colinesterasa fueron investigados inicialmente para el tratamiento de la enfermedad de Alzheimer, donde existe un déficit colinérgico en la corteza. Se sabe también que existe un déficit colinerico asociado con EM, sin embargo ensayos pequeños sugieren que el donepezilo puede tener un beneficio modesto en el tratamiento..

Organizadores de uso personal, como alarmas, pueden ser de utilidad en pacientes con dificultad en la áreas de la memoria y la organización. Recordatorios por escrito, directorios, logos telefónicos son otros instrumentos que cumplen con el mismo propósito.

La neurorrehabilitacion se plantea como un apoyo importante para estos pacientes. Tiene como objetivo intentar mejorar el déficit de atención y memoria, recuperar habilidades deterioradas, así como potenciar el desarrollo de mecanismos de adaptación y habilidades compensadoras. No obstante, la evidencia de su utilidad es escasa.

Entre el 15 y el 50 % de los pacientes con EM presentan síntomas de depresión mayor. El tratamiento con interferón no parece aumentar las tasas de depresión como inicialmente se sugirió. 
Se recomienda un abordaje integral, psicoterapéutico y farmacológico en estos pacientes. Los antidepresivos tricíclicos y los inhibidores de la recaptura de la serotonina pueden ser útiles en el tratamiento; la elección debe individualizarse en función de la situación basal de cada paciente y el perfil de efectos secundarios.

Es un síntoma frecuente en los pacientes con EM. Se debe intentar identificar y tratar los posibles factores concomitantes que puedan generar o empeorar la fatiga, como los trastornos del sueño, la depresión o el dolor. 

Algunos fármacos como el interferón o los empleados en el tratamiento sintomático de la espasticidad pueden ser causa de fatiga.

Ciertas medidas no farmacológicas como planificar la mayor parte de la actividad a primera hora de la mañana, realizar descansos regulares, ejercicio físico o estructurar los horarios pueden resultar útiles. Si la fatiga persiste, se puede valorar el tratamiento farmacológico.

El único tratamiento que ha demostrado tener cierta base para su uso es la amantadina.

Las alteraciones en la marcha en pacientes con EM pueden ser el resultado de diversas complicaciones como la espasticidad, debilidad, fatiga, alteraciones sensoriales, pérdida visual y disfunción vestibular. La fampridina, un bloqueador de los canales de potasio, es el primer tratamiento aprobado con la indicación de mejorar la capacidad de la marcha de los pacientes con EM. Esta contraindicada en pacientes con historia de crisis epilépticas y en pacientes que reciben fármacos inhibidores del transportador de cationes orgánicos 2 (OCT2) renal.

Existen síndromes dolorosos paroxísticos asociados a la EM como la neuralgia del trigémino y el fenómeno de Lhermitte. Episodios recurrentes del signo de Lhermitte, incluyendo aquellos sin lesión medular, pueden responder a terapia con gabapentina, pregabalina y cabamazepina. La terapia farmacológica utilizada de primera línea incluye carbamazepina, oxcarbazepina y como segunda línea baclofeno, lamotrigina y misoprostol Síntomas paroxísticos. Los ataques paroxísticos pueden responder con dosis bajas de antiepilépticos como carbamazepina, ácido valproico después de varias semanas a meses, usualmente sin recurrencia.

Es uno de los factores que más contribuye a la discapacidad en la EM, limitando la movilidad, generando dolor, condicionando posiciones articulares erróneas y, en fases más avanzadas, dificultando la higiene.

En pacientes con debilidad en las extremidades, el desarrollo de espasticidad puede conllevar una mejoría funcional, por lo que su desaparición implicaría un empeoramiento de la discapacidad. En otras ocasiones la espasticidad, o su empeoramiento se debe a la existencia de otros factores (por ejemplo, infecciones), por lo que es importante identificar la causa y tratarla. 

La decisión de tratar la espasticidad requiere una evaluación cuidadosa e individualizada. En los casos leves la espasticidad puede aliviarse con fisioterapia y ejercicio.

Hay que considerar el tratamiento con fármacos orales cuando el tratamiento rehabilitador fracase o la espasticidad sea moderada. Si la monoterapia no es efectiva, se pueden combinar fármacos con distintos mecanismos de acción. Dentro de las terapias farmacológicas para el tratamiento de la espasticidad se encuentran el baclofeno, el dantroleno, la tizanidina, diacepam, gabapentina, toxina botulínica tipo A.

Hasta un 80% de los pacientes con EM referirían temblor o ataxia a lo largo de la evolución de la enfermedad. Las terapias rehabilitadoras pueden resultar útiles en el tratamiento de la ataxia. 

Dentro de los medicamentos utilizados para el temblor se encuentran la isoniacida. El propranolol no ha demostrado utilidad en pacientes con EM . Entre los fármacos antiepilépticos, la carbamazepina puede ser útil en el tratamiento del temblor de origen cerebeloso. 

Requiere un manejo multidisciplinario, pues puede existir un componente psicogeno asociado. La disfunción eréctil en varones con EM es frecuente. La utilidad de los inhibidores de la fosfodiesterasa-5, como el sildenafilo, vardenafilo o tadalafilo, se ha estudiado en pacientes con EM. Los dispositivos de vacío y las prótesis peneanas constituyen el último escalón del tratamiento.

Entre las mujeres, los principales problemas son: anorgasmia, falta de lubricación vaginal y disminución de la libido. La falta de lubricación vaginal se resuelve con lubricantes. La anorgasmia en ocasiones puede mejorar con dosis bajas de estrógenos en crema de aplicación vaginal. El sildenafilo se ha probado en mujeres con EM y disfunción sexual sin éxito.

En EM entre 30 y 80% de los pacientes utilizan terapias alternativas y complementarias , que son terapias no convencionales pueden utilizarse como ayuda o en lugar de tratamientos establecidos de la enfermedad, principalmente como parte del manejo de síntomas específicos relacionados con la enfermedad . Es importante reiterar que ninguna de estas terapias ha demostrado tener un efecto en modificar la actividad y progresión de la enfermedad, aunque algunas de ellas han demostrado tener un impacto positivo en la calidad de vida de los pacientes , siendo algunas de ellas relacionadas con rehabilitación física y ejercicio, alimentación, técnicas de meditación-relajación o psicoterapia .

Las terapias de rehabilitación física tienen un impacto positivo en la recuperación de secuelas y en mejorar la movilidad, disminuir el grado de espasticidad e incrementar la fuerza de extremidades. Específicamente se ha descrito mejoría en velocidad de marcha y estabilidad .

Otras terapias alternativas como yoga, pilates o tai chi han mostrado en algunos estudios de calidad de vida mejoría similar o mejor a actividad física regular en fuerza muscular, estabilidad postural, disminuyendo niveles de estrés, específicamente en yoga, que es practicado en aproximadamente 30% de los pacientes con EM. Resultados similares se han visto en quienes realizan pilates y tai chi al ser ejercicios que involucran estiramiento muscular y ejercitan músculos que mejoran la estabilidad postural, incluso en ciertos estudios comentando mejoría en fatiga. 

Así como la discapacidad física está presente en pacientes con esclerosis múltiple, la presencia de síntomas neuropsiquiátricos como trastornos depresivos, alteraciones cognoscitivas y de la conducta son síntomas bastante frecuentes en la enfermedad. Si bien la terapia psicológica no interfiere con la actividad inflamatoria o la progresión de la discapacidad, si impacta positivamente en la calidad de vida de los pacientes con EM.

Terapias alternativas como biofeedback, mindfulness o meditación disminuyen niveles de estrés y mejora la capacidad de respuesta al mismo, encontrando en algunos estudios mejoría principalmente en la autopercepción de la discapacidad y calidad de vida.

Algunos estudios relacionan terapia con masajes y reflexología con disminución de niveles de estrés y reducción de secuelas sensitivas, como parestesias, en pacientes con EM. 

Canabis: Tanto la marihuana como sus derivados canabinoides han probado tener una efectividad como relajante muscular y como terapia para la espasticidad, ya sea por eventos traumáticos, vasculares, congénitos o como en este caso, en pacientes con esclerosis múltiple, así como el manejo de ciertos tipos de dolor neuropático crónico. 

Sin embargo, no ha logrado relacionarse con modificación en la tasa de recaídas o la aparición de lesiones nuevas en T2 en resonancia magnética o lesiones captantes de gadolinio. También es bien conocido el efecto negativo en la cognición a largo plazo, por lo que no puede ser recomendado como terapia complementaria en EM, salvo que sea para tratar específicamente espasticidad o manejo de dolor crónico, solo si no hay respuesta a tratamientos alternativos.  

Ginkgo biloba: No se ha demostrado su eficacia en mejorar la cognición, concentración o atención en pacientes con esclerosis múltiple, sin embargo hay reportes que afirman su efectividad en el manejo de la fatiga, otro síntoma muy importante y discapacitante en estos pacientes.              

Se ha utilizado la ozonoterapia para en múltiples enfermedades con el objetivo de mejorar la oxigenación tisular y disminuir el efecto nocivo de especies reactivas de oxígeno siendo este último punto uno de los target terapéutico de la EM, incluso con una TME que actúa a este nivel. Desgraciadamente no hay artículos confiables en los que se haya demostrado eficacia en reducir número de reacaídas, progresión discapacidad o presencia de lesiones nuevas en resonancia magnética (signos que indican actividad de la enfermedad) con este tipo de terapias en pacientes con EM

Es ampliamente conocida la relación negativa del consumo de tabaco con la actividad de la enfermedad. Múltiples estudios demuestran la asociación de mayor número de recaídas, aparición de nuevas lesiones y progresión de la discapacidad en pacientes fumadores versus su contraparte no fumadora, asociado principalmente al incremento de radicales libres generados. 

Al momento no hay una relación directa entre el consumo de alcohol y la progresión de la enfermedad, incluso un artículo sugiere disminución de riesgo. Sin embargo esto no es suficiente para poder recomendar su uso. Por otra parte, al haber en el paciente con EM síntomas y secuelas que alteran la función de equilibrio y cognitiva. 

Múltiples estudios han demostrado la incidencia que tiene el sobrepeso/obesidad en estados inflamatorios, dado principalmente por el incremento de estados proinflamatorios y reducción de estados antiinflamatorios demostrado a nivel celular y por la expresión de citosinas correspondientes. Específicamente en esclerosis múltiple, el sobrepeso/obesidad se ha relacionado con incremento de la actividad inflamatoria de la enfermedad, tanto en población mundial, como específicamente en población mexicana. Si bien como medida aislada, no puede considerarse el control de peso como tratamiento de la enfermedad, si se ha demostrado que estilos de vida saludables, que lleven a mantener un peso adecuado, puede estar relacionado con disminución de la actividad inflamatoria, con mejoría en el estado funcional de pacientes con EM.

La única intervención nutricional que ha demostrado resultados en algunos estudios, que ha mostrado cambios en el estado inflamatorio de los pacientes con esclerosis múltiple, así como la cantidad de recaídas es la complementación con ácidos grasos omega 3 derivados de pescado. Otros ácidos grasos, así como suplementación con vitaminas B, C, E, limitación calórica o dietas específicas no han mostrado diferencias significativas con respecto a placebo en enfermedad remitente-recurrente o en modalidades progresivas. 

Si bien es conocida la relación de la deficiencia de vitamina D como probable factor ambiental para el desarrollo de EM, la suplementación con esta vitamina no ha mostrado resultados significativos en la disminución de la actividad de la enfermedad per se. Sin embargo, la disminución de niveles de vitamina D se relaciona con osteopenia y aumento de fracturas en pacientes con y sin esclerosis múltiple, lo que empeora la funcionalidad, por lo que es recomendable complementación con vitamina D en todos los pacientes en los que se diagnostique EM. 

Hay un número considerable de estudios que sugieren una relación directa entre el nivel de estrés emocional con el incremento de recaídas clínicas y presencia de lesiones captantes de gadolinio por resonancia magnética. Al ser este el sustrato principal en la fisiopatogenia de la EM, es esperada esta respuesta en pacientes sometidos a estrés crónico o aumento transitorio de situaciones de estrés laboral, personal o social.  

Se ha propuesto en varios estudios la disminución en la función cognitiva y calidad de vida en pacentes con EM . Si bien es ampliamente conocido que una buena calidad de sueño mejora la calidad de vida percibida en la población general y disminuye riesgo de enfermedades cardiovasculares y estado anímico, no se han realizado estudios que relacionen este rubro con la actividad inflamatoria o estado de discapacidad en esclerosis múltiple.

Los pacientes con esclerosis múltiple no están exentos de desarrollar otro tipo de patologías relacionadas con estilos de vida. Mantener una vida sedentaria, tanto en personas con EM como en población sana, es un importante factor de riesgo para el desarrollo de enfermedades cardiovasculares y sus complicaciones, por lo que debe fomentarse la importancia de mantener estilos de vida saludables que incluyan actividad física regular. Cuando además hay secuelas motoras o de coordinación, realizar ejercicio de manera habitual, así como la terapia específica de rehabilitación física, puede ayudar a mantener un adecuado tono muscular, peso saludable y articulaciones suficientemente laxas para mejorar la movilidad de los pacientes, así como disminuir globalmente niveles de estrés.

Actualmente no hay pruebas clínicas establecidas que permitan un pronóstico o decidir una respuesta terapéutica, aunque existen investigaciones prometedoras que necesitan ser confirmadas, como la detección de los anticuerpos anti-MOG (anticuerpo sérico contra la MOG ["myelin oligodendrocyte glycoprotein:" ‘glicoproteína de la mielina de los oligodendrocitos’]) y anti-MBP (anticuerpo contra la MBP ["myelin basic protein:" ‘proteína básica de la mielina’]), como factores predictores de evolución a la enfermedad instaurada. La incertidumbre es uno de los aspectos psicológicos que resulta más difíciles de llevar en la esclerosis múltiple.

Debido a la mejora en el tratamiento de complicaciones como infecciones pulmonares o en la vejiga urinaria, la esperanza de vida de las personas diagnosticadas con esclerosis múltiple solo se ve ligeramente reducida.

Cuanto más joven se es cuando aparece la enfermedad, más lentamente avanza la discapacidad. Esto es debido a que en personas mayores es más frecuente la forma crónica progresiva, con una acumulación mayor de discapacidad.

La discapacidad tras cinco años se corresponde con la discapacidad a los 15 años: 2/3 de los pacientes con esclerosis múltiple que tengan poca discapacidad a los 5 años, no se deteriorarán mucho en los 10 años siguientes. Otros casos de esclerosis múltiple en la familia no influyen en la progresión de la enfermedad. Uno de cada tres pacientes seguirá siendo capaz de trabajar después de 15 a 20 años.

Pérdida de visión o síntomas sensoriales (entumecimiento u hormigueo) como síntomas iniciales son signos de un pronóstico benigno. Perturbaciones en el andar y cansancio son signos de un pronóstico negativo.

Una rápida regresión de los síntomas iniciales, edad a la que aparece la enfermedad por debajo de los 35, solo un síntoma inicialmente, desarrollo rápido de los síntomas iniciales y corta duración de la última recaída indican un buen pronóstico.

Si la forma es con recaídas y remisiones, estadísticamente serán necesarios 20 años hasta que la silla de ruedas sea necesaria. Esto quiere decir que muchos pacientes nunca la necesitarán. Si la forma es progresiva primaria, como media, se necesitará una silla de ruedas tras 6 o 7 años. Hay que tener en cuenta que estos datos a largo plazo fueron tomados antes de la llegada de los medicamentos inmunomoduladores modernos, a principios de los años 2000. Estos medicamentos consiguen retrasar el progreso de la enfermedad varios años.

Las personas que padecen Esclerosis Múltiple (EM) enfrentan diversas situaciones que afectan su calidad de vida. El vivir con EM puede causar deterioro en diversos ámbitos de la vida que pueden influir en la vida familiar, laboral y actividades de la vida diaria. La forma en que evoluciona la enfermedad puede causar incertidumbre y limitar el desarrollo de planes a futuro lo cual genera importante estrés psicológico. Las necesidades de los pacientes son muy variadas y dependerá de entre otros factores de la edad, sexo, gravedad de la enfermedad, tipo de empleo, redes de apoyo con los que cuenten los pacientes. Algunas de las principales necesidades identificadas tanto en pacientes como en familiares y cuidadores son: mayor conocimiento y entendimiento de la enfermedad, apoyo psicológico a pacientes, familiares y cuidadores sobre todo en pacientes cuya enfermedad ha causado algún grado de discapacidad, acceso a tratamientos multidisciplinarios, un diagnóstico y tratamientos oportunos que modifiquen el curso de la enfermedad y eviten discapacidad. Otros puntos que se han identificado son la necesidad de una mejor comunicación con todos los involucrados en el cuidado de los pacientes con EM (médicos, enfermeras, rehabilitadores, psicólogos, cuidadores). Acceso a grupos de apoyo a nivel local. Es muy importante para los pacientes el poder sentir que tienen control de su vida , sentirse productivos y tener soporte emocional.

Desde 2009 la Federación Internacional de Esclerosis Múltiple (MSIF, por sus siglas en inglés), inició con la campaña de celebrar cada año el día mundial de la EM. Con esta campaña se ha llegado a miles de personas y cada año se centra en un tema diferente. Se considera que es un día para celebrar la solidaridad mundial y la esperanza para el futuro. El Día Mundial de la EM está establecido el último miércoles del mes de mayo pero los eventos y campañas se celebran durante todo el mes de mayo. Reúne a la comunidad global de EM para compartir historias, sensibilizar y hacer campaña con y para todos los afectados por la esclerosis múltiple.

Los comités para el tratamiento e investigación en EM a nivel mundial son asociaciones sin fines de lucro las cuales tienen como principal objetivo entender y tratar a la esclerosis múltiple. Estos comités contribuyen a nivel global trabajando con investigadores y clínicos en el desarrollo de investigación tanto básica como clínica, apoyan educación y enseñanza en la materia, estimulan a investigadores jóvenes a involucrarse en la EM, organizan congresos anuales, crean alianzas y colaboraciones a nivel internacional todo con el fin de tener un mejor entendimiento de la enfermedad.

Existen diversos comités a nivel mundial divididos por continentes y dentro de estos hay países que han formado sus propios comités. Uno de los más grandes y que fue creado hace más de 30 años es ECTRIMS (European Committe for treatment and research in Multiple Sclerosis) organiza anualmente el congreso anual que convoca al mayor número de personas interesadas en una sola enfermedad. Otros comités a nivel continental son ACTRIMS (American Committe for treatment and research in Multiple Sclerosis) , LACTRIMS (Latin American Committe for treatment and research in Multiple Sclerosis) PACTRIMS (Pan Asian Committe for treatment and research in Multiple Sclerosis) MENACTRIMS (Middle East North Africa Committe for treatment and research in multiple sclerosis) , los cuales tiene como objeto el desarrollo de investigación y conocimiento de la enfermedad en las diferentes regiones. Existen también algunos países que han creado sus propios comités como MEXCTRIMS (Mexican Committe for treatment and research in Multiple Sclerosis) , BCTRIMS (Brazilian Committe for Treatment and research in Multiple Sclerosis) . Estas asociaciones también se encargan de difundir y concientizar acerca del conocimiento de la enfermedad.

Existen también otras iniciativas a nivel global como MS Brain Health las cuales buscan alertar de la importancia de un diagnóstico y tratamiento oportunos, generando información para población general y para todas las personas involucradas en el cuidado de los pacientes con EM.

En modelos murinos se ha logrado reducir la infiltración por linfocitos T CD4 de la vaina de mielina y las lesiones desmielinizantes características de la EM utilizando naltrexona en dosis bajas (LDN) . Dicho tratamiento restablece los niveles circulantes de 5-metaencefalina que se encontraban reducidos en sujetos con EM y tras 8 semanas de uso, se reducen citocinas proinflamatorias fuertemente implicadas en su fisiopatología . Tres ensayos clínicos han encontrado que este tratamiento es seguro, no causa eventos adversos serios y su uso es bien tolerado además de mejorar la calidad de vida y reducir la espasticidad en sujetos con EM . Todo ello ha justificado la realización de ensayos clínicos en diferentes patologías tales como: CHRON, fibromialgia, cáncer, etc. 




</doc>
<doc id="41587" url="https://es.wikipedia.org/wiki?curid=41587" title="ECHELON">
ECHELON

ECHELON es considerada la mayor red de espionaje y análisis para interceptar comunicaciones electrónicas de la historia (Inteligencia de señales, en inglés: Signals intelligence, SIGINT). Controlada por la comunidad UKUSA (Estados Unidos, Reino Unido, Canadá, Australia y Nueva Zelanda), ECHELON puede capturar comunicaciones por radio y satélite, llamadas de teléfono, faxes y correos electrónicos en casi todo el mundo e incluye análisis automático y clasificación de las interceptaciones. Se estima que ECHELON intercepta más de tres mil millones de comunicaciones cada día.

A pesar de haber sido desarrollada con el fin de controlar las comunicaciones militares de la Unión Soviética y sus aliados, se sospecha que en la actualidad ECHELON es utilizada también para encontrar pistas sobre tramas terroristas, planes del narcotráfico e inteligencia política y diplomática. Sus críticos afirman que el sistema es utilizado también para el espionaje económico de cualquier nación y la invasión de privacidad en gran escala. La existencia de ECHELON fue hecha pública en 1976 por Winslow Peck.

Los miembros de esta alianza de habla inglesa son parte de la alianza de inteligencia UKUSA, que lleva reuniendo inteligencia desde la Segunda Guerra Mundial. El sistema está bajo la administración de la NSA (National Security Agency). Esta organización cuenta con 100 000 empleados tan solo en Maryland (Estados Unidos) (otras fuentes hablan de 380 000 empleados a escala mundial), por lo que es probablemente la mayor organización de espionaje del mundo. La información es enviada desde Menwith Hill (Reino Unido) por satélite a Fort Meade en Maryland (EE. UU.).

A cada estado dentro de la alianza UKUSA le es asignado una responsabilidad sobre el control de distintas áreas del planeta. La tarea principal de Canadá solía ser el control del área meridional de la antigua Unión Soviética. Después de la Guerra Fría se puso mayor énfasis en el control de comunicaciones por satélite y radio en centro y Sudamérica, principalmente como medida para localizar tráfico de drogas y secuaces en la región. Los Estados Unidos, con su gran cadena de satélites espías y puertos de escucha controlan gran parte de Latinoamérica, Asia, Rusia asiática y el norte de China. Gran Bretaña intercepta comunicaciones en Europa, Rusia y África. Australia examina las comunicaciones de Indochina, Indonesia y el sur de China, mientras que Nueva Zelanda barre el Pacífico occidental. El desarrollo de estos sistemas se ha extendido por otros países, entre los que cabría destacar la creación de un centro OSINT en la universidad sueca de Lund.

Varias fuentes afirman que estos estados han ubicado estaciones de intercepción electrónica y satélites espaciales para capturar gran parte de las comunicaciones establecidas por radio, satélite, microondas, móviles y fibra óptica. Las señales capturadas son luego procesadas por una serie de superordenadores, conocidas como "diccionarios", las cuales han sido programadas para buscar patrones específicos en cada comunicación, ya sean direcciones, palabras, frases o incluso voces específicas. Según algunas fuentes el sistema dispone de 120 estaciones fijas y satélites geoestacionarios. Estos podrían filtrar más del 90 % del tráfico de Internet. Las antenas de Echelon pueden captar ondas electromagnéticas y trasmitirlas a un lugar central para su procesamiento. Se recogen los mensajes aleatoriamente y se procesan mediante los diversos filtros buscando palabras clave. Este procedimiento se denomina "Control estratégico de las telecomunicaciones".

Diversos medios de comunicación señalaron que el apoyo del expresidente español José María Aznar a las estrategias desarrolladas bajo la presidencia de G.W. Bush, habrían posibilitado la colaboración antiterrorista de los Estados Unidos, incluyendo el acceso a la red Echelon y "Carnivore". Dicha colaboración habría permitido la detención de algunos comandos de ETA y la detención del arsenal etarra en Sokoa en el País Vasco francés basado en el informe "Una aproximación a las tecnologías de control político" de la Fundación Omega de Mánchester y presentado en la Comisión de Libertades Públicas y Asuntos de Interior del Parlamento Europeo el 27 de enero de 1998.

La red Echelon espió al ingeniero español, José Ignacio López de Arriortúa, grabando la NSA una videoconferencia entre este y su superior de la empresa alemana Volkswagen entregando a los EE. UU. la información a la empresa estadounidense General Motors.

La actividad fue motivo de pregunta parlamentaria por parte de la diputada Begoña Lasagabaster (Eusko Alkartasuna).

El Parlamento Europeo decidió crear una comisión para investigar la red ECHELON el 5 de julio de 2000. La motivación fue el informe titulado "Capacidades de Intercepción 2000", en el que se informaba del uso de información recolectada por la red ECHELON para fines comerciales de los países UKUSA.

En 1994, el grupo francés Thompson-CSF habría perdido un contrato con Brasil por valor de 1300 millones de dólares en favor de la empresa estadounidense Raytheon, gracias a información comercial interceptada por ECHELON que habría sido suministrada a Raytheon. Ese mismo año Airbus habría perdido un contrato de 6000 millones de dólares con Arabia Saudita en favor de las empresas estadounidenses Boeing y McDonnell Douglas, gracias a que las negociaciones entre Airbus y sus interlocutores árabes habrían sido interceptadas por ECHELON, y la información facilitada a las empresas estadounidenses

En 2001, el Parlamento Europeo en Acta del 5 de septiembre de 2001, emitió un informe en el que se expresa que: ""considerando que no hay ninguna razón para seguir dudando de la existencia de un sistema de interceptación de las comunicaciones a nivel mundial"" constató la existencia de un sistema de interceptación mundial de las comunicaciones, resultado de una cooperación entre los Estados Unidos, Reino Unido, Canadá, Australia y Nueva Zelanda siendo ""la finalidad del sistema es la interceptación, como mínimo, de comunicaciones privadas y comerciales, y no militares""

La red de espionaje fuera del control judicial supone una privación de la libertad individual consagrada en diferentes textos legislativos internacionales y nacionales; siendo este el motivo por el que el 21 de octubre de 2001, se organizó a través de Internet un intento de colapsar o socavar a Echelon.

PRISM es un programa del Gobierno estadounidense, que puede ser considerado como parte de la red ECHELON. Es divulgada su existencia por los medios de comunicación en junio de 2013 (The Guardian y The Washington Post), y se caracteriza por capturar los datos de compañías como Google, Apple, Microsoft o Facebook. Aunque todas ellas niegan su participación activa, la filtración de una presentación de la NSA hace considerar que esto no es así.









</doc>
<doc id="41589" url="https://es.wikipedia.org/wiki?curid=41589" title="Mioclono">
Mioclono

El mioclono es una sacudida repentina e involuntaria de un músculo o grupo de músculos. Describe un signo médico y generalmente no constituye el diagnóstico de una enfermedad. 

El mioclono puede desarrollarse en respuesta a la infección, un traumatismo en las cervicales o la columna vertebral, tumores cerebrales, secundario a un evento vascular cerebral (EVC), o insuficiencia renal, o en respuesta a un problema en la eliminación de las grasas, a la intoxicación por drogas o sustancias química. La prolongada falta de oxígeno en el cerebro, llamada hipoxia, puede conducir a un mioclono post-hipóxico. Las mioclonías pueden presentarse solas, pero por lo general es uno de los síntomas que pueden observarse por un problema del sistema nervioso. Las sacudidas mioclónicas se generan principalmente en las personas con epilepsia, un trastorno en el cual se altera la actividad eléctrica en el cerebro y conduce a convulsiones.

Los tirones mioclónicos generalmente resultan de contracciones musculares repentinas llamadas mioclonos positivos, o de un relajamiento muscular llamado mioclono negativo. Las sacudidas mioclónicas pueden ocurrir solas o en secuencia, con o sin algún patrón determinado. Pueden ocurrir infrecuentemente, o muchas veces por minuto. Los mioclonos ocasionalmente ocurren como respuesta a un evento externo o cuando una persona trata de realizar un movimiento. Las contracciones son incontrolables.

En su manera más sencilla, el mioclono consiste en una contracción seguida por un relajamiento del músculo. El hipo es un ejemplo de este tipo de mioclono. Otros ejemplos familiares de mioclonos son los sobresaltos nocturnos o mioclonías del sueño que algunas personas tienen cuando están quedándose dormidas. Estas formas sencillas de mioclonos ocurren en personas normales y saludables sin causar problema alguno. Cuando son más prevalecientes, los mioclonos consisten en contracciones persistentes en forma de sacudidas violentas en un grupo de músculos. En algunos casos, los mioclonos comienzan en una región del cuerpo y se esparcen a los músculos en otras áreas. Los casos más severos de mioclonos pueden distorsionar el movimiento y limitar severamente la capacidad de comer, hablar o caminar. Estos tipos de mioclonos indicarían un trastorno asociado en el cerebro o los nervios.

Es difícil clasificar las diferentes formas de mioclonos porque las causas, los efectos y las respuestas a la terapia varían mucho. A continuación se describen los tipos más comunes.





</doc>
<doc id="41590" url="https://es.wikipedia.org/wiki?curid=41590" title="Síndrome de las piernas inquietas">
Síndrome de las piernas inquietas

El síndrome de las piernas inquietas es un trastorno neurológico caracterizado por sensaciones desagradables en las piernas (menos frecuente en los brazos) y un impulso incontrolable de moverse y andar cuando se está descansando, en un esfuerzo del paciente de aliviar estas sensaciones (se denomina en ciertas ocasiones "andadores nocturnos"). Algunos autores postulan que este trastorno pudiera afectar a un 10 % de la población mundial, y sus causas son desconocidas a comienzos del siglo XXI.
Un porcentaje pequeño de las personas es correctamente diagnosticado, debido a que su síndrome es dirigido a especialistas como neurólogos, reumatólogos, psicólogos, etc. No se trata de una enfermedad grave, que sea causa "per se" de muerte, pero sí de terribles desasosiegos que disminuyen la calidad de vida del paciente y de aquellos que le rodean. Si no se tratan pueden devenir en crisis nerviosas y depresión.
Con el objeto de aliviar, compartir y mejorar la convivencia se han creado en cada país asociaciones específicas. También se le denomina "síndrome de Ekbom", por lo que se puede confundir con el delirio de parasitosis o delirio dermatozoico.

Una revisión de 2019 concluye que se trata del segundo trastorno del movimiento relacionado con el gluten más frecuente. Estos se desarrollan independientemente de la presencia de síntomas o lesión intestinal, es decir, tanto en celíacos como en no celíacos. Más de la mitad de las personas mejora con la retirada estricta del gluten de la dieta, sin necesidad de ningún otro tratamiento adicional.

Se conocen las primeras descripciones médicas del trastorno ya en la segunda mitad del siglo XVII, procedentes del doctor inglés Thomas Willis (doctor de cámara de Carlos II). Willis describe un caso en 1672 de un granjero londinense. En el año 1861 el conocido clínico alemán Theodor Wittmaack la incluyó por primera vez en su lista de enfermedades describiéndola como inquietud de miembros inferiores: "Anxietas tibiarum". La denominación actual de "Piernas inquietas" se hizo oficial internacionalmente en 1945 gracias a la descripción del neurólogo de Estocolmo Karl Ekbom. En su publicación muestra los síntomas y describe ocho casos. Con posterioridad a su descripción se añadió la palabra síndrome para hacer descripción de la enfermedad, no de un proceso. Durante los años cincuenta se intentó ligar las causas de la enfermedad al período de gestación, deficiencia de hierro, y fallos renales crónicos. Estudios posteriores enlazaron algunas de causas a un fallo en el sistema límbico del cerebro humano.

Algunos investigadores estiman que este síndrome afecta hasta un 10 % de la población europea y estadounidense. Existen pocos casos diagnosticados en la India, Japón y Singapur. Esta aparición de la enfermedad en ciertos lugares, evidencia la existencia de factores raciales o étnicos. Sin embargo, otros consideran que la ocurrencia es mayor porque, en algunos casos, no se diagnostica correctamente. El síndrome de las piernas inquietas ocurre en ambos sexos, pero la incidencia puede ser ligeramente mayor en las mujeres. Aunque el síndrome puede comenzar a cualquier edad, aún tan temprano como en la infancia, la mayoría de los pacientes severamente afectados son de edad media o mayores. Además, la severidad del trastorno parece aumentar con la edad. Los pacientes mayores sufren los síntomas con más frecuencia y durante períodos de tiempo más largos. Algunos pacientes muestran síntomas ya desde la niñez. Algunas personas con el síndrome no buscan atención médica pensando que no se les va a tomar en serio, que sus síntomas son muy leves o que su problema no se puede tratar. Algunos médicos, equivocadamente, atribuyen los síntomas al nerviosismo, al insomnio, al estrés, a la artritis, a los calambres musculares o al envejecimiento.

El síndrome de piernas inquietas está asociado con frecuencia a una deficiencia de hierro (aproximadamente en el 20% de los casos). Los niveles elevados de estrógenos son otra causa relacionada con la aparición de los síntomas, como ocurre durante el embarazo. Entre los factores de riesgo se incluyen edad avanzada y el consumo de ciertos medicamentos, tales como los antagonistas de la dopamina, los antidepresivos tricíclicos y los inhibidores selectivos de la recaptación de serotonina. El tabaco y la ingesta excesiva de cafeína o alcohol pueden empeorar los síntomas.

Asimismo, es muy común en personas con enfermedades inflamatorias del tracto gastrointestinal, como la enfermedad celíaca sin diagnosticar o tratar (que puede cursar sin síntomas digestivos), la enfermedad de Crohn y el sobrecrecimiento bacteriano intestinal. Con frecuencia, estas enfermedades provocan una deficiencia de hierro y un aumento de los niveles de inflamación sistémica. Esta inflamación podría ser una causa directa del desarrollo del síndrome de piernas inquietas, al afectar a la neurotransmisión de la dopamina en el cerebro y la médula espinal.

El síndrome de las piernas inquietas es el segundo trastorno del movimiento relacionado con el gluten más frecuente (la ataxia por gluten es el primero). Estos trastornos se desarrollan independientemente de la existencia de síntomas o lesiones intestinales, es decir, tanto en celíacos como en no celíacos. Más de la mitad de las personas experimenta una clara mejoría con la retirada estricta del gluten de la dieta, sin necesidad de ningún otro tratamiento adicional. (véase Trastornos neurológicos relacionados con el gluten)

Las personas a menudo describen las sensaciones del síndrome de las piernas inquietas como «quemantes», como si algo se les deslizara, o como si insectos treparan por el interior de sus piernas (sensación de "hormigueo"). Estas sensaciones, a menudo llamadas parestesias (sensaciones anormales) o disestesias (sensaciones anormales desagradables), varían en gravedad de desagradables a irritantes, a dolorosas. Por regla general se desencadenan estas sensaciones cuando el paciente se encuentra en reposo. La sensación de alivio parece surgir si el paciente anda o se mueve, y es por esta razón por la que deviene en un trastorno del sueño, por su incapacidad de desarrollar una actividad de descanso. Esta situación genera una disminución de calidad de vida del paciente. Los síntomas del síndrome de las piernas inquietas en las extremidades se suelen desatar por la tarde, o noche. 

El aspecto más distintivo o poco usual del trastorno es que los síntomas son activados por el hecho de acostarse y tratar de relajarse. Como resultado, la mayoría de las personas con el síndrome de las piernas inquietas tienen dificultad para conciliar y mantener el sueño. En la literatura de finales del siglo XX se denominaba a los pacientes con síndrome de las piernas inquietas: "andadores nocturnos" "(nightwalkers)". En segundo orden de cosas puede causar trastornos sociales debido a la imposibilidad de asistir al teatro, al cine. En algunas ocasiones impedimento para viajar. Pérdida de concentración en el trabajo. Si no se trata, el trastorno provoca agotamiento y fatiga durante el día. Muchas personas con el síndrome de las piernas inquietas informan que su trabajo, sus relaciones personales y las actividades diarias resultan muy afectadas como resultado del cansancio. A menudo no se pueden concentrar, tienen la memoria deteriorada, o fallan en el cumplimiento de sus tareas diarias. Existe una categorización de severidad de la enfermedad que va desde los síntomas intermitentes de incomodidad, hasta los moderados (síntomas que aparecen a diario), hasta los extremos que por su habitual aparición puede causar serios problemas psicológicos. En casos graves también afecta los miembros superiores llegando hasta zarandeos de cuerpo completo parecido a la epilepsia (claro que si pasa en ese caso ambas se pueden confundir).

Los médicos que sospechan el síndrome de las piernas inquietas suelen tratar los pacientes en clínicas del sueño con el objeto de precisar mejor el diagnóstico. Para diagnosticar el RSL, debe detectarse cualquiera de los cuatro síntomas:
Debido a una posible causa de transmisión hereditaria, los pacientes que tengan entre los familiares un caso de síndrome de las piernas inquietas son potenciales pacientes. De la misma forma un análisis de sangre buscando bajos niveles de ferritinas que indiquen bajos niveles de hierro en sangre.

Más del 80% de las personas con el síndrome de las piernas inquietas también sufren una dolencia más común conocida como síndrome de movimientos periódicos de las piernas (PLMD por sus siglas en inglés). Se ha descrito la asociación de anemia ferropénica con el síndrome de piernas inquietas junto a acatisia. El PLMD se caracteriza por movimientos involuntarios bruscos de las piernas, como jalones o tirones, que ocurren durante el sueño, generalmente con una frecuencia de 10 a 60 segundos, a veces durante toda la noche. Estos síntomas hacen que el paciente se despierte repetidamente e interrumpen gravemente el sueño. A diferencia del síndrome de las piernas inquietas, los movimientos causados por el "PLMD" son involuntarios. Aunque muchos pacientes con este síndrome también desarrollan el síndrome de movimientos periódicos de las piernas, la mayoría de las personas con este trastorno no sufren el síndrome de las piernas inquietas.

De igual modo que ocurre con el síndrome de las piernas inquietas, no se conoce la causa del "PLMD", aunque estudios recientes realizados en la Escuela de Medicina de la Universidad de Harvard concluyen provisionalmente que detrás de la relación entre la disfunción eréctil y el movimiento involuntario de las piernas podrían estar los bajos niveles de dopamina, una hormona neurotransmisora del sistema nervioso asociada a ambos trastornos.
Se trata con diversos medicamentos que van desde la levodopa, los dopaminérgicos, los opiáceos, las benzodiazepinas, los antiepilépticos y los suplementos dietéticos de hierro.




</doc>
<doc id="41591" url="https://es.wikipedia.org/wiki?curid=41591" title="Anemia de células falciformes">
Anemia de células falciformes

La anemia de células falciformes, llamada también drepanocitosis o anemia drepanocítica, es una alteración de la sangre que hace que los glóbulos rojos se deformen hasta adquirir apariencia de hoz (de ahí el nombre de «eritrocitos falciformes» o «drepanocitos»). Esta deformidad estructural entorpece la circulación sanguínea y causa en el enfermo obstrucciones vasculares, microinfartos en múltiples órganos y hemólisis. Se trata de una anemia crónica y grave que se diagnostica mediante el estudio de la hemoglobina, la molécula que transporta el oxígeno en la sangre. 

La enfermedad tiene su origen en la sustitución, consecuencia de una mutación genética, de un aminoácido polar (el ácido glutámico) por otro no polar (la valina) en la sexta posición de la cadena de globina β, uno de los componentes de la hemoglobina adulta (HbA) de tal manera que disminuye la unión de oxígeno y el eritrocito se atrofia.

En la literatura científica se designa esta alteración con múltiples denominaciones, además de las mencionadas: «drepanocitemia», «enfermedad de células falciformes» o «hemoglobinopatía S homocigota»; otros nombres, ya en desuso, son: «anemia de la hemoglobina S», «anemia de Herrick», «anemia microdrepanocítica», «enfermedad de la hemoglobina S» o «meniscocitosis». La expresión «anemia falciforme» es inexacta, ya que lo que tiene forma de hoz son los eritrocitos, no la anemia en sí. El término "siclemia", híbrido impropio formado a partir del inglés "" es incorrecto.

A diferencia de los hematíes normales, que generalmente son bicóncavos, con una forma similar a la de una rosquilla, los glóbulos rojos falciformes tienen forma de media luna o de hoz. Este cambio en la morfología del glóbulo rojo se debe a una mutación puntual sustitutiva de un residuo de ácido glutámico por un residuo de valina en la sexta posición de la cadena de globina β, que forma parte de la molécula de hemoglobina. Su presencia determina alteraciones en la estructura de la célula. La membrana plasmática se vuelve rígida y viscosa. La célula se deshidrata por salida de potasio y entrada de calcio. Al exponerse a bajas presiones parciales de oxígeno, se forman fibras poliméricas de hemoglobina S; que produce las alteraciones en la morfología del eritrocito, a la vez que una menor capacidad para transportar oxígeno.

Estas células modificadas presentan menor deformabilidad, asociada a su vez con una mayor expresión de moléculas de adhesión en la superficie de la membrana plasmática. Estas características provocan atascamiento de dichos eritrocitos en la microcirculación, llevando a un obstrucción del flujo sanguíneo (isquemia) con la consiguiente disminución del aporte de oxígeno al tejido (hipoxia). Estas microisquemias pueden provocar crisis dolorosas, con posible lesión tisular y liberación de mediadores proinflamatorios y nociceptivos.

La anemia de células falciformes es una enfermedad genética autosómica recesiva resultado de la sustitución de timina por adenina en el gen de la globina beta, ubicado en el cromosoma 11, lo que conduce a una mutación de ácido glutámico por valina en la posición 6 de la cadena polipeptídica de globina beta y a la producción de una hemoglobina funcionalmente defectuosa, la hemoglobina S. El ácido glutámico tiene carga negativa y la valina es hidrófoba, entonces se forman contactos con alanina, fenilalanina y leucina, lo que promueve polímeros cruzados que deforman el glóbulo rojo.

La transformación del eritrocito se produce cuando no transporta oxígeno, pues con oxihemoglobina el glóbulo tiene la forma clásica bicóncava.

La transmisión génica se debe a un gen de herencia autosómica recesiva del cromosoma 11; así pues los individuos homocigóticos recesivos (SS) sólo producen globina beta con valina. El número de hematíes falciformes alcanza todo el tracto venoso y cualquier esfuerzo podría provocarles la muerte. Afecta a 4 por cada mil individuos de la población afroamericana.

Los individuos heterocigóticos (AS) fabrican la mitad de globina beta con ácido glutámico y la otra mitad con valina, de tal manera que sólo una centésima parte de sus eritrocitos son células falciformes. Pueden llevar una vida bastante normal, aunque no les es muy recomendable hacer grandes esfuerzos. Padecen una leve anemia a veces incluso inapreciable. Afecta al 8 % de la población afroamericana.

Los estudios muestran que en las zonas donde el paludismo o malaria era o es un problema, los individuos que heredan un solo alelo de la hemoglobina S —y que por tanto son portadores del rasgo de la célula falciforme— tienen una ventaja para sobrevivir sobre los individuos con genes de globina normales. A medida que las poblaciones iban migrando de un lugar a otro, la transformación de la célula falciforme se extendió a otras zonas del Mediterráneo y de allí al Oriente Medio, y finalmente al hemisferio occidental (a causa, entre otras cosas, del mercado de esclavos negros).

En los Estados Unidos y otros países en los que el paludismo no es un problema, el gen de la hemoglobina falciforme no constituye ya una ventaja para sobrevivir. En cambio, puede ser una seria amenaza para los hijos del portador, ya que estos pueden heredar dos genes anormales de hemoglobina falciforme y entonces desarrollar anemia falciforme.

Entre las opciones terapéuticas se cuenta con:

Los niños con anemia drepanocítica son más susceptibles a sufrir infecciones por bacterias como "Streptococcus pneumoniae" (también conocido como neumococo), que puede causar meningitis, neumonía y sepsis. La administración de penicilina V —por vía oral— reduce mucho el riesgo de enfermedad. Se debe dar desde el nacimiento hasta los cinco años o incluso prolongarse durante toda la vida si se ha sufrido una infección neumocócica grave, una esplenectomía, un trasplante de precursores hematopoyéticos o se tiene asplenia funcional.

En un artículo publicado en la revista "Essence" en septiembre de 2011 se menciona un tratamiento alternativo para pacientes de anemia falciforme a base de células madre. Es el caso de una familia en donde uno de sus hijos tenía esta condición. Los profesionales de salud optaron por el trasplante de células madre del cordón umbilical de un hermano que no era portador de la enfermedad. Los médicos extrajeron células madre del cordón y al paciente se le expuso a quimioterapia, para evitar la reproducción de glóbulos rojos deformes. Tras tres meses de haber introducido de forma intravenosa las células madre, al paciente se le declaró sano.

El mayor problema es el costo de conservar esas células madre para hacer el trasplante. Con respecto al índice de éxito de la operación, este se acerca al 90 % para niños y decrece en función de la edad, hasta el punto que para adultos no hay muchas probabilidades de que la operación sea exitosa.





</doc>
<doc id="41592" url="https://es.wikipedia.org/wiki?curid=41592" title="San Pedro Tidaá">
San Pedro Tidaá

San Pedro Tidaá es un municipio que pertenece al distrito de Asunción Nochixtlán del estado de Oaxaca, México. Pertenece a la Mixteca Alta. Colinda con las poblaciones de Magdalena Yodocono, San Francisco Nuxaño, San Juan Diuxi, Santiago Tilantongo y San Miguel Achiutla. Los habitantes de este municipio hablan mixteco.

Las festividades más representativas de este municipio son la Semana Santa, el día de carnaval, la fiesta de su patrono San Pedro y la más relevante: la fiesta de la Ascensión del Señor el día 6 de agosto. Esta última coincide con las vacaciones escolares, y debido a esto llegan al lugar los radicados en los diferentes estados de México y hasta del vecino país del norte.

La palabra "Tidaá" significa pájaro en el dialecto mixteco. Anteriormente, según cuentan los "tíos" (palabra usada con respeto para dirigirse a los mayores), hubo un cacique que quería que se nombrara San Pedro Tototitlán pero, como esta palabra no la comprendían, no fue aceptada y se quedó siempre con la terminación "tidaá".

Entre los personajes más destacados de su historia figura el Sr. Tomás Jiménez, el cual se rumorea que fue el fundador de la población.

El municipio tan sólo cuenta con tres poblaciones incluida la cabecera municipal.



</doc>
<doc id="41594" url="https://es.wikipedia.org/wiki?curid=41594" title="Amarillo">
Amarillo

El amarillo es el color que se percibe ante la fotorrecepción de una luz cuya longitud de onda dominante mide entre 574 y 582 nm. Se asemeja a la coloración característica de la piel del limón maduro, de la flor del diente de león, de las abejas o del oro.

Se encuentra estandarizado en catálogos de colores e inventarios cromáticos, sirviendo este amarillo «estándar» como modelo y referente para el color amarillo. El amarillo estándar puede verse en el recuadro de la derecha, arriba; los valores que se dan debajo corresponden al mismo.

La denominación de color «amarillo» abarca un conjunto de coloraciones o variantes semejantes al color estándar, llamadas amarillentas.

La palabra amarillo deriva del bajo latín hispánico "amarēllus", ‘amarillento, pálido’, diminutivo del latín "amārus", ‘amargo’. Para el filólogo Joan Corominas, esta asociación de conceptos puede haberse debido a la palidez de los aquejados de ictericia, por ser esta enfermedad un trastorno de la bilis o humor amargo.

El amarillo es uno de los cuatro colores psicológicos primarios, junto con el rojo, el verde y el azul. Además, es considerado un color cálido, junto con el rojo, el naranja y todas las coloraciones que tienden a estos.

El amarillo, junto con el magenta y el cian, es un color primario sustractivo. Esto significa que cuando se trabaja con pigmentos o tintes de cualquier clase (pinturas, colorantes, tintas) basta con mezclar esos tres colores en diferentes proporciones para obtener todos los demás, con el agregado de negro y ocasionalmente de blanco para lograr una tonalidad más clara o más oscura.

En el procedimiento de impresión por cuatricromía (que se usa para imprimir, por ejemplo, libros y revistas en color) también juega un papel fundamental el amarillo, ya que esta técnica también emplea los colores primarios sustractivos con el agregado de negro. De allí que un color para cuatricromía se describa mediante el porcentaje de cada uno de estos cuatro colores que entra en su composición. Así, un área impresa en color amarillo puro estará compuesta por C=0 (0 % de cian), M=0 (0 % de magenta), Y=100 (100 % de amarillo) y K=0 (0 % de negro). "Véase CMYK."

En este sistema de cromosíntesis, el color complementario del amarillo es el azul purpúreo. En artes plásticas se considera tradicionalmente que el complementario del amarillo es el violeta.

En el sistema aditivo de síntesis de color, en el cual los colores se obtienen mezclando luz de color en lugar de pigmentos, el amarillo es un color secundario. Los colores primarios de este sistema son el rojo, el verde y el azul; para obtener amarillo hay que superponer luz roja y luz verde. Opcionalmente, podemos partir de la luz blanca, que contiene a todos los otros colores, y filtrar el azul, tras lo cual quedará solo la combinación de rojo y verde: amarillo.

Este sistema aditivo de colores luz es el que utilizan los monitores y televisores para producir colores. En este sistema, un color se describe con valores numéricos para cada uno de sus componentes (rojo, verde y azul), indicando al rojo con «R», al verde con «G» y al azul con «B». En una escala de valores de 0 a 255, el amarillo aditivo puro se expresa como R=255 (rojo al valor máximo), G=255 (verde al valor máximo) y B=0 (nada de azul). "Véase RGB."

Este amarillo fue uno de los primeros colores que pudieron reproducir los ordenadores personales al abandonar la monocromía, a principios de los años 1980.

En este sistema de cromosíntesis, el color complementario del amarillo es el azul.

Amarillo espectral es, simplemente, el color amarillo de la región del espectro electromagnético que el ojo humano es capaz de percibir. Las frecuencias más altas que el amarillo se perciben como verde, y las más bajas como naranja. La longitud de onda de la luz amarilla se sitúa entre 570 y 590 nm y el amarillo puro (según diversos autores) es de 578, 580 o 582nm. 

En Occidente, la interpretación tradicional del cromatismo del arcoíris sostiene que este contiene siete colores, que corresponden a los siete colores en que Newton dividió el espectro de luz visible. En este contexto, el amárillo es considerado el tercer color, tanto del espectro newtoniano como del arcoíris.

Es el amarillo aditivo puro o vivo con la máxima brillantez. Algunos pigmentos alcanzan su tonalidad, lo que los convierte en colores primarios sustractivos ideales. Es un color HTML establecido por protocolos informáticos para su uso en páginas web y en programación puede invocárselo con el nombre "yellow" (amarillo). 

Entre los pigmentos que alcanzan su brillantez está el amarillo Hansa claro o amarillo monoazo, el amarillo Ford, amarillo Chrysler, amarillo Humbrol, etc. Crayola y RAL lo denominan amarillo brillante, ColorHexa lo llama amarillo eléctrico. Una comparación entre este amarillo y otros amarillos espectrales:

Es antiguo, su uso se remonta a más de 300 años, es el color primario del modelo tradicional de coloración y un ejemplo de su uso es el pigmento amarillo cadmio. Actualmente es muy común en la impresión CMYK (coordenada 0,0,100,0) y también se le denomina amarillo canario o "process yellow" (Pantone).

Desde la más remota antigüedad se ha buscado producir sustancias coloreadas para pintar o teñir que tuviesen buen color, poder de tinción y permanencia; que fuesen estables ante la luz, que secasen razonablemente rápido, que tuviesen la densidad necesaria y que pudiesen mezclarse sin problemas con otros colores. Debajo se listan algunos pigmentos amarillos que se han destacado en la historia de la pintura.

Los colores web establecidos por protocolos informáticos para su uso en páginas web incluyen el amarillo que se muestra debajo. Como se ve, coincide con el amarillo aditivo puro, y en programación puede invocárselo con el nombre "yellow" (amarillo). 

Para los demás amarillos HTML que se encuentran definidos por nombre, ver la siguiente tabla de colores:

En la naturaleza, el color amarillo cumple un papel importante en la coloración de advertencia, también llamada aposemática, de los animales. Ciertas especies utilizan este color —generalmente en combinación con negro u otros colores contrastantes— para advertir a los depredadores de su toxicidad y/o mal sabor. El amarillo usado en estos casos es intenso, apropiado para distinguirse en el entorno natural, donde son más frecuentes los verdes, azules y marrones.

Se conoce como xantismo a una anormalidad fenotípica en el patrón cromático habitual de una especie, la cual se expresa en la exhibición total o parcial de escamas, piel, pelaje o plumaje de color amarillo. Dichos ejemplares suelen ser denominados también "variedad amarilla", "mutación amarilla" o "lutino", este último término es especialmente característico en avicultura ornamental.


El amarillo no es un color heráldico. Cuando aparece en un blasón, generalmente representa al metal oro. Sin embargo, puede aparecer específicamente como amarillo si la descripción de las armas requiere la representación de alguna figura amarilla «al natural» (por ejemplo, una flor).

En vexilología, el color amarillo deriva del oro heráldico y es relativamente frecuente. La superficie amarilla en algunas banderas es considerable, y otras incluso emplean al amarillo como color de fondo, aunque esto no suele verse en banderas nacionales.

En los ejemplos bajo estas líneas: el rombo amarillo de la bandera del Brasil deriva del oro heráldico de la Casa de Habsburgo, familia de María Leopoldina de Austria, quien fuera emperatriz del Reino del Brasil entre 1822 y 1826; en la bandera de Colombia, el amarillo representa la riqueza del suelo, así como el sol y la soberanía; en la bandera de Ucrania, el amarillo representa los campos de trigo del país.

En la mayoría de los países europeos el amarillo se asocia a los partidos liberales y a los liberales libertarios incluyendo Renovar Europa y a los liberaldemócratas. Los partidos liberales se caracterizan por ser partidos transversales (que no se encuadran en el espectro izquierda-centro-derecha) sino que tienen una ideología política liberal en política económica y, al mismo tiempo, tienen una política ligada a la defensa de los derechos civiles.

También existe la expresión «hacer amarillismo» para los partidos que apoyaban al patrono en las huelgas. El sindicalismo amarillo surgiría en Francia en 1899. Sería fundado el primer sindicato de este tipo por un grupo de obreros contrarios a las huelgas y elegirían el color amarillo en contraposición al rojo de los sindicatos socialistas. Por la razón anterior, en algunos países "amarillo" puede ser un adjetivo calificativo de connotaciones despectivas para referirse a aquellos que sostienen posturas políticas moderadas, indefinidas o vacilantes, o que constituyen una "quinta columna" contra las ideologías defendidas, especialmente las de izquierda. 

"Véase colores políticos: amarillo político."



</doc>
<doc id="41598" url="https://es.wikipedia.org/wiki?curid=41598" title="Naranja (desambiguación)">
Naranja (desambiguación)

Naranja puede referirse a:



</doc>
<doc id="41602" url="https://es.wikipedia.org/wiki?curid=41602" title="PlayStation">
PlayStation

PlayStation (プレイステーション Pureisutēshon, oficialmente abreviada como PS1) es la primera videoconsola de Sony, y la primera de dicha compañía en ser diseñada por Ken Kutaragi, y es una videoconsola de sobremesa de 32 bits lanzada por Sony Computer Entertainment el 3 de diciembre de 1994 en Japón. Se considera la videoconsola más exitosa de la quinta generación tanto en ventas como en popularidad. Además de la original, en el año 2000 se lanzó la PSone (también llamado modelo slim). Tuvo gran éxito al implantar el CD-ROM dentro de su hardware a pesar de que otras compañías como SEGA (Sega CD), Panasonic (3DO), Philips (CD-i), SNK (Neo Geo CD), NEC (Super CD-ROM) y Atari (Atari Jaguar) ya lo habían empleado. Dichas compañías tuvieron poco éxito al utilizar el CD-ROM como soporte para almacenar juegos. Se estima que Sony pudo vender 105 500 000 unidades de su videoconsola en diez años. La consola fue retirada oficialmente del mercado el 23 de marzo de 2006.

La PlayStation fue lanzada a la venta el 3 de diciembre de 1994 en Japón, el 9 de septiembre de 1995 en Estados Unidos y el 29 de septiembre de 1995 en Europa. Todo empezó con un contrato roto con Nintendo a finales de la década de 1980.
Nintendo acordó con Sony, a finales de la década de 1980, desarrollar para su exitosa Super Nintendo un apéndice para incorporar juegos en disco, además del tradicional cartucho. La firma de videojuegos, sin embargo, rompió con la tecnológica japonesa, neófita entonces en la industria, porque consideró que cedió muchísimo en el control y beneficios derivados de la venta de juegos en CD.

Ken Kutaragi, que en aquella época fue un ingeniero informático de Sony apasionado por los videojuegos, propuso una consola que combinó las capacidades gráficas de una estación de trabajo y la unidad de CD-ROM de Sony. Durante dos años, Ken Kutaragi buscó sin éxito alguien en algún lugar del grupo audiovisual de Sony que respaldase su proyecto. Se trasladó, junto con su investigación, de un laboratorio a otro, hasta que Teruo Tokunaka le llevó a ver al entonces presidente Norio Ohga para exponer su idea.

La cúpula de Sony, reacia desde un principio a introducirse en el mercado de los videojuegos, intentó terminar la aventura aquí. Sin embargo, la obstinación de Kutaragi hizo que la compañía siguiese adelante. La empresa derivó el proyecto con Kutaragi a cargo a Sony Music para no responsabilizarse de las impredecibles consecuencias de la apuesta. La colaboración, a la postre, fue fundamental para la producción de discos. Hasta 1993, la compañía no tendría una sección propia de videojuegos, Sony Computer Entertaiment.

Sony lanzó la PlayStation en Japón el 3 de diciembre de 1994. El éxito fue inmediato. La clave estuvo en las facilidades ofrecidas por la compañía a los desarrolladores de videojuegos, entusiasmados con las grandes posibilidades técnicas, las tres dimensiones y el disco. Los desarrolladores tomaron varios riesgos económicos creando cartuchos para Sega o Nintendo; por el contrario, Sony ofreció todas las facilidades a fin de tener un catálogo variado de juegos. Enseguida se sumaron los grandes del sector. Títulos como Gran Turismo, Metal Gear o Final Fantasy son historia fundamental de los videojuegos.

El salto a Europa y los Estados Unidos también fue exitoso. Sony optó rebajar el precio de su consola por debajo del costo. El lanzamiento en los Estados Unidos fue a un precio de 299 dólares, muy por debajo de los 399 dólares de su principal competidora, la Sega Saturn, barrida completamente. Al contrario de la tendencia de la industria, Sony intentó obtener beneficios derivados del software y no solo del hardware. Y lo hizo: las ganancias de Sony Computer Entertaiment supusieron el 90% de la compañía.

La primera versión de la PlayStation superó los cien millones de unidades vendidas nueve años después de lanzarla.








El catálogo de los juegos populares de la consola son: 
El último videojuego publicado para PlayStation fue "FIFA Football 2005", el 12 de octubre de 2004 en los Estados Unidos, "" en el 2005 para Europa, y "" en el 2007 (aunque es un juego de 1999) para Japón.










</doc>
<doc id="41603" url="https://es.wikipedia.org/wiki?curid=41603" title="Constantin Brâncuși">
Constantin Brâncuși

Constantin Brâncuși ("Constantin Brancusi" en la grafía francesa), (Hobița, distrito de Gorj, Rumania, 19 de febrero de 1876 – París, 16 de marzo de 1957) fue un importante escultor, pintor y fotógrafo rumano, considerado pionero del arte moderno. Sus obras se encuentran en museos de Francia, Estados Unidos, Rumanía y Australia.

Brancuși nació en Pestisani Gorj, dentro de una numerosa familia campesina. Trabajó como pastor desde los siete años de edad.

Sin estudios básicos, aprendió a leer por sí mismo. Entre los nueve y once años combinaba su trabajo como ayudante de una tienda con su afición con las tallas de madera, parte importante de la cultura popular de su país.

Su trabajo llamó la atención de un filántropo, que decidió pagar sus estudios en la Escuela de Arte y Oficios de Craiova (ciudad del distrito Dolj, Rumania) desde 1894 a 1898. Posteriormente continuó sus estudios en la Escuela Nacional de Bellas Artes de Bucarest desde 1898 hasta 1901. En 1903 realizó su primer encargo: un busto del general rumano Carol Davila.

Llegó a París en 1904 para perfeccionar sus estudios, mientras lavaba platos en un restaurante o cantaba en ceremonias de la Iglesia ortodoxa rumana. Como estudiante de arte coincidió y se vio influido por Auguste Rodin y el Impresionismo. Se muestra como uno de los primeros artistas del arte moderno. En 1909-1910 trabajó con Amedeo Modigliani.

En 1912 el Salón Oficial de Bucarest le otorgó el primer premio de escultura y participó en el Salón de los independientes en París. En 1913, presentó en Nueva York, Chicago y Boston "Mademoiselle Pogany".

Fue ya en 1924 cuando visitó Rumania, siendo reconocido mundialmente. En 1926 y 1928 viajó nuevamente a Estados Unidos.

En 1952, obtuvo la nacionalidad francesa y donó al Museo de Arte Moderno de París su taller, con casi cien esculturas.

En 1955 presentó una retrospectiva de su obra en el Museo Guggenheim de Nueva York.

Tenía una amplia gama de intereses, desde la ciencia hasta la música. De hecho, era un buen violinista y solía tocar canciones populares rumanas. Entabló amistad y contactos con una parte del círculo intelectual y social de París como Erik Satie o Marcel Duchamp, Pablo Picasso o Guillaume Apollinaire, además de otros rumanos como Emil Cioran.

Brancuși murió en París el 16 de marzo de 1957. Se encuentra enterrado en el cementerio de Montparnasse.

Aunque de origen rumano, se desarrolló y se dio a conocer en París. Está considerado como uno de los grandes escultores del siglo XX. Su obra ha influido en nuevos conceptos de la forma en escultura, pintura y diseño industrial.

Su obra, 1200 fotografías y 215 esculturas, evolucionó desde 1908 hacia un estilo muy personal, geométrico, con una eliminación de los detalles que le condujo casi a la abstracción, proponiendo una realidad distinta. De esta manera, dejaba de lado el realismo escultórico del siglo XIX para dar paso al arte abstracto que se abría camino.

Inspirándose en el arte escultórico prehistórico y africano, intentó mostrar la naturaleza subyacente al desnudo mediante una simplificación extrema de la forma. Trabajó el mármol, piedra caliza, bronce y la madera. Predomina en sus obras dos formas simples: el huevo y el cilindro alargado.

Realizó una serie de esculturas en metal llamadas "Pájaro en el espacio". Entre otras obras de Brancusi se encuentran: "Madre durmiente" (1906-10), "El beso" (1908), "La sabiduría" (1909), "Prometeo" (1911), "El comienzo del mundo" (1924), "El pájaro" (1924-49), la "Columna del infinito" (1933), "El espíritu de Buda" (1933) y "La gallina" (1941).

En Targu Jiu hay un conjunto escultural Constantin Brâncuși que se considera la cima de su carrera. En 1938 presenta la "Columna del infinito", monumento a los jóvenes fallecidos durante la Primera Guerra Mundial. Conmemora el valor y el sacrificio del pueblo rumano que en 1916 se defendió de la invasión alemana. En el 2004 se terminaron unos trabajos de restauración de la obra primigenia que se encontraba deteriorada.

Estas obras se encuentran dispuestas en línea recta y son:

Este conjunto escultórico simboliza la vida del ser humano.

La "Mesa del silencio" y la "Puerta del beso" se encuentran en el parque principal de la ciudad; entre las mismas se encuentran sillas también hechas por Brâncuși y siguiendo la imaginaria línea determinada por estas dos, se llega a una de las iglesias ortodoxas de la ciudad. Continuando aún más por esta línea, se llega a la "Columna del infinito" (considerada su obra cumbre) ubicada también en el parque.

Algunas de sus obras alcanzaron cifras récord por una escultura. Subastadas en Christie's en Nueva York, "Danaide" (de 1913) se vendió por unos 18 millones de dólares en mayo de 2002, tres años después, superó esta cifra su obra "Pájaro en el espacio" (de 1922), por algo más de 27 millones de dólares.

En mayo de 2017, una escultura de bronce de Brâncuși, "La muse endormie" (1913), que pertenecía a una colección privada de París, fue subastada por 51,8 millones de euros por la casa Christie's en Nueva York.

Asimismo, su aportación al mundo del arte ha servido de inspiración para artistas de la talla de Isamu Noguchi o Andreu Alfaro.

 


</doc>
<doc id="41604" url="https://es.wikipedia.org/wiki?curid=41604" title="Campeones de AAPM">
Campeones de AAPM

La siguiente es una lista de Campeones Metropolitanos Nocturnos desde que se fundó la Asociación Argentina de Pilotos Midgets.



</doc>
<doc id="41608" url="https://es.wikipedia.org/wiki?curid=41608" title="Conservatorio Nacional de Música">
Conservatorio Nacional de Música

El término Conservatorio Nacional de Música se refiere, en esta enciclopedia:


</doc>
<doc id="41612" url="https://es.wikipedia.org/wiki?curid=41612" title="Departamento Islas del Atlántico Sur">
Departamento Islas del Atlántico Sur

El departamento Islas del Atlántico Sur es una de las cinco subdivisiones territoriales de la provincia de Tierra del Fuego, Antártida e Islas del Atlántico Sur en la República Argentina. Está conformado por las islas Malvinas y los archipiélagos agrupados con el nombre de Antillas del Sur subantárticas. Todo el territorio del departamento —y sus mares adyacentes— es objeto de disputa entre Argentina y el Reino Unido, por lo que la administración argentina no puede ser ejercida en él dado que lo hace el Reino Unido.

El departamento está conformado por cuatro archipiélagos reivindicados por la Argentina como "«parte integral e indivisible de su territorio que se halla ocupada ilegalmente por una potencia invasora»". Los archipiélagos fueron reivindicados como territorio argentino mediante la ley nacional 26.552, sancionada por el Congreso de la Nación Argentina en noviembre de 2009 y promulgada por Cristina Fernández de Kirchner en diciembre del mismo año, que detalla los límites de la provincia de Tierra del Fuego, Antártida e Islas del Atlántico Sur, al agregar un párrafo adicional en el artículo 1 de la ley 23.775.

Los tres archipiélagos están bajo administración del Reino Unido, que agrupa las islas componentes del departamento en tres territorios de ultramar:

Las islas Malvinas es uno de los 17 territorios en la lista de las Naciones Unidas de territorios no autónomos bajo supervisión del Comité de Descolonización de las Naciones Unidas, con el fin de examinar la situación con respecto a la aplicación de la Declaración sobre la concesión de la independencia a los países y pueblos coloniales, por lo que la situación del archipiélago es examinada anualmente por el Comité de Descolonización desde 1965. La Organización de las Naciones Unidas lo considera un territorio de soberanía aún .
Los espacios marítimos y aéreos adyacentes a las islas junto con el lecho, subsuelo marino y los derechos pesqueros correspondientes, son también objeto de disputa entre Argentina y el Reino Unido.

El continuo reclamo argentino sobre estos archipiélagos del Atlántico Sur ha quedado plasmado en la disposición transitoria primera de la Constitución de 1994, que dice:

El 11 de marzo de 2016 la Comisión de Límites de la Plataforma Continental (CLPC) de las Naciones Unidas aprobó la presentación de la plataforma continental de Argentina sin realizar recomendaciones, quedando desde ese momento reconocidos por la comunidad internacional los límites del territorio argentino en lo que respecta a su plataforma marítima en las áreas no sujetas a disputas. El 28 de marzo de 2016 el Ministeiro de Relaciones Exteriores y Culto, realizó una presentación sobre los nuevos límites, durante la cual se expresó que al aprobarse la presentación la ONU "reconoció que existe una disputa de soberanía" con el Reino Unido.

En casos como estos, en que haya una controversia territorial o marítima, está establecido que la Comisión "no examinará ni calificará la presentación hecha por cualquiera de los Estados Partes en esa controversia". Por ello, y ya en septiembre de 2009, la Comisión había determinado que de acuerdo con el reglamento, no se hallaba en condiciones de examinar ni de calificar la parte de la presentación que se refería a los espacios marítimos de las islas Malvinas, Georgias del Sur y Sandwich del Sur, ni a la plataforma continental perteneciente a la Antártida. Según una asociación privada la soberanía argentina no disputada reconocida por la Comisión (CLPC), llegaría al 20% de la superficie total delimitada de 1,7 millones de km², es decir aproximadamente 340.000 kilómetros cuadrados.

Las Antillas del Sur subantárticas están ubicadas al norte del paralelo 60° S, comprendiendo a las islas Georgias del Sur, rocas Clerke, Sandwich del Sur y las islas Aurora (Rocas Negras e islotes Cormorán).
La superficie total del departamento varía según las fuentes consultadas:

De acuerdo al Instituto Geográfico Nacional de Argentina es de 16 027 km², discriminado así:


Los mismos datos figuran en el "Atlas de Estadísticas" del gobierno de la provincia de Tierra del Fuego, Antártida e Islas del Atlántico Sur. 

El "CIA The World Factbook 2013" atribuye otras superficies a los archipiélagos de las:


La población permanente asentada en las islas Malvinas es de alrededor de 3400 habitantes en 2016 (excluyendo a personal y militares de la Base Aérea de Monte Agradable y sus familiares). Estos datos provienen del censo realizado por el gobierno británico, desconocido por la Argentina. Además, según estimaciones de especialistas en áreas de diplomacia y política internacional, la dotación de soldados británicos desplegados en dichas islas supera en cantidad a la población civil.

En las islas Malvinas se halla la única localidad del departamento, denominada Puerto Argentino en Argentina, y Stanley por el Reino Unido. Al igual que en los otros tres departamentos de esta provincia, no se le ha asignado formalmente una localidad cabecera, pero habitualmente se considera que corresponde a Puerto Argentino.

Debido a la ocupación británica de tres de los archipiélagos, el Instituto Nacional de Estadística y Censos de la República Argentina no puede censar su población.

Tras una centenaria ocupación de España, las islas Malvinas pasaron a la efectiva posesión argentina desde el 6 de noviembre de 1820, cuando se realizó la formal toma de posesión, hasta el 3 de enero de 1833, cuando fueron ocupadas por fuerzas británicas.

El departamento fue creado el 8 de abril de 1970 cuando el gobernador de Tierra del Fuego dictó el decreto N° 149:

El decreto no designó ninguna población cabecera para el departamento. Años más tarde, la carta orgánica de la ciudad de Ushuaia, sancionada en 2002, reafirmó en el artículo 22 su condición de capital del territorio, incluyendo todos los archipiélagos.

La Estación Científica Corbeta Uruguay fue construida por la Argentina en la isla Morrell del grupo de las islas Tule del Sur, perteneciente a las Sandwich del Sur, el 7 de noviembre de 1976. Fue mantenida hasta la ocupación británica de la base el 20 de junio de 1982.

El 2 de abril de 1982 el gobierno militar argentino retomó la posesión de las islas Malvinas, lo que dio lugar a la guerra de las Malvinas. El 14 de junio de 1982 el Reino Unido recuperó las Malvinas al rendirse las fuerzas argentinas en Puerto Argentino/Stanley, al día siguiente se produjo la entrega de la guarnición argentina de la isla Gran Malvina.

Los puertos de Grytviken y de Leith en la isla San Pedro de las Georgias del Sur fueron ocupados por fuerzas argentinas el 3 de abril de 1982 y mantenidos hasta la recuperación británica el 25 de abril de 1982, en el curso de la Operación Paraquat.

Durante la guerra de las Malvinas el gobierno militar argentino creó mediante el decreto secreto n.º S 681/82, del 3 de abril de 1982 la gobernación Militar de las Islas Malvinas, Georgias del Sur y Sandwich del Sur reduciendo el territorio del departamento al archipiélago de las Orcadas del Sur. Este decreto fue derogado por el decreto n.º 879/85 del 15 de mayo de 1985, que disolvió la gobernación militar y restauró los límites del departamento Islas del Atlántico Sur.

Con la provincialización de Tierra del Fuego, el Congreso Argentino sancionó la Ley 23.775 el día 26 de abril de 1990. Al promulgarla, el poder ejecutivo a cargo de Carlos Saúl Menem vetó el artículo primero, que precisamente fijaba los límites de la nueva provincia, estableciendo límites generales mediante el decreto N° 905/1990. La ley además expresa en su artículo 2º que «en lo que se refiere a la Antártida, Malvinas, Georgias del Sur, Sandwich del Sur y demás islas subantárticas, la nueva provincia queda sujeta a los tratados con potencias extranjeras que celebre el gobierno federal, para cuya ratificación no será necesario consultar al gobierno provincial».

Desde 2007, la Cancillería Argentina impulsó reuniones con diputados y senadores fueguinos hasta elaborar el proyecto de ley presentado en julio de 2009 por los diputados Nélida Belaous, Leonardo Gorbacz, Rosana Bertone, Rubén Sciutto y Mariel Calchaquí, y que se sancionó en noviembre de ese mismo año (con el número 26.552), 19 años más tarde de la provincialización, cuando el Poder Ejecutivo decidió someter la cuestión de límites «a la consideración del Congreso de la Nación, a la mayor brevedad, un texto sustitutivo».

En 2015 el gobierno de Tierra del Fuego envió a la Legislatura provincial una propuesta para modificar los límites departamentales de la provincia con el fin de «contribuir a la soberanía territorial de la Nación». La iniciativa fue impulsada por el Consejo Asesor Observatorio Cuestión Malvinas y establece la división política de la provincia en cuatro departamentos. En esta propuesta las islas Orcadas del Sur pasarían a formar parte del departamento Antártida Argentina.
En agosto de 2017, un grupo de periodistas egresados de la Universidad Nacional de La Plata presentó ante el Congreso Argentino un proyecto de ley que busca crear una nueva provincia argentina denominada Malvinas, cuya capital estaría transitoriamente en Puerto Parry, en la isla de los Estados. Además del departamento de Islas del Atlántico Sur, la hipotética provincia tendría la ya mencionada isla de los Estados, además de las Orcadas del Sur y adyacentes.

El 27 de octubre de 2017​ por ley de la Legislatura de la provincia de Tierra del Fuego fue derogado el decreto territorial 149/70 que creó el departamento Islas del Atlántico Sur y le asignó un nuevo límite pasando las islas Orcadas del Sur al departamento Antártida Argentina.



</doc>
<doc id="41613" url="https://es.wikipedia.org/wiki?curid=41613" title="Diabetes mellitus tipo 1">
Diabetes mellitus tipo 1

La diabetes mellitus tipo 1 (DM1) es una enfermedad autoinmune y metabólica caracterizada por una destrucción selectiva de las células beta del páncreas causando una deficiencia gradual y absoluta de insulina. Se diferencia de la diabetes mellitus tipo 2 porque es un tipo de diabetes caracterizada por darse en época temprana de la vida, generalmente antes de los 30 años. Sólo 1 de cada 20 personas diabéticas tiene diabetes tipo 1, afectando preferentemente a jóvenes y niños. La administración de insulina en estos pacientes es obligatoria, ya que el páncreas no produce insulina. La diabetes tipo 1 se clasifica en casos autoinmunes —la forma más común— y en casos idiopáticos. La diabetes tipo 1 se encuentra entre todos los grupos étnicos, pero su mayor incidencia se produce entre poblaciones del norte de Europa y en Cerdeña. La susceptibilidad a contraer diabetes mellitus tipo 1 parece estar asociada a múltiples factores genéticos, aunque solo el 15-20% de los pacientes tienen una historia familiar positiva.

La diabetes mellitus tipo 1 y la enfermedad celíaca comparten base genética y mecanismos inmunológicos, y pueden aparecer asociadas.

La prevalencia de la diabetes tipo 1 en el mundo no se conoce con exactitud, si bien se sabe que su incidencia está aumentando rápidamente tanto en niños como en adolescentes, con un incremento anual calculado aproximadamente en un 3%. Este tipo de diabetes representa el 5-10% de los casos totales de diabetes mellitus, cifra que se eleva al 80-90% en la franja de edad comprendida entre la infancia y la adolescencia. 

Afecta por igual a ambos sexos, sin diferencias de género en las tasas de prevalencia. 

Si bien los adultos también pueden desarrollar diabetes tipo 1, la tasa de incidencia más alta se da en los adolescentes.

Los gemelos monocigóticos tienen una concordancia del 60%, aunque sólo el 30% desarrolla la enfermedad dentro de los 10 años siguientes al diagnóstico del primer gemelo. En contraste, los gemelos dicigóticos tienen un riesgo de concordancia similar al encontrado entre otros hermanos, calculado aproximadamente en un 8%.

La frecuencia de desarrollo de la diabetes tipo 1 en los niños con una madre que tiene diabetes es del 2-3%; del 5-6% cuando el diabético es el padre; y aumenta al 30% si ambos progenitores padecen la enfermedad.

La mayoría de los casos de diabetes mellitus tipo 1, aproximadamente un 95%, son el resultado de una compleja interacción entre factores ambientales y genéticos, que provocan el desarrollo de un proceso autoinmune, dirigido contra las células productoras de insulina de los islotes pancreáticos de Langerhans. Como resultado, estas células son progresiva e irreversiblemente destruidas. El paciente desarrolla la deficiencia de insulina cuando aproximadamente el 90% de las células de los islotes han sido destruidas.

La influencia de los factores ambientales no se limita a iniciar el daño a las células productoras de insulina, sino que probablemente son responsables de mantener activada, acelerar o retardar su destrucción.

Existe un componente genético en la diabetes mellitus tipo 1, si bien no es suficiente por sí solo para iniciar el desarrollo de la enfermedad y se requiere de la interacción con ciertos factores ambientales. 

La diabetes mellitus tipo 1 está fuertemente asociada a moléculas DR3 y DR4 del complejo mayor de histocompatibilidad (CMH) dentro del grupo HLA clase II. El 55% de los pacientes con diabetes tipo 1 expresa el HLA-DR3/DQ2. Otro alelo que se considera un importante factor de riesgo para desarrollar la enfermedad es el HLA-DQ8. El CMH requiere de una composición específica de aminoácidos para su correcto funcionamiento. La alteración en el grupo HLA-II parece influir en la configuración propia del CMH. El resultado de esta alteración es una pérdida de la especificidad en la presentación de antígenos, lo que podría generar una respuesta inmunológica cruzada. Es esta respuesta cruzada lo que explica la respuesta autoinmune de la diabetes mellitus tipo 1.

Los pacientes que expresan DR3 también presentan un riesgo elevado de desarrollar otras endocrinopatías autoinmunes y enfermedad celíaca (EC). Desde hace años se conoce la asociación de la diabetes mellitus tipo 1 y la enfermedad celíaca. Ambas enfermedades comparten mecanismos inmunológicos comunes y genes de susceptibilidad, HLA-II predominantemente, e incluso alguno del tipo I. Una revisión sistemática reciente encontró que aproximadamente un 6% de los pacientes con diabetes tipo 1 padece enfermedad celíaca asociada, si bien la mayoría de los casos de enfermedad celíaca permanecen sin reconocer ni diagnosticar, debido a que suele cursar sin síntomas digestivos o estos son leves. Además de la genética común, esta asociación puede explicarse por la inflamación o las deficiencias nutricionales causadas por la enfermedad celíaca no tratada, que aumentan el riesgo de desarrollar diabetes tipo 1.

Las infecciones virales se han propuesto como un factor ambiental implicado en el desarrollo de la diabetes mellitus tipo 1, probablemente por iniciar o modificar un proceso autoinmune, en el que la respuesta inmune desencadenada contra las células infectadas por el virus se dirigiría además contra las células beta del páncreas. Se han implicado algunas infecciones víricas, tales como enterovirus, citomegalovirus, paperas, rubeola y rotavirus, pero las evidencias no son concluyentes. Una revisión sistemática con meta-análisis publicada en 2011 mostró una asociación entre las infecciones por enterovirus y la diabetes tipo 1, pero otros estudios han demostrado resultados opuestos, sugiriendo que, en lugar de desencadenar un proceso autoinmune, las infecciones por enterovirus (tales como el virus coxsackie B) podrían proteger contra la aparición y el desarrollo de la diabetes tipo 1.

El aumento de nuevos casos de diabetes mellitus tipo 1 en niños menores de 5 años desde comienzos del siglo XXI ha sido explicado por un factor ambiental: la “hipótesis de la higiene”. Según esta teoría, la falta de exposición a patógenos en el niño favorece el desencadenamiento de la autoinmunidad. Un reciente estudio (2013) concluye que los parámetros relacionados con la higiene, tradicionalmente considerados como parte del estilo de vida moderno, no juegan ningún papel importante en la etiología de la diabetes tipo 1.

Los datos actuales muestran que la gliadina (fracción proteica del gluten) parece estar implicada en la aparición y desarrollo de la diabetes mellitus tipo 1. El aumento de la permeabilidad intestinal causado por el gluten provoca la pérdida protectora de la barrera intestinal. Esto permite el paso a la sangre de sustancias proinflamatorias y puede provocar una respuesta autoinmunitaria en individuos genéticamente predispuestos a la diabetes tipo 1. La introducción temprana de cereales que contienen gluten en la dieta aumenta el riesgo de desarrollar los autoanticuerpos que destruyen las células productoras de insulina del páncreas. La eliminación del gluten de la dieta puede proteger contra el desarrollo de la diabetes.

Se ha sugerido que algunas proteínas presentes en la leche de vaca podrían aumentar el riesgo de desarrollar un proceso autoimune dirigido contra las células de los islotes pancreáticos. No obstante, los estudios observacionales encaminados a investigar la posible relación entre la introducción temprana de la leche de vaca en la dieta y el desarrollo de la diabetes tipo 1 muestran resultados contradictorios.

Una reciente revisión sistemática muestra que la lactancia materna ejerce un pequeño efecto protector sobre el riesgo de desarrollar diabetes tipo 1.

La epidemia de diabetes mellitus tipo 2 y obesidad van unidas, pero se desconoce su relación con la diabetes tipo 1. Se ha intentado explicar mediante la “hipótesis del acelerador” postulada por Wilkin, que unifica la diabetes mellitus tipo 1 y 2 en una sola enfermedad, con la única diferenciación de la velocidad en la destrucción de las células beta.

El pirinurón, un veneno para ratas comercializado bajo los nombre de Vacor o RH-787, introducido en los Estados Unidos en 1976, destruye selectivamente las células beta del páncreas. La intoxicación accidental con este producto puede provocar diabetes tipo 1. El pirinurón fue retirado del mercado de Estados Unidos en 1979 y su uso en este país no está aprobado por la Agencia de Protección Ambiental.

La estreptozocina, un agente antineoplásico, es selectivamente tóxico para las células beta de los islotes pancreáticos. Se utiliza en investigación para provocar diabetes tipo 1 en roedores y para el tratamiento de cáncer metastásico de las células de los islotes pancreáticos en los pacientes cuyo cáncer no puede ser eliminado por medio de cirugía.

Otros factores en el desarrollo de la diabetes mellitus tipo 1 son los siguientes:

La diabetes es una enfermedad autoinmune crónica para la que aún no existe ninguna cura. En este tipo de diabetes quedan afectadas las células β del páncreas, que producen poca o ninguna insulina; hormona que permite que el azúcar (glucosa) ingrese en las células del cuerpo.

Como consecuencia, se produce una acumulación de glucosa en sangre que presenta efectos citotóxicos tales como la glucosilación no enzimática; la glucosa se une a moléculas como la hemoglobina o los lipopolisacáridos de las paredes de los vasos sanguíneos y a las lipoproteínas de la sangre, causando su acumulación y la aparición de ateromas. Además, al no poder usarse la glucosa como combustible metabólico, se favorece la digestión de lípidos y proteínas que aportan menor cantidad de energía apareciendo síntomas de polifagia (sensación de hambre) y de adelgazamiento. El uso de las grasas como fuente energética provoca la liberación de ácidos grasos, que son oxidados a AcetilCoA. Altos niveles de AcetilCoA saturan el ciclo de Krebs, obligando a que el AcetilCoA siga la ruta de síntesis de cuerpos cetónicos. El exceso de cuerpos cetónicos provoca cetoacidosis, que origina graves problemas pudiendo conducir al coma o, incluso, a la muerte. Por último, el exceso de glucosa es eliminado en la orina junto a gran cantidad de agua aumentando la diuresis y la sensación de sed (poliuria y polidipsia).

El proceso de desarrollo de la diabetes tipo 1 es gradual, pudiendo ser necesarios varios años antes de que se manifieste clínicamente. La enfermedad se desarrolla por el ataque del sistema inmune contra las propias células beta del páncreas, encargadas de producir la insulina. Esto puede deberse:

Los posibles síntomas abarcan:

La enfermedad celíaca en las personas con diabetes tipo 1 es difícil de identificar, pues la mayoría de los pacientes no presenta ningún síntoma digestivo o estos se atribuyen erróneamente al mal control de la diabetes, a una gastroparesia o a una neuropatía diabética, por lo que en la mayoría de los casos la enfermedad celíaca se diagnostica después de la aparición de la diabetes tipo 1. La asociación de la enfermedad celíaca con la diabetes tipo 1 aumenta el riesgo de complicaciones, como los daños en la retina y la mortalidad. Esta asociación entre ambas enfermedades se puede explicar debido a que ambas comparten genética y a la inflamación o las deficiencias nutricionales causadas por la enfermedad celíaca no tratada, incluso si la diabetes tipo 1 se diagnostica primero.

Los siguientes exámenes se pueden utilizar para diagnosticar la diabetes:

Los objetivos inmediatos del tratamiento son tratar la cetoacidosis diabética y los altos o bajos niveles de glucemia (hiperglicemia e hipoglicemia según refiere). La aparición de los síntomas graves de la diabetes es súbita y de gravedad, por lo que es posible que las personas necesiten permanecer en el hospital. Las personas que la padecen deben recibir inyecciones diarias de insulina.
Es muy importante el tratamiento médico de las personas con diabetes, ya sea del tipo 1 o 2, para evitar problemas y mejorar la vida de las personas con esta enfermedad.

Los objetivos a largo plazo del tratamiento son:

Estos objetivos se logran a través de:

La insulina baja el nivel de glucemia permitiendo que salga del torrente sanguíneo y entre en las células del organismo. Todas las personas necesitan insulina. Las personas con diabetes tipo 1 no pueden fabricar su propia insulina y, por tanto, deben administrarse diariamente.

La insulina se inyecta generalmente debajo de la piel, en el tejido subcutáneo. En algunos casos, un microinfusor electrónico (bomba de insulina) libera la insulina en forma continua.

Las preparaciones de insulina se diferencian por la rapidez con que empiezan a hacer efecto y su duración. El médico revisará los niveles de glucemia para determinar el tipo apropiado de insulina que se debe utilizar. Se puede mezclar más de un tipo de insulina en una misma inyección para así lograr el mejor control de la glucemia (sólo en casos específicos).

Las inyecciones se necesitan por lo general de una a cuatro veces al día. El médico o un educador en diabetes enseña a las personas que requieren insulina cómo inyectarse ellos mismos para obtener un auto control propio. Inicialmente, la inyección en los niños debe ser aplicada por uno de los padres u otro adulto y hacia la edad de 14 años se puede esperar que la mayoría de los niños se aplique sus propias inyecciones.

Las personas con diabetes necesitan saber cómo ajustar la cantidad de insulina que están tomando en las siguientes situaciones:

La planificación de comidas para la diabetes tipo 1 debe ser coherente para así permitir que el alimento y la insulina trabajen juntos para regular los niveles de glicemia. Si las comidas y la insulina no están equilibradas, los niveles de glucemia pueden subir o bajar, produciendo por tanto hiperglicemia e hipoglicemia. La mejor forma de sobrellevarla con una dieta es limitar la ingesta de carbohidratos de rápida absorción.

Pautas básicas incluyen:

Las personas con diabetes tipo 1 y enfermedad celíaca no diagnosticada tienen peor control de los niveles de glicemia y una mayor prevalencia de daños en el riñón y la retina. La dieta sin gluten, cuando se realiza estrictamente, mejora los síntomas de la diabetes y tiene un efecto protector contra el desarrollo de complicaciones a largo plazo. Sin embargo, el manejo dietético de estas dos enfermedades es un reto, por lo que estos pacientes tienen un bajo cumplimiento de la dieta.

El ejercicio regular ayuda a controlar la cantidad de glicemia llegando incluso a disminuir la cantidad requerida, al igual que quemar el exceso de calorías y de grasa para lograr un peso saludable.

Las personas con diabetes tipo 1 deben tomar precauciones especiales antes, durante y después de cualquier ejercicio o actividad física intensa. Es importante:

El control de la glucemia se hace verificando el contenido de glucosa en una pequeña gota de sangre. Dicha prueba debe hacerse regularmente y le informa a la persona con diabetes qué tan bien está funcionando el tratamiento (la dieta, los medicamentos y los ejercicios) para controlar la enfermedad.

Los resultados se pueden usar para ajustar la dieta, la actividad física o los medicamentos con el fin de mantener los niveles de glicemia dentro de un rango apropiado. Los exámenes generalmente se hacen antes de las comidas y a la hora de dormir. Cuando uno está enfermo o con estrés, se pueden necesitar exámenes con más frecuencia debido a que estos aumentan la cantidad de glucosa en la sangre.

Los exámenes brindarán información importante, porque posibilita los cambios necesarios para el mejoramiento en los cuidados y el tratamiento. Las pruebas identificarán el alto o bajo nivel de glicemia antes de que se desarrollen problemas serios.

El aparato para medir los niveles más exactos posibles de glucosa en la sangre es el hemoglucotest. Hay diferentes tipos de dispositivos. Normalmente, uno punciona el dedo con una aguja pequeña llamada lanceta para obtener una gota diminuta de sangre. Se pone la tira reactiva correspondiente en el dispositivo y se coloca la sangre en la tira reactiva. Los resultados deben salir según la máquina que se posea, pudiendo ser en cuestión de segundos o más.

El hecho de mantener registros precisos de los resultados del examen le ayudará al diabético y al médico a planear la mejor manera de controlar su diabetes.

Las personas con diabetes deben hacerse revisar sus niveles de Hemoglobina glicosilada (HbA1c) cada 3 a 6 meses. El HbA1c es una medida del contenido promedio de glucosa en la sangre durante los últimos 2 a 3 meses. Puede ayudar el hecho de determinar qué tan bien está funcionando el tratamiento en mediano plazo.

La diabetes causa daños a los vasos sanguíneos y a los nervios, lo cual puede reducir la capacidad de uno para sentir lesiones o la presión en los pies. Uno puede no notar una lesión en el pie hasta que se presente una infección grave.
Además, la diabetes afecta el sistema inmunitario del organismo, disminuyendo la capacidad para combatir la infección. Las infecciones pequeñas pueden progresar rápidamente hasta provocar la muerte de la piel y otros tejidos, lo que puede hacer necesaria la amputación.

Para prevenir las lesiones en los pies, una persona con diabetes debe adoptar una rutina diaria de revisión y cuidado de los pies que consiste en lo siguiente:

Para prevenir la lesión a los pies, hay que adoptar la rutina de revisar y cuidar los pies diariamente.
tomar medidas drásticas.

El nivel bajo de glucemia, conocido como hipoglucemia, se puede presentar por demasiada insulina, demasiado ejercicio o muy poco alimento. La hipoglucemia se puede desarrollar rápidamente en los diabéticos y los síntomas aparecen particularmente cuando el nivel de azúcar cae por debajo de 60 mg/dl. Si este nivel se coloca por debajo de 40 mg/dl se pone en peligro la vida, pudiendo producirse coma y muerte.

Los síntomas más característicos son:

Si estos síntomas se presentan y se tiene un equipo disponible para medir el nivel de glucemia, hay que hacerse el chequeo. Si el nivel está por debajo de lo deseado, hay que comer algo con azúcar: jugo de frutas, algunas cucharaditas de azúcar, o una gaseosa normal. Si no se tiene el equipo a mano, hay que consumir azúcar de todas maneras, porque no le puede hacer daño a corto plazo. Los síntomas deben desaparecer en cuestión de 15 minutos, de lo contrario, hay que consumir más azúcar y verificar nuevamente el nivel de glucemia.

Hay que vigilar a la persona muy de cerca. Si los síntomas empeoran (confusión, convulsiones o pérdida del conocimiento), hay que aplicar a la persona una inyección de glucagón. Si no se tiene glucagón, hay que llamar al número local de emergencias de inmediato.

Cuando no hay suficiente insulina para movilizar la glucosa a las células, ésta se puede acumular en la sangre. El cuerpo busca entonces otras formas de energía y utiliza la grasa como fuente de combustible. A medida que las grasas se descomponen, unos moléculas llamadas cuerpos cetónicos se acumulan en la sangre y en la orina. Las cetonas, en niveles altos, son tóxicas. Esta afección se conoce como cetoacidosis que si se mantiene en el cuerpo por un tiempo puede producir coma diabético (urgencia médica) e incluso a la muerte.

Los signos de advertencia de que la cetoacidosis está empeorando podrían ser:

La diabetes es una enfermedad metabólica crónica que exige la adquisición de nuevas rutinas representadas, en su mayoría, por procedimientos médicos que son dolorosos o tediosos, pero indispensables para el control y la prevención de complicaciones posteriores. Además, supone cambios en los hábitos de vida, en la alimentación y en la actividad física.
Ser consciente de que se tiene una enfermedad crónica puede percibirse como una pérdida del equilibrio, biológico, psicológico o social. Su diagnóstico supone un gran impacto para el individuo.

Entre otras razones, implica un cambio en el estilo de vida, puesto que la diabetes pasa a formar parte de su identidad. Cualquier pérdida pone en marcha toda una serie de emociones y sentimientos. Expresar esas emociones es la mejor garantía de una apropiada adaptación individual y familiar a la diabetes.

La información adecuada sobre la condición y la edad en que se diagnostica son dos factores relevantes que pueden determinar la forma en que se reacciona ante su descubrimiento. La percepción del mundo es diferente en cada etapa del desarrollo de la persona, de allí la importancia del enfoque multidisciplinario, que ayude al paciente a responsabilizarse gradualmente de su tratamiento.

Además, el diabético funciona en varios ambientes sociales, de los cuales el más influyente es la familia. La aparición de la diabetes en uno de los miembros tiene un impacto en las áreas de comunicación, integración marital y otros patrones de funcionamiento. Cada miembro difiere en la forma de enfrentarla, algunos son capaces de solucionar problemas y otros pueden usar al individuo o a su enfermedad como pretexto para explicar problemas no relacionados con la condición.

Muchas veces quienes rodean al diabético se empeñan en asegurar que el individuo no es "diferente", sin embargo, sí lo es, y en variados aspectos. Reconocer esas diferencias, sin enfatizarlas, puede conducir a un mejor ajuste emocional y social. Para ello es necesario el apoyo de familiares y personas que rodean al paciente diabético.

El control de un paciente con diabetes de tipo 1 es personalizado por un médico. Depende de la edad de enfermo, de los años de evolución, del grado de control glucémico y la presencia de complicaciones o no. Además, hay situaciones especiales como el embarazo, la obesidad, el ejercicio físico, o las necesidades individuales, que obligan a controles y medidas específicas. Este control individualizado suele incluir las siguientes pruebas:


Son las que precisan atención médica al momento


El pronóstico para las personas con diabetes varía. Los estudios muestran que un estricto control de la glucemia puede prevenir o retrasar las complicaciones en los ojos, los riñones y el sistema nervioso. Sin embargo, pueden surgir complicaciones incluso en aquellas personas con un buen control de la enfermedad.




</doc>
<doc id="41616" url="https://es.wikipedia.org/wiki?curid=41616" title="Corporal">
Corporal

Corporal puede hacer referencia a los siguientes artículos de esta enciclopedia:


</doc>
<doc id="41620" url="https://es.wikipedia.org/wiki?curid=41620" title="Hueso occipital">
Hueso occipital

El hueso occipital es un hueso impar, central, plano y simétrico del cráneo que constituye la parte posterior, inferior y media del cráneo de mamíferos. Colabora en la formación tanto de la base como de la bóveda craneal. Es cóncavo hacia arriba y adelante y convexo hacia abajo y atrás. Tiene una forma irregularmente romboidal. En él se pueden distinguir una cara pósteroinferior (exocraneal) y una ánterosuperior (endocraneal). En anatomía comparada de vertebrados, el hueso occipital toma también el nombre de complejo occipital, al ser producto de la fusión de cuatro huesos distintivos presentes en otros vertebrados: el supraoccipital, impar y dorsal (porción tabular), los exoccipitales, pares, laterales al foramen magno (porciones condilares), y el basioccipital, impar y ventral (porción basilar).

Embriológicamente está formado por cuatro partes: una porción tabular posterior, dos porciones condilares a los costados del foramen magno y una porción basilar anterior. En el ser humano estas cuatro porciones se encuentran separadas en el nacimiento y se unen alrededor de los seis años de edad.

En su extremidad inferior se encuentra el agujero occipital, el cual en estado fresco da paso al bulbo y sus cubiertas, a los dos nervios espinales, a las raíces ascendentes del hipogloso y a las dos arterias vertebrales. 

Por delante del agujero occipital se encuentra la superficie basilar del occipital, la cual presenta en la línea media y el punto de unión de los dos quintos posteriores con los tres quintos anteriores una eminencia denominada tubérculo faríngeo donde se inserta la aponeurosis de la faringe. La porción de la superficie basilar ubicada por delante de este tubérculo se corresponde con la bóveda de la faringe, en la cual se suele observar la fosita navicular. 

Por detrás del agujero occipital se extiende la porción escamosa o concha del occipital. En su centro se encuentra la protuberancia occipital externa. Por encima de este accidente, el hueso es liso y se corresponde con los tegumentos. Por debajo de la protuberancia se encuentra la cresta occipital externa que llega hasta el agujero occipital. De cada lado de esta cresta surgen dos líneas curvas de concavidad anterior que parten de la línea media a la cara anterior del hueso: línea curva occipital superior y línea curva occipital inferior. 

A cada lado del Foramen magnum se encuentran los cóndilos del occipital que articulan con las cavidades glenoideas del atlas.

La cara externa del cóndilo está separada del borde del hueso por una superficie rugosa denominada superficie yugular. Por delante y detrás del cóndilo se encuentran las fositas condiloidea anterior (con el agujero condileo anterior) y posterior respectivamente.


Esta cara está en relación con la masa encefálica. 

Por delante del agujero occipital se encuentra el canal basilar, que aloja la protuberancia anular y una parte del bulbo raquídeo. 

Por detrás del agujero occipital nos encontramos con cuatro fosas occipitales, siendo las dos inferiores fosas cerebelosas (por su relación con el cerebelo) y las dos superiores fosas cerebrales. Estas fosas se encuentran separadas claramente por canales y crestas que confluyen en la parte media en la protuberancia occipital interna. Las dos fosas cerebelosas están separadas entre sí por la cresta occipital interna, que parte desde la protuberancia occipital interna al agujero occipital.Las dos fosas cerebrales están separadas entre sí por la terminación del canal longitudinal superior. Las fosas cerebrales están separadas de las cerebelosas por la presencia de los canales para el seno lateral.




</doc>
<doc id="41622" url="https://es.wikipedia.org/wiki?curid=41622" title="Grupo de Visegrado">
Grupo de Visegrado

El Grupo de Visegrado en castellano , conocido también como V4 o Visegrád Group en inglés (; ; ; ), es una alianza de cuatro países centroeuropeos: Eslovaquia, Hungría, Polonia y la República Checa.

El "Grupo Visegrado" tiene un antecedente histórico en el pacto de Visegrado en 1335, cuando el rey Carlos Roberto de Hungría convocó a una reunión en el palacio de Visegrado (en húngaro: "Visegrád") al rey Casimiro III de Polonia y al rey checo Juan I de Bohemia. En aquel entonces, los tres reyes acordaron un pacto de no agresión y colaboración mutua para una mejor relación política y económica.

El moderno grupo V4 tuvo su origen en una cumbre de los jefes de Estado y gobierno de Checoslovaquia, Hungría y Polonia el 15 de febrero de 1991: Václav Havel, por parte de Checoslovaquia; Lech Wałęsa, el presidente de la República de Polonia; y József Antall, el primer ministro de la República de Hungría. El encuentro se llevó a cabo 656 años después del organizado por Carlos Roberto de Hungría en la misma ciudad de Visegrado, con el fin de establecer una cooperación entre estos tres Estados (cuatro, con la posterior división de Checoslovaquia en 1993) para acelerar el proceso de integración europea.




</doc>
<doc id="41627" url="https://es.wikipedia.org/wiki?curid=41627" title="Hueso temporal">
Hueso temporal

El hueso temporal (Os temporale), hueso del cráneo, es un hueso par, irregular, neumático, situado en la parte lateral, media e inferior del cráneo. Contiene en su espesor el órgano vestibulococlear o de la audición.

De manera oficial el temporal solo tiene tres porciones: porción escamosa, porción mastoidea y porción petrosa. Para poder dar una mejor descripción se le agregan dos más: porción timpánica y la Apófisis estiloides (porción estiloidea) dando un total de cinco porciones.

Su forma varía según las edades, en el feto y recién nacido se pueden distinguir tres porciones diferentes:


Se osifica a mediados del segundo mes de vida fetal aparecen dos centros de osificación, uno origina la escama y el otro el hueso timpánico, junto a este se desarrollan otros centros de osificación, tres para cada una de estas partes. En el tercer mes de vida fetal se delimitan ocho centros que producen la petrosa. Otro centro origina la apófisis estiloide.

En el recién nacido en el que las tres porciones no se han soldado todavía a dicha porción se denomina Petromastoidea. En el primer año de vida estas tres porciones se unen mediante una sinostosis para formar un hueso único. Estas tres piezas, al soldarse unas con otras en su desarrollo, originan una serie de suturas más o menos visibles y permanentes. Debido a esta fusión sólo la porción escamosa conserva casi los mismos límites primitivos, las otras dos porciones cambian su disposición primordial, quedando de la siguiente manera:


La porción escamosa del temporal, plana, gruesa, irregularmente circular, se asemeja a una escama de pez, de donde deriva su nombre (de "squama", escama). Se encuentra en la parte superior y lateral del temporal.




Situada en la parte posteroinferior del temporal, por detrás del conducto auditivo externo, formada por la parte más externa del peñasco fetal. Se distinguen en ella una cara externa, otra interna y una circunferencial:

También se le denomina hueso timpánico o timpanal, es un segmento embrionario distinto de las demás porciones, en su desarrollo, desde los tres meses de gestación hasta los dos años de edad aproximadamente, tiene la forma de un anillo abierto en su parte superior llamado anillo timpánico ("anulus tympanicus"), el cual se osifica durante el proceso ontogenético. El hueso timpánico en el adulto se presenta como una superficie regular, y constituye solo la pared inferior del conducto auditivo externo ("meatus acusticus externus") en la parte posterior esta el orificio del conducto auditivo externo ("porus acusticus externus") y en la parte anterior encontramos la ranura timpanica ("sulcus tympanicus") para la membrana timpánica cuyo borde anterior contiene la espina timpánica mayor ("spina tympanica major") y en su borde posterior encontramos la espina timpánica menor (" spina tympanica minor" ). Entre estas dos espinas esta la insisura tímpánica ("incisura tympanica"), una pequeña prolongación del hueso timpánico que forma una vaina por delante de la base de la apófisis estiloides llamada apófisis vaginal de la apófisis estiloides ("vagina processus styloidei").

Tiene forma de pirámide cuadrangular, con la base vuelta para afuera y hacia atrás, en tanto que el vértice, truncado, se dirige hacia dentro y adelante, formado por la parte interna del peñasco y porción timpánica fetal. Esta porción contiene numerosas cavidades y múltiples conductos que lo atraviesan. Presenta cuatro caras, dos superiores o endocraneanas y dos inferiores o exocraneanas, que van a formar la cara anterosuperior y la posterosuperior, la cara anteroinferior y la posteroinferior.












Articulaciones extrínsecas

Articulaciones formadas entre los huesos del cráneo con el temporal:

Articulaciones intrínsecas

Vestigios de la fusión de las porciones del hueso temporal entre sí: 

Trece músculos, sin comprender los del oído medio, se insertan en el temporal:




</doc>
<doc id="41630" url="https://es.wikipedia.org/wiki?curid=41630" title="Muqtada al-Sadr">
Muqtada al-Sadr

Muqtada al-Sadr (مقتدي الصدر, también conocido como "Moqtada", "Múqtada"; "al-Sáder" o "Muctada al-Sáder") (Nayaf, 12 de agosto de 1973) es un clérigo chií y uno de los políticos iraquís más influyentes a pesar de que nunca ha ostentado un cargo en el gobierno. Dirige el partido Movimiento Sadrista especialmente implantado en los barrios pobres chiís y la milicia Ejército de al-Mahdi, una de las más poderosa de Irak.

Su perfil público se forjó a partir de 2003 durante la guerra de Irak. Fue uno de los líderes de la insurgencia más relevantes y su milicia, que se hizo fuerte en Ciudad de Sadr en Bagdad, golpeó duramente tanto a las fuerzas de la coalición liderada por Estados Unidos, como al ejército iraquí. También se enfrentó a las milicias suníes por lo que fue acusado de avivar los enfrentamientos sectarios. 

En el periodo de reconstrucción de Irak tras la guerra, su partido ha participado activamente en la política. En 2006 apoyó al primer ministro Nouri al Maliki, también chií, del partido Dawa, y varios miembros de sus partido fueron ministros pero se retiraron en 2007 del gobierno. En 2010 volvió a apoyar a al Maliki dándole una ventaja decisiva sobre sus rivales. 

Situado con en primera línea política, en 2016 lidera la oposición al primer ministro Haidar al Abadi al que reclama un gobierno tecnócrata frente al reparto tradicional de poder por cuotas de clanes y partidos. Algunos análisis interpretan que su posición está relacionada con la pugna entre diferentes clanes chiís y el temor de Sadr a perder protagonismo ante la emergencia de nuevas milicias que destacan por lucha contra el Estado Islámico.

Pertenece a una influyente familia chií. Su padre fue el Gran Ayatolá Mohamed Mohamed Sadeq al-Sadr asesinado por el dictador Sadam Husein en 1999. También otro familiar, Mohamed Baqir al-Sadr, filósofo y pensador chií fundador del Partido Islámico Dawa fue asesinado por Sadam en 1980.

Habitualmente reside en la ciudad santa de Nayaf y se desplaza a Bagdad en pocas ocasiones.

Muqtada al Sadr tiene ascendencia iraquí e iraní. Su bisabuelo fue Ismail al Sadr. Es el cuarto hijo del famoso clérigo iraquí gran ayatolá Mohamed Mohamed Sadeq al-Sadr asesinado, junto con dos de sus hijos, presuntamente por el gobierno de Sadam Husein en 1999. También es el yerno del ayatolá Mohamed Baqir al-Sadr filósofo y fundador del Partido Islámico Dawa ejecutado durante el régimen de Sadam en 1980. Muqtada se casó con una de las hijas del ayatolá en 1994. No tiene descendencia.

Por otro lado también tiene ascendencia libanesa. Sus antepasados emigraron desde el sur del Líbano a Irak. Es primo del desparecido Imam Musa Sadr uno de los fundadores del Movimiento Amal.

Hijo y nieto de reconocidos ayatolás, la prestigiosa línea familiar "al Sadr" en el marco de la comunidad chií, mayoritaria en Irak, le situó en 2003 en la escena política liderando la insurgencia durante la guerra de Irak. Lanzó el partido Movimiento Sadrista y creó la milicia Ejército de al-Mahdi (2004) que con 60 000 combatientes se convirtió pronto en una la milicia más poderosa de Irak. A causa de sus enfrentamientos con las milicias suníes fue acusada de ser una fuerza desestabilizadora y de avivar la tensión sectaria en Irak.

En 2006 al Sadr tuvo parte activa cuando las facciones chiís asumieron el gobierno de Irak tras la guerra, sucediendo a la dictadura de Sadam Husein.

En mayo de 2014 antes de la celebración de las elecciones Sadr critica al primer ministro Al Maliki, le llama "tirano" y "dictador" y dice que no debería presentarse para un nuevo mandato.

El 26 de febrero de 2016 Sadr convocó a un millón de personas que se manifestaron en la plaza Tahrir de Bagdad para protestar por la corrupción en Irak y el fracaso del gobierno para reformar el país. La protesta fue considerada como una de las más masivas en la historia moderna de Irak. En su intervención, exigió al primer ministro Abadi reformas profundas y animó a los manifestantes a "alzar la voz y gritar para que los corruptos tuvieran miedo".

El 18 de marzo los seguidores de Sadr organizaron una protesta en el exterior de la Zona Verde de Bagdad donde están ubicadas las embajadas y los edificios oficiales después de que Sadr denunciara que la Zona Verde era "el bastión de apoyo a la corrupción".

El 27 de marzo el propio Sadr viajó a Bagdad desde su residencia en Nayaf y él mismo entró en la Zona Verde desde donde realizó un discurso exigiendo cambios políticos. En este caso pidió a sus seguidores que se quedaran fuera de la zona. Cuando Sadr cruzó la barrera de seguridad, los responsables militares y policiales del acceso, lejos de impedirle el paso le saludaron con honores.

El 30 de abril centenares de seguidores de Sadr irrumpieron en la Zona Verde y asaltaron la sede del Parlamento exigiendo al primer ministro Haidar al Abadi un gobierno tecnócrata que acabe con la corrupción. Durante la noche decidieron acampar en la plaza principal de la Zona Verde, la plaza Al Ihtifalat (Celebraciones) y quedarse allí hasta ser escuchados. Finalmente el 1 de mayo se retiraron con la amenaza de regresar y ocupar las sedes de la Presidencia, del primer ministro y del Parlamento otra vez, si no se cumplen sus exigencias.





</doc>
<doc id="41634" url="https://es.wikipedia.org/wiki?curid=41634" title="Abdelaziz Buteflika">
Abdelaziz Buteflika

Abdelaziz Buteflika (عبد العزيز بوتفليقة) (Oujda, Marruecos, 2 de marzo de 1937) es un militar y político argelino que fungió como desde el 27 de abril de 1999 hasta su renuncia el 2 de abril de 2019, siendo también líder del partido hegemónico Frente de Liberación Nacional (FLN). Con casi veinte años en el cargo, Buteflika es la persona que más tiempo ha ocupado la jefatura de estado argelina. Fue también el ministro de relaciones exteriores de Argelia en 1963 y ministro de juventud, turismo y deporte en 1964, durante el gobierno del presidente Ahmed Ben Bella.

Tras ser elegido presidente controvertidamente, luego de que sus principales contendientes se retiraran de las elecciones alegando fraude, Buteflika presidió el país durante el final de la guerra civil argelina (1992-2002). Durante su largo mandato, Buteflika logró la pacificación progresiva del país tras lograr el "arrepentimiento" de los dirigentes del rebelde Ejército Islámico de Salvación luego de prometer una masiva amnistía. Al igual que gran parte de sus predecesores, Buteflika se sostuvo sobre una enorme élite militar, partidaria y civil conocida popularmente como ""le pouvoir"" (en francés: "El Poder") de la cual formaban parte varios miembros de su familia, conocida como el "Clan Buteflika". Fue reelegido en 2004 y 2009 con más del 80% de los votos, en elecciones denunciadas como fraudulentas por la comunidad internacional. Aunque el país en gran medida se vio alejado de los acontecimientos de la Primavera Árabe regional, en 2011, una serie de manifestaciones lograron que el gobierno derogara el estado de emergencia impuesto desde 1991.

A finales de su tercer mandato, en 2013, Buteflika sufrió un derrame cerebral debilitante. Desde entonces, la capacidad del presidente para dirigir el país comenzó a ser cuestionada tanto dentro como fuera de Argelia, siendo comunes los rumores sobre su muerte al encontrarse a menudo hospitalizado, y no haciendo apariciones públicas desde mayo de 2013. En ese marco, resultó reelegido por un cuarto mandato en 2014, por un margen similar a los anteriores, sin haber participado durante toda la campaña. A principios de marzo de 2019, Buteflika confirmó su intención de presentarse a un quinto mandato, lo que desató masivas manifestaciones pacíficas en todo el país. Luego de seis semanas, ante la fuerte presión política, social y militar, Buteflika confirmó la formación de un gobierno de unidad nacional, pospuso las elecciones presidenciales y renunció el 2 de abril, entregando el cargo al presidente del Consejo de la Nación, Abdelkader Bensalah.

Abdelaziz Bouteflika nació el 2 de marzo de 1937 en Oujda en el entonces Protectorado francés de Marruecos. En 1956, con tan solo 19 años de edad, Bouteflika militaría en el ejército de Liberación nacional, que era un brazo armado del Frente de Liberación Nacional, que dirigió la independencia en Argelia, conviritíendose durante 26 años en partido único en el país.

Durante la guerra de la independencia argelina fue un líder militar conocido por el nombre de "AbdelQader" (عبد القادر) que estuvo destinado en diferentes puestos, incluido el control de la frontera con Mali. Tras la independencia de Argelia fue elegido diputado durante el mandato de Ahmed Ben Bella. Fue nombrado ministro de juventud, turismo y deporte. En 1963, fue nombrado ministro de Exteriores y en 1964 pasó a formar parte del Comité Central del Frente de Liberación Nacional argelino. Muy unido a Huari Bumedian, el 19 de junio de 1965 participó en el golpe de estado para que Bumedian fuera presidente. Durante el gobierno de Bumedian siguió ocupando la cartera de exteriores que le permitió proyectar al nuevo régimen argelino por el mundo y comenzar a establecer relaciones diplomáticas fluidas con Francia, con quien firmó un amplio acuerdo económico en 1971. Partidario del Movimiento de Países No Alineados, fue un activo defensor de dicha organización, extendiendo la confianza de la misma en el mundo árabe y convirtiendo a Argelia en punto de encuentro de diversas reuniones internacionales. Dimitió como ministro de Exteriores en 1979 tras la muerte de Bumedian y el nuevo presidente, Chadli Bendjedid, lo nombró ministro de Estado con escasas competencias.

El régimen de Chadli Benjedid, condenó a Bouteflika al ostracismo político desde los años 1980. Poco tiempo después, fue acusado de malversación de fondos públicos por el Tribunal de Cuentas del Estado argelino, que le impuso una cuantiosa multa por este delito, que le obligó a llevar a cabo un autoexilo en 1983 por Francia, Suiza y algunos países del Golfo. Tras volver a Argelia en 1987, volvió a ser nombrado miembro del comité central del Frente de Liberación Nacional argelino. Bouteflika fue uno de las dieciocho figuras históricas que firmó una carta apelando a la democracia y a las reformas políticas tras la brutal represión con disturbios en Argel. Los disturbios, seguidos de la derogación de los partidos políticos, marcaron el principio de un capítulo doloroso en la historia argelina. Además las represalias del gobierno a estos disturbios fueron traumaticas para el país. Quinientas personas fueron asesinadas, y muchas más heridas tras todo lo sucedido en el país.

Tras el final del gobierno de Liamine Zéroual, se presentó como candidato independiente a las elecciones para la presidencia de la República el bajo un programa de reconciliación nacional con los islamistas y en contra de las posiciones del ejército partidario del combate y la represión de los elementos fundamentalistas. A pesar de presentarse como independiente, Buteflika obtuvo el apoyo del FLN y del constituido partido de Zéroual, el Agrupación Nacional Democrática y obtuvo cerca del 74 % de los sufragios. La candidatura de AbdelAziz Bouteflika, ministro de asuntos exteriores durante el periodo de Boumeddiene y artífice del prestigio internacional de Argelia en aquella época, fue a la que el régimen le decidió dar su apoyo al ser el que mejor se acercaba a los “criterios de gobernabilidad” considerados indispensables por la cúpula militar, y contó con el apoyo del sector no erradicador del ejército, con el de la seguridad militar y con el de cuatro partidos próximos al régimen el FLN, el RND, el Nahda y el MSP (de los cuales los últimos dos partidos eran islamistas). Según los resultados oficiales, AbdelAziz Bouteflika habría obtenido el 75,9 % de los votos, por otro lado el resto de los candidatos no habrían logrado alcanzar entre todos ellos el 25 % de los sufragios, contando que la participación oficial fue 60,2 %.

Tras la aplastante victoria, aprovechó para oficializar y legalizar la tregua previamente establecida por el Ejército Islámico de Salvación, brazo armado del FIS, en 1997, ofreciendo la paz, lo que condujo al fin de la lucha armada el 6 de junio de 1999. Dentro del Plan de Concordia Nacional, aprobado por la Asamblea Nacional, en julio de 1999 se decretó una amnistía para todos los presos fundamentalistas que estuvieran libres de delitos de sangre, al tiempo que se instrumentaba un programa de integración social.

Desde la firma de los acuerdos de paz desarrolló una amplia labor política internacional para mejorar la imagen de Argelia: mejora de las relaciones con el Marruecos de Mohammed VI, acudiendo al sepelio de Hassan II en julio de 1999, aceptación explícita del entonces presidente de los Estados Unidos, Bill Clinton, de la política interna de represión contra los islamistas todavía combatientes, reanudación de un diálogo fluido con Francia y firma de acuerdos comerciales ventajosos con España para ampliar el suministro de gas natural a este país y, en un futuro, a otras zonas de Europa. Por otro lado, las dificultades del presidente Bouteflika cinco meses después de su llegada a la presidencia para formar gobierno o la resistencia de la cúpula militar argelina hacia su política de normalización con Marruecos reflejan, como nos lo ha demostrado la reciente historia del país que la actitud del ejército fue decisiva y que condicionaría el éxito de la estrategia de Bouteflika.

El 8 de abril de 2004 fue reelegido presidente de la República Argelina con un inesperado 85 % de votos en las elecciones, de acuerdo con Reporters sans frontière, se le acusó de elecciones fraudulentas, y atacaron al presidente con declaraciones como el abuso de las instituciones de estado y apropiamiento de dinero público. Durante el primer año de su segundo mandato, Bouteflika llevó a cabo un referéndum en su plan nacional de reconciliación, que dio fin a la guerra de la independencia de los 90. La guerra civil fue un conflicto armado entre el gobierno argelino y varios grupos rebeldes islamistas, comenzó en 1991 y terminó en 1998.

El resto de candidatos con partidos consolidados y no divididos (Yaballah, Said y Hannun) se han hundido en número de votos. Esto a pesar de la campaña mediática lanzada contra Buteflika. La pregunta por tanto que habría que hacer es si importan algo los partidos en la vida política de Argelia. Quizá sean más relevantes las asociaciones y grupos de poder, las zagüias, los notables y jefes tribales y los comités ‘aruch, que parecen capaces de decantar las elecciones o producir una abstención mayoritaria. En última instancia, se trata también de un interrogante sobre la eficacia de los medios de comunicación, ya que en la política moderna los partidos necesitan a los medios como forma de expresión y visibilidad.

En cuanto a la participación, esta fue más elevada que en las citas legislativa y local de 2002, pero algo más baja que en las presidenciales de 1999. Un 58 % de los electores acudió a votar con normalidad, produciéndose solo algunos incidentes en la Cabilia, donde grupos radicales de los comités quemaron urnas e impidieron el acceso a algo más de un 20 % de los colegios electorales.

El periódico francés "Le Dauphine Libere", anunciaba en 2005 que Bouteflika habría sido ingresado en la unidad de cardiología de una clínica privada al sureste de Francia. Toda la planta del hospital donde se encontraba el presidente fue cerrada para asegurar la máxima seguridad.Tras esto el presidente reduciría notablemente sus apariciones públicas en medios de comunicación y compromisos políticos, debido a su delicado estado de salud.

En 2009 ganó Buteflika su tercer mandato con el 90.24 %, aunque hubo una clara intención de boicot por parte del partido de la oposición y a que alegaban que había habido un fraude a “escala industrial”, según la agencia de noticias Reuters.

En cuanto a la participación, el equipo de observación electoral del TEIM desplazado a Argelia cubrió 8 colegios electorales de la wilaya de Argel y estimó que la participación media en los mismos no superó el 25 %. Si bien Argel es una wilaya poco inclinada a ir a las urnas, los cálculos realizados por los interventores de los candidatos y los partidos en otras wilayas del país en ningún caso alcanzan el 50 % de participación.

En cuanto a la relevancia de estas elecciones según el informe del observatorio internacional del TEIM del mismo año de las elecciones en la República argelina, afirma que debido a la falta de competición, la poca representatividad y la previsibilidad de los resultados, estas elecciones solo tienen la relevancia que les haya querido atribuir el régimen. Es decir, continuidad absoluta y ausencia de cambio. Por un lado, transmite la imagen de estabilidad que el régimen quiere proyectar hacia el extranjero. Por otro lado, a nivel doméstico este era precisamente uno de los eslóganes de la campaña de Buteflika: 'no al cambio, sí a la continuidad'.

Respecto a estas elecciones, algunos observadores consideran que no tuvieron lugar, pero para otros lo consideraron retroceso de 20 años a la época del partido único y los plebiscitos electorales y respecto a las repercusiones internacionales fueron escasas, solo manifestó su preocupación Obama desde la Casa Blanca en Estados Unidos. En general, la reacción internacional proclama que la estabilidad y los intereses económicos son más importantes que las cuestiones democráticas en lo que a Argelia se refiere.

En 2014, el presidente argelino Abdelaziz Buteflika, quien ha estado en el poder durante 15 años, ganó el cuarto mandato con el 81,53 % de los votos, según anunció el ministro de interior Tayeb Belaiz el 16 de abril de ese mismo año. Estas votaciones, al igual que las anteriores, no estuvieron exentas de polémica, ya que según los opositores volvería a hablar de fraude en ellas.

Los dos primeros mandatos del presidente Buteflika están marcados por la promesa de poner fin a la discordia y considera que hay que pasar página sobre el terrorismo por todos los medios para garantizar el retorno de la paz tras un larga guerra civil conocida como la "década negra". En septiembre de 1999, la ley de la "concordia civil" fue votada y aprobada por referéndum. Se prevé una amnistía parcial para islamistas armados. Los maquis están vacíos y más de 6000 hombres deponen las armas. Con miras a la reconciliación nacional, en ella se establece una ayuda para las familias de las víctimas del islamismo y libera a los soldados detenidos por los ataques contra miembros del Frente Islámico de Salvación (FIS).

En 2005, es nombrado presidente de honor del FLN en el 8 Congreso del Partido, el presidente Buteflika convoca un referéndum para adoptar una serie de medidas destinadas a "restablecer la paz civil en Argelia." Se propone reconocer el derecho a la reparación a las familias de las personas desaparecidas, conceder una forma de amnistía para los miembros de los grupos armados no culpables de matanzas, violaciones y bombardeos y crear apoyo a viudas y huérfanos de miembros de grupos armados muertos. Estas medidas también proporcionan una amnistía total para los miembros de las fuerzas de seguridad (todos los componentes) responsables de graves violaciones de los derechos humanos. Posteriormente cualquier queja en contra de ellos se considerará inadmisible. El artículo 46 de la ordenanza de aplicación establece una pena de prisión de 3 a 5 años a "cualquier persona que, de palabra, por escrito o de cualquier otro acto, utiliza o explota las heridas de la tragedia nacional para dañar la instituciones de la República Democrática de Argelia, que debilitan el estado, dañan la reputación de sus agentes que sirvieron con honor, o empañar la imagen de Argelia a nivel internacional". Durante la campaña para el referéndum, la oposición y las familias y asociaciones de derechos humanos de las víctimas están excluidos del debate. Los medios de comunicación permanecen herméticos a toda contestación y mantienen la confusión entre el derecho de rechazar la propuesta y la traición al país. El estado utiliza para esta campaña todos los recursos materiales, incluyendo los fondos públicos para el beneficio exclusivo de los partidarios del presidente. Este referéndum no resuelve el problema de las personas desaparecidas cuya suerte no puede ser una investigación independiente o internacional. La oposición critica mascarada electoral donde se considera que los resultados ya se conocen de antemano (97,36 % a favor y el 79,76 % de participación, excepto en Cabilia con la participación de solo el 12 %).

El 2 de marzo de 2006, en línea con su política de arabización, Abdelaziz Buteflika de 42 instituciones francófonas.

Durante sus dos primeros mandatos, consigue librarse de las garras de quienes le han instalado y confirmado en el poder: el personal del Ejército y el Departamento de Información y Seguridad (DRS). Anuncia también su deseo de "civilizar" el régimen. La profesionalización del ejército, el retiro de los oficiales superiores y la promoción de nuevos hombres, próximos al jefe de Estado en esta línea.

Sin embargo la represión de los acontecimientos en la Kabilia (Primavera negra) entre abril de 2001 y 2002 desacreditada la imagen del estado. En agosto de 2005, el nombramiento como embajador en Rabat del general Larbi Belkheir, exdirector de la oficina de Bouteflika, también refleja una toma de control del ejército del poder civil.

El 15 de abril de 2011, cuando los disturbios y manifestaciones tienen lugar durante varios meses en Argelia en medio de protestas y revoluciones en el mundo árabe, se anunció la creación de un comité para enmendar la Constitución, una revisión de la ley electoral, la ley de partidos políticos y el código de la información. Esos anuncios son considerados poco innovadores además de considerarse que llegan con retraso por parte de la prensa independiente y de la sociedad civil.

El año 2015 estuvo marcado por la detención de los oficiales militares de alto rango anti-Buteflika, el general retirado Hocine Benhadid (exjefe de la región militar del suroeste) y el general mayor Abdelkader Ait Ouarabi (exjefe de la lucha contra el terrorismo) así como por la jubilación de exgeneral Toufik. Algunos medios de comunicación, recuerdan el deseo de Buteflika cuando llegó al poder en 1999, de vencer al poder militar para restaurar el poder de la presidencia del país. En enero de 2016 disuelve la DRS y la sustituye por un nuevo organismo, el Departamento de Seguridad y Vigilancia (DSS), bajo la tutela de la presidencia situándose al mando el general retirado Bachir Tartag, con rango de ministro consejero del jefe del Estado.

Desde 2005, el estado de salud de Abdelaziz Bouteflika es con frecuencia objeto de especulaciones en los periódicos argelinos. Fue hospitalizado varias veces en Francia entre 2005 y 2016, especialmente en el hospital militar de Val-de-Grâce y Grenoble.

El 26 de noviembre de 2005 ingresa en Val-de-Grâce y fue operado oficialmente en una úlcera de estómago. Durante este período, la información es muy escasa y el estado del presidente de la República es objeto de especulación en la opinión pública argelina y en la prensa: se menciona un cáncer de estómago (esta hipótesis será corroborada más adelante por revelaciones de telegramas de la diplomacia estadounidense en el caso WikiLeaks). Abdelaziz Bouteflika regresó a Argelia el 31 de diciembre de 2005.

El 27 de abril de 2013, fue hospitalizado nuevamente en Val-de-Grâce después de un ataque isquémico transitorio (TIA), es decir, un pequeño ataque cerebral. El 19 de mayo de 2013, los diarios argelinos Mon journal y Djaridati, que afirmaban que Bouteflika estaba en estado de coma, fueron incautados en la imprenta y prohibida su publicación, algo inédito desde la década de 1990. La opacidad en torno a esta hospitalización es denunciada por la prensa argelina. El 12 de junio de 2013, después de 47 días de ausencia, las imágenes de Bouteflika se transmiten en el canal estatal ENTV.

El diario El Khabar publica posteriormente que Bouteflika sufrió un segundo ataque isquémico el 31 de mayo en los Invalides que paralizaron su lado izquierdo. El 16 de julio de 2013, después de 80 días de ausencia, Bouteflika regresó en silla de ruedas a Argel.

Abdelaziz Bouteflika ha estado viviendo desde entonces en una residencia médica estatal en Zéralda.

En la década de 2000, su deficiente salud provocó que algunos medios de comunicación argelinos cuestionaran su capacidad para dirigir el país. En 2011, Mourad Medelci, Ministro de Relaciones Exteriores, habló de la necesidad de "avanzar hacia la próxima generación".

Durante las elecciones presidenciales de 2014, Abdelaziz Bouteflika no participó en ningún mitin electoral y votó en silla de ruedas. Posteriormente, su movilidad continuó disminuyendo y rara vez realiza apariciones públicas. Su último discurso a la nación se remonta a 2012. El periodista Frédéric Pons lo describió en 2017 como "incapaz de gobernar más de unas pocas horas al día, casi postrado en la cama y tonto".

En abril de 2016, el primer ministro francés, Manuel Valls, publica en su cuenta de Twitter una fotografía que lo representa en compañía de Abdelaziz Bouteflika, en la que este último aparece muy disminuido y ojeroso. La imagen provoca muchos comentarios en la prensa y en las redes sociales. Así, el diario argelino El Watan considera que el presidente "no puede liderar el país hasta el final de su mandato en 2019" y solicita la aplicación del artículo 102 de la Constitución sobre el estado de incapacidad. Se alzan voces y tienen lugar pequeñas manifestaciones para reclamar su impedimento.

En ausencia de un sucesor designado o potencial, su séquito lo impulsaría a postularse para presidente en 2019. En mayo de 2018, catorce políticos e intelectuales argelinos, entre ellos el escritor Yasmina Khadra y el ex primer ministro Ahmed Benbitour, firmaron una carta en la que instaban a Abdelaziz Bouteflika a no postularse para un quinto mandato. En octubre de 2018, el secretario general del FLN anunció que Abdelaziz Bouteflika es el candidato del partido para la próxima elección presidencial. A pesar de las críticas a su estado de salud, Abdelaziz Bouteflika disfruta del hecho de que aparece como un factor de estabilidad para el país, así como del hecho de que es una de las últimas figuras de la guerra en Argelia y del proceso de reconciliación de finales de los años noventa.

El 10 de febrero de 2019, en un mensaje enviado a la agencia APS anunció la candidatura de Abdelaziz Bouteflika a un quinto período presidencial. El 22 de febrero se inician las primeras protestas contra este quinto mandato. El 1 de marzo las protestas se incrementan en todo el país. Desde el 24 de febrero el Presidente está en Ginebra de nuevo hospitalizado.

El 2 de marzo de 2019 APS también anunció que Abdelmalek Sellal director de campaña en las elecciones de 2004, 2009 y 2014 era reemplazado por el actual ministro de Transporte Abdelghani Zàlene.

Abdelaziz Buteflika, presidente de Argelia renunció a su cargo el día 2 de abril de 2019. El mandatario de 82 años, quien llegó al poder en el país africano en 1999, renunció luego de semanas de protestas masivas ante el anuncio que hizo el político de presentarse a las elecciones por un quinto periodo.

“Esta decisión procede del deseo de evitar que los excesos verbales (...) degeneren en deslices potencialmente peligrosos para la protección de las personas y los bienes”, dijo en su carta de renuncia.




</doc>
<doc id="41636" url="https://es.wikipedia.org/wiki?curid=41636" title="Hueso palatino">
Hueso palatino

Masetero:
El hueso palatino es un hueso de la cara, par, corto y compacto de forma irregular. Ocupa junto con los maxilares superiores (de los cuales parecen ser la continuación hacia atrás) la porción más posterior de la cara. Concurren a la formación de la bóveda palatina, de las fosas nasales, de la órbita y de la fosa ptérigomaxilar. Se compone fundamentalmente de dos porciones: una porción horizontal y una vertical, con dos caras y cuatro bordes cada una de ellas. 

Función: Este hueso sirve de caja de resonancia cuando hablamos y contribuyen a la formación de las fosas nasales



El palatino está casi enteramente constituido por tejido compacto; únicamente la apófisis piramidal contiene tejido esponjoso.

El palatino se articula con seis huesos:

En el palatino se insertan seis músculos que son:



</doc>
<doc id="41638" url="https://es.wikipedia.org/wiki?curid=41638" title="Apache">
Apache

Apache es el nombre con el que se conoce a un grupo de naciones indígenas culturalmente cercanas del este de Arizona, noroeste de México (norte de los Estados de Sonora,Chihuahua, Coahuila, Nuevo León y Tamaulipas y ), Nuevo México y regiones de Texas y de las Grandes Llanuras. El término "apache" probablemente proceda del zuñi "apachu", que significa «enemigo»; de ahí el nombre que les pusieron los españoles. Se denominaban a sí mismos "Indé", que quiere decir «la gente». Hablaban un conjunto de lenguas atabascanas meridionales, que se han clasificado en apache de las llanuras, apache oriental y apache occidental.

Eran pescadores, cazadores y agricultores. Vivían en pequeños grupos basados en la familia. Los grupos se formaban con varias familias de carácter matriarcal. Compartían el mismo rito de los sioux y los cheyennes.

Cuando llegaron los españoles, los apaches habían alcanzado el suroeste de los que hoy es Estados unidos. y norte de México en una migración de unos 500 años desde Canadá. Fue una tribu poderosa y guerrera, en continua lucha tanto con los colonizadores de procedencia europea como con otras tribus indias. La rendición de la tribu tuvo lugar en 1886, cuando los chiricahuas fueron deportados a Florida y Alabama, donde estuvieron bajo confinamiento militar. Siempre mostraron una gran fiereza como guerreros y mucha habilidad como estrategas.

En el año 1900 vivían unos apaches en libertad. Su jefe más conocido, Gerónimo ("Gokhlayeh"), nació en 1829 y murió por causas naturales en Fort Sill, Oklahoma, en el año 1909. Sucedió como jefe de los apaches a Cochise, quien vivió 62 años. Fue el hijo de Cochise, Taza, quien designó a Gerónimo como sucesor de su padre. Ahora se encuentran en reservas en Arizona, Nuevo México y Oklahoma en un número de entre 5500 y 6000. Una pequeña minoría de 20 apaches mescaleros aún subsisten en la frontera norte de los estados mexicanos de Chihuahua, Sonora y Coahuila.

Lingüísticamente pertenecen a la familia atapascana, al igual que sus hermanos navajos. Los apaches vivían seminómadas e inestables en un lugar determinado entre los ríos Colorado y Brazos en Texas. Se caracterizaban por una fuerte resistencia cultural como grupo. Eran grandes exploradores y conocían perfectamente su vasto territorio, el cual siempre defendieron tenazmente. Eran cazadores y recolectores de productos silvestres, se convirtieron en invasores de tierras pastoriles y agrícolas de otros grupos indígenas, hacían trueque y practicaban el pillaje para subsistir. En la región, era habitual que los grupos nómadas saqueasen a los sedentarios para asegurarse los suministros y alimentos y los apaches aplicaron este sistema también a los colonos europeos que se establecieron en ella.

Los primeros contactos corroborados con europeos, españoles, tuvieron lugar a finales del siglo . En 1598, Juan de Oñate, al tomar posesión de las tierras de Nuevo México, mandó dividir sus poblaciones para reducir a los apaches; envió a fray Francisco de Zamora como encargado de evangelizar a estos, que vivían en el extremo norte de la Nueva España y al poniente de Sierra Nevada, en California; más tarde mandó traer del centro de las provincias novohispanas a indígenas aztecas y otomíes cristianizados para lograr un mejor entendimiento con los pueblos apaches. El esfuerzo fue en vano: los apaches y otras tribus indígenas no se dejaron someter y decidieron quemar y destruir muchas poblaciones españolas de los territorios norteños de la Nueva España. Los apaches apoyaron a los indios pueblo en los combates con los españoles en 1599.

La relación desde el comienzo, y en el siglo se produjeron una serie de combates e incursiones emprendidas por los dos grupos. Los apaches buscaban ganado y caballos —animales a los que habían aprendido a montar— y los españoles emprendían represalias y capturaban esclavos, pese a que la Corona lo había prohibido. Las últimas décadas de este siglo correspondieron al esplendor del grupo, que a comienzos del siglo siguiente comenzó a ser desplazado por los comanches, provenientes del norte y empujados en la migración a su vez por los siux. El avance hacia el sur de los apaches a comienzos del hizo que aumentasen los choques con los españoles del Virreinato de Nueva España.

Nunca unidos políticamente, los apaches se dividían en distintos grupos, que fueron cambiando con el tiempo. Algunos de ellos, como los llamados lipanes y jicarillas, llegaron a asentarse y a criar ganado, pero la mayoría eran cazadores y recolectores. Se conocía como Apachería el territorio por donde vagaban, unos seiscientos sesenta mil kilómetros cuadrados que abarcaban el este del estado de Arizona, gran parte del de Nuevo México, el sudeste del de Colorado, el oeste del de Oklahoma, una parte notable de Texas y la zona septentrional de los estados de Sonora, Chihuahua y Coahuila. A mediados del siglo , se calcula que eran unos ocho mil, aunque algunos autores calculan una cifra mayor, treinta mil.

El primer documento que menciona la existencia de los apaches se escribió en Taos en 1702 y en 1720 llega allí una embajada apache solicitando permiso para asentarse en el territorio; el gobernador español lo otorgó, dando comienzo a un largo y difícil proceso de asentamiento, ya que otros pueblos ya instalados allí no sentían demasiada simpatía hacia ellos.

En 1825, el gobierno mexicano inicia un segundo periodo para pacificar a los apaches y unificarlos en poblaciones unidas, el cual fue un gran fracaso para los intermediarios mexicanos. Con el Tratado de Guadalupe-Hidalgo de 1848, firmado por México y Estados Unidos, el territorio apache quedó dividido entre los dos Estados, lo que originó la separación del grupo y gran descontento con ambos Gobiernos. Los apaches se dispersaron por las tierras de Arizona, Nuevo México, Texas, Oklahoma, Chihuahua, Sonora, Coahuila, Durango y Zacatecas.

Durante la colonización angloamericana de los nuevos territorios del oeste americano, se constituyeron en un gran obstáculo para poder llegar a colonizar hasta las costas del Pacífico. Los apaches resentidos usaron las armas de fuego y los caballos para matar a aquel que se introdujera en sus territorios. Muchos españoles, mexicanos y angloamericanos murieron a manos de los apaches. En 1821 se inicia una etapa conocida como las guerras indias, que terminan con la rendición de Gerónimo, el último líder de la resistencia, quien provenía de la tribu chiricahua.

El Gobierno de México trató de aniquilar a Gerónimo, pacta con él y después lo traiciona matando a su familia. Gerónimo ataca y quema los fuertes fronterizos mexicanos. El gobierno de los Estados Unidos también le persigue y trata de someterlo, matando a su segunda esposa. Finalmente fue confinado a las reservas indias de Florida y muere en Oklahoma en 1909.

En 1928 el gobierno de México declara oficialmente extinta la etnia apache en territorio mexicano y las 3000 personas que sobrevivieron en tierra estadounidense fueron sometidos finalmente en reservas de los estados de Arizona, Nuevo México y Oklahoma.

No hay registros escritos de las razones de la migración desde Canadá. Existen testimonios indígenas donde consta que se dedicaban al pillaje y la depredación, especialmente los lipanes que habían absorbido a varios grupos coahuiltecos, sin saberse esto con verdadera exactitud. En el momento de la llegada de los primeros europeos, los españoles, las etnias del lugar se mostraban agresivas contra los apaches y contaban de sus múltiples ataques, probablemente buscando asentarse en tierras más fértiles.

Al principio del dominio español de las demás tribus, se trató de evangelizarlos sin éxito, en vista de lo cual se procedió a establecer la cadena de presidios que intentaba proteger la zona. Como el éxito fue mínimo, se estableció el sistema de soborno, por el cual el gobierno virreinal aprovisionaba a las distintas tribus para detener los ataques.

Sin embargo, en el momento de la independencia de México, ambos sistemas, el de presidios y el de sobornos, colapsaron, reanudándose los ataques. Estos alcanzaron proporciones épicas en Chihuahua y Sonora, contribuyendo a la guerra de Intervención Estadounidense, y gracias a ello los ataques se extendieron.

Después de la guerra (1848), los ataques se extendieron más al sur, llegando a Sinaloa y Durango, al tiempo que se iniciaban en los nuevos territorios de Nuevo México, Arizona, y Texas. La depredación llegó al punto de poblaciones completas exterminadas en diferentes lugares de Arizona, Sonora y Chihuahua. De especial ferocidad fueron los ataques en Cuencamé y zonas aledañas, donde todos los habitantes fueron exterminados. Esto provocó la respuesta de ambos lados de la frontera, donde se envió al ejército para controlar los ataques, de nuevo con poco éxito.

Los gobiernos locales tomaron control de la situación. En Chihuahua, el gobernador Ángel Trías ofreció 200 pesos por la cabellera de cada indígena, esto disminuyó los ataques en Chihuahua, llegándose a pactar el fin de la cacería. El mismo modelo se seguía en Estados Unidos. Sin embargo, siendo estos pactos locales o regionales, provocaban que los ataques se detuvieran en un lugar y continuaran en otro. El ejemplo más claro es el de 1851, cuando Chihuahua sostenía un pacto de paz con las tribus del noroeste, llegando varias de ellas a asentarse en Janos. Sin embargo, los propios anales de los apaches, así como la autobiografía de Gerónimo, reconocen que se usaba el campamento como base para atacar los establecimientos de Sonora. Ante esto, el gobernador Elías González persiguió a los indígenas hasta Janos, exterminándolos o capturando a algunos de regreso a Sonora. Este acontecimiento en particular reanudaría la guerra por el norte de México y suroeste de Estados Unidos, que continuaría hasta 1886.

Debe notarse la lógica diferencia entre la narración de los mismos hechos desde los tres puntos de vista diferentes, estadounidense, mexicano y apache. Al analizar las crónicas de la época se debe tomar en cuenta las motivaciones y justificaciones de un punto de vista en particular. Así, cronistas de cualquiera de las partes omiten convenientemente sus crímenes. El ejemplo del ataque a Janos es el más claro, donde las tres narrativas de la mayoría de los cronistas de la época son muy parciales en sus apreciaciones, a la vista de lo cual debe analizarse con cuidado cualquier material de referencia. Incluso en la actualidad se usan estas fuentes parciales del siglo XIX.

Actualmente, la mayoría de apaches son adherentes del cristianismo, en particular, del catolicismo romano. La mitología prehispánica Apache hablaba de dos héroes mitológicos. Uno es el del sol/calor, frente a los monstruos asesinos, el segundo es la del agua/la luna/ y el trueno, hijo del Agua, Nacido fuera del agua, que es perjudicial para los seres humanos. Otras leyendas hablan de un juego de pelota secreto en el que los animales buenos y malos jugaban para decidir si el mundo debería permanecer en la oscuridad eterna o entrar en un nuevo amanecer. Ocupa un lugar importante las leyendas del coyote y la de triksteris. Los Indios navajos, apaches occidentales, jicarilla y Lipanes tienen sus antecedentes legislativos, sin embargo, esto no incluye a los Chiricahuas y los mescaleros.

Algunos animales estaban demonizados, ya que podían causar diversas enfermedades; entre estos, estaban los búhos, serpientes, osos y coyotes.

En muchos de los "dioses" de los Athabascanos del sur se personificaban las fuerzas de la naturaleza que la gente utilizaba para sus propios fines a través de diversos rituales. A continuación la definición Basso paeiktas sobre los apaches orientales de la palabra "Dios":

El término "Diyi" 'se refiere a una o un grupo de fuerzas invisibles que se derivan de una serie de animales, plantas, minerales, fenómenos meteorológicos y las criaturas mitológicas del universo apache occidental existentes. Cualquiera de una variedad de fuerzas puede ganar y un hombre puede hacerlo correctamente, puede ser utilizado para diversos fines.

En estas ceremonias eran muy conscientes los hechiceros (chamanes) de ello, pero era posible, una manifestación directa de un individuo. Variaban los rituales y sus formas en las diferentes tribus apache. Muchas ceremonias chiricahuas y mescaleras eran hechas con lo aprendido durante visiones religiosas personales, mientras que los jicarilla y apaches occidentales tuvieron rituales estandarizados. La pubertad en las niñas (danza del amanecer), rituales y celebraciones con canciones navajo, estaban bien establecidas desde la antigüedad entre los apaches jicarilla de los llanos; las ceremonias de consagración eran de los rituales más abundantes. Muchas máscaras ceremoniales se utilizaban para representar espíritus religiosos. Los navajos, apaches y los jicarillas occidentales tienen considerable conocimiento y filosofía religiosa, incluyendo símbolos que marcaban en la arena del desierto. Se cree que el uso de máscaras y pintura sobre la arena fue tomada de la cultura pueblo.

Todos los apaches vivían en familias grandes, donde cada familia tenía su propia vivienda. La familia extendida consistía en marido y mujer, sus hijos todavía no casados, los que se casaron con sus hijas, sus hijos que se casaron con las hijas de otros hombres y los niños de cada uno de ellos. De esta manera, un grupo de familias unidas formaban un linaje femenino (modelo de madre) en la que los hombres eran capaces de conseguir un matrimonio después de salir de la familia de sus padres. Cuando una hija se casa, ella y su esposo colocan nuevas viviendas. Era muy grande el rol de las mujeres en la organización de la aldea. Mientras que en la parte sur del Oeste también estuvo el matriarcado extendido, a veces el hijo mayor se quedaba con su esposa a vivir con sus padres. Todas las tribus reconocían matrimonio religioso y libre.

Algunas familias trabajaban juntos en grupos locales, las cuales llevaban a cabo ciertas ceremonias, el comercio y la guerra. Este grupo local estaba encabezado por un hombre-comandante de avanzada edad que tuvo la mayor influencia desde joven entre los miembros del grupo gracias a su reputación. El Comandante de grupo venía por sus culturas como el líder Apache. La ocupación no era hereditaria y por lo general era diferente en todos los miembros de la familia. El Comandante era una influencia fuerte en el grupo - ni un solo miembro del grupo no se veía obligado a seguir al comandante. El comandante Apache occidental bien considerado tenía que ser trabajador, generoso, objetivo, tolerante, honesto y elocuente.

Muchos grupos locales de base se fusionaban en escuadrones. Estos grupos de organización fueron más fuerte entre los Chiricahuas centrales y del oeste, mientras que entre los Lipan y mescaleros, era débil. El Grupo Navajo no formaba escuadrones, muy probablemente debido al hecho de que no era favorable en la búsqueda del bisonte. Sin embargo los navajos organizaron grupos, que eran más grandes que la familia extensa Apache, pero menor que el grupo local o pelotón.

Los mayores niveles en la jerarquía social de los apache del oeste acudieron en masa a las órdenes, que Grenville Goodwin bautizó. Llamó a los cinco grupos apaches occidentales: los Tontos del Norte, los Tontos del sur, Cibekuje, San Carlos y White Mountain. Los Escuadrones Jicarilla consistieron partes influenciadas probablemente por la cultura Pueblo. Los apaches occidentales e indios Navajo tenían sistema de clanes matriarcales.

El concepto de nación apache no existía entre ellos y no se consideraban parte de la misma unidad a pesar de hablar idiomas semejantes. Las Siete tribus apaches no estaban unidas políticamente (a pesar de la opinión generalizada) y, a veces incluso eran enemigos mutuamente, tales como los Lipanes, que eran enemigos de los Mezcalero.

Las tribus apaches se caracterizaron por tener dos sistemas diferentes: la familia o clan y el tipo entre los Chiricahuas y Jicarillas. Los tipo de sistema fueron ciertemente parecidos entre los Chiricahuas, Mezcaleros y en los apaches occidentales, el último sistema era un poco diferente de los dos primeros ya que era más parecido al sistema navajo.

El sistema Jicarilla, se asemejaba a el de los Dakota y a los sistemas de parentesco Iroqueses siendo parecido a el de los Navajo, Lipan, y apaches de la planicie. El sistema Navajo era algo diferente y tenía similitudes con sistema Chiricahua. El sistema Lipan, y el de los apaches de la planicie eran sistemas muy similares.

Los Chiricahuas tenían cuatro palabras diferentes para nombrar a los abuelos:"-chu" - abuela materna, "-tsúyé" - abuelo materno, "-ch'iné" - abuela y del lado del padre "-Nalé" - abuelo paterno. Además, hermanos y hermanas mayores son llamados por la misma palabra; por lo tanto, la abuela materna, hermanas y hermanos de la abuela materna se llaman "-chu". Además, todo el nivel de los abuelos es recíproca, es decir, los abuelos también llaman a sus nietos por la madre o la línea del padre. Por ejemplo, la abuela materna del hablante se llamará "-chu" e identicamente también la del receptor se llamara "-chu".

Los Chiricahuas no son poseedores de los primos de la familia principál y hermanas de hermanos y hermanas. De esta manera, la misma palabra se usa para designar hermano y primo. Además, otro término es utilizado para describir el género del hablante: "-k'is" - son hermanos del mismo sexo, hermana o primo, prima y "-ląh" - el sexo opuesto. Esto significa que si el que habla es un hombre, entonces él va a llamar "-k'is" a su hermano y a su hermana "-ląh". Si el que habla es una mujer, entonces su hermano se llamará "-ląh" y su hermana "-k'is".

Dos palabras diferentes son conocidas para designar a los padres, de acuerdo con el sexo: "-máá" - madre y "-taa" - padre. También, estas dos palabras describen a los niños en función del sexo: "-yách'e" '- hija y "-ghe" ' - hijo.

Hermanas y hermanos de los padres se identifican juntos, sin importar el género: "-ghúyé" - una tía o un tío materno (hermano de la madre o hermana), "-deedéé" '- tía o un tío del lado de su padre (hermano del padre o hermana). Estos términos se usan mutuamente por los abuelos y los nietos. Esto significa que "-ghúyé" también significa sobrino o sobrina.

A diferencia del sistema Chiricahua, los abuelos Jicarilla solo se podían nombrarse con dos palabras distintas para diferenciar el sexo: "-chóó" - abuela y "-tsóyéé" - abuelo. No hay términos separados para los abuelos maternos o paternos. Estos términos también se utilizan para nombrar a los abuelos de los niños en función del género. De esta manera, la palabra "-choo" llama a la abuela y sus hermanas (tanto de la madre y la otra mitad del padre); "-tsóyéé" llaman al abuelo y sus hermanos. Estos términos no son mutuamente. Sólo hay una palabra para el nieto, independientemente de su sexo "-tsóyíí".

Hay dos palabras para cada padre. Estos términos también se utilizan para nombrar a los padres de los hermanos del mismo sexo: "-'nííh" - madre o tía (hermana de la madre), "-ka'éé" - padre o tío (hermano del padre). Además, el uso de estos términos para describir el tío o tía del sexo opuesto: "-da'áá" - tío materno (hermano de la madre) y "-béjéé" - tía paterna (hermana del padre).

Los dos términos son utilizados por hermanos y hermanas. También utilizan los primos del sexo opuesto: "-k'isé" - hermano del mismo sexo, hermana o primo, prima; "-láh" - es el hermano del sexo opuesto, hermana o primo, prima. También hay tres términos que dependen de la edad del hablante: "-ndádéé" - hermana mayor, - "na'áá" - hermano mayor, "-shdázha" - el hermano o hermana más joven. También se utiliza, además, diferentes palabras para la madre y sus hermanos, hermanas, hijos de sus padres: "-zeedń" - padre, hermana, hermano, madre o hijo, "-iłnaa'aash" - hermana del padre varón o el hijo del hermano de la madre (utilizado sólo en relación con los hombres).

Es conocido como los hijos de los padres, así como los hermanos del mismo sexo o los hijos de los niños del mismo sexo o los primos de primos: "-zháche'e" - la hija del mismo sexo en relación con el hermano o la hermana del orador, hija, "-ghe" '- hijo del hermano del mismo sexo o hijo de la hermana. Existen términos individuales del sexo opuesto con respecto al hermano o hermana de un niño del que se habla: "-da'áá" - la hermana del sexo opuesto o hija de la hermana, "-da" '- el hermano del sexo opuesto o hijo de la hermana.

Todos los apaches vivían en tres tipos de vivienda. La primera era una tienda india utilizada por vivir en las llanuras. El segundo tipo de vivienda eran las vikiupas, una especie de choza, llamada vigvam. Esta casa se trata de un marco de madera de 2,5 m de altura reforzada con fibras de yuca y cubierto de arbustos y hierba. Estas chozas generalmente eran de los apaches que vivían en las montañas. Si un miembro de la familia que vivía en la choza muere, se incinera. El tercer tipo de vivienda era un Hogan, se utilizaba durante los tiempos cálidos cuando se adentraban al norte de México, el cual es perfectamente fresco. Su construcción aún es común en la Nación Navajo.

A continuación una descripción de la antropóloga Morris (Morris Opler) sobre la casa de los chiricahuas:

Los apaches obtenían sus alimentos de diversas maneras:


La dieta del Apache occidental consistía en carne (35-40 por ciento). Y los alimentos vegetales (60-65 por ciento). Debido a que las diversas tribus apaches vivían tan lejos como en diferentes condiciones climáticas, esto tuvo un impacto en sus diferencias nutricionales.

La caza era un asunto masculino, aunque hubo excepciones dependiendo del tiempo y la cultura (por ejemplo, las mujeres Lipan podían ayudar cazando conejos).

Antes de la caza por lo general tenían lugar elaborados rituales como el ayuno y los curanderos (ang. Curandero) llevaban a cabo ciertas ceremonias religiosas antes y después de la caza. En la cultura Lipan, los ciervos eran custodiados por el espíritu de la montaña, por lo que se invocaba a dicho espíritu en sus rituales para que la caza tuviera éxito. El juego de cazar que cumplir con ciertas normas religiosas (muchas de los cuales se describen en los relatos religiosos), designadas para un animal sacrificado, cómo orar y cómo destruir los huesos. Entre los criollos del sur era generalizada la división y repartición del cuerpo. Por ejemplo, entre el cazador mezcalero, que tuviera éxito en la caza, se esperaba que diera la mitad de sus presas a sus compañeros y los que carecían de alimentos en el campamento. La preocupación por otros individuos de la comunidad muestra un gran compromiso social y generosidad. Era posible ganar respeto entre los miembros de la tribu, precisamente quien tuviera mayor respeto y carisma podía convertirse en su líder.

El arma más común durante la invasión de los europeos fue el arco y la flecha. Era utilizada en la caza en diversas tácticas. En casos especiales utilizaban cabezas de animales como carnada, y silbatos seductores para que el animal se acercara. También trataban de cansar al animal - los cazadores localizaban varios puntos rodeándolo y defendían su posición hasta que el animal caía agotado o muerto.

Algunos animales no podían comerse, ya que era una falta a la moral pública. Diferentes tribus tenían diferentes tabúes. Algunos de los ejemplos más comunes: osos, pecaríes, pavo, pescado, ranas, serpientes, insectos, búhos, pumas y coyotes. Dos ejemplo de tabúes diferentes: el oso negro fue siendo parte de la dieta (como los bisontes, ciervos o antílopes), pero los Jicarilla nunca comían osos, porque los consideraban animales malignos. Algunos tabúes eran típicas de toda la región. Tal como ir sin peces, que era tabú en todo el suroeste, o que las serpientes eran la encarnación física del mal.

Los apaches occidentales en su mayoría cazaban venado y berrendo a finales de otoño. En noviembre, comían carne seca y migraban a campamentos de invierno en Salto, Negro, Gilo o incluso los valles del Río Colorado.

Los Chiricahuas generalmente cazaban ciervos y berrendo. Sin embargo en el juego de la caza se mataban conejos (liebres), zarigüeyas, ardillas, caballos salvajes y mulas, venados, pecaríes, hámsteres.

Para los Mezcaleros era mucho más importante el venado, aunque también cazaban borrego cimarrón, bisonte (los que vivían más cerca de los llanos), y según los estadounidenses conejos, ciervos, caballos y mulas, zarigüeyas, berrendo y hámsteres.

Los Jicarilla cazaban borrego cimarrón, bisontes, ciervos y šakiaragis. También eran cazados los castores, conejos, ardillas, palomas, urogallo, mulas, puercoespín, conejos, ardillas, pavos y hámsteres. Los caballos solo se comían después de haber sido introducidos por los europeos. No comían pecarí, comadrejas, gatos y lobos salvajes, pero los cazaban para obtener ciertas partes de su cuerpo como la piel o los dientes. El zorrillo solo se comía en casos extremos de hambruna.

El Lipán se defendió sobre todo por la carne de bisonte. Su caza se prolongó durante tres semanas en el otoño y después de unos pocos días hasta la primavera. Otro partido clave fue el venado. Bebían la sangre fresca de venado porque pensaban que era más saludable. Aunque también los lipanes cazaban castores, borrego cimarrón, osos negros, patos, caballos, pumas, palomas, perros de la pradera, berrendo, codornices, conejos, ardillas, pavos, tortugas y hámsteres.

Entre los Apaches de los llanos era más común que se cazara bisontes y ciervos. Otros animales cazados eran tejones, osos, castores, coyotes, lobos, gallinas salvajes, gansos, caballos, mulas, zarigüeyas, nutrias, conejos y tortugas.

La recolección de plantas y la cocina era el trabajo de las mujeres. Sin embargo, en algunas cosas, como la recolección de coronas de agave que estaban pesadas, los hombres ayudaban. Muchas plantas silvestres se han utilizado no solo por la comida, sino también por los servicios médicos y religiosos. Algunas plantas se utilizaban exclusivamente para los rituales religiosos y como medicinas.

Los apaches occidentales recogían agave, que se cuece y se seca. A finales de junio - principios de julio el mercado de las grandes llanuras se llenaba de frutas, verduras y raíces. Entre julio y agosto se recogían vainas de los árboles del género Prosobis, de bayoneta española (género yuca), frutas y bellotas de roble Emory. A finales de septiembre, la planta ya no se recogen, porque en ese momento toda la atención se centraba en los cultivos de invierno. Al final de otoño recogían bayas de enebro y piñones de pino.

Los Chiricahuas utilizan la corona de la planta (base del tubérculo) (la cuecen en grandes hornos subterráneos y la secan al sol) y brotes. El agave, fue elegido de entre otras plantas, las bayas, frutos y demás partes comestibles - agaritos (Mahonia trifoliata) bayas, bayas de enebro, Dátil de yuca (Yucca baccata) frutas, pimientos, pasas de Corinto, bellotas Gambela y corteza de roble (utilizada para el té), verduras, frutas de espino, hojas de abril de bellotas de roble de hoja perenne, vainas de acacia, papas silvestres y muchos otros cultivos. También miel de panales silvestres.

El agave crece no era menos utilizado entre la nación mezcalera, cuya corona recogían antes del invierno. También se alimentaban de bayas agaritos, Dátil de yuca (Yucca baccata), fruta, fruta de cactus, papas silvestres y muchas otras plantas no venenosas que crecían en territorio mezcalero.

Los cultivos domésticos en los diferentes grupos apaches difieren fuertemente. La mayoría de los cultivos se cultivaron por los indios Navajo, en menor medida, los Lipan. Los Apaches occidentales y Jicarilla practicaban muy poco la agricultura. Mientras que los apaches de las llanuras y los Chiricahuas no la utilizaban.

A diferencia de los europeos o los euro-americanos, los apaches claramente se dedicaban al saqueo y al pillaje, quedándose con el botín de guerra, no exactamente comerciaban, aunque si existía en sí mismo cierto trueque. Los comerciantes estaban en pequeños grupos, para un determinado propósito económico. Durante el robo se constituían reservas de alimentos adicionales, perseguían y buscaban ciertos artículos necesarios (sobre todo mujeres). La guerra se llevaba a cabo en grandes grupos hasta perdición.

El comercio también era una fuente de alimento. Sus trofeos de caza, en su mayoría eran pieles, las cuales cambiaban por armas, herramientas de utilidad y otros artículos útiles o alimentos.

En nuestros días abarcan las tribus de los jicarillas y mescaleros de Nuevo México, los chiricahuas de Nuevo México y la frontera con Arizona, los apaches occidentales de Arizona, los lipan de Texas y los apaches de la llanuras del suroeste en Oklahoma. Probablemente vivieron aquí durante siglos, otros grupos apache del sur que ahora han desaparecido son desconocidos para los antropólogos.

Los apaches occidentales son ahora los únicos que todavía residen en Arizona. Los grupos de Apache viven en varias reservas, las tribus individuales se fundaron juntas. Los Apaches occidentales que viven en las reservas White Hill Fort Apache (ang. Fuerte Apache Montaña Blanca), San Carlos, Yavapai-Apache, Tonto-Apache, y McDowell Mohave-Apache Fort (ang. Fuerte McDowell Mohave-Apache). Parte de los Apaches occidentales viven en Yavapai-Prescott ((ang. Yavapai-Prescott). La tribu apache White mountain vive en la parte oriental del centro de Arizona, 312 kilómetros al noreste de Phoenix. La reserva de los Tonto-Apache fue fundada en 1972 cerca Paisono al este de Arizona. Situado en 344.000 m² en el noreste de "Fikinkso Tonto National Forest", hogar de cerca de 100 personas. Los familiares poseen un casino. La reserva Yavapai-Apache, que está al oeste de Flagstaff, en Arizona.

A los chiricahuas ya no se les considera una unidad cultural ya que se han dividido en dos grupos. Algunos se trasladaron a las reservas mescaleras junto con los Lipan que se incluyeron en un grupo político mescalero más grande. Otros chiricahua permanecieron en Oklahoma e hicieron una fortaleza que con el tiempo se convirtió en reserva (ang. Fort Sill Apache Tribe).

La reserva mezcalera está ubicada en el sureste de Nuevo México, cerca de la histórica Fort Stanton. La reserva jicarilla está situada en el río de Arriba y el condado Sandovali en el noroeste de Nuevo México. Algunos lipanes se conservan en la reserva mescalera. Otros Lipanes viven en Texas. Los apaches de las planicies viven en Oklahoma, en la ciudad de Andarko.

Mientras tanto, los navajo viven en una gigantesca reserva-nación semiautónoma al noreste de Arizona llamada Nación Navajo que goza de cierta autonomía.

La siguiente división se basa en diferencias lingüísticas.







</doc>
<doc id="41642" url="https://es.wikipedia.org/wiki?curid=41642" title="Hueso hioides">
Hueso hioides

El hueso hioides es un hueso impar, medio y simétrico, situado en la parte anterior del cuello, por debajo de la lengua y por encima del cartílago tiroides. Tiene forma de herradura, siendo convexo hacia delante. Se pueden distinguir en él tres porciones diferentes:
El hueso hioides, generalmente aislado en el ser humano, puede estar unido en ocasiones al resto del esqueleto por un conjunto de formaciones óseas, que con él constituyen el aparato hioideo.

A diferencia de otros huesos, el hioides no se encuentra articulado mediante contacto directo con ningún otro hueso Está suspendido por músculos y ligamentos que lo unen a la mandíbula, a la apófisis estiloides, al cartílago tiroides, al manubrio del esternón, a la escápula y al ligamento estilohioideo. 

Es aplanado de adelante atrás y posee para su estudio dos caras, dos bordes y dos extremidades.

Las astas mayores o astas tiroides se dirigen horizontalmente hacia fuera y atrás, describiendo una curva de concavidad posterior e interna.
Aplanadas de arriba abajo, hemos de considerar en ellas las siguientes regiones:

Las astas menores o astas estiloides, situadas por dentro de las precedentes, sobresalen del borde superior del hueso. A la vez prolongadas y redondeadas, se parecen mucho a dos granos de cebada, dirigidos oblicuamente de abajo arriba, de dentro a fuera y de delante atrás.
Se consideran en ellas:

El hioides presta inserción a doce músculos:

Está formado por tejido compacto, excepto en la base de las astas mayores, donde presenta una grandísima cantidad de tejido esponjoso.

El hioides se desarrolla a la vez a expensas de los segundo y tercero arcos branquiales. Contribuyen a su formación seis puntos de osificación, a saber:

El hioides se desarrolla en el embrión a partir del cartílago de Reichert, proveniente del segundo arco branquial. Este cartílago se divide en tres segmentos en el transcurso de su desarrollo:
Estos tres segmentos van a formar la cadena hioidea, y a esta se le va a añadir otro segmento proveniente del tercer arco branquial, el tirohioidal, que van a formar las astas mayores del hueso hiodes.

Algunas veces el segmento medio del cartílago de Reichert, en lugar de volverse fibroso, constituye una pieza ósea. De ello resulta la formación de un aparato en forma de herradura, cuyas dos extremidades se articulan con los temporales; en esto consiste el aparato hioideo.
Se compone a cada lado por cuatro piezas :

Debido a su posición, el hueso hioides no suele tender a fracturarse.
En casos de muerte dudosa, un hioides fracturado es un fuerte indicio de estrangulación y de un suicidio por ahorcamiento.


</doc>
<doc id="41644" url="https://es.wikipedia.org/wiki?curid=41644" title="Escápula">
Escápula

La escápula (en latín, "escarbar") u omóplato / omoplato (del griego "omo", hombro, y "plato", ancho) es un hueso plano y triangular. Se ubica en la parte posterior o dorso-lateral del tórax según la especie de la que se trate, específicamente en el esqueleto humano se encuentra en la región comprendida entre la segunda y séptima costilla. Conecta con el húmero (hueso del brazo) y con la clavícula (en aquellas especies que poseen tal hueso) y forma la parte posterior de la cintura escapular. El cuerpo es delgado, incluso traslúcido a contraluz por encima y por debajo de la espina, aunque es más grueso en sus bordes.

Consta en términos generales, y específicamente en el ser humano, de un cuerpo, una espina que termina por fuera en el acromion y una apófisis coracoides.

El resto de este artículo hace referencia puntual a sus características particulares en la especie humana, pero puede ser de utilidad como base comparativa para el estudio de otras especies de animales superiores.

Los siguientes músculos se insertan en la escápula:

Esta cara es la que se encuentra en el tórax, del cual está separada por el serrato mayor. Es cóncava en casi toda su extensión, a esta concavidad se le llama fosa subescapular, donde se insertan las láminas tendinosas del músculo subescapular. En el límite lateral se advierte un saliente alargado es el denominado "pilar de la escápula". Medialmente a la fosa subescapular y a lo largo del borde medial, se observa una superficie rugosa y alargada en la cual se fija el músculo serrato anterior.

Las dos terceras partes de la sección medial de la fosa están marcados por muchas cordilleras oblicuas, que van de los lados hacia arriba. Las cordilleras permiten la adhesión a las inserciones de los tendones, y las superficies entre ellas a las fibras carnosas, del músculo subescapular. El lateral de la tercera parte de la fosa es liso y está cubierto por las fibras de este músculo.

Es convexa y está dividida por la espina de la escápula en la fosa supraespinosa y la fosa infraespinosa. En sentido medial, la espina se pierde hacia el borde medial del hueso en una pequeña superficie triangular. Lateralmente, en cambio, se eleva y espesa cada vez más, hasta separarse por completo del hueso, terminando en una amplia saliente en forma de paleta: el acromion. El acromion presenta: una cara superior con numerosos foramenes nutricios, situada directamente debajo de la piel; una cara inferior, cóncava; un borde lateral, espeso y rugoso, donde se insertan los fascículos medios del deltoides; un borde medial, más delgado, que presenta la carilla articular para la clavícula. La espina de la escápula es aplanada de arriba hacia abajo y presenta dos caras, superior e inferior. La superficie de la fosa suprespinosa es lisa y sirve de inserción al músculo supraespinoso, mientras que la superficie de la fosa infraespinosa es rugosa y cóncava y cubre poco más de los dos tercios inferiores de la superficie dorsal de la escápula, por debajo de la espina escapular, estando dividida en dos partes por una cresta que discurre a lo largo de su borde lateral, la parte medial sirve para la inserción del músculo infraespinoso. El área de la fosa es de forma más o menos triangular, su lado superior recorre por debajo el trayecto de la espina y su vértice se dirige hacia abajo, cerca del ángulo inferior de la escápula. La parte lateral está subdividida, por una cresta oblicua, en dos zonas secundarias: una superior donde se inserta el redondo menor y otra inferior donde se inserta el redondo mayor. El borde posterior de la espina da inserción a dos músculos muy potentes: en el labio superior, al trapecio y en el labio inferior, en la parte lateral, al deltoides.

Es corto, delgado y afilado, y está interrumpido en su unión con la apófisis coracoides por la escotadura coracoidea (escotadura escapular), por la que discurre el nervio supraescapular. A menudo, esta escotadura está cerrada parcial o totalmente por un ligamento que se osifica, llamado ligamento coracoideo. Medialmente a la escotadura se inserta el vientre inferior del músculo omohioideo.

Es el más largo de los tres, sigue paralelo y unos 5 cm lateral a las apófisis espinosas de las vértebras torácicas. Superiormente a la espina se inserta el músculo romboides menor, e inferiormente el romboides mayor.

Es una cresta delgada pero rugosa que en su parte inferior presenta el tubérculo infraglenoideo, donde se fija el tendón de la cabeza larga del tríceps braquial. Este borde suele presentar un surco para la arteria circunfleja escapular.

Se sitúa en la unión de los bordes superior y medial. En casi 90°.

Se sitúa en la parte más medial o interna de la escápula. 45°

Es grueso, redondeado y rugoso. Resulta de la unión de los bordes medial y lateral. En él convergen las potentes inserciones del músculo infraespinoso, del músculo romboides mayor y del fascículo inferior del serrato anterior. En este punto se inserta a veces un fascículo del músculo dorsal ancho. Este ángulo presenta movimientos amplios cuando se abduce (aleja) el brazo, y es un punto de referencia importante al estudiar los movimientos de la escápula.

Resulta de la unión de los bordes lateral y superior. Presenta dos formaciones de importancia: la cavidad glenoidea y la apófisis coracoides.

En los peces, la hoja escapular es una estructura unida a la superficie superior de la articulación de la aleta pectoral, y se acompaña de una placa coracoides similar sobre la superficie inferior. Aunque robusto en el pescado cartilaginoso, ambas placas son generalmente pequeños en la mayoría de otros peces, y pueden estar parcialmente cartilaginoso, o consistir múltiples elementos óseos.

En los primeros tetrápodos, estas dos estructuras, respectivamente, se convirtieron en la escápula y un hueso denominado el procoracoides (comúnmente llamado simplemente el "coracoides", pero no homóloga con la estructura de mamífero de ese nombre). En los anfibios y reptiles (aves incluidas), estos dos huesos son distintos, pero juntos forman una estructura única que lleva muchas de las inserciones musculares de la extremidad anterior. En estos animales, la escápula suele ser un plato relativamente simple, carente de las proyecciones y la columna vertebral que posee en los mamíferos. Sin embargo, la estructura detallada de estos huesos varía considerablemente en grupos de vida. Por ejemplo, en ranas, los huesos procoracoides pueden ser arriostrados entre sí en parte inferior del animal para absorber el choque de aterrizaje, mientras que en las tortugas, la estructura combinada constituye una forma de Y con el fin de permitir la escápula para retener una conexión a la clavícula (que es parte de la cáscara). En las aves, los procoracoides ayudan a prepararse el ala contra la parte superior del esternón.

En los terápsidos fósiles, un tercer hueso, el verdadero coracoides, formó justo detrás del procoracoides. La estructura de tres huesos resultante todavía se ve en los monotremas modernos, pero en todos los demás mamíferos vivos, el procoracoides ha desaparecido, y el hueso coracoides ha fusionado con la escápula, para convertirse en la apófisis coracoides. Estos cambios están asociados a la marcha en posición vertical de los mamíferos, en comparación con la disposición del miembro más extenso de reptiles y anfibios; los músculos anteriormente unidos al procoracoides ya no son necesarios. La musculatura alterada también es responsable de la alteración en la forma del resto de la escápula; el margen delantero del hueso original se convirtió en la columna vertebral y acromion, de la cual el estante principal de la lámina del hombro surge como una nueva estructura.

En dinosaurios los principales huesos de la cintura pectoral eran la escápula (omóplato) y el coracoides, ambos de los cuales articula directamente con la clavícula. La clavícula estaba presente en los dinosaurios saurisquios pero en gran medida ausente en dinosaurios ornitisquios. El lugar en la escápula donde se articula con el húmero (hueso superior de la extremidad anterior), es el hueso llamado glenoideo. La escápula sirve como sitio de unión para los músculos de la espalda y las extremidades anteriores de un dinosaurio.




</doc>
<doc id="41653" url="https://es.wikipedia.org/wiki?curid=41653" title="(524) Fidelio">
(524) Fidelio

(524) Fidelio es un asteroide perteneciente al cinturón de asteroides descubierto el 14 de marzo de 1904 por Maximilian Franz Wolf desde el observatorio de Heidelberg-Königstuhl, Alemania.
Está nombrado por la ópera "Fidelio" del compositor alemán Ludwig van Beethoven (1770-1827).


</doc>
<doc id="41655" url="https://es.wikipedia.org/wiki?curid=41655" title="(530) Turandot">
(530) Turandot

(530) Turandot es un asteroide perteneciente al cinturón de asteroides descubierto el 11 de abril de 1904 por Maximilian Franz Wolf desde el observatorio de Heidelberg-Königstuhl, Alemania.
Está nombrado por Turandot, un personaje de la ópera homónima del compositor italiano Giacomo Puccini (1858-1924).


</doc>
<doc id="41656" url="https://es.wikipedia.org/wiki?curid=41656" title="(531) Zerlina">
(531) Zerlina

(531) Zerlina es un asteroide perteneciente al cinturón de asteroides descubierto el 12 de abril de 1904 por Maximilian Franz Wolf desde el observatorio de Heidelberg-Königstuhl, Alemania.
Está nombrado por Zerlina, un personaje de la ópera "Don Giovanni" del compositor austriaco Wolfgang Amadeus Mozart (1756-1791).


</doc>
<doc id="41657" url="https://es.wikipedia.org/wiki?curid=41657" title="(539) Pamina">
(539) Pamina

(539) Pamina es un asteroide perteneciente al cinturón de asteroides descubierto el 2 de agosto de 1904 por Maximilian Franz Wolf desde el observatorio de Heidelberg-Königstuhl, Alemania.
Está nombrado por Pamina, un personaje de la ópera "La flauta mágica" del compositor austriaco Wolfgang Amadeus Mozart (1756-1791).


</doc>
<doc id="41658" url="https://es.wikipedia.org/wiki?curid=41658" title="(540) Rosamunde">
(540) Rosamunde

(540) Rosamunde es un asteroide perteneciente al cinturón de asteroides descubierto el 3 de agosto de 1904 por Maximilian Franz Wolf desde el observatorio de Heidelberg-Königstuhl, Alemania.
Está nombrado por Rosamunde, un personaje de la ópera homónima del compositor austriaco Franz Peter Schubert (1797-1828).

Rosamunde forma parte de la familia asteroidal de Flora.


</doc>
<doc id="41660" url="https://es.wikipedia.org/wiki?curid=41660" title="(861) Aïda">
(861) Aïda

(861) Aïda es un asteroide perteneciente al cinturón de asteroides descubierto el 22 de enero de 1917 por Maximilian Franz Wolf desde el observatorio de Heidelberg-Königstuhl, Alemania.
Está nombrado por la ópera homónima del compositor italiano Giuseppe Verdi (1813-1901).


</doc>
<doc id="41662" url="https://es.wikipedia.org/wiki?curid=41662" title="Alejandría">
Alejandría

Alejandría (en árabe: الإسكندرية Al-ʼIskandariya, árabe egipcio: اسكندريه Isindireyya, , copto: Ⲣⲁⲕⲟⲧⲉ Rakotə) es una ciudad del norte de Egipto, en la zona más occidental del delta del Nilo, sobre una loma que separa el lago Mariout del mar Mediterráneo. Es capital de la gobernación del mismo nombre, y el principal puerto del país. Es la segunda ciudad más importante de Egipto tras El Cairo. Fundada por Alejandro Magno en el año 331 a. C. en una estratégica región portuaria, se convirtió en pocos años en el centro cultural del mundo antiguo.

Está asentada sobre una península y se extiende hasta la isla de Faros y por tierra firme se extiende al sur del puerto oriental. Esta parte continental está habitada por europeos mientras que en la parte de la península se encuentra el barrio egipcio. 

Desde la antigüedad han existido en Alejandría dos puertos. En 1870 se construyó una escollera, reformada en 1906, que ha ampliado el puerto occidental convirtiéndolo en el mejor del Mediterráneo oriental, que soporta el 80% del tráfico marítimo exterior de Egipto, ya que puede acoger hasta 250 buques de gran calado, y en donde está la terminal del oleoducto Suez-El Cairo-Alejandría, con una refinería de petróleo y el centro comercial, la aduana y numerosos almacenes. También se usa como base por los barcos pesqueros. El puerto oriental se ha convertido en puerto deportivo.

El edificio religioso más importante de la ciudad es la mezquita de Abu al-Abbas al-Mursi, un jeque murciano del , patrón de los pescadores alejandrinos.

En el año 332 a. C., Egipto estaba bajo el dominio persa. Ese mismo año, Alejandro Magno entró triunfante en Egipto como vencedor del rey persa Darío III y los egipcios lo aceptaron y lo aclamaron como libertador y lo proclamaron faraón. Hay que tener en cuenta además, que en Egipto había desde mucho tiempo atrás gran cantidad de colonias griegas y que por lo tanto no eran considerados extranjeros.

En abril de 331 a. C., fundó la ciudad que llevaría su nombre en un lugar del delta del Nilo, sobre un poblado llamado Rakotis habitado por un puñado de pescadores. La elección del emplazamiento fue muy afortunada pues estaba al abrigo de las variaciones que pudiera tener el río Nilo, y por otro lado, lo suficientemente cerca de su curso como para que pudiesen llegar a través de sus aguas las mercancías destinadas al puerto, a través de un canal que unía el río con el lago Mareotis y el puerto.

Al este de Alejandría en la antigüedad (donde ahora está la bahía de Abu Kir) hubo varias islas y pantanos donde desde el  a. C. existían importantes ciudades como Canopo y Heracleion. Esta última fue redescubierta recientemente bajo el agua.

El lugar estaba frente a una isla llamada Faro, que con el tiempo y las múltiples mejoras que se harían quedaría unida por un largo dique a la ciudad de Alejandro. El arquitecto que realizó esta obra se llamaba Dinócrates de Rodas. El dique tenía una longitud de siete estadios (185 m cada estadio), por lo que se le llamó Heptastadio (Επτασταδίων). La construcción del dique conformó dos puertos, a ambos lados: el Gran puerto hacia el este, el más importante; y el Puerto del buen regreso (Εύνοστος), al oeste, que es el que continúa utilizándose en la actualidad. 

En los amplios muelles del gran puerto atracaban barcos que habían surcado el Mediterráneo y el Atlántico. Traían mercancías que se apilaban en los muelles: lingotes de bronce de la península ibérica, barras de estaño de Bretaña, algodón de las Indias, sedas de China. El famoso faro construido en la isla de Faros por Sóstrato de Cnido, en 280 a. C., dispuso en su cúspide un fuego permanentemente alimentado que guiaba a los navegantes, hasta 1340, cuando fue destruida la edificación.

El arquitecto Dinócrates se ocupó también del trazado de la ciudad y lo hizo según un plan hipodámico, sistema que se venía utilizando desde el  a. C.: una gran plaza, una calle mayor de treinta metros de anchura y seis kilómetros de largo que atravesaba la ciudad, con calles paralelas y perpendiculares, cruzándose siempre en ángulo recto. Se construyeron barrios, semejantes a los que levantaron los españoles en las ciudades hispanoamericanas, las llamadas "cuadras". Las calles tenían conducciones de agua por cañerías. Administrativamente se dividió en cinco distritos, cada uno de los cuales llevó como primer apelativo una de las cinco primeras letras del alfabeto griego. Cuando Alejandro se marchó de Egipto para continuar sus luchas contra los persas dejó como administrador de Alejandría a Cleomenes de Naucratis.

Fue una ciudad opulenta. Los Ptolomeos construyeron un palacio de mármol con un gran jardín en el que había fuentes y estatuas. Al otro lado de ese jardín se levantaba otro edificio construido en mármol al que llamaban Museo (Μουσείον). Fue una innovación del rey Ptolomeo I Sóter y en él se reunía todo el saber de la época. El museo tenía una gran biblioteca. Cerca de este edificio se levantaba el templo de Serapis, el nuevo dios greco-egipcio. En el centro de la ciudad se hallaban la Asamblea, las plazas, los mercados, las basílicas, los baños, los gimnasios, los estadios y demás edificios públicos y necesarios para las costumbres de aquellos siglos. Los habitantes de esta magnífica ciudad eran en su mayoría griegos de todas las procedencias. También había una colonia judía y un barrio egipcio, de pescadores, el más pobre y abandonado de la gran urbe.

Alejandría se convirtió pronto en el centro de la cultura griega en la época helenística, pero aún con todo, solo se helenizó a las altas esferas y la administración; se mantuvo el arte y la arquitectura mayormente egipcia, con pequeños rasgos e influencias griegas. Era tan grande la separación cultural, que la única persona de toda la dinastía de Ptolomeo I que hablaba egipcio, la lengua de la mayor parte de su pueblo, fue Cleopatra, la última de los faraones. Tan importante llegó a ser y tan grandiosa que la llamaron "Alexandria ad Aegyptum", es decir, "Alejandría que está cerca de Egipto", perdiendo importancia el resto del país.

El escritor griego Plutarco (c. 46-125) que escribió la biografía de Alejandro Magno, cuenta cómo este se inspiró para tomar la determinación de fundar la ciudad en este sitio. Según parece, tuvo un sueño en el que se le apareció un anciano de cabellos muy blancos y que le recitaba insistentemente cierto pasaje de la Odisea: ""Hay a continuación una isla en el mar turbulento, delante de Egipto, que llaman Faros (Φάρος)"". Cuando se levantó quiso ir a la isla y se dio cuenta de su situación privilegiada y más aún si, por medio de un dique, se la unía a la costa. Entonces mandó traer harina para marcar él mismo el enclave de la futura Alejandría (pues no se disponía del yeso con que solía hacerse) y él mismo dibujó el círculo en forma de manto macedonio. No bien hubo terminado cuando empezaron a llegar desde el río y desde el mar pájaros grandes y diversos que se dedicaron a comer toda la harina esparcida. Cuando vio lo que estaba ocurriendo, Alejandro se turbó muy preocupado pensando que se trataba de un mal augurio. Pero Aristandro, el vidente que lo acompañaba supo interpretar el buen augurio y que el proceder de los pájaros pronosticaba que la ciudad sería tan rica y próspera que podría nutrir a todos los hombres de todas las razas.

Ptolomeo I mandó construir el gran palacio que serviría de alojamiento a toda la dinastía ptolemaica. Su hijo, Ptolomeo II Filadelfos fue el impulsor y creador del edificio levantado al otro lado del jardín y conocido desde el principio con el nombre de museo. Le llamaron así por respeto a la sabiduría, porque lo consideraron como un santuario consagrado a las musas, que eran las diosas de las artes y de las ciencias. Se considera como el establecimiento científico más antiguo del mundo, con una Universidad de enseñanza superior.

El edificio constaba de varios apartados dedicados al saber, que con el tiempo fueron ampliándose y tomando gran importancia. Uno de esos apartados se dedicó a biblioteca y fue quizás el que más creció y el que más fama adquirió en el mundo de la antigüedad. Había también un jardín botánico con plantas de todos los países conocidos, una colección zoológica, un observatorio astronómico y una sala de anatomía donde se hacía la vivisección en cuerpos de criminales y donde, durante algún tiempo, se llegaron a disecar cadáveres. Contenía habitaciones a modo de residencia para sabios, gramáticos y médicos y todos los gastos corrían por cuenta de los reyes que estaban orgullosos de esta institución y comían muchas veces allí en su compañía. Los sabios además de investigar y estudiar, daban conferencias y lecciones a los jóvenes que quisieran aprender. En Alejandría llegó a haber hasta 14.000 estudiantes. Allí vivieron los famosos gramáticos alejandrinos que determinaron las leyes de la retórica y la gramática, los famosos geógrafos que diseñaron mapas del mundo y los famosos filósofos cuyo grupo acabó fundando una especie de religión.

Entre los grupos de sabios se encontraban personajes tan famosos en la Historia como Arquímedes (ciudadano de Siracusa), Euclides, que desarrolló allí su geometría; Hiparco de Nicea, que explicó a todos la trigonometría y defendió la visión geocéntrica del Universo, enseñó que las estrellas tienen vida, que nacen y después se van desplazando a lo largo de los siglos y finalmente, mueren; Aristarco de Samos, que defendió todo lo contrario, es decir, el sistema heliocéntrico (movimiento de la Tierra y los demás planetas alrededor del Sol); Eratóstenes, que escribió una geografía y compuso un mapa bastante exacto de "el mundo conocido", consiguiendo medir la circunferencia terrestre con un error inferior al 1%; Herófilo de Calcedonia, un fisiólogo que llegó a la conclusión de que la inteligencia está en el cerebro y no en el corazón; Apolonio de Pérgamo, gran matemático; Herón de Alejandría, un inventor de cajas de engranajes y también de unos aparatos de vapor asombrosos (es el autor de la obra "Autómatas", la primera obra que conocemos en el mundo sobre los robots), etc. 
Más tarde, ya en el , allí mismo trabajaron y estudiaron el astrónomo y geógrafo Claudio Ptolomeo y el médico Galeno, que escribió bastantes obras sobre el arte de la curación y sobre la anatomía; sus enseñanzas y sus teorías fueron seguidas hasta muy entrado el Renacimiento. 

En el  a. C. nació en este templo del saber una nueva ciencia: la alquimia, basada en la sabiduría y conocimientos de los egipcios sobre las sustancias materiales y en las teorías griegas sobre los elementos. Esta ciencia fue el embrión de lo que siglos más tarde sería la química, cuyas bases como ciencia experimental sentó Antoine Laurent Lavoisier.

La denominación Escuela de Alejandría, en filosofía, se ha empleado en tres sentidos:

Se denomina así a una corriente de la filosofía neoplatónica que se desarrolló entre los siglos y en la ciudad, con ciertos vínculos con la escuela neoplatónica ateniense, caracterizada por la erudición, el sincretismo y el eclecticismo, incluso tendiendo puentes con el cristianismo, alternando con estallidos de violencia. A ella pertenecen, entre otros, Olimpiodoro e Hipatia.

Se da también este nombre a la filosofía de Filón de Alejandría, judío, que en el a. C. interpretó la Biblia aplicando los métodos del platonismo y del estoicismo. Es la escuela filosófica seguida por los pensadores cristianos alejandrinos o vinculados a esta ciudad, de los siglos y , cuyas ideas tienen una poderosa influencia en toda la teología del cristianismo primitivo. Los principales representantes de esta llamada escuela catequística de Alejandría fueron Clemente de Alejandría (filósofo cristiano griego) y Orígenes (Padre de la Iglesia, alejandrino). Según el "Diccionario filosófico" de Mark Rosental y Pavel Iudin, es a esta escuela de Filón y los primeros cristianos alejandrinos a la que corresponde en rigor el apelativo de escuela de Alejandría.

En un sentido más amplio, hasta comienzos del recibieron el nombre de escuela de Alejandría las escuelas filosóficas, científicas, geográficas y lingüísticas que surgieron y se desarrollaron principalmente en la ciudad durante los primeros siglos e influyeron en otras ciudades con ideas eclécticas y del neopitagorismo pagano tanto como el conjunto de las corrientes neoplatónica y el pensamiento filosófico del judaísmo y del cristianismo primitivo. En ese conjunto muy diverso de ideas y tendencias destacaron Aristarco de Samos (astrónomo y matemático griego), Hiparco de Nicea (astrónomo, geógrafo y matemático griego), Claudio Ptolomeo (geógrafo y matemático greco-egipcio), Diofanto (matemático greco-egipcio), Eratóstenes (matemático, astrónomo y geógrafo griego, quien calculó la circunferencia de la Tierra), Ammonio Saccas (fundador del neoplatonismo ateniense), Filón de Alejandría (filósofo judío greco-egipcio) y Clemente de Alejandría.

Julio César, a petición de Cleopatra, tomó el control de las fuerzas de la ciudad (Sitio de Alejandría (47 a. C.)), para zanjar la guerra dinástica entre la reina y su hermano (y corregente) Ptolomeo XIII. Durante la batalla en el mar se produjo un incendio en Alejandría, en el cual ardieron algunos almacenes de libros en el puerto, pero no esta claro si llegó a la Gran Biblioteca. Después de asegurar a Cleopatra en el trono egipcio y casarla estratégicamente con su hermano mucho menor, Ptolomeo XIV, Julio César regresó a Roma. Julio Cesar y Cleopatra mantenían un estrecho vínculo amoroso, y dejaron descendencia, Cesarión. A la muerte de Julio César, le sucede su sobrino e hijo adoptivo, Octavio Augusto. Cleopatra y Marco Antonio inician un vínculo amoroso, y paralelamente se libra una disputa entre Marco Antonio y Octavio, que pretendía el reino de Egipto (repercutiendo en la Batalla de Alejandría (30 a. C.)). Derrotado, muere Marco Antonio, y Octavio visita a Cleopatra. La reina, al ver que no tenía posibilidades de presentar batalla, se suicida, haciéndose morder por una serpiente. De esta manera, Octavio toma de la ciudad en el año 30 a. C., y convirtió Egipto (último bastión no romano del "Mundo conocido"), en provincia romana, y en propiedad particular suya, acabando así con la independencia del país. 

Los romanos convirtieron al país en el granero del Imperio, con lo que aumentó la importancia de la ciudad, en cuyos almacenes debía depositarse toda la cosecha: cada año, debía enviarse a Roma una cantidad de trigo que era el equivalente a la tercera parte de su abastecimiento, cantidad y precio que se fijaba en la bolsa de Alejandría por la "annona" egipcia. Para mantener aislado al país, se prohibió el uso de la moneda romana, que debía cambiarse por la local de Alejandría. Todos estas disposiciones convirtieron a la ciudad en una próspera metrópolis con varios cientos de miles de habitantes, cosmopolita y centro financiero de la zona.

Durante el período romano la ciudad experimentó numerosos desastres militares: sufrió parte de la guerra de Kitos en el 117 durante Trajano, luego fue saqueada por un capricho de Caracalla en 215, después destrozada por Valeriano en 253, posteriormente conquistada por las tropas de Zenobia, reina de Palmira en 269, y reconquistada por Aureliano en 273, quien saqueó y destruyó completamente el Bruchión, desastre que dañó el Museo y la Biblioteca (se dice que en aquella ocasión los sabios griegos se refugiaron en el Serapeo, que nunca sufrió con tales desastres, y otros emigraron a Bizancio). Finalmente, en 297 la revuelta del usurpador Lucio Domicio Domiciano acabó con Alejandría, tomada y saqueada por las tropas de Diocleciano, tras ocho meses de asedio (victoria conmemorada por el llamado «Pilar de Pompeyo»). Se dice que tras la capitulación de la ciudad, Diocleciano ordenó que la carnicería continuara hasta que la sangre llegara a las rodillas de su caballo, librando a los alejandrinos de la muerte la caída accidental de este, al resbalar en un charco de sangre.
Además hubo en el período varios desastres naturales.Particularmente devastador fue el terremoto de Creta en julio del 365, que fue seguido horas después de un tsunami que devastó particularmente en las costas de Libia y Alejandría. Hubo alrededor de 50,000 víctimas. El equipo de Franck Goddio del "Institut Européen d´Archéologie Sous-Marine", ha encontrado en el fondo de las aguas del puerto cientos de objetos y pedazos de columnas que demuestran que al menos el veinte por ciento de la ciudad de los Ptolomeos se hundió en las aguas, incluyendo el Bruchión, supuesto enclave de la Biblioteca.

En 616 los persas de Cosroes II tomaron la ciudad.

Una tradición muy antigua asegura que el primer cristiano que llegó a Alejandría para predicar la nueva religión fue san Marcos. Esto sucedía en el año 61 después de Cristo. La misma tradición cuenta que el primer cristiano convertido fue Aniano, de oficio, zapatero. San Marcos le curó la herida de una mano y al mismo tiempo le habló del significado del cristianismo. Desde esos tiempos de predicación, los cristianos de Alejandría y del resto de Egipto mantuvieron una gran tradición evangélica. San Marcos fue perseguido bajo el mandato del emperador Nerón y en el año 68 fue martirizado y muerto. Desde entonces hasta la época del emperador Trajano (comienzos del ), los cristianos tuvieron que ocultar sus creencias, amenazados por las persecuciones. A partir de este momento se les permitió con tolerancia extenderse por toda la ciudad de Alejandría y poco a poco, a lo largo de todo el valle del Nilo.

En el , Panteno y, posteriormente, Clemente de Alejandría y su discípulo Orígenes establecieron en esta ciudad un verdadero semillero de teólogos, hasta tal punto que el resto de la cristiandad les miraba con cierto recelo. Es la que se conoce como Escuela catequística de Alejandría. Al llegar al , con el emperador Constantino I el Grande, existían graves disensiones cristianas en el norte de África y en Alejandría. Las tensiones con el resto de la comunidad cristiana condujeron al cisma con la aparición además del presbítero Arrio y su doctrina el arrianismo. Por esta razón, el emperador convocó el concilio de Nicea, donde se establecieron las bases del credo (declaración resumida de la fe católica).

Por otra parte, se desencadenó una abierta rivalidad entre las dos ciudades más importantes del momento: Constantinopla y Alejandría. Esta rivalidad afectó bastante a los eternos debates teológicos sobre la naturaleza o naturalezas de Cristo. Era la “guerra” entre los monofisitas y los ortodoxos de Calcedonia.

Pero las luchas y disputas entre cristianos continuaron sin remedio y ya en el , en el año 553, en el segundo concilio de Constantinopla, con el emperador romano Justiniano I al frente, fue declarada herética la ortodoxia de los cristianos de Alejandría que seguían enfrentados a los cristianos de Calcedonia. En los últimos años de mandato de este emperador, los monofisitas de Siria empezaron a organizar su iglesia separada del resto de los cristianos, con una estructura propia.

Cuando el pueblo árabe musulmán llegó en plan de conquista a Egipto en el 641 dieron el nombre de "qubt" al cristiano de Alejandría. Esta es la palabra que nosotros conocemos como copto. El símbolo de la cruz de Cristo se empezó a emplear en Alejandría, entre los cristianos coptos, fue una costumbre que nació allí; se sabe que no existía en las catacumbas ni en el lábaro de Constantino que llevaba un crismón.

Los cristianos coptos son la mayor religión en esta región del norte de Egipto, aunque en el , los misioneros católicos y protestantes convirtieron algunos de los seguidores ortodoxos a sus respectivas iglesias.

Alejandría seguía siendo una de las mayores metrópolis mediterráneas en el momento de la conquista musulmana. Su patriarca, Ciro, capituló ante los invasores en abril de 641, al ser derrotadas las fuerzas imperiales locales. Sin embargo, el gobierno imperial no reconoció la capitulación, y sus habitantes se alzaron contra el yugo musulmán. Tras catorce meses de asedio, la ciudad fue conquistada por los musulmanes a finales de 642. El historiador Eutiquio cita una carta escrita el viernes de la luna nueva de Moharram del año vigésimo de la Hégira donde el comandante musulmán Amr ibn al-As, al entrar en la ciudad, se dirigió al segundo sucesor de Mahoma, el califa Umar ibn al-Jattab e hizo un inventario de lo encontrado en la ciudad de Alejandría: «4.000 palacios, 4.000 baños, 12.000 mercaderes de aceite, 12.000 jardineros, 40.000 judíos y 400 teatros y lugares de esparcimiento». El cronista Ibn al-Kifti afirmó en su "Crónica de los sabios" que en aquel momento fue destruida la Gran Biblioteca. Aunque los árabes pudieran destruir numerosos libros, lo cierto es que ni la Biblioteca ni la biblioteca-hija del Serapeo existían ya por entonces, víctimas de las guerras civiles entre romanos, de los desastres naturales y el fanatismo de los coptos.

Una flota imperial desembarcó en la ciudad a comienzos de 645 para reconquistar Egipto, pero el ejército que transportaba fue derrotado por las superiores fuerzas árabes, y acabó por retirarse. Tras un nuevo y largo asedio, en 646 los árabes tomaron la ciudad por tercera vez, destruyéndola en buena parte para evitar que los bizantinos volvieran a atrincherarse en ella vía marítima. Acabaron así 975 años de pertenencia al mundo grecolatino.

Durante un intervalo, entre 811 y 827, la ciudad estuvo en manos de piratas andalusíes, en cierto modo antecedentes de los almogávares, para retornar a manos árabes. En 828, el cadáver de San Marcos fue recuperado de la ciudad por navegantes venecianos, que lo depositaron en la Basílica de San Marcos, construida expresamente para albergar sus restos. 

Tras un largo declive, Alejandría resurgió como gran metrópoli en la época de las Cruzadas y vivió un período floreciente gracias al comercio, con convenios con los aragoneses, genoveses y venecianos que distribuían los productos llegados de Oriente a través del mar Rojo. En 1365 la ciudad fue brutalmente saqueada tras ser tomada por los cruzados dirigidos por el rey Pedro I de Chipre. En los siglos y , Venecia eliminó a la competencia y su almacén alejandrino se convirtió en el centro de la distribución de especias hasta que los portugueses abrieron la ruta del Cabo en 1498, fecha que marca el declive comercial, agravado por la invasión turca. Cuando Napoleón entró en la ciudad, era un pueblo medio arruinado de sólo 7000 habitantes. Mehmet Alí la reconstruyó en el , convirtiéndose nuevamente en el gran puerto egipcio. 

El 19 de mayo de 1798, los franceses salen de Toulon con más de 400 barcos, hombres y marineros y llegan a Alejandría el 30 de junio, el desembarco de tropas se hizo en barcas de remo hasta la noche por lo que se coloca su flota en la amplia bahía de Abukir, donde es destruida por Nelson un mes más tarde, tomando los ingleses el control del puerto Alejandría y dando fin a la expedición francesa.

La flota británica bombardeó el puerto en el año 1882, lo que provocó un gran incendio y el saqueo de las ruinas por parte de los beduinos. Al cabo de un mes desembarcó un gran ejército británico que restauró el orden y dio inicio el protectorado británico sobre Egipto.

Los papiros de Elefantina nos dan información acerca de la vida de la comunidad judía asentada en la zona de Alejandría tras la toma de Jerusalén en 586 a. C. por Nabucodonosor II, aunque existen datos de asentamientos en época de Manasés. Desde los reyes lágidas, los judíos de la Diáspora se establecieron en la ciudad atraídos por el Museo, protegidos por la tolerancia del mundo pagano en materia de diversidad religiosa, y crearon un activo foco intelectual con un centro de estudios hebraicos.

Los judíos gozaban de todos los derechos civiles, como cualquier ciudadano griego, pero mantenían las prerrogativas concedidas por los reyes persas, y constituían una comunidad política independiente y autónoma, limitada solo por la subordinación a los Ptolomeos primero y a los romanos después. A su frente tenían los cargos de las comunidades de la diáspora: arcontes, que regían los asuntos administrativos y judiciales, y el archisinagogo a quien correspondía todo lo referente al culto, además de un etnarca con grandes poderes civiles que le permitían tratar con los funcionarios de Egipto o del Imperio romano. Constituyeron así un grupo étnico apartado de la población de Alejandría, con un aislamiento lingüístico, económico y cultural que les permitió conservar su identidad y religión, fieles a la ley y a las tradiciones ancestrales.
Los romanos, que antes del Imperio habían sido aliados de los judíos, les otorgaron algunos privilegios más, como la celebración del "shabat". Sin embargo, el sentimiento antijudío fue alentado por los escritores griegos alejandrinos, que les acusaban de exclusivismo, grosería y deslealtad. 

Probablemente a los egipcios les irritaba la tolerancia que el imperio había otorgado a los judíos, y no faltaba entre ellos el descontento por la dominación foránea, primero griega y luego romana. Ese resentimiento se tradujo en una xenofobia que terminó por descargarse contra el pueblo hebreo. Esto, más la envidia social frente al florecimiento de esa colectividad, fue caldo de cultivo para las primeras agresiones escritas, como las de Apión, iniciador de las agitaciones antijudías que el año 38 provocaron que decenas de miles de judíos fueran asesinados. Dos personajes se enfrentaron a Apión: Flavio Josefo, que tituló una de sus obras "Contra Apión", y el filósofo "Filón de Alejandría", que encabezó una delegación para entrevistarse con Calígula, intentando acabar con la violencia en la ciudad. 

La negativa judía a practicar el culto oficial al Emperador, junto a las dos revueltas judías, provocó la hostilidad romana y diezmó la población judía en Alejandría (al igual que en Jerusalén), que constituía un 40% de la ciudad hasta el  d. C. Las relaciones entre judíos y griegos siguieron siendo tensas y Alejandría se convirtió paulatinamente en un foco de antisemitismo. El mismo Lisímaco, director de la Biblioteca de Alejandría, fue uno de los instigadores de desórdenes contra los judíos. Aunque en los siglos siguientes Egipto fue casi siempre un lugar relativamente seguro para los judíos, Alejandría conservó su tradición antisemita y se producían brotes esporádicos antijudíos. 

Helenizados en la época macedónica, tuvieron una gran influencia sobre sus correligionarios en la época de los seleúcidas y asmoneos. Tradujeron al griego la Biblia, la llamada versión de los setenta o Septuaginta en los siglos y  a. C., además de producir una abundante literatura hebrea en lengua griega: epopeyas, dramas, obras moralizantes. Las más conocidas son la Carta de Aristea, los Oráculos sibilinos, el Libro de la Sabiduría de Salomón. Entre los autores conocidos, se puede citar a Eupolemo, Artipon Demetrio, Aristeo y Filón.

La que se llamó escuela judía de Alejandría está fuertemente influenciada por la filosofía griega. Al estudiar esta filosofía encuentran conceptos espirituales y morales que desean conciliar con la Ley mosaica, considerando esta ley como fuente en la que se inspiraron aquellos filósofos, especialmente Platón. El método para demostrar esta identidad fue la interpretación alegórica, ya conocida por los judíos de Palestina y muy estimada en los ambientes griegos. 

El primer representante conocido es Aristóbulo, del que sólo se sabe que era vecino de Alejandría en tiempos de Ptolomeo VI Filometer. Explica alegóricamente los pasajes bíblicos, limando las dificultades que presentan la Biblia y los mitos griegos. Filón, contemporáneo de Jesucristo, dedica su obra a unir sistemáticamente las ideas judías y griegas, y es el predecesor del neoplatonismo de Plotino y de gran parte de las ideas de los Padres de la Iglesia. 

La Escuela exegética de Alejandría, que intenta hermanar la filosofía griega y el cristianismo, se considera sucesora de la judía.

La persecución contra las religiones monoteístas emprendida por los emperadores romanos acabó con esta actividad literaria.

A principios de la década de 1940, tras siglos de convivencia relativamente pacífica como dhimmíes, los judíos comenzaron a sufrir persecuciones y atentados en todo Egipto. Tras la independencia de Israel y la subsiguiente guerra árabe-israelí de 1948, los cerca de cien mil judíos egipcios quedaron bajo sospecha y la hostilidad contra ellos fue en aumento. La situación se agravó aún más tras la crisis de Suez: cerca de judíos fueron expulsados y sus bienes y tierras confiscados. La mayor parte se refugiaron en la vecina Israel, aunque otros emigraron a Francia y a América. En solo unos años se extinguió la presencia milenaria de judíos en Egipto, incluidas comunidades judías antiquísimas como la de Alejandría, muy anteriores a la arabización e islamización de esas tierras.

El 11 de junio de 1882, estalla en Alejandría un movimiento xenófobo que se extiende a otras ciudades del delta del Nilo y en el transcurso del cual son asesinados 200 extranjeros. El conflicto tuvo su origen en la entrada de barcos ingleses y franceses en el puerto de Alejandría para oponerse al derrocamiento del Jedive Tewfik. El 11 de julio de 1882, una escuadra británica abre fuego sobre la ciudad y la ocupa. El 13 de septiembre de 1882, Egipto es declarado protectorado británico, "statu quo" que se mantuvo hasta 1946.

La Alejandría del es una ciudad moderna, con un trazado en cuadrícula (plan hipodámico), al estilo griego, o europeo del , que difiere de las laberínticas ciudades islámicas. Es un centro del comercio del algodón, principal producto agrícola del país, y con un importante núcleo de industrias textiles, químicas, de construcción mecánica y naval y centro bancario. Su aeropuerto es el segundo de Egipto, con un gran tráfico internacional.

El Plan Toshka o "New Valley", inaugurado en enero de 1997, cuya finalidad es hacer un delta alternativo paralelo al valle del Nilo que recuperará tierras del desierto, ampliará sus perspectivas de negocio. 

La comunidad internacional, por medio de la Unesco, ha financiado el Proyecto de Reconstrucción de la Antigua Biblioteca de Alejandría: la Bibliotheca Alexandrina, que tiene un centro de conferencias, un museo de las ciencias, un planetario, un centro de estudios y el Instituto Caligráfico y Museo. Ocupa un área de y guarda 8 millones de libros, manuscritos antiguos y libros raros, además de material electrónico y audiovisual y bases de datos.

Aunque por la proximidad al mar las costumbres en el vestir resultan un poco más relajadas que en El Cairo, sigue existiendo algo de puritanismo en los espacios públicos, como la playa o los cafés, solo con presencia masculina. La oración es respetada, y el alcohol, que en Cairo resulta habitual, es raro. Con el buen tiempo, los habitantes están en la calle: bien en la corniche (20 kilómetros de paseo marítimo), bien en los cafés jugando al dominó y fumando la tradicional pipa de agua, o de compras: desde la plaza de Mohammed Alí hacia el interior, toda la ciudad es una mezcla de zoco tradicional y centro comercial moderno. Al final de la playa, el fuerte de Qaitbey, que aloja el museo naval y una mezquita cuyo minarete fue destruido por los británicos en el , se ha convertido en un centro de reuniones, desde donde se contempla la ciudad y el mar y se puede tomar el té en alguna de las ventanas.

La cosmopolita y occidentalizada vida de la Alejandría de principios del , cuando el 60% de la población la formaban coptos, griegos, armenios, judíos, británicos e italianos, desapareció a partir de la proclamación de la república y de la crisis de Suez. La emigración de la mayor parte de las comunidades griega, europea y judía acabó con el carácter más cosmopolita de la ciudad. En los últimos años han aparecido conflictos interreligiosos entre musulmanes radicales y cristianos coptos (12% de la población).

Desde el año 2004 se celebra la Feria Internacional del Libro de Alejandría, siendo la segunda más importante por número de visitantes y expositores de Egipto, hospedándose en el recinto cultural de la Bibliotheca Alexandrina.

Aquellos antiguos monumentos de que habla la historia de Alejandría desaparecieron casi todos; solo de algunos han llegado hasta nuestros días restos y ruinas desperdigados:












</doc>
