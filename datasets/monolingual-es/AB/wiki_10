<doc id="29586" url="https://es.wikipedia.org/wiki?curid=29586" title="Fernando de la Rúa">
Fernando de la Rúa

Fernando de la Rúa (Córdoba; 15 de septiembre de 1937-Loma Verde; 9 de julio de 2019) fue un abogado, profesor universitario de derecho procesal y político argentino, presidente de la Nación Argentina entre 1999 y 2001, primer Jefe de Gobierno de la Ciudad Autónoma de Buenos Aires entre 1996 y 1999, senador nacional por la Capital Federal en los períodos 1973-1976, 1983-1989 y 1992-1996 y presidente del Comité Nacional de la Unión Cívica Radical entre 1997 y 1999.

De la Rúa asumió como presidente el 10 de diciembre de 1999, luego de vencer en las elecciones presidenciales apoyado por la Alianza para el Trabajo, la Justicia y la Educación, denominada simplemente como «La Alianza»; coalición entre la UCR y la formación de centroizquierda y sectores peronistas FREPASO. De la Rúa anunció su programa de gobierno mediante una «Carta a los Argentinos», en donde se comprometió a mantener el sistema monetario imperante desde 1991 bajo la Ley de Convertibilidad del Austral, que establecía una paridad entre el peso argentino y el dólar estadounidense, a luchar contra «la corrupción inherente a la concentración del poder y la debilidad de los controles», generar una «cultura exportadora» con mayor valor agregado, profundizar en el Mercosur, tener una tasa de inversión no menor al 30% financiada con recursos propios y colocar a la educación como «eje central de la transformación». 

Ya en gobierno, sin embargo, el agravamiento de la crisis económica iniciada en 1998, la creciente demostración de insostenibilidad de la convertibilidad a largo plazo y las medidas económicas procíclicas tomadas al inicio de su gobierno, llevaron a su gobierno a depender cada vez más de la toma de deuda externa —principalmente por parte del Fondo Monetario Internacional (FMI)— a fin de sustentar el cambio fijo; estas medidas llevaron a un deterioro rápido y persistente de la economía y los indicadores sociales. Durante el período temprano de su presidencia se destacaron la intervención federal de la provincia de Corrientes a fines de 1999 —la cual tuvo fuerte represión en la que fueron muertas dos personas y heridas otras cincuenta— y el anuncio del primer plan económico que incluyó la célebre Tablita de Machinea y un sonado escándalo por posibles sobornos en el Senado de la Nación para lograr la aprobación de la Ley de Reforma Laboral —que motivó la renuncia de su vicepresidente, Carlos Álvarez, en octubre del 2000—. Más tarde en su gobierno se destacaron las dos grandes operaciones financieras internacionales conocidas como «Blindaje» y «Megacanje», la designación como ministro de Economía de Domingo Cavallo —quien había ocupado el mismo cargo durante el gobierno de Carlos Ménem y contribuyó a la Ley de convertibilidad del Austral— y el «Decreto de Necesidad y Urgencia» —el cual fue propuesto por la entonces ministra de Trabajo Patricia Bullrich, el cual rebajó un 13% las jubilaciones y los salarios de la administración pública (incluidos los docentes) generalizando un movimiento piquetero de desocupados en contra de las medidas del gobierno—.

La situación del país empeoró visiblemente en 2001, agravándose la crisis económica y social, hasta convertirse en una crisis política e institucional. El plano político terminó de deteriorarse con la derrota abrumadora de «La Alianza» en las elecciones legislativas de medio término de ese mismo año, en las que una enorme proporción del electorado emitió votos en blanco o anulados. El 2 de diciembre, una impopular disposición del gobierno, conocida como «Corralito», que restringía la extracción de dinero en efectivo de los bancos y la cual fue diseñada por el ministro Cavallo, terminó provocando un estallido social generalizado, con manifestaciones, bloqueos de rutas y calles, ataques a bancos y saqueos a supermercados en las principales ciudades del país. El 19 de diciembre, De la Rúa anunció por televisión el Estado de sitio e inmediatamente después de finalizado el anuncio miles de personas salieron a la calle con el lema «Que se vayan todos». Al día siguiente las manifestaciones populares continuaban y fueron reprimidas por las fuerzas de seguridad con un saldo de treinta y nueve víctimas fatales en distintos puntos del país. Esa tarde De la Rúa presentó su renuncia a la Presidencia, abriendo un período de dos semanas de alta inestabilidad política y caos social, durante las cuales cuatro funcionarios estuvieron a cargo del , dos de ellos con el título de presidente de la Nación (Adolfo Rodríguez Saá y Eduardo Duhalde). 

De la Rúa tras su renuncia se retiró de la vida pública y política y evitó realizar declaraciones públicas. Continuó siendo una figura controvertida por su papel en la crisis económica de 2001, al igual que por la represión policial durante el estallido social posterior y los eventuales actos de corrupción durante su gobierno, que lo llevaron a ser incluido en varias causas judiciales en su contra, en ninguna de las cuales resultó condenado. Falleció el 9 de julio de 2019, a la edad de 81 años.

Fernando de la Rúa nació el 15 de septiembre de 1937, en la ciudad de Córdoba, capital de la provincia homónima, en el seno de una familia de clase media-alta. La familia De la Rúa estaba muy ligada a la política argentina en más de una provincia y varios de sus miembros habían integrado previamente la Unión Cívica Radical. Su padre, Antonio de la Rúa Catani (1905-1979), era descendiente de gallegos y provenía de Santiago del Estero, habiéndose instalado en la provincia de Córdoba para estudiar abogacía; fue militante radical desde muy joven, llegando incluso a ser legislador y funcionario durante la de Amadeo Sabattini. Su madre, Eleonora Felisa Bruno Boeri (1908-1999), era hija de inmigrantes italianos. De la Rúa tuvo un hermano, Jorge de la Rúa, cinco años menor que él, que ejercería como funcionario durante la gobernación de Eduardo Angeloz y posteriormente como secretario y ministro durante la presidencia de su hermano. Un primo hermano suyo, Vicente Rodríguez de la Rúa, fue candidato por un sublema radical a en las elecciones de 1999, en una misma boleta que lo contenía como candidato presidencial.

De la Rúa asistió a la escuela primaria Olmos, y completó sus estudios secundarios en el Liceo Militar General Paz, manteniendo promedios muy altos de 9,53 en el primer año y 9,92 en el último, lo que le permitió ser abanderado. Manifestó sus inclinaciones políticas iniciales en ese período como opositor al gobierno de Juan Domingo Perón y al movimiento justicialista, sobre los cuales escribió sátiras bajo el seudónimo de «Lauchín». Quienes lo conocieron en su adolescencia lo describieron como una persona extrovertida y risueña, con una personalidad muy diferente al perfil serio y moderado que mantendría durante toda su carrera política. Se afilió al radicalismo al alcanzar la mayoría de edad, en 1955, instigado por su tío materno Víctor Arraigada, aproximadamente cuando ocurrió el golpe de estado de septiembre de 1955 que derrocó a Perón e instauró una dictadura militar, la cual proscribió al justicialismo. De la Rúa se mantuvo ligado a los sectores más antiperonistas y derechistas de la UCR, y cuando el partido se dividió en 1957 permaneció en la Unión Cívica Radical del Pueblo (UCRP), manteniéndose así hasta que esta obtuvo nuevamente el título «Unión Cívica Radical» en junio de 1972. Además de su filiación radical, desde su niñez De la Rúa fue fervientemente católico y tuvo participación activa en grupos religiosos como el Movimiento Católico de Juventudes y la Juventud Católica de Córdoba.

Aunque inicialmente tenía la intención de estudiar medicina, finalmente resolvió convertirse en abogado, instigado mayormente por su padre. Completó su carrera rápidamente y se recibió con medalla de honor en la Universidad Nacional de Córdoba en 1958, a la temprana edad de veintiún años. Ejerció brevemente su profesión desde un estudio jurídico instalado en la casa de su familia en Obispo Trejo 636, Nueva Córdoba, pero posteriormente lo abandonó para comenzar a trabajar como docente en la universidad.

En 1960, con alrededor de veintitrés años, De la Rúa viajó a Italia a hacer una beca en derecho procesal, permaneciendo en dicho país por meses, durante los cuales aprendió italiano y alemán. Durante este viaje se mantuvo en compañía de Juan Carlos Palmero, hijo de Juan Palmero, quien sería más tarde ministro de Gobierno del presidente de la Nación Arturo Umberto Illia (1963-1966). Por medio de esta amistad De la Rúa ingresó por primera vez a la función pública como miembro del equipo de asesores de Palmero y funcionario del gobierno de Illia. De la Rúa era la figura más joven del gobierno (contaba por entonces con veintiséis años) y su aspecto era marcadamente juvenil, lo que le valió el apodo de «Chupete», aunque algunas fuentes citan que obtuvo ese apodo mucho después, al momento de ser elegido senador por primera vez. A pesar de su origen cordobés, De la Rúa desarrolló la totalidad de su carrera política posterior en la Ciudad de Buenos Aires, de la que sería elegido representante legislativo cuatro veces. Sobre su experiencia en este período, De la Rúa declaró:
Su primer período como funcionario terminó abruptamente con el golpe de Eestado del 28 de junio de 1966, que instauró una dictadura militar de tipo permanente, la cual proscribió la actividad política. Durante este intervalo, en 1970, De la Rúa se casó con Inés Pertiné, con quien tuvo tres hijos: Agustina, Antonio y Fernando de la Rúa. La herencia de su esposa y la suya, sumada a su ejercicio destacado de la abogacía, le permitieron acumular una considerable riqueza.

En 1973, con la apertura democrática que condujo a la legalización del peronismo y el llamado a elecciones libres, De la Rúa (que pertenecía al sector de la UCR ligado a Ricardo Balbín) se postuló como candidato del radicalismo a segundo Senador Nacional en representación de la Capital Federal. La dictadura gobernante había instaurado un sistema de segunda vuelta electoral o balotaje para la elección tanto de cargos ejecutivos (presidente y gobernadores), como de senadores, instaurando por primera vez la figura de dos senadores por la mayoría y uno por la minoría. Una lista senatorial debía obtener mayoría absoluta de votos (50% más uno) para quedarse con las dos bancas por la mayoría, mientras que la banca restante correspondería a la lista que le siguiera en sufragios. En caso de que la lista más votada no lograse la mayoría, se consideraría virtualmente electos a los dos candidatos que encabezaran las dos listas senatoriales más votadas, y se realizaría una segunda vuelta entre los segundos candidatos de ambas listas, resultando electo segundo senador aquel que obtuviese más votos. Aunque De la Rúa contaba para entonces con una actuación meritoria dentro de su partido, su candidatura correspondió más bien al rechazo de figuras de mayor peso del balbinismo a presentarse, creyendo que tenían pocas expectativas de vencer al peronismo.
La primera vuelta de las elecciones tuvo lugar el 11 de marzo de 1973. La lista del Frente Justicialista de Liberación (FREJULI), que postulaba como candidato presidencial a Héctor José Cámpora (debido a una estratagema legal empleada por la dictadura para impedir la candidatura del propio Perón), obtuvo la victoria a nivel nacional. En Capital Federal, la lista frejulista para el Senado encabezada por Alejandro Manuel Díaz Bialet obtuvo la primera minoría de votos, seguido por la lista radical encabezada por Raúl Jorge Zariello, resultando de este modo elegidos los dos como senadores. El FREJULI había superado a la UCR por más de diez puntos, en medio de la atomización de la elección porteña en varios partidos no peronistas que tuvieron su mayor despliegue en la capital, pero se mantuvo muy lejos de lo requerido para obtener los dos senadores por la mayoría. De este modo, correspondió realizarse una segunda vuelta entre De la Rúa, que secundaba la lista radical, y Marcelo Sánchez Sorondo.

Durante los meses de marzo y abril, la campaña porteña se polarizó totalmente entre De la Rúa y Sánchez Sorondo. Este último no pertenecía al peronismo, sino al Partido Conservador Popular, fuerza de derecha encabezada por el vicepresidente electo Vicente Solano Lima dentro del FREJULI, y había accedido a la candidatura senatorial en gran medida debido a que la ciudad constituía uno de los distritos más esquivos al peronismo del país, junto con la provincia de Córdoba. Por entonces la Capital Federal era un distrito no autónomo que contaba con un intendente designado, por lo que la senaduría era uno de los cargos electos más representativos del distrito, y el único que se dirimió en segunda vuelta, mientras que varias provincias realizaron desempates gubernativos. La gran mayoría de las listas derrotadas apoyaron a De la Rúa, mientras que otros sectores se abstuvieron.

Sánchez Sorondo era una figura nacionalista muy conservadora, que dio numerosos pasos en falso al emitir declaraciones escandalosas durante la campaña de la primera vuelta, destacando el ejemplo de una entrevista en la que cuestionó «los vicios del sistema democrático». En clara desventaja ante un candidato más joven, que a su vez era apoyado por casi todos los partidos opositores al peronismo, Sánchez Sorondo limitó su actividad proselitista y siguió los lineamientos del comando superior del justicialismo, evitando desmarcarse demasiado de la corriente nacional. De la Rúa, por su parte, realizó una campaña vigorosa, centrándose en cuestiones de defensa de la democracia y la búsqueda de una oposición fuerte al gobierno entrante. Concedió entrevistas en las cuales defendió la protección de la libertad de prensa, la autonomía universitaria y los derechos individuales, llamando a la derogarción inmediata todas las leyes represivas del gobierno "de facto". Sobre la victoria aplastante del FREJULI en las elecciones presidenciales, declaró que si bien consideraba que las fuerzas no peronistas se habían visto condicionadas por el accionar represivo de la dictadura saliente, el radicalismo debía a su vez realizar una profunda autocrítica respecto a su desempeño, y que esta autocrítica «no será silenciosa, ni subterránea, sino sonora», afirmando que «en las derrotas también existe la grandeza». De la Rúa evitó atacar directamente a su competidor, expresando que: «prefiero que a Marcelo Sánchez Sorondo lo juzgue el pueblo en las urnas».

La segunda vuelta electoral tuvo lugar el domingo 15 de abril. En trece de los catorce distritos que realizaron segunda vuelta ese día triunfó el FREJULI en doce, mientras que en Neuquén se impuso el peronista disidente Movimiento Popular Neuquino de Felipe Sapag. En ese contexto, De la Rúa obtuvo un amplio y sorpresivo triunfo con 934.831 votos (54,13%) contra los 791.560 (45,87%) que logró Sánchez Sorondo, constituyendo la única victoria de un partido no relacionado con el peronismo en la jornada electoral.

El radicalismo porteño festejó profusamente el resultado, finalizando con una conferencia de prensa durante la cual De la Rúa agradeció a la militancia radical por apoyarlo y a los ciudadanos que lo votaron sin ser radicales, afirmando: «Hoy Buenos Aires ha demostrado ser la capital de la liberación, y el cambio somos nosotros». Aunque reconoció la derrota, Sánchez Sorondo acusó a De la Rúa y la UCR en un análisis posterior de haber «servido a un nuevo ensayo de Unión Democrática», en referencia a la confluencia de fuerzas dispares con el solo propósito de vencer al peronismo.

La victoria en Capital Federal dejó a De la Rúa muy bien posicionado dentro del radicalismo en particular y el espectro político no peronista en general. La Juventud Radical celebró la noche del 15 de abril con el cántico «¡Balbín, De la Rúa, la lucha continúa!». Los principales medios de comunicación conservadores y refractarios del peronismo en el país resaltaron su triunfo en sus titulares posteriores a las elecciones, destacando el diario "La Nación", que tituló «Nace una estrella», mientras que el diario "La Prensa" publicó un análisis duramente antiperonista tiulado «Buenos Aires ha dicho NO», el cual afirmaba:

La dimisión de Cámpora el 29 de julio condujo al llamado de nuevas elecciones presidenciales para septiembre, las cuales no tendrían proscripción alguna, pudiendo presentarse Perón como candidato del FREJULI. La Unión Cívica Radical definió su postura de cara a los nuevos comicios en agosto, resolviendo volver a presentar a Ricardo Balbín como su candidato presidencial, luego de que fracasaran las tentativas para que este concurriera como compañero de fórmula de Perón en un binomio de «unidad nacional». La Convención Nacional se realizó el 11 de agosto, siendo Balbín proclamado candidato por unanimidad. Inicialmente, se buscó un consenso entre la Línea Nacional, facción interna mayoritaria encabezada por Balbín, y el Movimiento de Renovación y Cambio liderado por Raúl Alfonsín para presentar una fórmula que representara a las dos facciones. Sin embargo, el alfonsinismo puso como condición la intervención de los comités radicales de Capital Federal, Entre Ríos, Santa Fe y Tucumán, lo que fue rechazado por la conducción partidaria. De la Rúa fue postulado como precandidato vicepresidencial en la Convención, contando con el precedente de su victoria electoral.

Ante la abstención del alfonsinismo después del fracaso del consenso, De la Rúa solo tuvo como rival a Luis León, senador por el Chaco. En la primera ronda de votación de la Convención Nacional, De la Rúa obtuvo 81 votos (43,09%) contra 72 de León (38,29%) y 35 abstenciones (18,62%), muy lejos de lo requerido para ser proclamado candidato. En la segunda instancia, ya con la ausencia definitiva de los convencionales alfonsinistas, De la Rúa logró 102 votos (64,15%) sobre 57 de León (35,85%), siendo de este modo proclamada la fórmula Balbín - De la Rúa. De la Rúa acompañó a Balbín durante la campaña proselitista, dando discursos en los que defendió los valores de la democracia y el desarrollo, describiéndose como un «representante de la juventud» y manteniendo los conceptos de «cambio» y «liberación nacional» que el radicalismo había empleado para las elecciones de marzo.

Las elecciones tuvieron lugar el 23 de septiembre con un triunfo aplastante para el FREJULI, resultando Perón elegido para un tercer mandato no consecutivo con el 61,85% de los votos sobre el 24,42% de Balbín, una diferencia de 37,43 puntos que no ha vuelto a ser superada en la historia electoral argentina. La presencia de De la Rúa en la fórmula no incrementó demasiado las posibilidades del radicalismo, obteniendo en Capital Federal 557.121 votos (31,46%), y 374.295 (33,32%) en su provincia natal de Córdoba. En general, sin embargo, la fórmula radical obtuvo un incremento neto de 368.114 votos y recibió más que el doble de sufragios que la tercera fuerza más votada, la Alianza Popular Federalista, con un 12,19% de los votos.

De la Rúa ejerció su primer mandato como Senador Nacional en representación de la Capital Federal desde el 25 de mayo de 1973 hasta el golpe de estado del 24 de marzo de 1976, que implicó la disolución del Congreso. Al momento de su juramentación se comprometió a apoyar las leyes que fueran «de carácter popular», al tiempo que aseguraba que buscaría representar un «control democrático» sobre el oficialismo. El 27 de mayo, dos días después de asumir, votó a favor de la liberación de los presos políticos, cumpliendo una de sus principales promesas de campaña. Durante su primer período como senador fue uno de los principales dirigentes de la oposición radical a las sucesivas y breves administraciones peronistas, finalizando con el gobierno de María Estela Martínez de Perón (1974-1976). Destaca de su gestión parlamentaria la redacción y defensa de la Ley de Acefalía Presidencial, que fue aprobada el 11 de julio de 1975 tras un intenso debate y que en 2001 se emplearía para reemplazarlo a él como presidente tras su renuncia. En su defensa del proyecto el 8 de julio De la Rúa trazó un panorama crítico de la situación del gobierno de Martínez de Perón, acusando al gobierno de aislarse y cuestionando la situación económica del país después del reciente Rodrigazo. La ley mantenía el orden de sucesión presidencial establecido pero reemplazaba el llamado a nuevas elecciones directas por una convocatoria a la Asamblea Legislativa (senadores y diputados) que deberían elegir un nuevo presidente entre los legisladores y gobernadores, garantizando de este modo que la presidencia solo pudiera ser ocupada por alguien que hubiera accedido previamente a un cargo electo.

Al igual que casi todos los funcionarios elegidos en 1973, su gestión como senador finalizó abruptamente con el golpe de estado del 24 de marzo de 1976, que derrocó a Martínez de Perón y disolvió el Congreso.

Tras la derrota de la Guerra de las Malvinas y el colapso de la Junta Militar gobernante, se dio comienzo a un proceso de liberalización política que desembocó en una nueva transición a la democracia. De este modo, la Unión Cívica Radical comenzó a reorganizarse políticamente con el llamado a elecciones internas. Tras el fallecimiento de Balbín en septiembre de 1981, la UCR estaba presidida por el exgobernador balbinista de Entre Ríos, Carlos Raúl Contín, y De la Rúa era uno de los principales referentes de la Línea Nacional, siendo de este modo perfilado como uno de los principales contendientes por la nominación radical para las inminentes elecciones presidenciales. Los otros precandidatos destacados serían Raúl Alfonsín, líder del socialdemócrata Movimiento de Renovación y Cambio (MRC), y Luis León, del izquierdista y americanista Movimiento de Afirmación Yrigoyenista (MAY). El compañero de fórmula de De la Rúa fue el exvicepresidente Carlos Humberto Perette. Las elecciones internas se realizaron por partes para escoger a delegados de la Convención Nacional Radical, que se encargaría de proclamar la fórmula presidencial del partido por medio de sus 95 delegados el 14 de agosto. Las primarias comenzaron el domingo 12 de junio de 1983, con internas en la provincia de Formosa y Chubut. La interna formoseña dio como resultado un triunfo casi unánime para el alfonsinismo, que obtuvo el 90,06% de los votos. En Chubut se dio otro triunfo alfonsinista, aunque esta vez la Línea Nacional obtuvo la representación de la minoría con un 36,18% de los votos.

El clima no mejoró para el sector de De la Rúa. El 26 de junio tuvieron lugar comicios internos en La Rioja, Neuquén, Misiones, Salta, San Juan, San Luis, Santa Cruz y en el Territorio Nacional de la Tierra del Fuego, Antártida e Islas del Atlántico Sur. Con el alfonsinismo en un ascenso creciente, Alfonsín se impuso ante De la Rúa en seis de los ocho distritos por diversos márgenes, imponiéndose De la Rúa solo en La Rioja y León en San Luis. Una semana más tarde, el 3 de julio, tuvo lugar una nueva primaria, esta vez en cuatro distritos, imponiéndose Alfonsín en la mayoría de ellos. El 10 de julio tuvieron lugar nuevas internas en Capital Federal, Córdoba, Santa Fe y Tucumán, cuatro de los distritos más poblados del país solo por detrás de la provincia de Buenos Aires.
Con un triunfo estrecho en la Capital Federal, hasta entonces un bastión de la Línea Nacional cuyo peso fue decisivo para que Balbín derrotara a Alfonsín en la primaria de 1973, y más triunfos en otros distritos, el alfosinismo consiguió una mayoría en la Convención Nacional con más del 50% de los delegados, garantizándole, "de facto", una preeminencia absoluta al MRC para proclamar al binomio presidencial. El 17 de julio tuvieron lugar las primarias en el distrito bonaerense, la provincia más poblada del país que albergaba a un 38% del electorado nacional y a un porcentaje similar de la afiliación radical. En estos comicios el alfosinismo logró un rotundo triunfo, al punto que la Línea Nacional ni siquiera pudo superar el 25% de los votos requerido para alcanzar una representación por la minoría de la provincia en la Convención. Aunque todavía faltaban cuatro primarias (Chaco, Santiago del Estero, Catamarca y Jujuy), el resultado en la práctica volvía la diferencia irremontable.

El 18 de julio, tras constatarse la derrota en Buenos Aires, De la Rúa y Perette concedieron una conferencia de prensa en la que anunciaron que declinaban su postulación. De la Rúa justificó su postura admitiendo que continuar su campaña implicaría «negar la realidad» con respecto a la inminente victoria de Alfonsín, y que el progreso de la interna podría acrecentar la fricción entre el alfonsinismo y el balbinismo y comprometer las posibilidades de triunfo (percibidas como cada vez mayores) del radicalismo de cara a las elecciones presidenciales. Gran parte del arco político del radicalismo elogió la postura de De la Rúa, que calificó como «inédita», y el propio Alfonsín declaró estar dispuesto a negociar la postulación de listas con él, de acuerdo con su nueva actitud. Al momento de la retirada de De la Rúa, Alfonsín había obtenido el 86,21% de los votos, contra el 13,48% de De la Rúa y el 0,31% de León. Al día siguiente, cuando se preguntó a Alfonsín si consideraba la posibilidad de una fórmula Alfonsín-De la Rúa, este rechazó de plano la idea, sugiriendo de forma irónica que De la Rúa era un «buen senador».

Fracasada su primera carrera presidencial, De la Rúa se postuló nuevamente para el cargo que había ocupado antes de la dictadura: Senador Nacional por Capital Federal. El retorno a la constitución de 1853 condujo a que los senadores fueran elegidos nuevamente por las legislaturas provinciales, mientras que los dos senadores por Capital Federal eran designados por un Colegio Electoral de 54 miembros, que era elegido popularmente. El senador por la minoría, presente en las anteriores elecciones, no sería restaurado hasta 1994. De la Rúa concurrió acompañado por Juan Trilla, también ligado al balbinismo. En paralelo con la amplia victoria de Alfonsín en los comicios presidenciales, el binomio senatorial De la Rúa-Trilla obtuvo un aplastante triunfo en la Capital Federal con un 61,36% de los votos y 36 de los 54 escaños del Colegio Electoral contra el 26,07% de la lista justicialista encabezada por Carlos Ruckauf, garantizando que el radicalismo se quedara con los dos senadores porteños. De la Rúa asumió su cargo el 10 de diciembre de 1983.

Durante su segundo período en el Senado, De la Rúa fue un legislador destacado, ejerciendo como presidente de la Comisión de Asuntos Constitucionales de la cámara alta. Entre los primeros y más destacados proyectos redactados por De la Rúa se encuentra la ley N.º 23.098 de "habeas corpus", conocida como «Ley De la Rúa», que regulaba el control judicial en el caso concreto de limitaciones a la libertad ambulatoria en el caso de ser declarado estado de sitio en el país. La misma fue sancionada el 19 de septiembre de 1983 y promulgada por el poder ejecutivo el 28 de octubre. A finales del mismo año, presentó un proyecto por el cual se otorgaría una pensión a las madres que tuvieran cinco o más hijos, y otro por el cual se extendería al esposo el derecho a percibir una pensión por el trabajo de su mujer. Ambos proyectos fueron aprobados, el primero el 24 de septiembre de 1984 y el segundo recién el 22 de agosto de 1985. Mientras que el primero finalmente caducó en diputados, el segundo se convirtió en ley tras ser aprobado por la cámara el 25 de septiembre de 1985. Impulsó también un proyecto para establecer los derechos de voto de los ciudadanos argentinos residentes en el extranjero, recibiendo este la media sanción del Senado el 31 de agosto de 1988 pero nuevamente fracasando en la Cámara de Diputados. Algunos radicales creen que fue quien redactó la ley de Punto Final y la de obediencia debida sancionadas el 4 de junio de 1987, durante el gobierno del radical Raúl Alfonsín, que estableció que los delitos cometidos por los miembros de las Fuerzas Armadas cuyo grado estuviera por debajo de coronel durante el Terrorismo de Estado y la dictadura militar no eran punibles. 

En medio del debate sobre la ley de divorcio vincular de 1987, iniciativa apoyada por un 62% de la sociedad de acuerdo con las encuestas, De la Rúa se declaró inicialmente a favor de la misma, aunque impulsó numerosas modificaciones, exigiendo una mayor potestad por parte de los jueces para impedir un divorcio cuando este «implicara consecuencias negativas para el cónyuge o los hijos». De la Rúa defendió estas modificaciones en nombre de «humanizar» el proceso, sugiriendo también la necesidad de que pasaran cinco años para que una persona pudiera volver a casarse. Finalmente votó en contra del proyecto presentado, aunque este fue de todas formas aprobado en el Senado con 26 votos a favor y 14 en contra. En 1988, ligado a las problemáticas enfrentadas por grupos de jubilados, logró la aprobación de la ley N.º 23.592 de Actos Discriminatorios, de su autoría, que exigía compensaciones por los actos discriminatorios con base a raza, religión, nacionalidad, ideología, sexo, posición económica, condición social o caracteres físicos. Considerada avanzada para su época, dicha ley fue objeto de debate en la década de 2010 por tenerse en cuenta que no criminalizaba la discriminación en un sentido estricto y se creía que en realidad fue aprobada en forma apurada por la necesidad del país de adaptarse al Pacto de San José de Costa Rica, luego del fracaso de un intento por parte del ejecutivo de impulsar una ley similar en 1984.

Su banca en el Senado debía renovarse en 1989, debido a las disposiciones de la transición democrática que establecían mandatos acortados para los cargos que se renovaban en forma escalonada, pues por entonces el mandato completo de un senador era de nueve años. De la Rúa se postuló para la reelección por un segundo mandato teniendo como principales competidores a Eduardo Vaca, del Frente Justicialista de Unidad Popular (FREJUPO) y a María Julia Alsogaray, de la Unión del Centro Democrático (UCeDé) dentro de la Alianza de Centro (AC). El eslogan de campaña empleado durante sus anuncios televisivos fue «De la Rúa, un senador en serio». Las elecciones se realizaron anticipadamente el 14 de mayo de 1989, al mismo tiempo que las elecciones presidenciales. Mientras que la lista de diputados del FREJUPO superó en votos a la lista radical, y el candidato presidencial Eduardo Angeloz solo venció en Capital Federal debido al pacto entre la UCR y la Confederación Federalista Independiente, De la Rúa por sí solo fue el candidato más votado con el 33,11% de los votos sobre el 32,60% de Vaca y el 19,62% de Alsogaray. Sin embargo, en el Colegio Electoral el radicalismo empató con el justicialismo con 19 escaños cada uno, contra 11 de la UCeDé, 3 de la CFI y 2 de la Izquierda Unida. Finalmente, un pacto entre la Alianza de Centro y el FREJUPO posterior a las elecciones, que consagraron a Carlos Menem como presidente, sumó una mayoría de 30 escaños y consagró a Vaca como senador.
Tras finalizar su mandato como senador, De la Rúa mantuvo la presidencia del Comité Capital de la Unión Cívica Radical. En abril de 1990, Menem le ofreció un puesto en la Corte Suprema de Justicia, que De la Rúa rechazó por considerar que esta estaba subordinada al oficialismo. Después de dos años fuera de los cargos públicos nacionales se presentó como candidato a diputado nacional por Capital Federal en las elecciones de 1991, integrando el segundo lugar en la lista radical, por debajo de Jorge Enrique Benedetti. La lista obtuvo un triunfo holgado sobre el justicialismo, con el 40,35% de los votos sobre el 29,02% del Frente Justicialista para la Producción y la Estabilidad, que encabezaba Jorge Argüello. De este modo, De la Rúa asumió como diputado el 10 de diciembre de 1991, ocupando la presidencia del bloque de la Unión Cívica Radical (compuesto entonces por 84 diputados) en la cámara baja. Su período como diputado fue en realidad muy corto, ya que se postuló rápidamente como candidato a senador en las elecciones destinadas a realizarse el 28 de junio de 1992, siete meses después, para renovar la banca de Juan Trilla. De la Rúa pretendía permanecer en la Cámara de Diputados, pero fue convencido por la dirigiencia radical de que presentara su candidatura debido a que era la figura del partido con mayor intención de voto.

El principal oponente de De la Rúa sería el fundador y decano de la Universidad de Belgrano Avelino Porto, candidato por una gran coalición menemista entre el PJ, la UCeDé y el Partido Intransigente (PI). De corte conservador liberal, Porto realizó una campaña vigorosa empleando elementos que emulaban, de acuerdo a varios analistas, las campañas electorales estadounidenses, y evitó hacer referencias al justicialismo durante la campaña. Si bien pretendía atraer a los votantes de un distrito particularmente antiperonista, el estilo de Porto erosionó la base de votos originaria del justicialismo porteño, al mismo tiempo que su coalición con el PJ le restó el apoyo de gran parte del electorado no peronista. Del mismo modo, el debate en torno a la posible reelección de Menem giró en torno a la campaña, considerando que una victoria menemista en la capital del país podría jugar a favor del bando reeleccionista, pues ayudaría al oficialismo a alcanzar los dos tercios requeridos para iniciar una reforma constitucional. En este contexto, De la Rúa (que era un opositor acérrimo a la reelección presidencial) obtuvo un aplastante triunfo con el 50,01% de los votos y una mayoría absoluta de 29 electores contra el 31,67% de Porto, que obtuvo 18 electores. El Frente País Solidario (FREPASO), recientemente fundado, obtuvo el 7,47% de los votos y el Movimiento por la Dignidad y la Independencia (MODIN), el 5,62%, ingresando ambas fuerzas al colegio electoral con 4 y 3 votos. De la Rúa logró de este modo retornar al Senado el 10 de diciembre, después de solo un año en la Cámara de Diputados.

Durante su tercer período en el Senado, De la Rúa integró la comisión de Economía. Su margen de maniobra legislativa se redujo considerablemente, pues en ese momento la cámara estaba integrada por 32 senadores justicialistas y solo 8 radicales (con el resto de las bancas repartidas entre partidos provinciales). En 1993 presentó un proyecto de ley para exigir la incorporación de un mínimo de escritura braille en los documentos públicos y un proyecto de ley Orgánica de la Municipalidad de Buenos Aires, los cuales caducaron durante la etapa de comisión. Destino similar tuvieron un proyecto de 1994 destinado a controlar los aportes privados a la financiación de partidos políticos, y otro a la creación de un «sistema libre y solidario de salud». La mayoría de su historia parlamentaria posterior a su segunda elección se restringe a una serie de proyectos de resolución.

Durante la década de 1990, De la Rúa mantenía una actitud fuertemente opositora al gobierno de Carlos Menem, y se opuso tajantemente a que el presidente buscara la reelección en 1995, algo hasta entonces prohibido por la constitución de 1853. La legitimidad de dicha constitución se encontraba en duda desde 1957, cuando el derrocamiento del gobierno peronista derogó la constitución de 1949 y reimplantó la de 1853 mediante una convención constituyente elegida en elecciones no libres, con el peronismo proscripto e impedido para presentarse, y que no llegó a sesionar completamente por falta de quorum. La posibilidad de convocar a una nueva constituyente bajo un amplio consenso político se barajó durante gran parte de la década de 1980 y principios de 1990. Finalmente, ante la posibilidad de que Menem recurriese a un fallo de la Corte Suprema de Justicia para acceder a una segunda candidatura, Alfonsín, entonces presidente del Comité Nacional de la UCR, optó por negociar con el gobierno el apoyo radical (necesario para alcanzar los dos tercios del Congreso) a una reforma constitucional que permitiera una sola reelección consecutiva para el presidente, acortando el mandato a cuatro años, y aceptando una serie de demandas para el radicalismo. De la Rúa rechazó de plano este acuerdo, conocido como Pacto de Olivos, afirmando que «se habría cortado la mano» antes que firmarlo, y criticó duramente a Alfonsín por esta decisión. Las elecciones para convencionales constituyentes se realizaron en 1994, y De la Rúa no fue candidato.

Entre las principales exigencias del radicalismo para apoyar la reforma constitucional se encontraba la autonomización de la Capital Federal, entonces un distrito no autónomo con un intendente designado por el presidente de turno de la Nación y un Concejo Deliberante como único organismo electo. Siendo la ciudad de Buenos Aires desde la democratización uno de los principales bastiones electorales de la UCR, se encontraba entonces bajo la intendencia de Oscar Camilión, dirigente del Movimiento de Integración y Desarrollo, y posteriormente por el justicialista Jorge Domínguez, ambos ligados al menemismo. Bajo el nuevo régimen municipal, el intendente pasaría a denominarse «Jefe de Gobierno» y sería elegido directamente por los ciudadanos porteños, y el Concejo Deliberante pasaría a ser un poder legislativo autónomo con facultades similares a las de una legislatura provincial. Después de amagar brevemente con la idea de presentarse en las primarias presidenciales radicales de 1994, De la Rúa optó por disputar la naciente jefatura de gobierno, debiendo competir como precandidato del «Movimiento de Participación» en las internas radicales contra el alfonsinista Facundo Suárez Lastra, intendente durante el gobierno de Alfonsín. Las primarias tuvieron lugar el 20 de marzo de 1995 con un rotundo triunfo para De la Rúa, del 70% de los votos. De la Rúa fue proclamado de este modo como candidato radical a Jefe de Gobierno, con el exdiputado nacional Enrique Olivera como compañero de fórmula y candidato a vicejefe. De cara a las elecciones, la fórmula De la Rúa-Olivera configuró una colectora con otros cuatro partidos minoritarios: el Frente Progresista Desarrollista de los Jubilados, Generación Intermedia, Solidaridad y el Partido Social Demócrata.

Las elecciones porteñas para las autoridades autónomas tuvieron lugar más de un año más tarde, el 30 de junio de 1996. La candidatura de De la Rúa a la jefatura de gobierno se produjo en un contexto sumamente desfavorable para el radicalismo, en franco declive después de la derrota abrumadora en las elecciones de 1995, en las que su candidato presidencial, el Horacio Massaccesi, se ubicó en tercer puesto detrás del recientemente fundado FREPASO. El 8 de octubre de ese año, la UCR sufrió un nuevo revés al perder las elecciones para la renovación senatorial, con el FREPASO imponiéndose y arrebatándole gran parte del voto opositor en favor de Graciela Fernández Meijide, antigua militante de la Democracia Cristiana y miembro de la CONADEP. Fernández Meijide se presentó como primera candidata del FREPASO a la Legislatura Estatuyente, que debía redactar la constitución o estatuto del nuevo distrito autónomo y cuya elección se realizaría al mismo tiempo que la elección para Jefe de Gobierno. El FREPASO postuló al socialista Norberto La Porta como candidato, mientras que el intendente menemista Jorge Domínguez se presentó para el continuar al frente de la ciudad por el Partido Justicialista, enfrentando la disidencia de Gustavo Béliz, de la fuerza Nueva Dirigencia. Ante este escenario, el oficialismo aparecía desgastado por la división y la difícil situación económica del país, y la competencia se centró entre De la Rúa y La Porta.

Al igual que en anteriores elecciones, De la Rúa mantuvo un discurso con fuerte énfasis en denunciar la corrupción política imperante, tanto en el gobierno de Menem como en la intendencia de Domínguez, a quien definió como un «intendente "de facto"» luego de casi de dos años de realizada la reforma constitucional sin que se hubieran normalizado las instituciones autónomas. Se comprometió mayormente a fortalecer la transparencia institucional y buscar el desendeudamiento de la ciudad. De la Rúa y La Porta manifestaron ciertas coincidencias antes de la campaña y se comprometieron a cooperar para que Domínguez traspasara el mando al Jefe de Gobierno electo si perdía las elecciones tan rápido como fuera posible, independientemente de quien resultara ganador. Respaldado por el fuerte aparato radical presente en la ciudad y con un historial favorable por su carrera como legislador y opositor al justicialismo, De la Rúa lideró todas las encuestas de intención de voto por márgenes holgados y obtuvo una cómoda victoria con el 39,89% de los sufragios, triunfando en todos los distritos de la capital, contra el 26,50% de La Porta y el solo 18,62% de Domínguez, que se ubicó en un lejano tercer puesto. Además de representar un fuerte deterioro para el gobierno menemista, la elección puso de manifiesto las popularidades personales tanto de De la Rúa como de Fernández Meijide, pues hubo un masivo corte de boleta en favor de ambos, al punto que mientras De la Rúa se impuso en toda la ciudad en la elección ejecutiva, Fernández Meijide logró lo propio en la categoría legislativa, otorgándole al FREPASO la primera minoría en la Legislatura Estatuyente.

Durante la gestión municipal se crearon órganos como la sindicatura del gobierno de la ciudad, la defensora del consumidor, dirección general de higiene y seguridad alimentaria y el ente regulador de servicios. Durante su período se produce la sanción de la Constitución de la Ciudad de Buenos Aires.

A principios de 1998 la Legislatura porteña aprobó el Código de Convivencia Urbana que, entre otras medidas, eliminaba los edictos policiales e imponía el concepto de "tolerancia" hacia manifestaciones de diversa índole como el travestismo y la oferta de sexo en la vía pública, lo que desató muchas protestas de los vecinos y de la oposición.

Las obras para la prolongación de la Línea D con rumbo a Belgrano ya habían sido reiniciadas por Jorge Domínguez en 1996.

Poco después, el 13 de noviembre de 1997 fue inaugurada "José Hernández" y el 21 de junio de 1999 comenzó a prestar servicios la estación "Juramento", en pleno centro de Belgrano. Los trabajos para la prolongación de la Línea D culminaron en el barrio de Nuñez, con la nueva terminal denominada "Congreso de Tucumán", la cual fue habilitada el 27 de abril de 2000.

Mientras tanto, en noviembre de 1999, comenzó la construcción de las estaciones "Tronador" y "De Los Incas - Parque Chas" de la Línea B. Paralelamente el gobierno porteño comienza a gestionar la construcción de la flamante Línea H (Retiro - Nueva Pompeya), aunque su construcción recién sería iniciada durante la gestión de AnÍbal Ibarra.

Entre medio, el 21 de abril de 1999, el aún presidente Carlos Menem firmó el Decreto PEN Nº 393/99 en favor de la transferencia del ejercicio de la fiscalización y el control de los subterráneos en favor de la ciudad. Si bien la Legislatura de la Ciudad Autónoma de Buenos Aires, aprobó una ley adhiriendo al decreto nacional mencionado, la posterior crisis socio-económica de 2001 provocó que el efectivo traspaso de dichas funciones se diluyera en el tiempo.

En 1994, como parte de los contratos de concesión de autopistas, se anuncia la intención de extender la ruta Panamericana dentro de la Capital Federal mediante un viaducto elevado hasta la Av. Congreso aprovechando los terrenos remanentes de lo que iba a ser la Autopista Central AU-3 (ver Plan de Autopistas Urbanas de 1976). Sin embargo, los vecinos se oponen terminantemente y el intendente Jorge Domínguez, propone en su lugar la construcción de una avenida de acceso rápido, abarcando casi la totalidad del espacio vacante entre las calles Holmberg y Donado. Sin embargo no se llega a un consenso y el plan queda en suspenso. 

Recién durante la gestión de Fernando De la Rúa, atendiendo al reclamo de los vecinos, se vuelve a modificar el proyecto de la avenida rápida incorporando una calle lateral de tránsito vecinal (separada de la vía rápida) y otras modificaciones que tendieron a privilegiar los espacios verdes. Los vecinos se mostraron satisfechos y la construcción de la obra se realizó entre los años 1997 y 1998, constituyendo la actual avenida Roberto Goyeneche.

Otra obra que reinicia De la Rúa, es la de la Autopista Occidental AU-7. Esta obra, que contemplaba unir los barrios de Villa Soldati y Villa Pueyrredón, había sido iniciada en 1980 con algunos viaductos en el sector de la avenida Lacarra (parque Almirante Brown), sin embargo su ejecución se había suspendido en 1982, quedando estas estructuras abandonadas. En 1998, se reinician los trabajos aunque en una versión recortada del proyecto, yendo solo desde la Avenida Dellepiane hasta el Riachuelo. En 2000 se inaugura el primer tramo desde Av. Dellepiane hasta la Av. Roca y en 2002 se completa la obra hasta el Riachuelo, recibiendo en ese momento el nombre de Autopista Cámpora.

En septiembre de 1997, ante el reiterado reclamo de los ciclistas porteños, De la Rúa también inauguró el primer tramo de 7,12 kilómetros de lo que sería posteriormente la Red de Bicisendas de la Ciudad de Buenos Aires. Este primer tramo partía de Av. Libertador y Carlos Casares, frente al Jardín Japonés, luego corría paralela a la avenida Figueroa Alcorta y terminaba en la esquina de Av. Libertador y García del Río. 

Esta nueva senda para bicicletas contaba con todo el trayecto debidamente señalizado para que los peatones no usurparan el paso a las bicicletas. Además, en los cruces con las avenidas se colocaron carteles indicadores para los automovilistas y se instalaron semáforos especiales para los ciclistas.

Una de las primeras medidas de la gestión De la Rúa fue anular, en 1997, las polémicas concesiones del restaurante bailable "Ski Ranch" (Costanera Norte) y del Campo de Golf-Velódromo por haber encontrado graves incumplimientos en los contratos.

El "Ski Ranch", que ocupaba el terreno del tradicional Espigón Dorrego, fue demolido por la comuna y en su lugar se construyó un parque público. La iniciativa del Gobierno era recuperar el paseo costanero. 

Inmediatamente De la Rúa firmó un decreto por el que obligaba a casi todos los concesionarios a presentarse en la Secretaría de Hacienda y Finanzas para analizar cada contrato en particular.

El principal efecto de esta medida se sintió en la avenida Intendente Güiraldes, un tradicional paseo de la costa porteña, donde se ubicaban numerosos locales gastronómicos concesionados por la Municipalidad de Buenos Aires en la década del ´70. El paseo gastronómico fue muy popular durante varios años, con una oferta muy variada para todas las clases sociales.

Sin embargo, su decadencia comenzó con los años ´90 y se profundizó cuando Fernando De la Rúa tomó la decisión de clausurar varios locales, porque las concesiones estaban en su mayoría vencidas o presentaban inconsistencias. La mayoría de los restaurantes fueron demolidos y convertidos en parte del paseo público. Finalmente solo sobrevivieron unos pocos restaurantes. 

A fines de 1999, el paseo costanero incorporó un nuevo atractivo: el parque temático Tierra Santa. Con edificaciones de estilo helénico, romano, judío, egipcio y babilónico, los visitantes podían recorrer los distintos momentos de la vida de Cristo e interactuar con actores caracterizados como personajes de la época. La nueva atracción fue construida sobre terrenos que se hallaban disponibles del Balneario Parque Norte.

Otro lugar recuperado durante la gestión de De la Rúa fue la Costanera Sur (Puerto Madero) y la Laguna de los Coipos, un espejo de agua de 11 hectáreas ubicado en la Reserva Ecológica. Las tareas habían consistido en la remoción de 50.000 metros cúbicos de barro, desmalezamiento, retiro de residuos sólidos y profundización del terreno.

La obra incluyó, en una segunda etapa, la reconstrucción del veredón inferior de la rambla que pasa junto a la Laguna los Coipos.

Dentro de la trama urbana, cabe destacar que, si bien durante la gestión de Jorge Domínguez se procedió a cercar el Paseo del Rosedal para su protección, es durante la gestión de De la Rúa cuando se populariza la polémica tradición de enrejar plazas y monumentos históricos, para impedir que sean vandalizados.

Al año siguiente de su asunción como jefe de Gobierno, en agosto de 1997, se formó la Alianza por el Trabajo, la Justicia y la Educación, con varios partidos políticos de centro e izquierda moderada, siendo los principales la Unión Cívica Radical y el Frente País Solidario (Frepaso). El principal objetivo de la Alianza era conformar listas comunes en la mayor cantidad posible de distritos de cara a las legislativas de ese mismo año, y además, disputarle el poder al Justicialismo en las presidenciales de 1999. Buena parte de esas aspiraciones se cumplieron cuando, con listas conjuntas en 14 distritos (incluyendo la Capital y la Provincia de Buenos Aires) en octubre de 1997 la UCR y el Frepaso triunfaron con el 45 por ciento de los votos en todo el país, causando la primera derrota electoral nacional del Partido Justicialista desde 1985. 

Como resultado de los acuerdos entre los dos partidos mayoritarios de la coalición hasta entonces opositora, la candidatura presidencial para las elecciones de 1999 se definiría en internas abiertas entre un candidato de la UCR y un postulante del Frepaso. Tras lograr el respaldo de la mayoría del Partido Radical —en especial el aval clave del expresidente Raúl Alfonsín— De la Rúa se convirtió en 1997 en presidente del Comité Nacional de la UCR y luego, meses después, en precandidato presidencial por su partido. El Frepaso le opuso a Graciela Fernández Meijide, que contaba con el antecedente de la elección anterior de haber derrotado al justicialismo en la provincia de Buenos Aires (hasta entonces un distrito fuertemente duhaldista).

Fernando De la Rúa logró la victoria en la interna abierta en noviembre de 1998, alcanzando el 62 % de los votos contra el 38 % del Frepaso en todo el país. Consagrado Fernando de la Rúa como candidato presidencial, el líder del Frepaso, Carlos Álvarez, decidió acompañarlo como candidato a vicepresidente para reforzar la unidad de la coalición.
Fernando De la Rúa, candidato de la Alianza, fue elegido presidente en las elecciones del 24 de octubre de 1999, y el justicialismo perdió la mayoría en la Cámara de Diputados. La Alianza y su fórmula De la Rúa-Álvarez obtuvo el 48,5 % de los sufragios, contra el 38,09 % del binomio peronista Eduardo Duhalde-Ramón Ortega. En tercer lugar, con el 10,09 % de los votos, aparecía el exministro de Economía Domingo Cavallo. 

Uno de los éxitos de la campaña electoral fue la campaña publicitaria televisiva de De la Rúa, en el cual pronunciaría la frase ""Dicen que soy aburrido..."" con la cual se lo relacionaría más adelante. Dicha publicidad buscaba contrastar al candidato presidencial con la frivolidad que el público percibía en el gobierno menemista. La campaña electoral estuvo a cargo de Ramiro Agulla, David Ratto (publicista de Raúl Alfonsín en las elecciones de 1983) y Antonio de la Rúa, este último hijo del propio Fernando De la Rúa. El hijo del presidente lideraría al ""Grupo Sushi"", un entorno con influencia en las decisiones de Fernando De la Rúa.

La victoria de De la Rúa se debió al fuerte rechazo público hacia la figura de Carlos Menem, así como también al deterioro de la situación económica del país, que en 1999 terminaba con una caída del PBI de alrededor de 3,4 puntos porcentuales respecto al año anterior. El desempleo se acercaba al 14 por ciento, luego de haber alcanzado la cifra récord de 18,6% algunos años antes, y la pobreza era del 30% aunque seguía siendo menor a la que había antes que él asumiera. El país tenía serios problemas en materia educativa y sanitaria, y la dirigencia política tenía una mala imagen pública. Además, el gobierno peronista dejaba un elevado déficit fiscal, con un rojo de más de mil millones de pesos, una deuda externa del orden de los 150 mil millones anuales con vencimientos de casi 25 mil millones en el año próximo. La inestabilidad económica provocó constantes cambios en el Ministerio de Economía, pasando por José Luis Machinea (1999 - marzo del 2001), Ricardo López Murphy (marzo-abril del 2001) y por último Domingo Cavallo, que ya había sido Ministro de Economía entre 1991 y 1996 y que había impulsado la Ley de Convertibilidad.

Debido a esto, Machinea tomó severas medidas de ajuste con el propósito de sanear las finanzas. A principios del 2000 se aprobó una Ley de Reforma Tributaria, que entre otras cuestiones, aumentó el mínimo no imponible del Impuesto a las Ganancias y generalizó la aplicación del IVA. A esto se le sumaron recortes de sueldos y despidos de empleados estatales, entre los que se encontraban docentes, fuerzas de seguridad, administrativos y judiciales. La economía continuaba contrayéndose y el desempleo aumentaba. Para reducir la presión de la deuda externa el gobierno negoció un paquete de salvataje de cerca de 40.000 millones de dólares, conocido como "Blindaje financiero". Sin embargo no fue suficiente para reactivar la economía y en marzo de 2001 Machinea decidió presentar su renuncia.

Machinea fue reemplazado en el cargo por Ricardo López Murphy quien duró apenas 16 días en el cargo tras realizar un nuevo ajuste del gasto público con recortes de salarios estatales que no fue acompañado por la Unión Cívica Radical. El último ministro de Economía de De la Rúa fue Domingo Cavallo, quien ya había ocupado ese lugar durante la presidencia de Menem. Sus primeras medidas fueron la creación del impuesto al cheque y el recorte del 13 % en haberes previsionales y del salario de empleados estatales. La situación económica empeoró rápidamente con aumentos del desempleo, la pobreza y el riesgo país. En noviembre se inició una reestructuración de la deuda externa, conocida como "Megacanje". La desconfianza en el sistema financiero produjo fuertes retiros de depósitos bancarios. Para frenarlos, el ministro de Economía impuso restricciones al retiro de fondos, medida que recibió el nombre de "corralito". Estos hechos desembocaron en la crisis de diciembre de 2001 en Argentina.

La primera medida trascendente antes de finalizar 1999 fue la aprobación de la Ley de Reforma Tributaria, que preveía aumentar el impuesto a las ganancias, realizar quitas a las jubilaciones mayores a los 3100 pesos, generalizar la aplicación del IVA, entre otras modificaciones que suponían un aumento de casi todos los impuestos internos. El aumento impositivo fue parte de un paquete que procuró en general mejorar la economía, así como atender deudas pendientes como el Fondo para el Incentivo Docente, pero esto resultó sin embargo insuficiente para resolver el deterioro de las finanzas públicas. A lo largo del año 2000 el gobierno buscó controlar el gasto público, bajar las tasas internas de interés y mantener la estabilidad monetaria y financiera. De la Rúa tomó severas medidas de ajuste: dispuso un recorte de sueldo entre 8 y 20% a los empleados públicos, docentes, fuerzas de seguridad y empleados judiciales que afectaron a más de 140.000 personas, recortes en el presupuesto de las Universidades Nacionales y el despido de 10.000 empleados, como parte de un paquete de ajuste exigido por el FMI. Sin embargo las medidas no dieron resultado, para el año 2000 la crisis continuó, la economía se contrajo 0.5% del PBI y la desocupación alcanzó el 14.7%.A partir de diciembre de 2000 la actividad industrial cayó. En Tierra del Fuego, la producción de electrodomésticos fue solo de 610 artefactos, cuando el peor año había sido el 2000 con 19 115 artefactos. la producción de artefactos de televisión cayó 89 % con respecto al igual periodo del año anterior, la producción de videocaseteras fue del 94 %, la de microondas del 84 %, auto-radios del 73 %. Cerraron muchas fábricas con una antigüedad de más de cien años, como la empresa elaboradora de alfajores Balcarce.

En octubre el desempleo alcanzó al 18,3% de la población activa. La deuda pública llegó a 132.000 millones de dólares, se registró una contracción mayor al 11% en la actividad fabril y al 20% en construcción en términos anuales, el PBI per cápita bajó 10% y la inversión un 30% y un déficit de 8.500 millones.

El gobierno de De la Rúa pidió ayuda complementaria al Fondo Monetario Internacional (FMI) y a los bancos privados para reducir la presión de la deuda externa. En diciembre de 2000, el ministro de Economía José Luis Machinea negoció un paquete de salvataje de cerca de 40.000 millones de dólares, conocido como "Blindaje financiero" para ganar confianza y credibilidad en el exterior y bajar los intereses y renovar más fácilmente los vencimientos. Sin embargo, el "Blindaje" no fue suficiente para reactivar la economía y en marzo de 2001 Machinea decidió presentar su renuncia.

En enero de 2001 las reservas internacionales del BCRA habían alcanzado el récord histórico de 37.380 millones de dólares, el cual sería superado en el año 2007 durante el gobierno de Néstor Kirchner.

En marzo del 2001 asumió como ministro Ricardo López Murphy que llevó a cabo un severo programa de ajuste fiscal por 2500 millones de dólares, de los cuales 1100 millones correspondían al área educativa. También se eliminaron partidas por 660 millones de pesos/dólares destinadas a las provincias, recorte en el presupuesto universitario por 361 millones para el 2001 y 541 millones para el 2002, recortes en sueldos docentes por 220 millones, rebajas en el salario familiar por 129 millones, recortes en jubilaciones por 127 millones y postergación en el pago de las mismas, anulación de pensiones y becas estudiantiles, achique y recorte en los programas sanitarios por 50 millones. También incluyó el aumento del IVA del 15 al 21 % para espectáculos culturales, fútbol, teatro y cine, la eliminación de ayuda a productores rurales de siete provincias por 180 millones, el despido inmediato de 40.000 empleados públicos, flexibilización laboral aumentando el período de prueba de 3 a 12 meses y recortes en las indemnizaciones por despido, privatización de las casas de juego y de parte del Banco Nación. 
El manejo del gobierno radical deterioró la economía rápidamente: las tasas interbancarias orillaban el 900% anual y el riesgo país se disparó dificultando las inversiones. Sus proyectos chocaron con una muy fuerte oposición popular, particularmente dentro del mismo Partido Radical y en sus brazos juveniles y universitarios. También puso en situación de ruptura a la coalición gobernante ya que los miembros del Frepaso dentro del Gabinete anunciaron su renuncia luego de que fue presentado el programa económico. 

Debido a esto, López Murphy se vio obligado a retirarse luego de apenas 16 días en el ministerio de Economía.

Se convocó a ocupar la cartera a Domingo Cavallo, expresidente del BCRA durante la dictadura y Ministro de economía del menemismo. Cavallo inició su gestión prometiendo un crecimiento anual del 5% e intentando rebajar impuestos distorsivos y reanimar la industria, presentado como "Planes de Competitividad". Se aprobó el impuesto a las operaciones bancarias y se efectuaron delegaciones de algunas de las atribuciones del poder legislativo en el poder ejecutivo. 

Sin embargo, los mercados reaccionaron tan mal como los organismos internacionales de crédito. En julio de ese año, debido a la presión fiscal y la imposibilidad de normalizar la economía, Cavallo viró hacia una fuerte ortodoxia económica. Para ello presentó un plan de ""Déficit cero"", con un nuevo recorte general de gastos en la administración pública para evitar gastar más de lo que ingresaba en el Estado. La resistencia para obtener del Congreso la ley en cuestión fue muy grande, incluso dentro del radicalismo en los sectores adherentes al alfonsinismo, pero De la Rúa la obtuvo pidiendo un esfuerzo tanto a los legisladores opositores como a los propios y a la población en general. Se argumentaba que ""si no hay arreglo, llega el caos"". Esto tampoco ayudó, y el continuo ajuste contraía aún más la economía en el marco de un contexto internacional de recesión regional y global, que tampoco ayudaba a la Argentina a crecer.

Cavallo junto a su equipo económico y Patricia Bullrich, entonces Ministra de Trabajo, anunciaron más ajustes. Se aprobó el impuesto a las operaciones bancarias, un recorte del 13 % en haberes previsionales que afectaron a 533.401 jubilados, recortes del 13 % sobre el salario de empleados estatales, y se emitió deuda por 3000 millones. Estas medidas enfriaron el consumo y conllevaron a una mayor caída de los niveles de empleo. El déficit fiscal se disparó a 4000 millones de dólares. El desempleo pasó de 14,7 % en el año 2000 a 25 % a comienzos del 2001, niveles que marcaron un récord histórico en el país, superior incluso a los de la crisis de 1930.

Durante el 2001 la situación económica se deterioró rápidamente: las tasas interbancarias orillaban el 900 % anual, el riesgo país se disparó, (en marzo superó 800 puntos y en octubre llegó a 1859 puntos, el más alto del mundo), acompañado de una baja de 540 millones de pesos en depósitos bancarios en un solo día. En octubre el desempleo fue récord 4.8 millones entre desocupados, 18.3 % de la población activa. La deuda pública llegó a 132.000 millones de dólares, se registró una contracción mayor al 11 % en la actividad fabril y al 20 % en construcción en términos anuales, el PBI per cápita bajó un 10 % y la inversión un 30 %, y un déficit de 8.500 millones, sin contar el de las provincias.
En noviembre, el gobierno de De la Rúa inició una reestructuración de los compromisos de la deuda externa, denominada "Megacanje". Hacia fines de ese mes, el agravamiento inusitado de la situación económica, con inversiones que se alejaban debido a la complicada situación política, provocó desconfianza pública en el sistema financiero, por lo que se produjeron fuertes retiros de depósitos bancarios. Para frenarlos, el ministro de Economía impuso restricciones que implicaban el congelamiento de los fondos depositados en los bancos, medida conocida como el "corralito". La medida fue promulgada el 1 de diciembre y originalmente permitía sólo un retiro de 250 pesos en efectivo semanales, la prohibición de enviar dinero al exterior del país y la obligación de realizar la mayor parte de las operaciones comerciales mediante cheques, tarjetas de crédito o de débito, y tenía prevista una duración por 90 días. La deuda pública llegó a 132.000 millones de dólares, se registró una contracción mayor al 11% en la actividad fabril y al 20% en construcción en términos anuales, el PBI per cápita bajó 10% y la inversión un 30% y un déficit de 8.500 millones, sin contar el de las provincias. 

Ante una brusca caída de los depósitos y la fuga de divisas, el 1 de diciembre de 2001 se emitió el decreto 1570/2001, que establecía prohibiciones para el retiro de dinero de las entidades financieras por parte del público. Al restringir bruscamente la liquidez monetaria estas medidas ahogaron todo movimiento económico, paralizando el comercio y el crédito, rompiendo las cadenas de pago. Las restricciones al retiro de dinero, sumada a la incautación de los depósitos promovida por Domingo Cavallo fueron conocidas popularmente como Corralito. Estos hechos desembocaron en la crisis de diciembre de 2001 en Argentina.

El corralito fue altamente impopular y perjudicó todavía más a numerosos sectores de la economía argentina. El FMI, en tanto, endureció su posición y se negó a enviar 1.260 millones con los que se había comprometido a colaborar en el marco del préstamo conocido como "Blindaje", argumentando que la Argentina no habría cumplido sus compromisos de mantener el "déficit cero".

Al asumir intervino la Provincia de Corrientes que desde hacía meses estaba en un grave conflicto político y financiero, con paros ininterrumpidos y la destitución del gobernador Hugo Rubén Perie. La tarea le fue encomendada a Ramón Mestre que debió normalizar la situación provincial.

Lanzó iniciativas como el Plan de Infraestructura que mediante acuerdos con los gobiernos provinciales y financiamiento privado buscaba realizar obras de caminos, agua y vivienda en todo el país por 20 mil millones de dólares. La medida fue tomada por decreto ya que, según el ministro del interior Federico Storani, no había seguridad de que el Congreso sancionara la ley con la rapidez necesaria.El gobierno logró que en mayo del 2000 se apruebe la ley de flexibilización laboral que había impulsado los entonces ministros de Trabajo, Alberto Flamarique y Economía, José Luis Machinea. Los objetivos de la Ley de Reforma Laboral eran por un lado debilitar el poder gremial de los sindicatos grandes en favor de los pequeños y por otro bajar los costos laborales. La fuerte resistencia del justicialismo a votar el proyecto produjo modificaciones que atenuaron casi totalmente su efecto sobre la estructura gremial. En estas negociaciones participaron el entonces ministro de trabajo Alberto Flamarique y el líder sindical Hugo Moyano. Moyano se opuso con firmeza a la reforma, argumentando que la misma tendría como consecuencia una rebaja de los salarios y que estaría impulsada por el Fondo Monetario Internacional. Flamarique intentó conseguir el apoyo del sindicalismo prometiendo que los gremios nacionales seguirían siendo los destinatarios de la cuota sindical que aportan los afiliados, pero esto dividió internamente a la CGT.

Al poco tiempo de la sanción de la ley se desataría el escándalo de las coimas en el Senado, debido a denuncias de que el gobierno había sobornado a la oposición para lograr la sanción. El vicepresidente Carlos Álvarez renunció a su cargo el 6 de octubre de 2000, denunciando corrupción en la administración de De la Rúa y en el Senado nacional. La renuncia de Álvarez produjo un quiebre en la Alianza; aunque los funcionarios del Frepaso siguieron en sus cargos, muchos de ellos hasta el final de la gestión. Dicho quiebre se profundizaba en el Congreso: allí, la escasa mayoría que tenía el oficialismo en la Cámara de Diputados se iba reduciendo mes a mes a medida que legisladores de partidos de centroizquierda iban abandonando la coalición por diferencias políticas con el estilo de gobierno llevado adelante por De la Rúa.

La situación política en general era desfavorable. En el Senado la mayoría era del Partido Justicialista. En la Cámara de Diputados se tenía la mayoría pero ésta era mínima. El sindicalismo realizó 7 paros generales durante el gobierno delarruista, y la mayoría de las provincias argentinas tenían gobernadores propios del PJ. Tampoco disponía de un apoyo partidario claro, y varios sectores del radicalismo y el Frepaso comenzaron a distanciarse por la renuncia de Carlos Álvarez, el nombramiento de Domingo Cavallo y la política económica sostenida. El titular de la UCR, Raúl Alfonsín, intentaba evitar la ruptura partidaria. 

A mediados de año logra la aprobación parlamentaria de la ley de intangibilidad de los depósitos bancarios. En octubre la desocupación había trepado al 18,3%. A fin de año las reservas internacionales del BCRA bajarían a cerca de 20 mil millones de dólares.

En medio de estos problemas, también hubo aspectos positivos, como fue lograr que en el ciclo lectivo del año 2000 se cumplieran 180 días de clases, récord en más de una década, gracias al interés que se puso en aumentar las semanas del periodo escolar como en evitar conflictos docentes con el pago del incentivo docente.

En este marco de amplia problemática política y económica, se produjeron las elecciones legislativas de 2001, donde el Justicialismo se impuso con el 37 por ciento en todo el país, contra el 24% de una diezmada Alianza que perdía más de 4.500.000 votos respecto de lo logrado apenas dos años antes. Para el último bienio de gestión, el gobierno radical enfrentaría un Congreso totalmente opositor. El voto en blanco o nulo alcanzó cifras récord en la historia de la democracia argentina: la combinación de voto en blanco, voto nulo y ausentismo se elevó al 41%, equivalentes a 10,2 millones de argentinos.

La situación social motivó la generalización de grupos piqueteros a lo largo del país, una forma de manifestación que recurre al bloqueo total o parcial de rutas o calles como forma de protesta. Mientras que la derrota electoral, sumada a la renuncia de Álvarez, dejaba a una línea de sucesión presidencial completamente justicialista, con Ramón Puerta ejerciendo la presidencia provisional del Senado, y Eduardo Camaño la presidencia de la Cámara de Diputados.

Hacia el 19 de diciembre, la situación social se volvió incontrolable, con saqueos y desmanes en los puntos más importantes del país. El Presidente llamó a la población a la calma. De la Rúa respondió decretando el estado de sitio en todo el país. Sin embargo el vandalismo no disminuyó, e incluso aumentó a pesar del estado de sitio. Finalmente se produjeron 27 muertos y más de dos mil heridos. La rebelión popular, en lugar de terminar, sumó el apoyo de la clase media, histórico bastión electoral del radicalismo. La misma se implicó por el congelamiento de los depósitos bancarios. A la medianoche renunció el ministro de economía Domingo Cavallo y el resto del gabinete puso sus renuncias a disposición del Presidente.

El 20 de diciembre la Ciudad de Buenos Aires y el Gran Buenos Aires estaban desbordadas por una ola de saqueos a supermercados y establecimientos comerciales de diversos tipos. A esto se le sumó un cacerolazo generalizado y marchas de miles de personas autoconvocadas que reclamaban la renuncia del gobierno. En el centro porteño la Policía Federal lograba mantener la violencia fuera de la Plaza de Mayo.

Los gremios convocaron a huelgas como protesta por el estado de sitio. La CTA inició una huelga de 24 horas el 20 de diciembre. Al día siguiente (a pesar de haber tenido lugar la renuncia de De la Rúa) se sumarían la CGT de Rodolfo Daer en una huelga de 36 horas y la de Hugo Moyano por tiempo indeterminado. El presidente perdió el respaldo de la mayoría de su propio partido, la Unión Cívica Radical, y aferrado al escaso sector radical que aún le respondía intentó convocar al justicialismo a un acuerdo de gobernabilidad, sumándose al gobierno. El rechazo del PJ forzó, sin más remedio, a De la Rúa a presentar su renuncia al Parlamento a las 19:45 horas del 20 de diciembre de 2001, cuando no había completado sino apenas la mitad de su mandato. No obstante, tuvo que volver a la Casa Rosada el día siguiente para oficializarla, porque la Asamblea Legislativa no se había reunido aún.

La imagen del renunciante Presidente saliendo en helicóptero de la Casa Rosada, quedó grabada para siempre en el recuerdo de los argentinos. Este hecho tuvo además como afectado al partido político del expresidente, perdiendo la mayoría de las elecciones posteriores, debilitando a la Unión Cívica Radical frente a un peronismo en alza, que ante la ausencia de su rival clásico llegó a dividirse en dos frentes.

De la Rúa se retiró de la vida política y evitó apariciones o formular declaraciones, incluso respecto de las causas judiciales en su contra. Una de ellas se refiere a los eventos que tuvieron lugar al final de su mandato, durante los cuales murieron 30 personas en distintos lugares del país. Enrique Mathov, el exsecretario de Seguridad, acusó a De la Rúa de haber ordenado la represión. La causa fue llevada por el juez Claudio Bonadío, y estuvieron procesados el entonces subjefe de la Policía Federal, Osvaldo Cannizzaro, y otros comisarios que actuaron durante aquella tarde: Daniel Manzini, Próspero Treseguet, René Derecho y Alfredo Salomón. De la Rúa asegura que el 20 de diciembre ignoraba la gravedad de la situación: 
El juez, en cambio, desestimó la defensa de De la Rúa, y declaró lo siguiente:

Dicho procesamiento fue revocado el 29 de abril de 2008, cuando la Cámara Federal dictó la falta de mérito.

De la Rúa también fue procesado en una causa en que se lo acusa de sobornar legisladores para conseguir la aprobación de la Reforma Laboral del 2000. La causa por "cohecho activo agravado", y es llevada adelante por el juez federal Daniel Rafecas; fue promovida por el exsecretario parlamentario Mario Pontaquarto, quien confesó en los medios su intervención en dicha operación. Pontaquarto quedó procesado, al igual que Fernando de Santibañes, que era jefe de la SIDE; José Genoud, entonces presidente provisional del Senado, y Alberto Flamarique, ministro de Trabajo. Los exsenadores acusados de recibir las "coimas" (sobornos) fueron Alberto Tell, Remo Costanzo, Emilio Cantarero, Ricardo Branda y Augusto Alasino.

Durante 2006 el juez Jorge Ballestero procesó a De la Rúa, Domingo Cavallo y miembros de su equipo por el megacanje. Fundamentó su decisión en que éstos habrían cometido un crimen, excediendo el margen de discrecionalidad propio de las medidas políticas.

El 7 de abril de 2009, De la Rúa fue sobreseído por el Tribunal Oral Criminal 16 en la causa que investigaba la presunta contratación de un jardinero privado con sueldo del ex Concejo Deliberante. Se trataba de un desprendimiento de la megacausa por los denominados "ñoquis" del ex legislativo comunal. En el mismo día el juez Bonadío sobreseyó al expresidente en la causa en la que se lo investigaba por su presunta responsabilidad en las cinco muertes que hubo como consecuencia de la represión policial a las protestas que antecedieron su salida del Gobierno en diciembre de 2001.

Con respecto a su salud, mientras fue presidente fue operado mediante una angioplastia en el Instituto Cardiovascular de Buenos Aires en junio de 2001, y luego recibió una segunda angioplastia coronaria con estent en el Instituto del Diagnóstico en 2010 a manos del jefe de cardiología intervencionista de esa institución, doctor Luis de la Fuente, el médico pionero en Argentina de la medicina cardiovascular mínimamente invasiva quien en agosto de 2014 volvió a colocarle dos estents coronarios con medicamento.

El estado de salud del exjefe de Estado se había deteriorado mucho durante 2019. Había sido internado en el Hospital Austral el 1 de enero de ese año por problemas coronarios y renales que lo mantuvieron en terapia intensiva por cuatro semanas. En esas circunstancias tuvieron que realizarle una traqueotomía. Tras 28 días fue dado de alta y se realizó un tratamiento de rehabilitación en el Sanatorio Fleni de la localidad de Loma Verde.

El expresidente había sido tratado de su afección en el Instituto Alexander Fleming -especializado en tratamientos contra el cáncer- y con el empeoramiento de su cuadro fue enviado al Fleni sede Escobar.

Fernando de la Rúa falleció a las 7:10 del martes 9 de julio de 2019 en el Instituto de Rehabilitación y Educación Terapéutica Fleni sede Escobar, donde se encontraba internado a causa de una descompensación cardíaca y renal derivada de su cuadro oncológico.

El presidente Mauricio Macri decretó tres días de duelo nacional. El funeral de Estado tuvo lugar en el Salón de los Pasos Perdidos de la Cámara de Diputados. Sus restos fueron sepultados en el Cementerio Memorial de Pilar.




</doc>
<doc id="29588" url="https://es.wikipedia.org/wiki?curid=29588" title="Castellar de la Frontera">
Castellar de la Frontera

La villa de Castellar de la Frontera es un municipio español situado en la provincia de Cádiz, en la comunidad autónoma de Andalucía. Forma parte de la comarca del Campo de Gibraltar. Limita con los municipios de San Roque, San Martín del Tesorillo, Jimena de la Frontera, Los Barrios y Alcalá de los Gazules, en el parque natural de los Alcornocales, en la llamada Ruta del Toro. El municipio cuenta con tres núcleos de población, que son Castellar Viejo (conocido coloquialmente como El Castillo), Castellar Nuevo y La Almoraima. El pueblo viejo fue declarado Monumento Histórico Artístico en 1963. Castellar es económicamente fructífero por el descorche de los chaparros de la zona.
La temperatura es cálida, debido a la brisa mediterránea.

Castellar es popular por su castillo medieval, un destacado atractivo turístico.

Castellar tiene un escudo partido en dos campos. El primero de plata con tres fajas jaqueladas de oro y gules, cargadas cada una de una burela de oro, perteneciente al linaje Saavedra, condes de Castellar. El segundo de gules con un castillo de oro aclarado de azul. Va timbrado con la corona real española cerrada.

Las coordenadas geográficas de Castellar son 36º 19' N, 5º 27' O, a 137 kilómetros de la capital de provincia, Cádiz. Se encuentra situado a una altitud de 47 msnm. La extensión superficial de su término municipal es de 180,28 km². Según el INE, en 2018 contaba con 3013 habitantes, con una densidad de población 16,71 hab/km².

Por su estratégica situación sobre una peña, el pueblo viejo de Castellar de la Frontera, ha sido un lugar de asentamiento humano desde fechas muy tempranas (Cueva de las Estrellas). Durante el Paleolítico y el Neolítico la presencia del hombre está documentada a través de los hallazgos de un importante industria lítica y por numerosas pinturas rupestres, enmarcadas en el llamado arte rupestre del extremo sur de la península ibérica y situadas en la Cueva de los Maquis, de la Abejera, del Arquillo, de los Barrancones, de la Buitrera, del Cambulló, del Cancho, Cantarazo o de los Churretales, de las Cotillas, de los Números, del Rayo, de los Tajos, de las Tumbas y de la Ventana.
Se cree que los íberos construyeron la Torre Lascutana en Alcalá de los Gazules, cercana a Castellar. Durante la colonización romana, junto a dicha torre, que defendía la calzada romana que iba desde Carteia hasta Corduba, se creó un pequeño asentamiento del que se conservan restos de viviendas.

Del reino visigodo se conservan hallazgos. Con la islamización de la península ibérica en el 711, se construyó la ciudadela y se fundó la villa del pueblo viejo que, dado su emplazamiento en plena frontera granadina, jugó un importante papel en las Guerras de Granada en el . Asimismo en el periodo andalusí también en La Almoraima existió un pequeño asentamiento humano, donde habitaba la guarnición de la torre almenara allí situada.

Castellar de la Frontera fue uno de los eslabones de la cadena de fortalezas del reino nazarí, enlazando por el sur con la torre de Palmones y la bahía de Algeciras y al norte con Jimena de la Frontera. En ocasiones era cedida con otros castillos por los reyes de Granada a los benimerines como pago de su socorro contra los cristianos.

Juan de Saavedra, alcaide de Jimena de la Frontera, tomó el castillo en 1434 para la corona de Castilla. Juan II concedió a Juan de Saavedra la alcaldía de la villa, dándosela más tarde en señorío. Conquistada de nuevo para el reino de Granada, volvió a ser conquistada, una segunda vez, por Juan de Saavedra que recobró su señorío, siendo heredado durante muchas generaciones por los Arias de Saavedra, a los que Carlos I concedió en 1539 el título de condes de Castellar, título que ostentaron posteriormente sus descendientes. Desde entonces la economía local se basó en la agricultura, en la ganadería y en otras actividades relacionadas con el monte.

En el se construyó el Molino del Conde de Castellar, sobre el río Guadarranque. En 1603 se construyó el Convento de San Miguel de La Almoraima, de frailes descalzos de La Merced.

El tronco de la casa pasó al ducado de Santistevan en 1654 y más tarde al de Medinaceli. A principios del y mediante pleito, los Marqueses de Moscoso obtuvieron la posesión de la villa, que volvió a los Duques de Medinaceli en 1852, junto con el término de la Almoraima, hasta hace pocos años.

En 1868 el convento de la Merced pasó a ser propiedad del duque de Medinaceli.

En 1916 el Gobierno de la Nación añade a "Castellar" la denominación "de la Frontera" por el decreto de toponimia. Si bien existen pruebas en algunos escritos que se denominaba de la Frontera con anterioridad.

En 1945 se creó la Empresa Corchera Almoraima, que se convirtió en el motor de la economía local. Alrededor de ella se construyeron numerosas chabolas donde vivían los trabajadores de dicha empresa. En 1962, los propietarios construyeron viviendas para los empleados, aunque la mayoría de ellas siguió siendo chabolas. Por entonces La Almoraima contaba con más de 1500 habitantes.

En 1960 comenzaron las obras del embalse de Guadarranque. En 1968 el Instituto de Reforma y Desarrollo Agrario del Campo de Gibraltar, dentro del Plan de Desarrollo del Campo de Gibraltar, expropió 700 hectáreas de tierra a la empresa "La Almoraima S.A.", perteneciente a la Casa de Medinaceli, para la creación de un pueblo de colonización a 9 kilómetros de Castellar Viejo, siendo el último pueblo planificado
por el Instituto Nacional de Colonización en Cádiz. La tierra se dividió en parcelas que se entregaron a los colonos que las solicitasen. En 1971 se terminó el nuevo asentamiento y la mayoría de los castellarenses se trasladaron a Castellar Nuevo desde Castellar Viejo y desde La Almoraima, esta última a tan solo 1 kilómetro de distancia.

En 1980, gracias a la intervención Felipe González Márquez, posteriormente Presidente del Gobierno de España, se concedió definitivamente al ayuntamiento la potestad de gestionar y explotar económicamente las tierras de La Boyal, una finca de 526 hectáreas que había sido objeto de litigio entre los Condes de Castellar y los vecinos del municipio durante siglos. En 1981 Felipe González fue nombrado hijo adoptivo de Castellar por el papel que desempeñó en la recuperación de La Boyal.

En 1948 se construyó un apeadero junto a la línea de ferrocarril Bobadilla-Algeciras, para uso exclusivo de la Casa de Medinaceli. Hoy en día este apeadero es la estación de Almoraima.

Pese a la crisis económica actual, el de Castellar de la Frontera es uno de los tres únicos ayuntamientos de la provincia de Cádiz que no tienen deudas.

La construcción de la primitiva fortificación data de los siglos y , con estructura y detalles de torres y puertas característicos del reino de Granada. Aunque algunos elementos fueron añadidos más tarde. En la época cristiana (siglos al ), ya contaba con la muralla, torres y el alcázar-palacio.
El recinto está en parte almenado y protegido con torreones cuadrangulares en los ángulos que forman los lienzos, protegidos por el norte y suroeste con barbacanas. La puerta de acceso a la barbacana del norte se abre en una torre albarrana, existiendo otra torre en la que se abre la puerta con un arco apuntado de ladrillo. Entrando por el arco de la Villa, se encuentra el Alcázar o palacio de los condes de Castellar, formando parte de la fortaleza y muy transformado. Los materiales empleados en este conjunto de fortificaciones son de mampostería con piedras más o menos labradas y regulares, y ladrillos en los arcos. A partir del recinto fortificado, la población se desarrolla en su interior, incorporando algunos elementos a la muralla, con la intención de comunicar las casas adosadas a ella, que son las que primero se desarrollan. La acumulación del caserío intramuros a través del tiempo se corresponde con sus orígenes tardomedievales.

Las cubiertas de las casas son de teja árabe a una o dos aguas. Las fachadas no poseen salientes ni retranqueos y están encaladas, con guardapolvos o molduras en algunas de ellas. Las ventanas y puertas son de madera, con portillos en vez de cristales. La mayoría son de carácter unifamiliar, compuestas de una sala con cocina incluida, alcoba contigua y una o dos alcobas en la planta alta. Algunas poseen una cuadra. Las ventanas de la planta baja se protegen con rejas sencillas de hierro. En general la tipología corresponde a una arquitectura popular de carácter rural. Entre los edificios singulares, merece destacarse la Iglesia Parroquial del Salvador y el Ayuntamiento, con entradas desde la plaza mayor. La iglesia, de una sola nave, muy posiblemente se emplaza en el lugar de la antigua mezquita. Se sabe que su capilla mayor fue ejecutada por Juan Arias de Saavedra a principios del . En el fue ampliada, desapareciendo la primitiva estructura bajo pesadas yeserías y bóvedas de cañón. En el camino de Castellar y junto a la muralla, al pie del castillo se construye en 1603 el convento de Mercedarios de la Almoraima, del que se conserva el gran claustro con arquerías de piedra y de orden toscano, junto a la Ermita de Nuestra Señora de los Reyes de 1562. El desarrollo de la historia en este núcleo poblacional es un elemento decisivo que contribuye a valorarlo, pues parte de la historia ha quedado inmortalizada en su imagen y conformación singular.


El municipio cuenta con un colegio de educación primaria (CEIP Tierno Galván), un instituto de Educación Secundaria (IES Almoraima), un centro de educación de adultos (situado en el IES Almoraima), así como con una biblioteca pública y un teatro-cine.

Además del Castillo de Castellar, está prevista la creación de un centro de interpretación de la "Cueva de las Estrellas", declarada Bien de Interés Cultural.

La principal actividad económica es la agricultura, en la que predominan los cultivos de algodón, trigo y naranjo. Gran parte de la población se dedica al sector servicios, debido a la cercanía del municipio con otros núcleos de población del Campo de Gibraltar, como San Roque, La Línea de la Concepción, Los Barrios y Algeciras. La actividad turística también tiene una incidencia positiva en la economía de este municipio y en el la Campo de Gibraltar.








La principal vía de comunicación de Castellar de la Frontera es la . Esta vía lleva, hacia el norte, a la Serranía de Ronda pasando por Jimena de la Frontera y San Pablo de Buceite; y hacia el sur, a la Estación, Taraguilla y Miraflores, en San Roque, enlazando con la hacia Algeciras y La Línea.

Otras carreteras de Castellar son la a Sotogrande, la a San Martín del Tesorillo y la de Castellar Nuevo al Castillo de Castellar.

La estación ferroviaria de Castellar de la Frontera es la Estación de Almoraima, situada en la barriada del mismo nombre a un kilómetro al sur de Castellar Nuevo. En esta estación paran trenes de Media Distancia Renfe (Línea 70) con destino a Algeciras y Granada.



</doc>
<doc id="29589" url="https://es.wikipedia.org/wiki?curid=29589" title="Ceguera">
Ceguera

La ceguera, es una diversidad funcional de tipo sensorial que consiste en la pérdida total o parcial del sentido de la vista. Existen varios tipos de ceguera parcial dependiendo del grado y tipo de pérdida de visión, como la visión reducida, el escotoma, la ceguera parcial (de un ojo) o el daltonismo.


De acuerdo con la estimación de la OMS en 2002, las causas más comunes de ceguera alrededor del mundo son:

En España los accidentes, especialmente en los menores de 30 años, hacen perder la vista generalmente en uno de los ojos.

Personas con daños en el lóbulo occipital, a pesar de tener intactos los ojos y nervios ópticos, tendrían ceguera parcial o total.

Recientes descubrimientos en el genoma humano han identificado otras causas genéticas de baja visión o ceguera. Una de ellas es el síndrome de Bardet-Biedl.

Ciertos productos químicos, como el metanol (alcohol de quemar), que se utiliza para adulterar bebidas alcohólicas.

La malnutrición junto a las enfermedades son las causantes principales de la ceguera.
Exposición a ambientes que requieren gran esfuerzo visual durante largos periodos de tiempo.

En el 2002, la WHO (World Health Organization: Organización Mundial de la Salud) estimó que había 162 millones de personas (2.6 % de la población mundial) en el mundo con deterioro de la vista, de los cuales 124 millones (2 % aproximadamente) tenía baja visión y 37 millones eran ciegos (cerca de 0.6 %).

Existen organizaciones que han desarrollado programas para prevenir la ceguera.
Se recomienda ir al oculista cada 6 meses para un chequeo de la vista.

Además de la ceguera total existe la baja visión (ceguera parcial).

Una persona con baja visión es aquella persona que presenta en el mejor ojo, después de un tratamiento médico, quirúrgico y con corrección convencional, una agudeza visual que va de 20/70 hasta perdida de luz, o un campo visual desde el punto de fijación de 20 grados o menos, pero que es potencialmente capaz de utilizar la visión residual con propósitos funcionales.

Muchas veces (sobre todo en países en desarrollo) las personas con baja visión son tratadas como ciegas, un gran error ya que estas personas todavía tienen posibilidades de usar su resto visual con ayudas ópticas (telescopios, lupas potentes).

El braille es un sistema de lectura y escritura táctil pensado para personas ciegas. Fue inventado por el francés Louis Braille a mediados del siglo XIX, que se quedó ciego debido a un accidente durante su niñez mientras jugaba en el taller de su padre. Cuando tenía 13 años, el director de la escuela de ciegos y sordos de París –donde estudiaba el joven Braille– le pidió que probara un sistema de lecto-escritura táctil inventado por un militar llamado Charles Barbier para transmitir órdenes a puestos de avanzada sin tener necesidad de delatar la posición durante las noches. Louis Braille, al cabo de un tiempo descubrió que el sistema era válido y lo reinventó utilizando un sistema de 8 puntos. Al cabo de unos años lo simplificó dejándolo en el sistema universalmente conocido y adoptado de 6 puntos.

Importante, por ejemplo, para poder determinar el color de la ropa que el ciego se pone o compra, para separar la ropa que se ponga en la lavadora, saber si hay la luz encendida en una habitación (y poder encenderla o apagarla).

Así existen aparatos del tamaño de un mando de un TV, que se conecta a unos auriculares y la persona ciega entonces puede escuchar con voz humana la identificación del color. Se pone en contacto al lector del aparato con el objeto del que se quiere identificar el color, se pulsa un botón y el aparato dice el color. Es necesario que el objeto esté iluminado. Así puede decir rojo marrón oscuro o gris pálido. No puede identificar tramados de colores, deben ser sólidos.

También hay en la etapa final de diseño un aparato, basado en la sinestesia, que asociaría los colores a música. La tonalidad sería indicada por la nota musical (así una nota aguda indicaría un color de tonalidad clara y una nota grave una de oscura) y el color por el instrumento (así la flauta dulce indicaría el amarillo, el clarinete el azul, los tambores el rojo o el piano el verde). Se ha trabajado en niños y adolescentes.

Se trata de un sistema de identificación por el tacto, por tanto, similar al Braille. Actualmente utilizado en talleres educativos y de ocio para la identificación del color en obras de arte, que deben estar preparadas, es decir, que tengan un relieve con los signos y, mejor, unos límites del color y la tonalidad a identificar. Es, a diferencia de los sistemas electrónicos, independiente de un aparato o de un idioma. Desarrollado por Constanza Bonilla (Sistema Constanz), identifica el color por los colores básicos (amarillo: una línea recta; rojo: una línea en zigzag; azul: una línea ondulada), o en su combinación (así el verde sería una línea recta -amarillo- y una línea ondulada -azul-), y por tonalidades (claro: una redonda; oscuro: un punto; muy oscuro: cuatro puntos, etc.).

Son perros entrenados para guiar a personas ciegas o con daño visual.
Además, no solamente guía a personas ciegas a dirigirse a tal lugar, sino también, les ayuda a los quehaceres (tareas) de la casa como: vestirse, traer lo que el no vidente diga con el fin de satisfacer en totalidad las cosas de la vida diaria.

El crecimiento del uso educativo de las TIC ha sido partidario de la inclusión intermitentemente. Inicialmente, dado que dicho uso se abordó desde una sola materia del currículo escolar, el desempeño de niños videntes e invidentes era similar, pues juntos se acercaban al conocimiento tecnológico y digital. Esto fomentaba la inclusión y la autoestima de los infantes con discapacidad visual. Más adelante, en una segunda etapa, el crecimiento del uso educativo permitió que el acercamiento a las TIC se intensificara, haciendo que éstas pasaran a convertirse en una herramienta necesaria en todas las materias, lo que dificultó el desarrollo particular de los niños ciegos que no podían acceder a las TIC con la misma facilidad que sus símiles.

Finalmente, en la medida en que los agentes de la educación con TIC, desde los diseñadores de software educativo, hasta los docentes en el aula, adquieren un mejor entendimiento de las necesidades de los niños con ceguera, el crecimiento de las nuevas tecnologías termina por ofrecer nuevas oportunidades de inclusión. El desarrollo tecnológico en este sentido, visto en aplicaciones móviles, lectores de pantalla, herramientas táctiles, etc., es capaz de promover el desarrollo de los niños invidentes y su inclusión con los demás niños en el aula.

Se especula sobre la posibilidad en un futuro de curar la ceguera con células madre. Se trata de un proyecto totalmente inédito, que ayudaría a reparar las retinas dañadas utilizando para ello células obtenidas de cultivos de células madre de embriones humanos. Los creadores de esta técnica informaron que la cirugía necesaria es tan simple que algún día podría volverse tan rutinaria como lo son hoy las operaciones de cataratas.

Esta técnica es capaz de permitir a la gran mayoría de los pacientes con degeneración macular relacionada con la edad (DME) recuperar la vista. DME es una de las principales causas de ceguera entre los mayores de 50 años, que solo en Europa afecta a unos 14 millones de personas.

Actualmente existen algunos medicamentos, como el fabricado por Genentech Inc. (Lucentis) que pueden ayudar a uno de cada diez pacientes con un tipo de DME, llamada “DME húmeda”. El otro 90% de los pacientes tienen “DME seca”, para la que no hay tratamiento.

La degeneración macular relacionada con la edad es causada por una falla en las células del epitelio pigmentario de la retina (EPR), que forman una capa protectora bajo los conos y bastones sensibles a la luz que se encuentran en la retina. En algunos casos esta disfunción se ha asociado también al tabaquismo o los antecedentes familiares, entre otras causas.

El nuevo procedimiento ideado por los británicos consigue generar en el laboratorio células del epitelio pigmentario de la retina que sirven de “recambio”, a partir de células madre. Luego, los expertos inyectan en el ojo del paciente un pequeño parche de unos 4 a 6 milímetro formado con las células nuevas.

Los cirujanos logran restaurar la visión de algunos pacientes utilizando células madre de sus propios ojos, pero es un proceso es complicado y poco efectivo. La nueva técnica, que ha dado resultado en ratas, es mucho más prometedora. "Si no se ha vuelto algo de rutina en unos 10 años, significará que no hemos tenido éxito," dijo uno de los responsables. "Tiene que ser algo que esté disponible para una gran cantidad de personas," añadió. Se espera que la operación pueda ser realizada como un simple procedimiento de 45 minutos bajo anestesia local.

Extrañamente, este proyecto ha sido posible gracias a una donación de 8 millones de dólares que hizo un ciudadano estadounidense anónimo. Según los científicos del proyecto, esta persona se habría sentido frustrado por los límites que impone su país (EE.UU.) a la investigación con células madre.

Las células madre embrionarias son células maestras del cuerpo, capaces de generar todos los tejidos y órganos. Su uso es controvertido, porque muchas personas se oponen a la destrucción de embriones, aunque Gran Bretaña fomenta este tipo de investigación.

Se debe contactar con el médico o acudir a la sala de emergencias inmediatamente. La mayoría de las formas graves de pérdida de la visión son indoloras y la ausencia de dolor de ninguna manera disminuye la necesidad urgente de conseguir atención médica. Muchas formas de pérdida de la visión sólo dan un margen de tiempo breve en el cual se pueden tratar en forma exitosa.

Esta comunidad necesita tecnología adecuada debido a que esta discapacidad afecta al individuo evitando ver correctamente un documento en su forma original, ya sea en una hoja de papel o una pantalla de ordenador, por lo cual necesita que se le presente con un formato modificado. Necesita recurrir al tacto (es decir, al sistema braille, primordialmente), escuchar grabaciones sonoras, leer textos en caracteres agrandados, o bien utilizar un ordenador cuya tecnología permita agrandar en la pantalla el material de información visual o transformarlo en documentos sonoros o táctiles. Es decir necesitan un medio entre la información y ellos, que les permita cierta independencia y un acceso pleno al conocimiento.

Esto lo pueden obtener a través de elementos Tiflológicos los cuales son implementos que sirven a las personas con limitación visual para desempeñarse en su vida cotidiana de una manera autónoma. Ejemplos de ellos son el ábaco, la regleta y el bastón.

También podemos incluir la Tiflotecnología la cual conjunta técnicas, conocimientos y recursos para procurar a las personas con discapacidad visual, ceguera o sordo ceguera, los medios oportunos para la correcta utilización de la tecnología, contribuyendo a su autonomía personal, plena integración social, laboral y educativa.

Estos dispositivos pueden ser conjuntados en dos grupo.

Los que facilitan o permiten el acceso a la información del ordenador (sistemas de reconocimiento óptico o inteligente de caracteres, sistemas de reconocimiento táctil, revisores de pantalla, etc.)

Los que pueden conectarse al ordenador para intercambiar información, aun cuando también funcionan de forma autónoma y tienen su propia utilidad (sistemas portátiles de almacenamiento y procesamiento de la información, impresoras braille, aparatos de reproducción y grabación, calculadoras parlantes, diccionarios y traductoras parlantes, periódicos electrónicos adaptados para personas con discapacidad visual, programas de gestión bibliotecaria y de acceso a Internet, ampliación de la imagen, códigos de barras comprimidos para información de consumo y audiodescripción, etc.)




</doc>
<doc id="29590" url="https://es.wikipedia.org/wiki?curid=29590" title="Suprema Corte del Caribe Oriental">
Suprema Corte del Caribe Oriental

La Corte Suprema del Caribe Oriental (en inglés, "Eastern Caribbean Supreme Court") es el máximo tribunal judicial en seis países independientes (Antigua y Barbuda, Dominica, Granada, San Cristóbal y Nieves, Santa Lucía y San Vicente y las Granadinas) y tres territorios británicos de ultramar (Anguila, las Islas Vírgenes Británicas y Montserrat).

Está ubicado en la región este del Mar Caribe. Inició sus labores en 1967, sustituyendo a la Suprema Corte de los Estados Asociados de las Indias Occidentales y se compone de 19 ministros elegidos por una comisión a excepción del Ministro en Jefe, el cual es designado por el monarca del Reino Unido. La Corte es presidida desde 2003 por Sir Brian Alleyne.




</doc>
<doc id="29595" url="https://es.wikipedia.org/wiki?curid=29595" title="El Cabaco">
El Cabaco

El Cabaco es un municipio y localidad española de la provincia de Salamanca, en la comunidad autónoma de Castilla y León. Se integra dentro de la comarca de la Sierra de Francia. Pertenece al partido judicial de Ciudad Rodrigo y a la Mancomunidad Las Dehesas.

Su término municipal está formado por las localidades de El Cabaco, Peña de Francia, Zarzosillo y Zarzoso, ocupa una superficie total de 47,35 km² y según los datos demográficos recogidos en el padrón municipal elaborado por el INE en el año , cuenta con habitantes.

Aunque en la actualidad la denominación del municipio es la de El Cabaco, hasta hace relativamente poco se decía y escribía sistemáticamente "EL CAVACO", que algunos autores relacionan con una corrupción prosódica de "CAVADO" (forma romance del latín "CAVATUS") que enlaza con el significado de ""sitio donde se han hecho excavaciones"". De aquí provendrían el resto de nomenclatura asociada al pueblo: Las Cavenes, grandes zanjas existentes en el término ligadas a excavaciones romanas a cielo abierto; Arroyo Cavaquillo y Río Cavín o Gabín, denominación que recibe el arroyo lindante al núcleo principal, y que también responde al nombre de Arroyo de la Barranca, en la cartografía oficial.

El escudo heráldico que representa al municipio fue aprobado el 23 de julio de 1993 con el siguiente blasón:

El término municipal de El Cabaco se extiende principalmente en dirección norte sur y contiene variada geomorfología, extendiéndose por la vertiente norte de la Sierra de Francia. En la zona sur se encuentra la Peña de Francia, que con sus 1727 metros sobre el nivel del mar, corresponde a la mayor elevación del municipio; en las faldas de la misma se extiende una zona de relieve alomado con una cota media de 1000 metros. Estas estribaciones quedan condicionadas por los cursos del Arroyo de la Barranca al este y el Arroyo de Zarzosillo al oeste. En la zona central del término, entre estos cursos fluviales, se extiende una llanura conocida como "Campo el Potro" que está desprovista de vegetación arbórea. Al norte se encuentran las lomas finales de la Sierra de la Quilama y el inicio de la Submeseta Norte, que se extiende a una cota de unos 800 metros sobre el nivel del mar.

Su término municipal linda al norte con los de Tamames y Puebla de Yeltes, al este con Aldeanueva de la Sierra y Cereceda de la Sierra, al oeste con El Maíllo y al sur con Monsagro, La Alberca y Nava de Francia.

El núcleo poblacional principal se encuentra en la zona medio-oeste del municipio y se desarrolla siguiendo las vías de comunicación principales: Las carreteras de Salamanca a las Hurdes y la de Béjar a Ciudad Rodrigo y Portugal.

Cercano al término municipal de Tamames, se encuentra el Convento de Puerta Coeli, perteneciente a la finca agrícola de Zarzoso, aproximadamente a 7 kilómetros del casco urbano.

Siguiendo la carretera de La Alberca, a 15 km del pueblo, se encuentra el Santuario de Nuestra Señora de la Peña de Francia y un convento regentado por frailes dominicos. Se encuentra enclavado en lo alto de la Peña de Francia, junto a una reciente hospedería.

La finca de Zarzosillo, cuenta con unos accesos difíciles y está situado en la zona sudoriental del municipio.

Discurriendo por el término municipal se encuentra el Arroyo de la Barranca o Gabín que nace en la falda norte de la Peña de Francia; dicho curso fluvial pasa por el casco urbano y continúa en dirección norte hasta verter sus aguas en el Río Yeltes, proveniente de la cercana población de Cereceda de la Sierra.

La vegetación de El Cabaco está relacionada en gran medida por el clima y la morfología geológica de la zona. Son de gran importancia los robles; que dan forma a grandes extensiones de robledales por todo el término municipal. Existen además otros árboles de gran porte como castaños, nogales y, en menor medida, quejigos y encinas (más abundantes en la zona norte cercano al campo charro).

Por otro lado es importante destacar que el clima severo de la región ha permitido a los lugareños el cultivo de productos de la huerta, árboles frutales, fresas y, hasta hace poco, se empleaban grandes extensiones para la producción cerealística, fundamentalmente trigo y cebada.

Según el Instituto Nacional de Estadística, El Cabaco tenía a 31 de diciembre de 2018, una población total de 232 habitantes, de los cuales 117 eran hombres y 115 mujeres. De Respecto al año 2000, el censo refleja 285 habitantes, de los cuales 144 eran hombres y 142 mujeres. Por lo tanto, la pérdida de población en el municipio para el periodo 2000-2018 ha sido de 62 habitantes, un 32% de descenso.

El municipio se divide en tres núcleos de población. De los 232 habitantes censados en 2018, 219 corresponden a El Cabaco y 13 a Zarzoso. La Peña de Francia figura como despoblado y Zarzosillo no figura.

El Cabaco, como pueblo organizado, existía ya en 1215, año que fue anexado por Alfonso IX de León a la jurisdicción de la cercana villa de Miranda del Castañar; su independencia formal se inicia en 1811, cuando el gobierno de la nación abolió los señoríos y la jurisdicción de Miranda sobre El Cabaco.

El asentamiento poblacional primitivo se extendía por la zona lindante al río Gabín, a un lado y otro del curso, comunicándose ambas zonas con la construcción de un puente que aún existe. Esas construcciones, que constituyen en la actualidad el "barrio del puente", fue el núcleo germen de lo que en la actualidad es El Cabaco (S XIII y S XIV).
Fue en los siglos subsiguientes, cuando se comenzaron a construir los barrios anejos del Altozano, calle Estrecha y las Huertas siguiendo la dirección Oeste. Fue a partir de este momento cuando comenzó la construcción de la Iglesia de El Cabaco y el ayuntamiento, en torno a los ejes principales de la Calle Mayor y Calle Larga (S XVI y siguientes).

Con la creación de las actuales provincias en 1833, El Cabaco quedó encuadrado en la provincia de Salamanca, dentro de la Región Leonesa.

En la actualidad, en El Cabaco su núcleo urbano alcanzó su máxima extensión en la década de 1960, aunque comienzan a realizarse construcciones en la carretera de Ciudad Rodrigo y Béjar y por la zona sur-occidental (cercana a los establecimientos hosteleros del nudo viario Salamanca - La Alberca / Béjar - Ciudad Rodrigo.

El alcalde de El Cabaco no recibe ningún tipo de prestación económica por su trabajo al frente del ayuntamiento (2017).

En El Cabaco han tenido desde siempre una gran importancia las fiestas, siempre asociadas a eventos agrícolas destacados como la siembra y la cosecha. La fiesta principal del pueblo, y a la que refiere su Santo y Patrón se celebra en la última semana de agosto, concretamente el día grande es el 25, en honor de la Degollación de San Juan Bautista (el Martirio de San Juan Bautista se conmemora el 29 de agosto, pero se adelanta unos días para facilitar la afluencia de gente).

También tienen importancia otras festividades como la Emperrá ( domingo de Pentecostés), la Fiesta de los Mozos (26 de diciembre), la Santa Cruz( 3 de mayo) y la Semana Santa con la subida en procesión ( emulando el calvario) a la cruz situada en una loma cercana al núcleo poblacional denominada " la cuesta".

Durante estos períodos festivos, es común que se ponga a la vista la desarrollada bisutería charra, fruto de una artesanía propia del lugar con base generalmente en plata, y cuyo mayor exponente, sobre todo en la vestimenta masculina, se encuentra en el botón charro (el cual evoca a un rosario de mano con sus 10 cuentas en forma de pequeñas protuberancias).

Es también común el uso de rosarios grandes, diademas, anillos y una sobrecargada ornamentación (típica de esta zona meridional de Salamanca) que se luce en los bailes regionales característicos ( bailes serranos) , parecidos a la jota castellana, pero con un baile más lento.




</doc>
<doc id="29597" url="https://es.wikipedia.org/wiki?curid=29597" title="La Cabeza de Béjar">
La Cabeza de Béjar

La Cabeza de Béjar es un municipio y localidad española de la provincia de Salamanca, en la comunidad autónoma de Castilla y León. Se integra dentro de la comarca de la Sierra de Béjar. Pertenece al partido judicial de Béjar.

Su término municipal está formado por un solo núcleo de población, ocupa una superficie total de 13,91 km² y según los datos demográficos recogidos en el padrón municipal elaborado por el INE en el año , cuenta con una población de habitantes.

Según el Instituto Nacional de Estadística, La Cabeza de Béjar tenía, a 31 de diciembre de 2018, una población total de 86 habitantes, de los cuales 46 eran hombres y 40 mujeres. Respecto al año 2000, el censo refleja 106 habitantes, de los cuales 58 eran hombres y 48 mujeres. Por lo tanto, la pérdida de población en el municipio para el periodo 2000-2018 ha sido de 20 habitantes, un 19% de descenso.

Tenemos noticia de una mujer (llamada Isabel García Martín) por la que el rey Alfonso VIII de Castilla concedía tierras a la ciudad de Ávila en 1193, y le señalaba como frontera, del lado de Occidente, el conocido todavía hoy con el nombre de Arroyo de la Mula. Reza de esta manera:"Luego, desde lo alto del puerto de Xerit hasta el lugar donde nace Corpedumne (el río Cuerpo de Hombre); de aquí abajo, donde pasa la calzada que está en Corpedumne (Calzada de la Plata); después por la Calzada del "arroyo de la Mula"; desde el arroyo de la Mula abajo hasta donde cae en el Tormes".

El Arroyo de la Mula es un regatillo que atraviesa la pequeña cuesta por la que,viniendo de Béjar, se sube hacia Guijuelo. Nace en el alto de Tonda y bordeando las tierras de Guijuelo, la Cabeza y el Guijo de Ávila, por este pueblo rinde sus aguas al Tormes. El Arroyo de la Mula seguirá siendo punto de referencia para señalar, mirando hacia el norte, los límites de la futura Comunidad de Villa y Tierra de Béjar y de la nueva Diócesis de Plasencia.

Sabemos que la Reconquista, por lo que toca a Ávila, es decir, de parte de Castilla, había llegado hasta aquí, hasta los límites naturales de la Cabeza: pasando el arroyo de la Mula, se entraba en tierras del Reino de León en las que por Salvatierra se llegaba a la ciudad de Salamanca. De este modo, La Cabeza, con toda la Tierra de Béjar, era de Castilla. Del castillo, puesto defensivo y a la vez de vigilancia, y del asentamiento que se iba formando a su alrededor, sale, pues, el nombre de la Calzada de Béjar, es decir , principio o comienzo primero del Concejo (ó Comunidad de villa y tierra) de Béjar y más tarde de la diócesis de Plasencia. No es simplemente " Cabeza", sino " La Cabeza", lugar en el que, frente al condado de Salvatierra y las tierras de Salamanca, empezaba una nueva demarcación: la de la tierra de Béjar. Puede decirse también que en ese lugar terminaba el Reino de León y empezaba el de Castilla.

Como parte de la comunidad bejarana, tras la pérdida del voto en Cortes de Béjar y su paso a depender de Salamanca en ese aspecto a partir de 1425, hecho favorecido por el paso de Béjar y su territorio a manos de los Zúñiga en 1391, La Cabeza pasó a formar parte del Reino de León, en el que se ha mantenido en las divisiones territoriales de Floridablanca en 1785 y finalmente en la de 1833 en que se crean las actuales provincias, quedando integrado La Cabeza de Béjar en la provincia de Salamanca, dentro de la Región Leonesa, formando parte del partido judicial de Béjar.

Iglesia parroquial católica bajo la advocación de La Purísima Concepción, ermita de San Roque, en la Archidiócesis de Mérida-Badajoz, Diócesis de Plasencia, Arciprestazgo de Fuentes de Béjar.



</doc>
<doc id="29599" url="https://es.wikipedia.org/wiki?curid=29599" title="Cabeza del Caballo">
Cabeza del Caballo

Cabeza del Caballo es un municipio y localidad española de la provincia de Salamanca, en la comunidad autónoma de Castilla y León. Se integra dentro de la comarca de Vitigudino y la subcomarca de La Ramajería. Pertenece al partido judicial de Vitigudino.

Su término municipal está formado por las localidades de Cabeza del Caballo y Fuentes de Masueco, ocupa una superficie total de 36,71 km² y según los datos demográficos recogidos en el padrón municipal elaborado por el INE en el año , cuenta con habitantes.

Por el término municipal pasa el río Uces, el cual da agua al conocido Pozo de los Humos, que es una cascada de unos 50 m de altura, situado entre las localidades vecinas de Pereña y Masueco.

Cabeza del Caballo se encuentra situada en el noroeste salmantino. Dista 90 km de Salamanca. 

Se integra dentro de la comarca de La Ramajería. Pertenece a la Mancomunidad Centro Duero y al partido judicial de Vitigudino.

En su término municipal están integradas las localidades de Cabeza del Caballo y Fuentes de Masueco.

Parte de su término municipal se encuentra dentro del parque natural de Arribes del Duero, un espacio protegido de gran atractivo turístico.

El escudo heráldico que representa al municipio fue aprobado el 8 de noviembre de 2008 con el siguiente blasón:

La bandera municipal fue aprobada también el 8 de noviembre de 2008 con la siguiente descripción textual:

Según el Instituto Nacional de Estadística, Cabeza del Caballo tenía, a 31 de diciembre de 2018, una población total de 291 habitantes, de los cuales 157 eran hombres y 134 mujeres, de los cuales 240 en Cabeza del Caballo y 51 en Fuentes de Masueco. Respecto al año 2000, el censo refleja 482 habitantes, de los cuales 251 eran hombres y 231 mujeres, de los cuales 379 en Cabeza del Caballo y 103 en Fuentes de Masueco. Por lo tanto, la pérdida de población en el municipio para el periodo 2000-2018 ha sido de 191 habitantes, de los cuales 139 en Cabeza del Caballo y 52 en Fuentes de Masueco, un 40% de descenso.

El pueblo posee muchos cursos fluviales poblados por peces como la tenca, muy apreciada por los pescadores, que aprovechan su tiempo de ocio para pescar.

La fundación de Cabeza del Caballo se remonta a la Edad Media, obedeciendo a las repoblaciones efectuadas por los reyes leoneses en la Alta Edad Media, quedando encuadrado dentro del Alfoz de Ledesma tras su creación por el rey Fernando II de León en el siglo XII.

Con la formación de las actuales provincias en 1833, Cabeza del Caballo quedó integrado en la provincia de Salamanca, dentro de la Región Leonesa, pasando a formar parte del partido judicial de Vitigudino en 1844.

Su economía está basada fundamentalmente en la ganadería y agricultura.

Las fiestas mayores de la localidad se celebran el primer domingo de octubre y están dedicadas a la Virgen del Rosario. La devoción es muy grande y las protagonistas son las madrinas.

El juego tradicional es el "tiro al palancón", que es una barra grande de hierro usada para picar las piedras, de peso considerable, que hay que lanzar de parado de tal forma que haga y pequeño giro en el aire y caiga de punta.


El alcalde de Cabeza del Caballo no recibe ningún tipo de prestación económica por su trabajo al frente del ayuntamiento (2017).





</doc>
<doc id="29600" url="https://es.wikipedia.org/wiki?curid=29600" title="Taquigrafía">
Taquigrafía

La taquigrafía o estenografía es todo aquel sistema de escritura rápido y conciso que permite transcribir un discurso a la misma velocidad a la que se habla. Para ello se suelen emplear trazos breves, abreviaturas y caracteres especiales para representar letras, palabras e incluso frases.

Generalmente la escritura taquigráfica omite partes de los textos, y un texto recogido por un taquígrafo no puede ser entendido fácilmente por otro que no haya escuchado previamente el original. Por tanto, los mismos taquígrafos deben transcribir posteriormente el texto taquigráfico que han tomado a la escritura habitual.

El aprendizaje de la taquigrafía es relativamente fácil, pero su uso rápido requiere mucha práctica.

Es importante resaltar que cada taquígrafo posee una forma muy propia de escribir sus signos taquigráficos por lo que es imposible estandarizar el tamaño y la forma exacta de los mismos.

Los orígenes de la taquigrafía se remontan a la época del historiador griego Jenofonte, que se valió de esta técnica para transcribir la vida de Sócrates.

La taquigrafía; de las voces griegas "taxos" (‘celeridad’, ‘rapidez’) y "grafos" (‘escritura’), se vale de signos más sencillos que los de la escritura corriente para escribir tan deprisa como se habla. La usaron los fenicios y griegos, y en Roma desde la época de Cicerón, según Plutarco. En Roma se llamó «"notae tironianae"», pues la usó y perfeccionó Marco Tulio Tirón, esclavo y luego liberto de Cicerón, desde aproximadamente el 70 a. C. Un sistema parecido al de Tirón parece atribuírsele a Mecenas, de acuerdo con Dión Casio. Poco después el sistema fue ordenado en forma de diccionario por Séneca, llamándose «"Notas de Tirón y Séneca"». Sin embargo, cayó en el olvido con la invasión de los bárbaros. 

Posteriormente, se ha restablecido en la Edad Moderna. Comenzó de nuevo en Inglaterra, en 1588 por Timothy Bright. Siguió por Francia, Países Bajos, Alemania desde el siglo XVII. Llegó a Italia en el siglo XVIII y a España en 1800. Se considera que el inventor de la taquigrafía española es el valenciano Francisco de Paula Martí, en 1802. Su sistema se considera como el más perfecto de los conocidos hasta el siglo XIX.

El término fue utilizado por primera vez por Thomas Shelton en 1641, luego por Coulon de Thévenot en 1776 y finalmente adoptado por Martí. El inglés John Willis, inventor del primer sistema geométrico, la había denominado estenografía.

La taquigrafía era empleada comúnmente en los juicios, en cuyo caso se hacía uso de taquígrafos o estenógrafos para plasmar por escrito lo hablado. También era utilizado por secretarias y ayudantes de administración para apuntar las notas, cartas, recados y pedidos que reciben. Asimismo su uso era común para registrar los debates parlamentarios, y para la transmisión en directo de programas de televisión que requieren subtitulaje, por ejemplo, con el sistema Closed Caption (CC). También los intérpretes solían usar la taquigrafía como apoyo para la memoria cuando debían pasar un mensaje de un idioma a otro. Teóricamente era una herramienta muy útil para un estudiante cuando se asistía a clases para tomar apuntes.

El método Pitman utiliza los renglones de una hoja; esto significa que la posición de los símbolos es importante en relación a las horizontales de la tablilla de escritura.

Isaac Pitman nació en Trowbridge Wiltshire en 1813 y falleció en Bath en 1897. Aprendió el sistema de Taylor, y pensaba editarlo con las reformas que le había introducido, pero un amigo suyo, Bagsters, le sugirió la idea de crear un sistema propio.

En 1837, Pitman lanzó su famosa obra "Stenographic Soundhand". A pesar de que los signos de ambos autores puedan ser parecidos, se trata de sistemas muy distintos. 

Si bien se valió de recursos que ya habían sido utilizados por otros autores —la escritura fonética ya había sido utilizada por Tiffin (1750), Lyle (1762), Roe (1802) y Towndrow (1831); los signos semejantes para sonidos parecidos habían sido usados por Hervey (1779); las posiciones habían sido empleadas por farr (1819) y Taylor; los trazos finos y gruesos datan de la época de Tirón (50 a. C.); y ni siquiera fue el primero en utilizar el círculo pequeño para representar la S—, fue el primero en ordenarlos sistemáticamente, basándose en un análisis científico de su idioma. 

En 1840 apareció la segunda edición, con el título "Phonography, or Writing by Sound; being a natural method of writing, applicable to all languages, and a complete system of Short-hand", con un alfabeto de 36 signos. En 1842 se publica el primer número del "Phonetic Journal" y en 1846 se crea en Londres la primera sociedad pitmaniana, llamada Phonetic Society. 

En 1863 aparece en Londres la 11.ª edición de su obra: "A manual of Phonography, or, Writing by Sound: a natural method of writing by signs that represent spoken sounds; adapted to the English language as a complete system or Phonetic Short-hand". En ella, su autor dice: "«¿Quién, que esté en el oficio de escribir, no ha deseado en algún momento expresar mediante dos o tres trazos de la pluma aquello que en la actualidad requiere tanto tiempo y trabajo para llevarlo al papel?»". 

La obra de Pitman se ha adaptado a todos los idiomas. Isaac Pitman dice de su obra: "«Una vigorosa propaganda y métodos a bajo precio han contribuido a darle esta distinción, si bien su mérito sobre los demás sistemas ha sido apreciado por el público. Mi pequeño método ha sido vendido a un penique, y de mi libro" Phonografic Teacher "van vendidos más de un millón seiscientos mil ejemplares»". En su sistema, las consonantes forman pares representados por el mismo trazo, que se diferencian por el grosor. Representa las vocales con puntos, comas y guioncitos delgados y gruesos. La colocación de las consonantes, en la línea, arriba o debajo del renglón, indica la vocal que las acompaña y llama a la unión de dos o más palabras sin levantar el lápiz. Las consonantes R y L que siguen a la consonante base de la sílaba son representadas por una modificación al inicio de ésta. El "Manual Pitman's Shorthand-Phonography - New Era Edition" contiene un total de 189 reglas. 

La biografía de Pitman fue escrita en 1902 por su hermano Benn y en 1908 por Alfred Baker. 

La velocidad máxima alcanzada es de 360 palabras por minuto (durante dos minutos), por Nathan Behrin en 1922 (según el "libro Guinness de los récords").

En 1864 el sacerdote y filósofo español Pedro Garriga Marill publicó su obra «"La taquigrafía sistemática"» en la que detallaba el, posteriormente conocido como, "Sistema de Taquigrafía Garriga". Este sistema junto con el desarrollado por Francisco Martí, llamado "Sistema de Taquigrafía Martí" fueron los más utilizados en España dado el apoyo institucional que ambos recibieron y a su especial adaptación a la lengua castellana.

En 1872 se funda en Barcelona la primera sociedad taquigráfica española, la Corporación Taquigráfica del Sistema Garriga, formada por el propio Garriga, el catedrático José Balari Jovany y otros seguidores del sistema con el fin de fomentar y expandir su uso en España.

El irlandés John Robert Gregg publicó en Liverpool "Light Line Phonography" que traduce al español «"Fonografía de escritura sencilla"» en 1888, cuando apenas tenía veinte años. Fue enjuiciado por plagio por Thomas Malone, un antiguo compañero de estudios, y su libro no pudo ser vendido en Inglaterra. Por ese motivo en 1893 viajó a Estados Unidos, donde pudo difundir su sistema y logró gran resonancia. 

La taquigrafía Gregg está basada en la elipse, siguiendo la inclinación de la escritura corriente. Las vocales se escriben en su orden natural, no diferenciando los signos por el grosor o la posición sino solamente por los diferentes tamaños. La escritura es esencialmente horizontal. 

La primera adaptación del sistema Gregg al español fue realizada por el mexicano Camilo E. Pani en 1904, que apareció con el título "Taquigrafía fonética Gregg-Pani". A esta siguió otra en 1921 titulada "Taquigrafía Gregg", la cual se asemeja más a la escritura de la versión inglesa. Una edición revisada por Louis Leslie se publicó en 1923. Esta edición, conocida como "Taquigrafía Gregg Edición Aniversaria", se difundió por América Latina. Después de la muerte de John Robert Gregg en 1948, se pública en 1953 una revisión del sistema, conocida como "Taquigrafía Gregg Simplicada", que contiene cambios introducidos en la versión inglesa del sistema destinados a reducir la carga mental del taquígrafo y simplificar el aprendizaje de la taquigrafía. En 1969 surge la "Taquigrafía Gregg Edición Diamante", una nueva edición con numerosas simplificaciones destinadas a aligerar la escritura y aprendizaje. En esta edición se reenfoca el uso de la taquigrafía exclusivamente al área comercial. La edición Aniversaria fue reimpresa en 1970 bajo el título "Curso de taquigrafía Gregg", que en adición al texto básico, contiene un manual de ejercicios "(Ejercicios progresivos de la taquigrafía Gregg)" y un libro de lecturas "(Ejercicios graduados)". Una nueva edición fue lanzada en 1978, llamada "Taquigrafía Gregg Serie 90", con aún mayores simplificaciones. La última edición en español publicada en 1990, "Taquigrafía Gregg Edición Centenaria", retiene los principios de la Serie 90 e incluye actualizaciones del vocabulario para la oficina electrónica, además de un rearreglo en la presentación de los principios del sistema. Cada simplificación del sistema trae consigo, no sólo un aumento en la facilidad de aprendizaje y escritura, sino también una disminución en el potencial de rapidez en el dictado, ya que los trazos resultan más largos y demorados con la eliminación de reglas. Por este motivo, aunque la Edición Aniversaria resulte la más difícil de aprender y dominar por las muchas abreviaciones y reglas contenidas en ella, es la que se considera la más rápida de todas las versiones y la más adecuada para trabajo parlamentario y legal. 

Se han publicado otras adaptaciones del sistema Gregg. En 1931, Salvador F. Seguí publicó "Taquigrafía Seguí", sistema basado en el Gregg. En 1932, publicó su "Exposición de la taquigrafía Gregg", revisada en 1974 "(Exposición revisada de la taquigrafía Gregg)" y en 1984 "(Nueva exposición revisada de la taquigrafía Gregg, Edición Oro)".

El sistema de Gregg se ha adaptado a muchas lenguas, incluyendo afrikáans, esperanto, francés, alemán, hebreo, irlandés, italiano, japonés, polaco, portugués, ruso, español, catalán y tagalo. Con unas pocas adaptaciones, puede ser utilizado en casi cualquier lengua. Después del inglés, la adaptación al español es la más popular.

Con el sistema Gregg, se han registrado velocidades de hasta 280 palabras por minuto en inglés.

Gabriel Hilario Larralde (1853-1941) fue el creador de la "Estenografía Argentina", también denominada "Taquigrafía Larralde", la cual es utilizada desde fines del siglo XIX y hasta la actualidad por la mayoría de los taquígrafos de las Cámaras de Senadores y de Diputados del Congreso de la Nación Argentina.

El sistema Carissimi fue inventado en Uruguay en la década de 1940. Este sistema se compuso basándose en tres sistemas: Gregg, Pitman y un tercer sistema de origen inglés. Se compone de signos rectos, como los sonidos K, P, D y F, por signos de medio círculo, como los sonidos J, CH, Ñ, y los signos de cuarto de círculo, como los sonidos L, T, S y B. Este sistema es el más usado en Uruguay.

"Teeline" es una taquigrafía aceptada por el NCTJ, una asociación de periodistas del Reino Unido. Es adaptable a diferentes idiomas, pero principalmente es utilizado en los países de la Mancomunidad de Naciones.

Fue desarrollado en 1970 por James Hill, un profesor de Taquigrafía Pitman, y básicamente consiste en eliminar las letras innecesarias y hacer las letras de manera que sean más rápidas de escribir. Las vocales son por norma general eliminadas cuando no están al inicio o al final de la palabra, y las letras mudas también son ignoradas. Los prefijos, sufijos y letras «pares» (como «pr» o «bl») se reducen a un simple símbolo. Los símbolos derivan de las letras del alfabeto occidental, eliminando las partes más innecesarias.

El Teeline difiere de Pitman y Gregg en que Teeline es un sistema alfabético, mientras que Pitman y Gregg son fonéticos. Por tanto, Teeline es mucho más fácil de aprender, pero no es tan rápido como los sistemas fonéticos.

Teeline se parece mucho a los "grafitis" utilizados con el lápiz de las agendas electrónicas tipo Palm. 

La velocidad a que se puede llegar es de 140 palabras por minuto.




</doc>
<doc id="29601" url="https://es.wikipedia.org/wiki?curid=29601" title="Cabezabellosa de la Calzada">
Cabezabellosa de la Calzada

Cabezabellosa de la Calzada es un municipio y localidad española de la provincia de Salamanca, en la comunidad autónoma de Castilla y León. Se integra dentro de la comarca de La Armuña. Pertenece al partido judicial de Salamanca y a la Mancomunidad La Armuña.

Su término municipal está formado por un solo núcleo de población, ocupa una superficie total de 8,49 km² y según los datos demográficos recogidos en el padrón municipal elaborado por el INE en el año , cuenta con una población de habitantes.

El nombre de Cabezabellosa se repite aplicado a distintos parajes y pueblos; así, por ejemplo, en el cacereño lugar de Cabezabellosa. Riesco Chueca sitúa su origen en metáforas capilares, del tipo del francés "pelouse" ‘césped’, partiendo del latín VILLŌSUS 'velludo, hirsuto'. En tales denominaciones puede haber tenido influencia el frecuente hecho de que los accidentes del relieve o del roquedo vayan acompañados de una cubierta vegetal diferenciada: pequeños o grandes parches de matorral aislado, o de vegetación herbácea densa o enmarañada pueden haber suscitado la comparación con barbas, cabellos o vellos.

Similar origen tendrán topónimos como Peñavellosa y Valvellosa (Molinaferrera, León) y también el cerro de Capeloso (La Faba, León) o el pueblo de Capileira (Granada). Puede compararse también el topónimo, frecuente en Portugal, Barbosa, que sin duda tiene un referente vegetal, a la vista de su sufijo. El árabe vulgar ŠA‛RA, del que procede el castellano jara, es el femenino de un adjetivo que significa ‘velloso, peludo’, pero pasó a significar en el norte de África y Al-Andalus ‘bosque, bosquecillo’ y luego ‘matorral, mata’. El uso metafórico en Cabezavellosa se basa en una coordinación de símiles: un altozano es equiparado a una cabeza y su vegetación, al vello de ésta (entendido como pelo hirsuto, crespo o rizado). Tal hipótesis, que implica una coordinación o refuerzo metafórico, parece darse también en el abundante topónimo Cabezamesada (en Toledo y otras partes), que habrá de entenderse como 'cabeza cuyo pelaje parece arrancado'; ello siempre aludiendo a la cubierta vegetal del altozano en cuestión, y sobre la base del antiguo "mesar" 'arrancar[se] los pelos'.

El determinativo, "de la Calzada", hará alusión a la ubicación del pueblo sobre la antigua calzada que unía Salamanca y Medina del Campo.

Fundado por los reyes de León en la Edad Media, Cabezabellosa de la Calzada quedó encuadrado en el cuarto de Villoria de la jurisdicción de Salamanca, dentro del Reino de León, denominándose entonces Cabeçavellosa. Con la creación de las actuales provincias en 1833, Cabezabellosa de la Calzada quedó encuadrado en la provincia de Salamanca, dentro de la Región Leonesa.



</doc>
<doc id="29605" url="https://es.wikipedia.org/wiki?curid=29605" title="Cabrerizos">
Cabrerizos

Cabrerizos es un municipio y localidad española de la provincia de Salamanca, en la comunidad autónoma de Castilla y León. Se integra dentro de la comarca de La Armuña. Pertenece al partido judicial de Salamanca y a la Mancomunidad La Armuña.

Su término municipal está formado por las localidades de Aldehuela de los Guzmanes, Arenal del Ángel, Cabrerizos, Casablanca, La Carcesa, La Flecha, La Granja, Las Dunas y Vivero Forestal, ocupa una superficie total de 11,54 km² y según los datos demográficos recogidos en el padrón municipal elaborado por el INE en el año , cuenta con una población de habitantes.

La fundación de Cabrerizos se remonta a la repoblación efectuada por los reyes de León en la Edad Media, quedando integrado en el cuarto de Villoria de la jurisdicción de Salamanca, dentro del Reino de León, teniendo en el siglo XIII la denominación Cabrarizos. Con anterioridad, el actual despoblado de Ribas, situado en el término de Cabrerizos, había sido repoblado por el rey Ramiro II de León en el siglo X. Con la creación de las actuales provincias en 1833, Cabrerizos quedó encuadrado en la provincia de Salamanca, dentro de la Región Leonesa.

Cabrerizos está situado a orillas del río Tormes, al lado de Salamanca. 

Linda por el sur con Santa Marta de Tormes, Pelabravo y Calvarrasa de Abajo, por el este con Aldealengua, por el norte con Moriscos, Castellanos de Moriscos y Villares de la Reina y por el oeste con Salamanca.

Está situado a cierta altura porque años atrás cuando había inundaciones todo lo que estaba alrededor del río se cubría con agua y producía destrozos. por eso al estar en altura el agua no llegaba al pueblo en esos momentos de inundación.

Entre 1996 y 2006, Cabrerizos ha experimentado un explosivo aumento, de un 128,8%, en su población. Este crecimiento demográfico lo transforma en el 5º municipio de la provincia de Salamanca en cuanto al aumento de población en dicho período. Su cercanía a la capital provincial, de cuyo alfoz forma parte, le permite recoger parte de la migración residencial que se está produciendo desde aquella.
El municipio se divide en varios núcleos de población, que poseían la siguiente población en 2019 según el INE.



</doc>
<doc id="29618" url="https://es.wikipedia.org/wiki?curid=29618" title="Dispositivo Braille">
Dispositivo Braille

Por dispositivo braille (electrónico) se enmarca a cualquier aparato electrónico que sirva para la interpretación o generación de sistema braille, tanto de forma física como virtual.

Podemos distinguir entre los siguientes dispositivos:

Los dispositivos braille suelen ser utilizados como periféricos externos en un ordenador o en una PDA.También pueden ir embebidos en otro dispositivo. Por ejemplo una impresora braille puede llevar incorporado una línea braille para su uso por personas con discapacidad visual.

Una línea Braille es un dispositivo Braille que está basado en un mecanismo electro-mecánico capaz de representar caracteres Braille mediante la elevación de puntos a través de una superficie plana con agujeros hechos a tal efecto. 

Debido a la complejidad y elevado coste de estos dispositivos, sólo suelen tener 20, 40 u 80 celdas. También suelen disponer de botones para desplazar el texto y parar realiza otras funciones especiales. En algunos modelos la posición del cursor se representa por la vibración de los puntos y en muchos de ellos existe un botón por cada celda para llevar el cursor a esa posición asociada.

Tiflobuntu es la versión de la distribución Ubuntu para personas ciegas y con visión reducida.




</doc>
<doc id="29622" url="https://es.wikipedia.org/wiki?curid=29622" title="Línea braille">
Línea braille

Una línea braille es un dispositivo electrónico que permite la salida de contenido en código braille desde otro dispositivo, al cual se ha conectado, permitiendo a una persona ciega o con baja visión acceder a la información que éste le facilita. En algunos sitios se pueden referir a las líneas braille como pantallas braille.

Las líneas braille muestran de forma táctil la información que un "lector de pantalla" leería. Por ello son útiles para las personas ciegas y en especial para las sordociegas.

Las líneas braille están compuestas de un conjunto de celdas, cada una con seis u ocho puntos, que permiten mostrar caracteres braille. Los puntos, a diferencia del braille impreso, pueden alternar entre las posiciones de subido y bajado, de forma que pueden variar de manera dinámica.

La lectura en una línea braille es igual a la del braille impreso, la diferencia radica en que solo se lee sobre una o varias líneas que se van actualizando. Para ello los puntos pueden subir o bajar según los caracteres o símbolos que se deban representar en cada momento.

Las líneas braille están compuestas de celdas dispuestas en una o más filas de manera que la persona que la utiliza debe ir leyendo de un lado al otro y pulsar un botón para que la línea se actualice.

En los ordenadores se suele acoplar una línea braille debajo del teclado de manera que sea rápido el cambio de un periférico a otro.

Algunas líneas braille contienen celdas de solo seis puntos. Estos dispositivos utilizan el braille convencional. Estas líneas se utilizan para personas que no desean aprender braille de ocho puntos. Normalmente, las líneas braille incluyen celdas de ocho puntos. De esta forma pueden representar tanto braille de seis puntos como de ocho.

Las líneas braille suelen incluir un conjunto de botones que le dan mayor funcionalidad. Además de los botones para la actualización del contenido se pueden encontrar botones para cambiar de línea o párrafo, que permiten moverse por los textos de manera rápida sin tener que recurrir a otros teclados.

Una línea braille está compuesta de varias celdas con seis u ocho pequeños cilindros metálicos que o sobresalen de la superficie por un agujero o se encuentran debajo de la superficie. Un mecanismo electro-mecánico permite que los cilindros suban y bajen, formando los caracteres Braille del texto que se desea leer, con cada celda siendo equivalente a una letra, un número, etc. La terminación superior de los cilindros metálicos es redondeada para facilitar su detección sin causar molestias en el dedo del lector, permitiendo al dedo ir resbalando por los sucesivos puntos de la línea.

Los dispositivos braille más antiguos suelen utilizar la línea serie o el cable paralelo para su conexión al ordenador o a otros periféricos. Estos dispositivos tienen el inconveniente de que deben ser conectados con el ordenador apagado.

Hoy en día los dispositivos braille ya usan las conexiones más habituales en los equipos informáticos actuales, como son el cable USB y el Bluetooth. Esto ha permitido que se puedan aplicar a otros tipos de aparatos como pueden ser los móviles y las PDA.

Las líneas braille al igual que otros periféricos necesitan un driver para ser utilizadas. Este driver suele ser igual o parecido al que se utiliza en los lectores de pantalla.

Este se conecta dentro de una sucesión de driver que va recibiendo la información que la pantalla le envía. Cada uno de los driver va realizando una serie de operaciones con ella y luego le pasa la información recibida al siguiente driver.

De esta manera se pueden utilizar varios dispositivos simultáneamente. Por ejemplo, la información puede ser recibida por el lector de pantalla, que toma los datos necesarios y los lee, y a continuación el driver del magnificador toma la imagen de la pantalla y la amplía, enviándole al driver de la tarjeta gráfica solo la parte de la pantalla que se debe mostrar.

La línea braille puede funcionar de varias maneras, según estemos en un entorno de texto o gráfico.

En los entornos shell la línea solo tiene que reproducir en braille los caracteres que se muestran en pantalla.

En entornos gráficos es necesario añadir más información. Las líneas braille y los lectores de pantalla pueden acceder a textos y menús de forma fácil, pero otros componentes son más difíciles de interpretar. Por ello las APIs suelen permitir a los programadores y diseñadores de interfaces añadir a sus componentes de ventana información que permita ser leída por este tipo de programas y dispositivos.

Por ejemplo la etiqueta "<img>" para poner imágenes en código HTML, que se utiliza en las páginas web, incluye el atributo "alt", que permite dar una descripción (no visible, salvo que el enlace a la foto esté roto) que sirve para dar una descripción a la persona ciega.

<IMG src = "…" alt = "logo de Wikipedia">

Es altamente recomendable que los programadores y diseñadores añadan este tipo de información, dando una información clara y descriptiva a las personas que usan estos medios.

Hay que tener en cuenta que la información debe ser acorde con el contenido y que no se dedique a describir lo que ya es obvio (por ejemplo decir "esto es un botón"), ni dar un exceso de información que pueda hacer más lenta y/o difícil el uso de las aplicaciones por parte del usuario ciego.




</doc>
<doc id="29623" url="https://es.wikipedia.org/wiki?curid=29623" title="Geología de la Luna">
Geología de la Luna

El conocimiento de la geología lunar aumentó significativamente a partir de los años sesenta con las misiones tripuladas y automatizadas. Pese a todos los datos recogidos, todavía quedan preguntas sin responder que únicamente serán contestadas con la instalación de futuras bases permanentes y un estudio más amplio de la superficie. Gracias a su cercanía, la Luna es el único cuerpo —además de la Tierra— cuya geología se conoce detalladamente y del que se obtuvieron muestras de distintas regiones. Las misiones tripuladas Apolo contribuyeron en la recolección de 382 kilogramos de rocas y muestras del suelo lunar, los cuales siguen siendo objeto de estudio útil para la comprensión acerca de su formación y la de otros cuerpos celestes. Algunas sondas del programa Luna de la Unión Soviética también trajeron de vuelta a la Tierra pequeñas muestras del suelo lunar: la Luna 16 (101 gramos), la Luna 20 (55 gramos) y la Luna 24 (170 gramos).

Por mucho tiempo el problema fundamental concerniente a la historia lunar fue el de su origen. Las hipótesis que han sido elaboradas a este respecto son tan variadas como diferentes una de la otra. Las hipótesis más importantes son:

Es la hipótesis más aceptada. Aunque propuesta en 1984, sus orígenes se remontan a mediados de los años setenta. Esta teoría sí satisface las condiciones orbitales de la Tierra y la Luna y las causas por las que la Tierra tiene un núcleo metálico más grande que la Luna. Las teorías modernas de cómo se forman los planetas a través de cuerpos más pequeños —que habrían sido formados por cuerpos aún más pequeños— predicen que, cuando la formación de la Tierra estaba casi terminada, podría haber habido un protoplaneta o cuerpo planetario primitivo del tamaño de Marte (Tea), con cerca de un décimo de la masa de la Tierra, en las cercanías de la órbita terrestre, de forma tal que entró en colisión. Por todo esto, la teoría del gran impacto, según la cual la Luna se originó con los restos de un gran choque entre planetas, es un evento plausible, e incluso que podría haber sido inevitable.

La energía involucrada en esta colisión es impresionante: miles de billones de toneladas de material se habría evaporado y derretido. En algunos lugares de la Tierra la temperatura habría llegado a los 10 000 °C. Esto explicaría el tamaño inusual del núcleo metálico de la Tierra; el cuerpo del tamaño de Marte se habría fusionado con la Tierra incorporando su material al interior de nuestro planeta. Si este evento nunca hubiera sucedido, no solo es que la Tierra no tendría luna, sino que además los días serían más largos y sus duraciones serían de cerca de un año.

El primer evento importante de la formación lunar fue la cristalización del magma oceánico. No se sabe con certeza cuál era su profundidad, pero según diferentes estudios, el océano de magma estaba ubicado a unos 500 km de profundidad. Los primeros minerales en formarse en este océano en proceso de cristalización fueron los silicatos de hierro y magnesio olivino y piroxeno. Debido a que estos minerales eran más densos que el material confinante, se hundieron. La ulterior formación de feldespato plagioclasa, de menor densidad que el magma, se ubicó en la parte superior del océano de magma formando las montañas de anortositas, dando a lugar a la primera corteza lunar. La etapa del océano de magma terminó hace unos 4400 millones de años.

Tan rápido como se formó la corteza lunar, e incluso cuando todavía se estaba formando, otros tipos de magmas que formarían las noritas y las troctolitas en las tierras altas se empezaron a formar en lo profundo de la Luna, pero todavía no se sabe a qué profundidad. Los magmas subieron a través de la superficie infiltrándose a través de la corteza de anortosita, formando grandes rocas e incluso erupcionando sobre la superficie. Algunos de estos cuerpos magmáticos reaccionaron químicamente con los remanentes del océano de magma (KREEP) y otros pueden haber disuelto a las anortositas. Este periodo de historia lunar terminó hace cerca de 4000 millones de años.

Durante estas primeras etapas de la formación lunar, varios eventos de impacto siguieron modificando la superficie hasta una profundidad de unos pocos kilómetros (incluso hasta 20 km). Aunque no ha sido comprobado fehacientemente, el promedio de impactos parece haber declinado entre 4500 y 4000 millones de años atrás, pero después creció dramáticamente produciendo la mayor parte de las cuencas visibles en la Luna. Este bombardeo habría ocurrido en un lapso de entre 4000 a 3850 millones de años atrás.

Una vez disminuido el promedio de impactos, los mares tuvieron tiempo para formarse. Los basaltos se formaron hace más de 3850 millones de años. Sin embargo, entre 3700 y cerca de 2500 millones de años atrás (la última cifra es muy incierta), las lavas fluyeron sobre la superficie lunar, formando los mares y otras características típicas. Junto con los basaltos vinieron las erupciones piroclásticas arrojando restos de basalto derretido a cientos de kilómetros de distancia. Desde que cesó el vulcanismo, la única fuerza geológica en la Luna han sido los impactos de meteoritos.

Algunos de los cráteres más importantes de la Luna son Copérnico, con 93 km de diámetro y una profundidad de 3,76 km, y Tycho con un diámetro de 85 km. Ambos cráteres expulsaron gran cantidad de material. La misión Apolo 17 alunizó en el valle de Taurus-Littrow, un área en la que se había distribuido el material proveniente del cráter Tycho; el estudio de rocas permitió llegar a la conclusión de que el impacto habría ocurrido hace unos 110 millones de años.

El paisaje lunar está caracterizado por la presencia de cráteres de impacto, el material eyectado por éstos, algunos volcanes, depresiones rellenadas por el océano de magma, colinas y las marcas dejadas por los flujos de lava.

El aspecto más distintivo de la Luna es el contraste de zonas claras y oscuras. Las zonas claras son las tierras altas y reciben el nombre de "terrae" (del latín tierra; forma singular: "terra") y las planicies más oscuras llamadas "maria" (del latín mares; forma singular: "mare"), nombres acuñados por Johannes Kepler.

Las tierras altas presentan la mayor cantidad de cráteres de impacto desde un diámetro de cerca de un metro hasta 1000 kilómetros. Antes de que cualquier misión robótica pudiera llegar a la Luna, los científicos pensaban que el origen de algunos de estos cráteres era volcánico, idea que cambió radicalmente con el retorno de muestras de suelo y rocas lunares con las misiones Apolo mostrando claramente el importante rol del proceso de impacto en la formación del terreno. Los impactos ocurren a velocidades cercanas a los 20 km/s (70 000 km/h). En cada impacto ondas de alta presión rebotan al proyectil y el cuerpo impactado, proceso en el cual el proyectil (un meteorito) es destruido por el pasaje de la onda de choque haciendo que se evaporice casi en su totalidad. El material del cuerpo impactado es comprimido fuertemente y descomprimido brevemente después. Una porción de este material es evaporizado y otra parte es derretida, pero la mayor parte (una masa 10 000 veces superior a la del meteorito) es expulsada fuera del cráter formando el anillo que lo rodea. La parte central del cráter es un área más deprimida que el resto del terreno.<br>
La diferencia con las calderas volcánicas o conos de cenizas es que no tienen anillos de material acumulado y sus cimas están por encima del nivel de la superficie.

Una pequeña parte del cuerpo impactado es expulsada a grandes distancias dando lugar a unas figuras que se asemejan a líneas rectas llamadas radios.

Los mares ("maria") cubren cerca del 16 % de la superficie lunar y fueron formados por coladas de lava que principalmente llenaron las enormes cuencas de impacto. Aunque se piensa que en la actualidad la Luna no posee ninguna actividad volcánica, sí la tuvo en el pasado. Según estudios, la actividad volcánica de la Luna tomó lugar después de que las tierras altas fueran formadas y después de que la mayor parte del proceso de craterización sucediera, por este motivo, los mares lunares son más recientes que las tierras altas.

Antes de ser confirmado por las misiones "Apolo", los científicos ya creían que los mares lunares eran planicies de lava ya que éstas poseían unas características particulares: patrones de flujos de lava y colapsos atribuidos a tubos de lava. El material recogido durante las misiones lunares de los años sesenta y setenta confirmaron la sospecha: las cuencas están formadas de un tipo de roca volcánica llamada basalto.

Los mares llenan la mayor parte de las cuencas de impacto del lado visible. En los años sesenta, algunos científicos sugirieron que esto demostraba una causa y efecto: los impactos no solo causaron la formación de grandes cráteres sino también produjeron el derretimiento del interior lunar disparando el proceso volcánico. Sin embargo un examen más detallado de los mares muestran que éstos deben haber sido más jóvenes que las cuencas en las que residen.

Ejemplo: el impacto que formó la gran cuenca de Imbrium del Mare Imbrium (Mar de las Lluvias) arrojó material hacia fuera de la cuenca formando las montañas que rodean a la cuenca Serenitatis, es decir, del Mare Serenitatis (Mar de la Serenidad). Por eso el Mar de la Serenidad es más antiguo.

La característica visible más importante acerca de la relativa juventud de los mares respecto al terreno circundante es que los mares poseen menos cráteres, lo que supone que han estado menos tiempo presentes. De hecho, con los datos recogidos en las misiones lunares se sabe que los mares pueden formarse incluso miles de millones de años después de que se formen las cuencas.

Otro tipo de depósito asociado con los mares, aunque también cubre a las áreas de las tierras altas, son los depósitos de manto oscuro. Estos depósitos no pueden ser vistos a simple vista sino con la ayuda de telescopios o la cercanía de naves espaciales. Antes de las misiones "Apolo", los científicos creían que se trataba de depósitos producidos por erupciones piroclásticas. Algunos depósitos parecen estar asociados con conos de cenizas oscuros y anchos reforzando la idea de las erupciones piroclásticas, posteriormente confirmadas por el hallazgo de perlas de vidrio como las que se encuentran en las erupciones piroclásticas de la Tierra.

Todavía persisten algunos misterios sobre los mares:

En algunos casos es visible que la lava proviniera de las enormes cuencas de impacto, o quizás a lo largo de grietas concéntricas a la cuenca, aunque en la mayoría de los casos no se puede ver de dónde erupcionó. Otra de las características curiosas de la Luna es que casi todos los mares están presentes en el lado visible a la Tierra. La mayoría de los científicos cree que esta asimetría está causada porque la corteza de las tierras altas es más espesa en el lado opuesto dificultando el ascenso del basalto hasta la superficie; también se ha sugerido que la diferencia entre las dos caras puede deberse a la colisión de una segunda luna que pudo formarse también en el impacto que se piensa formó la Luna; de acuerdo con ésta idea, esa hipotética segunda luna tendría un diámetro de 1200 kilómetros aproximadamente (1/3 del tamaño de nuestro satélite) y colisionó con la mayor a una velocidad relativamente baja con el resultado de que, en vez de formar un gran cuenca de impacto, la luna más pequeña se desintegró cubriendo sus restos lo que hoy es la cara oculta.

La sonda japonesa SELENE descubrió tres agujeros circulares profundos en la superficie lunar causados tal vez por el colapso o caída de trozos de techo de varios tubos de lava o intumescencias; en la Tierra también se producen cuando se solidifica la superficie de una corriente de lava y la pasta fundida del interior sigue fluyendo, dejando un tubo hueco de roca. Así se forman amplias redes de galerías y grandes bóvedas o intumescencias huecas que, algunas veces, se derrumban, formando agujeros. Más tarde, la sonda Lunar Reconnaissance Orbiter pudo precisar sus dimensiones. Uno de cien metros de profundidad se encuentra en el Mar de la Tranquilidad. Otro agujero, detectado en el Mare Ingenii, posee 70 metros de profundidad y 120 de ancho. Un tercer hoyo, más pequeño, situado en las colinas de Marius, cae 34 metros bajo la superficie. Estos lugares se consideran prometedores para futuros asentamientos humanos en la Luna, pues permitirían un gran ahorro de costes y protección ante los rayos cósmicos y las extremas temperaturas. Por otra parte, un nuevo escaneo de los datos recogidos por la sonda LRO ha permitido localizar con un nuevo algoritmo informático hasta 200 agujeros más. Estos lugares pueden ofrecer acceso al interior de sistemas de cavernas utilizables como refugio o almacén. El diámetro de los agujeros oscila entre los 5 y los 900 metros.

La superficie de la Luna es de color gris y presenta una gran cantidad de fino sedimento producto de los innumerables impactos de meteoritos. Este polvo recibe el nombre de regolito lunar, un término acuñado para describir las capas de sedimento producidas por efectos mecánicos sobre las superficies de los planetas. El espesor del regolito varía de 2 metros en los mares más jóvenes hasta unos 20 metros en las superficies más antiguas de las tierras altas.

El regolito está formado por el material rocoso de la región en donde se encuentre, pero además contiene restos de material expulsado por impactos lejanos, por lo cual el regolito constituye una roca de gran valor científico.

El regolito contiene rocas, fragmentos de minerales derivados del lecho de roca original, partículas vidriosas formadas por los impactos. En la mayor parte del regolito lunar, la mitad de las partículas están compuestas de fragmentos minerales que están unidos por vidrios de impacto; estos objetos se llaman aglutinados. La composición química del regolito varía de acuerdo a su locación; el regolito en las tierras altas, como sus rocas, es rico en aluminio. El regolito en los mares es rico en hierro y magnesio, como las rocas basálticas.

El regolito lunar es también muy importante porque almacena la información de la historia solar. Las partículas que forman al viento solar, compuesto principalmente de átomos de helio, neón, carbono y nitrógeno golpean la superficie lunar y se insertan en los granos minerales. Al analizar la composición del regolito, especialmente su composición isotópica es posible determinar si la actividad del Sol ha cambiado con el tiempo.

Los gases del viento solar podrían ser útiles para futuras bases lunares, ya que el oxígeno y el hidrógeno (agua), carbono y nitrógeno no solo son esenciales para la vida sino que también son de gran utilidad para la elaboración de combustible. Al respecto es especialmente importante la existencia de grandes cantidades de Helio-3, que podrían utilizarse como material energético combustible en generadores de fusión nuclear.

Existe una gran cantidad de oxígeno almacenado en los silicatos,
dióxido de silicio (SiO), minerales de las rocas lunares, óxidos de calcio (CaO), hierro (FeO) y magnesio (MgO). Cerca del 43% de la masa del suelo es oxígeno y el viento solar provee el resto.

Las primeras rocas recogidas por el "Apolo 11" correspondían a basaltos. A pesar de que la misión "Apolo 11" transcurrió sobre el Mar de la Tranquilidad, también se recogieron fragmentos milimétricos de rocas de las tierras altas. Estas están principalmente compuestas por el mineral feldespato plagioclasa; algunos fragmentos únicamente contenían plagioclasa. Estas rocas se llaman anortositas.

Las tierras altas están formadas principalmente de plagioclasa porque este mineral se fue acumulando en la parte superior del océano de magma por flotación, dando lugar a la hipótesis de que la Luna estuvo alguna vez cubierta por un océano de magma.

El concepto del océano de magma fue comprobado en 1994 con la sonda estadounidense "Clementine", la cual en su órbita polar durante dos meses tomó fotografías en diferentes longitudes de onda. Los científicos analizaron el contenido de hierro en la superficie lunar a través de las variaciones de la intensidad de la luz reflejada a diferentes longitudes de onda. La hipótesis del océano de magma predice que las tierras altas lunares deberían tener un bajo contenido en hierro, menos de aproximadamente 5 % por peso (registrado como óxido de hierro FeO). De acuerdo a las mediciones de la "Clementine", la presencia promedio en las tierras altas es menor al 5 % de FeO por peso. Estos datos fueron confirmados en 1998 cuando otra sonda estadounidense, la "Lunar Prospector" orbitó la Luna.

Las tierras altas contienen otro tipo de rocas ígneas: las más abundantes son las noritas y las troctolitas, rocas formadas por cantidades iguales de plagioclasa y olivino o piroxeno (siendo ambos minerales de silicatos que contienen hierro y magnesio). La datación radiométrica de estas rocas sugiere que son más jóvenes que las anortositas que fueron formadas después que el océano de magma se había cristalizado.<br>
Las rocas de las tierras altas son además bastante complejas debido al proceso de craterización. La mayoría de estas rocas son complejas mezclas de otras. Las rocas originales fueron derretidas, mezcladas, e impactadas durante los primeros 500 millones de años de la Luna. Estas rocas se llaman brechas. Algunas de estas brechas están tan mezcladas que contienen brechas dentro de brechas. La mayor parte de las anortositas, noritas y troctolitas son en realidad fragmentos de rocas dentro de brechas.

Lo interesante de las brechas de las tierras altas, especialmente las brechas de impacto (rocas parcialmente derretidas por un evento de impacto) es que la mayoría de ellas se ubica en una edad que se expande desde los 3850 a los 4000 millones de años. Esto lleva a la idea de que la Luna experimentó un bombardeo de meteoritos muy intenso durante ese lapso, sin embargo, se debe tener en cuenta que el muestreo de rocas regresados por las misiones "Apolo" es muy reducido y corresponde a una pequeña región de la Luna.

Muchas brechas y algunas rocas ígneas están enriquecidas con un conjunto de elementos que no son comunes en la Tierra. Estos elementos no tienden a ser parte fundamental de los minerales presentes en las rocas. Su presencia se origina cuando el magma se cristaliza, y la parte que todavía está líquida progresivamente se va enriqueciendo de estos elementos especiales. Las rocas que los contienen se llaman KREEP, nombre que representa las siglas del potasio (símbolo químico K), elementos raros de la Tierra, del inglés "Rare-Earth Elements" (REE) y fósforo (símbolo químico P). Actualmente se cree que los KREEPs representan los últimos restos de la cristalización del magma de océano. Grandes impactos excavaron la corteza expulsando el material inferior mezclándolo con otros escombros formando brechas KREEP.

La principal características de las rocas basálticas respecto de las rocas provenientes de las tierras altas es que los basaltos contienen una mayor cantidad de olivino y piroxeno y menos plagioclasa. Llamativamente muchas de ellas también tienen un óxido de mineral de hierro-titanio llamado ilmenita. Debido a que el primer muestreo de rocas tenían un gran contenido de ilmenita (y otros minerales relacionados) recibieron el nombre de basaltos de “alto titanio” en referencia a los concentraciones excepcionales de este metal. El "Apolo 12" regresó a Tierra con basaltos de menores concentraciones y fueron llamados basaltos de “bajo titanio”. Misiones subsecuentes y las misiones automatizadas soviéticas regresaron con basaltos con una concentración aún menor, son los basaltos de “muy bajo titanio”.

La sonda "Clementine" proporcionó datos que muestran un amplio rango de contenido de titanio en las rocas basálticas, siendo las de alto contenido las de menor abundancia.

Las formas de los granos minerales en la que están presentes en los basaltos de los mares indican que estas rocas fueron formadas en coladas de lava, algunas delgadas (de un metro de espesor) y otras más espesas (hasta 30 metros). Muchas de los basaltos lunares contienen pequeños agujeros llamados vesículas, los cuales fueron formados por burbujas de gases atrapados cuando se solidificó la lava. No se sabe con certeza cuáles fueron los gases que escaparon de estas rocas. En la Tierra las vesículas se forman con la salida de dióxido de carbono, vapor de agua acompañada de algo de sulfuro y cloro. En la Luna no hay señales de la existencia de agua. Es probable que hayan sido dióxido de carbono y monóxido de carbono, con algo de sulfuro.

Las muestras de vidrios piroclásticos se presentan de color verde, amarillo y rojo. La diferencia en color reflejan la cantidad de titanio que poseen, de esta manera, las partículas verdes tienen las menores concentraciones (cerca de 1 %) y las rojas son las de mayores concentraciones con un 14 %, mucho más que los basaltos de mayores concentraciones.

Los experimentos llevados a cabo en las rocas basálticas y vidrios piroclásticos muestran que se formaron cuando el interior de la Luna estaba parcialmente derretido. Las rocas, no tienen una temperatura específica de fundición ya que se derriten en una gama de temperaturas: los basaltos se funden a unas temperaturas entre 1000 y 1200 °C. Los experimentos mostraron que el derretimiento en la Luna tomo lugar a una profundidad de entre 100 a 500 km, y que las rocas que se derritieron parcialmente contenían principalmente olivino y piroxeno con algo de ilmenita en las regiones que formaron los basaltos de alto titanio.

La Luna no posee tectónica de placas y por lo tanto no se renueva constantemente como la Tierra. Los temblores lunares, los lunamotos, son mínimos y los más grandes (de magnitud 5) solamente ocurren cerca de una vez por año. El interior lunar es bastante diferente del de la Tierra; la corteza lunar tiene un espesor de unos 70 km en el lado visible a unos 150 km en el lado oculto. Los mares tienen cerca de 1 km de espesor (dato derivado de estudios fotogeológicos). Las muestras regresadas a la Tierra y los datos de sondas, sugieren que la parte inferior de la corteza contiene menos plagioclasa que la mitad superior de la misma. Debajo de la corteza se encuentra el manto lunar, la capa más extensa de la Luna. Puede que haya una diferencia en la constitución de las rocas por encima y debajo de una profundidad de 500 km, representando la profundidad del océano de magma. Debajo del manto se encuentra el núcleo lunar cuyo tamaño es incierto aunque estimaciones lo ubican entre unos 100 a 400 km.

Si bien la Luna no posee un campo magnético como la Tierra, sí lo tuvo en el pasado. Las rocas lunares están magnetizadas, siendo las más antiguas las que presentan el mayor magnetismo. Esto supone que en el pasado el campo magnético era más intenso. El porqué de su debilitamiento es incierto aunque sirve para teorizar acerca de la ausencia de un núcleo de hierro líquido como en el caso terrestre que en su movimiento interno produce las corrientes eléctricas necesarias para la creación del campo. Otra de las diferencias así derivadas, es que la densidad media de la luna es de unos 3,3 g/cm³, mientras que la densidad media de la Tierra es de 5,5 g/cm³.

En algunas regiones de la Luna la intensidad del campo gravitacional es más intenso, este misterio fue resuelto con la Lunar Prospector al asociarlos con grandes concentraciones de masas (mascons) presentes en los mares de las cuencas.

A unos 80° del polo sur existen los remanentes de la enorme cuenca de Aitken, la más grande del sistema solar, con unos 2500 km de diámetro. La mayor parte de esta área, unos 15 000 km² no reciben luz solar gracias a las superficies elevadas de las que están rodeadas. Tanto imágenes de radar de la sonda "Clementine" y los datos del espectrómetro de neutrones del "Lunar Prospector" indican que la región contiene depósitos de agua congelada. Hasta ese momento se sospechaba la presencia de un depósito de 10 a 300 millones de toneladas. La Lunar Prospector también descubrió que el polo norte contiene cerca del doble de hielo que el polo sur.

La mayor parte de las rocas lunares están almacenadas en el Laboratorio de Recepción Lunar en el Centro espacial Lyndon B. Johnson, en Houston (Texas). Un pequeño porcentaje está distribuido en instalaciones auxiliares en la Base de la Fuerza Aérea Brooks, cerca de San Antonio (Texas). Muchas muestras lunares se encuentran en los laboratorios de investigadores en todo el mundo. Un pequeño número de rocas lunares están expuestas al público en museos y solo tres piezas pueden ser tocadas. Estas son las “rocas tocables” cortadas de rocas basálticas de la misión "Apolo 17". Una de estas rocas está ubicada en el Museo del Aire y el Espacio Smithsonian en Washington, D. C. Otra pieza está en el Centro Espacial de Houston cercano al Centro Espacial Johnson. Una tercera roca que se puede tocar está en el Museo de las Ciencias en la Universidad Nacional Autónoma de México.





</doc>
<doc id="29625" url="https://es.wikipedia.org/wiki?curid=29625" title="Mars Gravity Biosatellite">
Mars Gravity Biosatellite

El Mars Gravity Biosatellite o "Biosatélite de Gravedad Marciana" es un proyecto organizado por estudiantes del Instituto Tecnológico de Massachusetts (MIT) y la Universidad de Queensland (Australia) para estudiar los efectos sobre los seres humanos en un medioambiente a 0,38-g (entendiéndose como el 38% de la aceleración gravitatoria en la superficie de la tierra), es decir, las condiciones de gravedad presentes en Marte.

Para ello en 2006 lanzarán ratones en un satélite artificial(ver imagen) que rotará sobre su propio eje unas 34 veces por minuto para lograr el efecto de la gravedad de Marte. Los ratones estarán expuestos a una gravedad de 0,38-g terrestres durante cerca de 5 semanas para regresar a salvo en paracaídas y aterrizar en las cercanías de Woomara, Australia dentro de una pequeña cápsula.

Financiado en parte por la NASA, el proyecto es único ya que es la primera vez que se llevarán a cabo estudios sobre animales a este nivel de gravedad. Otro de los aspectos destacables es la participación activa de más de 250 estudiantes universitarios en todos los niveles del proyecto: desde el planeamiento de la misión, experimentos, y diseño de la nave.

El proyecto se centrará en la investigación de los cambios, pérdida y debilitación de la estructura ósea, la atrofia muscular y los cambios en el oído interno que afectan el balance.

Cada ratón tendrá su compartimento individual y serán observados minuciosamente con cámaras para cada uno. Tendrán además un sensor de masa corporal y abastecimiento de agua controlado. Los ratones dispondrán de algunos juguetes para mantenerse ocupados, sin embargo los científicos descartaron el uso de las ruedas ya que esto alteraría la masa muscular de los ratones.

Tres universidades participan del proyecto, el cual es coordinado por el MIT. La Universidad de Washington será la responsable del abastecimiento de electricidad, propulsión, control de actitud, control térmico y las comunicaciones a Tierra. La Universidad de Queensland estará a cargo de los sistemas de entrada, descenso y aterrizaje, incluyendo los escudos térmicos y paracaídas.



</doc>
<doc id="29626" url="https://es.wikipedia.org/wiki?curid=29626" title="Ciego (desambiguación)">
Ciego (desambiguación)

Ciego puede referirse a:


Asimismo, puede hacer referencia a las siguientes localidades o divisiones administrativas:

Además, puede referirse a:


</doc>
<doc id="29627" url="https://es.wikipedia.org/wiki?curid=29627" title="Organización Nacional de Ciegos Españoles">
Organización Nacional de Ciegos Españoles

La Organización Nacional de Ciegos Españoles (ONCE) es una corporación de derecho público de carácter social sin ánimo de lucro que tiene el propósito fundamental de mejorar la calidad de vida de las personas ciegas, personas con resto visual y personas con discapacidad de toda España.

Cuenta con 72 231 afiliados (a 31 de diciembre de 2019), de los cuales el 87 % son personas con deficiencia visual grave y el 13 % personas con ceguera total.

Se trata de una organización muy activa que participa en los diversos foros nacionales e internacionales sobre ceguera y discapacidad, promoviendo también distintas iniciativas para lograr su función. Es socio fundador del Comité Español de Representantes de Personas con Discapacidad (CERMI).

Tiene reconocida una concesión estatal en materia de juego para la comercialización de loterías, que le permiten financiar su labor social y crear empleos para sus afiliados.

A principios del siglo XX nacieron en España las primeras rifas callejeras organizadas por asociaciones de invidentes, un colectivo que hasta entonces estaba mayoritariamente abocado a la mendicidad. Pese a ser ilegales, a partir de los años 1930 estos sorteos locales fueron extendiéndose por distintas ciudades, especialmente por Levante, Cataluña y Andalucía. Destacaban la Associació de Cecs de Catalunya —creada en 1934 por el republicano Roc Boronat, entonces comisario de Beneficencia del Ayuntamiento de Barcelona—, la Sociedad de Socorros Mutuos y Defensa del Ciego de Cádiz (fundada por Antonio Calvo García, Alfredo Romero Cantero, Manuel Mera Gago, Emilio García Rincón y Vázquez, Silverio de la Pascua y Díaz de la Serna, Arturo Osiel Benazuli y Manuel Muñoz García), el Centro Instructivo y Protector de Ciegos de Granada (fundado por José Recuerda Rubio), y la asociación sevillana La Hispalense.

El primer intento de crear una organización a nivel nacional fue la Federación Hispánica de Ciegos, impulsada en 1930 por José Ezquerra. En 1931 nació el Patronato Nacional de Protección de Ciegos, un organismo estatal dirigido por videntes, heredero del Patronato Nacional de Sordomudos, Ciegos y Anormales de 1910.

En 1937, en plena guerra civil española, Javier Gutiérrez de Tovar, presidente de La Hispalense sevillana, impulsó la Federación Bética de Ciegos, que aglutinaba a las asociaciones locales de Andalucía y Extremadura, y que aspiraba ser el embrión de una federación nacional. Paralelamente, en Burgos, sede del Gobierno nacional, las autoridades falangistas y un grupo de invidentes (encabezado por Fernando Martínez-Burgos, Primitivo Pérez y Mariano Ortega) habían empezado a trabajar en la reconstitución del Patronato de Ciegos, inactivo desde el inicio de la guerra. En enero 1938 una delegación de la Federación Bética se reunió con el grupo de Burgos. Acordaron la creación de una organización nacional de ciegos, renunciando al modelo benéfico del Patronato (una pensión diaria de seis pesetas para los invidentes, sufragada por las administraciones públicas) en favor de la propuesta de Gutiérrez de Tovar, defensor de la autonomía económica de los invidentes a través de la venta de cupones de lotería.

Tras este acuerdo, el 25 de agosto de 1938, en el Congreso de la Asociación Española para el Progreso de las Ciencias celebrado en Santander, Gutiérrez de Tovar expuso una ponencia en defensa de la tiflología y de la creación una organización nacional de ciegos, destinada a la integración social de los invidentes. Al día siguiente la presentó al Gobierno, obteniendo el apoyo del Director General de Beneficencia, Javier Martínez de Bedoya, y del Ministro de Interior, Ramón Serrano Suñer. El 4 de diciembre una Orden Ministerial disolvió el Patronato Nacional de Protección de Ciegos. El 11 de diciembre Serrano Suñer presentó el proyecto de creación de la Organización Nacional de Ciegos (ONC) al Consejo de Ministros, donde fue aprobado. El 13 de diciembre de 1938, festividad de Santa Lucía (patrona de los invidentes), Francisco Franco firmó el decreto fundacional de la ONC.

El mismo decreto estableció la creación del Consejo Superior de Ciegos, organismo dependiente del Estado y dirigido por videntes, que durante toda la Dictadura franquista tuteló a la ONC, administrando sus recursos económicos, aprobando sus planes y designando a su Jefe Nacional, siendo Gutiérrez de Tovar el primer elegido para este puesto.

En 1939 se aprobó el Reglamento de la ONC, que autorizaba su participación en el monopolio estatal del juego mediante la comercialización del «cupón prociegos» como «forma excepcional y exclusiva de ingresos para los no videntes imposibilitados de desempeñar una profesión». El primer sorteo se celebró en Madrid el 8 de mayo de 1939. En los años siguientes, la ONC inició un proceso de integración de las distintas asociaciones locales, reemplazando al mismo tiempo las múltiples rifas existentes por el nuevo cupón que, no obstante, mantuvo los sorteos por separado en cada delegación provincial.

Durante los años 1940 la ONCE abrió sus primeros colegios para invidentes y fomentó la inclusión laboral con la creación de fábricas de dulces y talleres industriales. Los desencuentros de Gutiérrez de Tovar con el Consejo Superior de Ciegos provocaron su cese en 1948. Fue reemplazado transitoriamente por Benito Hermida, el único vidente al frente de la organización en toda su historia, y en 1949 José Ezquerra fue designado Jefe Nacional. Desempeñó el cargo durante una década, apostando por un modelo mutualista, que le llevó a cerrar las fábricas abiertas en la etapa anterior. En 1952 se añadió el término "Españoles" a la denominación de la organización.

La inserción laboral de los invidentes, más allá de la venta de los cupones, se convirtió en uno de los objetivos prioritarios de la ONCE a partir de los años 1960, con Ignacio de Satrústegui al frente. Para ello se impulsaron centros de formación profesional y talleres ocupacionales. La culminación de este proceso fue la creación de la Escuela de Fisioterapia (1964), la Escuela de Telefonía (1966) y el Centro de Rehabilitación y Formación Profesional (1966). Paralelamente, durante esta etapa se potenciaron las bibliotecas y las imprentas braille y se impulsó el audiolibro.

Al finalizar la Dictadura, la organización inició un convulso proceso de democratización y autogestión, impulsado por un grupo emergente de jóvenes sindicalistas, que culminó el 19 de enero de 1982, con la celebración de las primeras elecciones entre los afiliados. Antonio Vicente Mosquete fue el primer presidente elegido democráticamente. En los años siguientes se llevó a cabo un importante proceso de modernización y democratización de la ONCE que pasó de ser una organización caritativa a una institución de servicios. Cambio que se produjo en paralelo con el desarrollo empresarial de la ONCE gracias fundamentalmente a la reconversión del Cupón Pro-Ciegos en un sorteo a nivel nacional, con una mayor dotación en premios. Esta iniciativa comercial, lanzada el 2 de enero de 1984, logró una gran popularidad, aumentando las ventas de cupones un 300 %. En las siguientes elecciones democráticas, en 1986, Vicente Mosquete fue reelegido, pero esta vez logrando su grupo, UP, la mayoría absoluta. Miguel Durán, accedió en esta segunda legislatura a la Dirección General de la ONCE. Se abrió entonces una etapa marcada por dos acontecimientos, que sucederían un año después. El primero, la muerte prematura de Antonio Vicente Mosquete. El segundo, el rotundo éxito comercial del «cuponazo», que llevó a la institución a multiplicar sus ingresos.

Acompañando ese éxito comercial la ONCE amplió su campo de acción también a otras discapacidades distintas a la ceguera. En 1988 se creó la Fundación ONCE para la cooperación e inclusión social de las personas con discapacidad. Un año después la Fundación impulsó el grupo empresarial Fundosa, con el objetivo de generar empleo estable para personas con discapacidad.

A partir de 1989, con Miguel Durán como director general, la ONCE inició un proceso de inversiones en sectores estratégicos como las finanzas (a través de Finonce), la industria (Indonce), los servicios (Servionce), el negocio inmobiliario (Surba) y la comunicación (Divercisa). En este último sector desarrolló una notable actividad, destacando la compra de los periódicos "El Independiente" y "Diario de Barcelona", así como la participación en la creación de la agencia de noticias Servimedia, de la cadena televisiva Telecinco y de la radiofónica Onda Cero, las dos últimas presididas por el propio Durán. La mayor parte de esas inversiones en medios de comunicación resultaron altamente deficitarias para la ONCE. Esta etapa finalizó en 1993, con la dimisión de Durán, tras discrepancias con el Consejo General. Ese mismo año se creó la Corporación Empresarial ONCE (CEOSA), con el objetivo de aglutinar las distintas empresas que la organización había ido creando desde 1989. 

En los años 2000, las ventas del cupón entraron en un progresivo declive, motivado por la creciente competencia en el sector de las apuestas y loterías. Esta situación llevó a la ONCE a introducir nuevos juegos en el mercado. En 2004 lanzó El Combo, su primer juego activo y en 2006 creó El Rasca, su primera lotería instantánea. Tras quedar fuera de la comercialización de la lotería europea Euromillones (gestionada por el Estado), en 2012 inició la venta exclusiva en España de su principal competidora, el Eurojackpot.

En el año 2013, la ONCE recibió el Premio Príncipe de Asturias de la Concordia como reconocimiento a su labor en la integración social de las personas con discapacidad. El 2 de mayo de 2018 recibió la gran cruz de la Orden del 2 de mayo, bajo la forma de Placa de Honor. En 2020 la "Acción Social" de la ONCE fue acreditada como Embajador Honorario de la Marca España, reconocimiento entregado por los reyes de España por haber contribuido significativamente a la generación y al fortalecimiento de una imagen positiva de España en el exterior.

Desde 1982 el máximo órgano de gobierno de la ONCE es el Consejo General. Su composición, al igual que la de los 17 consejos territoriales (uno por cada comunidad autónoma) la eligen por sufragio los afiliados de la organización, cada cuatro años. La Dirección General es el órgano encargado de la gestión y ejecución de los acuerdos del Consejo General.

La administración pública supervisa y controla la actividad de la ONCE mediante el Consejo de Protectorado, en el que participan los ministerios de Sanidad, Servicios Sociales e Igualdad; Empleo y Seguridad Social; Economía y Competitividad; Hacienda y Administraciones Públicas; e Interior.
Jefes Nacionales

Presidentes del Consejo General

Bajo el paraguas de la ONCE se han constituido varias fundaciones, mediante las cuales la organización desarrolla su labor social:


ILUNION es el grupo de empresas de la ONCE y la Fundación ONCE, que se reparten la titularidad con un 47,5 % y 52,5 %, respectivamente. Fue creado en noviembre de 2014, fruto de la convergencia de los grupos empresariales pertenecientes a la ONCE y a su Fundación, CEOSA y Grupo Fundosa, respectivamente. Estas empresas nacieron con una doble finalidad: económica, para diversificar las fuentes de ingresos de la ONCE, y social, para facilitar la integración laboral de las personas afiliadas a la ONCE en otras actividades diferentes a los de la propia organización.

ILUNION cuenta con más de 50 líneas de negocio, repartidas en cinco divisiones: sociosanitario (clínicas de fisioterapia, centros de día, etc.), facility services (seguridad, limpieza, lavandería industrial, etc.) turismo (hoteles, cáterin y ocio), comercialización (tiendas de conveniencia y correduría de seguros) y consultoría. Cuenta con una plantilla de 32 000 trabajadores, de los cuales el 35 % son personas con discapacidad.

La ONCE ofrece servicios especializados para personas con ceguera o deficiencia visual en múltiples campos: educación, empleo, rehabilitación, ocio, deporte, comunicación y acceso a la información, etc.

El modelo de intervención educativa de la ONCE es la inclusión educativa, de modo que más del 98 % del alumnado con discapacidad visual en España se escolariza en colegios ordinarios. Paralelamente, la organización ofrece a este alumnado una atención complementaria, en función de sus necesidades específicas. Para ello, cuenta con cinco Centros de Recursos Educativos (CRE), ubicados en Alicante, Barcelona, Madrid, Pontevedra y Sevilla. La ONCE también gestiona la Escuela Universitaria de Fisioterapia, adscrita a la Universidad Autónoma de Madrid.

Las Agrupaciones artísticas de la ONCE son grupos estables de personas, que tienen como fin favorecer el desarrollo artístico, convivencial y la integración de los afiliados participantes. Las Agrupaciones Artísticas son de dos tipos: de Teatro y de Música y en cada una de ellas existen dos niveles. Las de Nivel I son aquellas cuya calidad artística está en proceso de consolidación y las de Nivel II cuya puesta en escena y repertorio tienen una calidad artística notable. Esta clasificación se hace anualmente a través de una Comisión Técnica que evalúa el nivel artístico de las mismas.

Es numerosa la participación de dichos grupos musicales y teatrales en ámbitos artísticos nacionales e internacionales. Cabe destacar, entre otras, la participación de la Agrupación "La Esfera" (Gijón), de la Delegación Territorial de Asturias, en el Festival Internacional de Zagreb (Croacia).

Otro elemento muy importante a señalar son las Muestras Estatales que se celebran anualmente alternando cada una de las modalidades, es decir, Teatro y Música. Este acontecimiento, que se realiza en una ciudad distinta cada año, es una cita en donde se presenta al público la producción artística de las mejores Agrupaciones de cada modalidad.

La ONCE, su Fundación y el grupo Ilunion dan trabajo a 67 710 empleados (datos de 2015), de los que un 56 % son personas con discapacidad. Adicionalmente, a través de terceras empresas y entidades con el apoyo de la ONCE, entre 1995 y 2015 se crearon 101 119 puestos de trabajo, un 98 % de los cuales para personas con discapacidad. Durante el mes de enero de 2019 hubo distintas manifestaciones de tipo sindical en Galicia, en las distintas delegaciones provinciales, reivindicando un "empleo estable". 

La Organización Nacional de Ciegos Españoles es, junto a la Sociedad Estatal Loterías y Apuestas del Estado (SELAE), el único operador designado para la comercialización de los juegos de loterías de ámbito estatal en España, según la Ley 13/2011.

Las loterías son una de las principales fuentes de financiación de la labor social de la organización. La ONCE destina a su Fundación el 3% de los ingresos brutos de la comercialización de sus distintas loterías. La más popular y longeva de ellas es el cupón de la ONCE, nacido a la par que la entidad. En los años 2000 se introdujeron nuevos juegos de azar, como las loterías activas y las instantáneas. 

La actividad comercial la lleva a cabo una red principal de 20 000 vendedores —personas con ceguera, deficiencia visual grave y otras discapacidades— y una red complementaria de más de 7000 establecimientos autorizados (estancos, estaciones de servicio, quioscos de prensa, etc.), además de la plataforma de juegos en línea «JuegosONCE.es», creada en 2013.






</doc>
<doc id="29630" url="https://es.wikipedia.org/wiki?curid=29630" title="Sons and Daughters of Saint Lucia">
Sons and Daughters of Saint Lucia

"Hijos e hijas de Santa Lucía" (en inglés, "Sons and Daughters of Saint Lucia") es el himno nacional de Santa Lucía, una pequeña nación caribeña ubicada al norte de Venezuela y Trinidad y Tobago. La letra es del reverendo Charles Jesse y la música del compositor Leton Felix Thomas. Fue adoptado como canto nacional en 1967 y desde su independencia del Reino Unido en 1979 es el himno nacional.




</doc>
<doc id="29634" url="https://es.wikipedia.org/wiki?curid=29634" title="Sordera">
Sordera

La sordera es la dificultad o la imposibilidad de usar el sentido del oído debido a una pérdida de la capacidad auditiva parcial (hipoacusia) o total (cofosis), y unilateral o bilateral. Así pues, una persona sorda será incapaz o tendrá problemas para escuchar. Esta puede ser un rasgo hereditario o puede ser consecuencia de una enfermedad, traumatismo, exposición a largo plazo al ruido, o medicamentos agresivos para el nervio auditivo.


La sordera también se da por desgaste de los oídos; esto explica por qué los adultos no pueden escuchar algunas frecuencias que personas de menor edad sí pueden.

Para comprobar el grado de sordera de una persona, se le hace una prueba de audiometría, de manera que una persona con sordera puede tener problemas en la percepción correcta de la intensidad (decibelios) o de la frecuencia (hertzios) de sonidos relacionados con el lenguaje oral, y es frecuente que se den resultados diferentes para cada oído. La pérdida de la capacidad auditiva generalmente se describe como leve, benigna, moderada, severa o profunda, dependiendo de dicha prueba. Generalmente, cuando una persona cuya pérdida de la capacidad auditiva supere a los 90 dB, se considera entonces persona sorda.

Podemos considerar diversos criterios a la hora de clasificar las diferentes tipologías de pérdida auditiva o sordera.

Causadas por enfermedades u obstrucciones en el oído exterior o medio (las vías de conducción a través de las cuales el sonido llega al oído interior), las pérdidas auditivas conductivas normalmente afectan a todas las frecuencias del oído de manera uniforme.

Son en los casos en los que las células ciliadas del oído interno, o los nervios que lo abastecen, se encuentran dañados. Estas pérdidas auditivas pueden abarcar desde pérdidas leves a profundas. A menudo afectan a la habilidad de la persona para escuchar ciertas frecuencias más que otras, de manera que escucha de forma distorsionada el sonido, aunque utilice un audífono amplificador. No obstante, en la actualidad, las grandes prestaciones tecnológicas de los audífonos digitales son capaces de amplificar solamente las frecuencias deficientes, distorsionando inversamente la onda para que la persona sorda perciba el sonido de la forma más parecida posible como sucedería con una persona oyente.

Se refiere a aquellos casos en los que existen aspectos de pérdidas conductiva y sensoriales, de manera que existen problemas tanto en el oído externo o medio y el interno. Este tipo de pérdida también puede deberse a daños en el núcleo del sistema nervioso central, ya sea en las vías al cerebro o en el mismo cerebro. Es importante tener cuidado con todo tipo de golpes fuertes en la zona auditiva, ya que son los principales causantes de este tipo de sordera.

Autores como Valmaseda y Díaz-Estébanez (1999) hablan de esta cuarta tipología, que hace referencia solo y exclusivamente a lesiones en los centros auditivos del cerebro.

Umbral por encima de 90 dB. Puede ser debido a malformaciones internas del canal auditivo o a la pérdida total de los restos auditivos por motivos genéticos. Entre todas las personas sordas, el porcentaje de personas que padecen cofosis es muy pequeña, casi insignificante, ya que se trata de una malformación (ausencia de cóclea, por ejemplo).

La etiología de la discapacidad auditiva puede ser por causas exógenas como la rubeola materna durante el embarazo, incompatibilidad del factor Rh... y que suelen provocar otros problemas asociados (dificultades visuales, motoras, cognitivas). O bien puede ser una sordera hereditaria, la cual, al ser recesiva, no suele conllevar trastornos asociados.

El momento en el que aparece la discapacidad auditiva es determinante para el desarrollo del lenguaje del individuo, por lo que se pueden distinguir 3 grupos:

La pérdida auditiva se describe como:

Sordera verdadera o pérdida del umbral de intensidad de audición.

En Argentina las obras sociales y las empresas de medicina prepaga cubren el tratamiento de la hipoacusia, los audífonos y prótesis auditivas y la rehabilitación. También cubren el implante coclear, que es un dispositivo electrónico que se pone detrás del oído y posibilita a la persona sorda la recuperación parcial de la audición.

En Argentina las empresas de telefonía domiciliaria y móvil deben proveer equipos para personas con discapacidad auditiva y dar un servicio adecuado para permitirles la comunicación.

Las personas con discapacidad auditiva tienen prioridad en la instalación de este servicio de telefonía cuando el teléfono sea su única forma de comunicarse y no deben pagar una mayor tarifa por usar estos teléfonos, las tarifas deben ser iguales a las de los teléfonos domiciliarios convencionales.

Las empresas de telefonía móvil están obligadas a vender equipos compatibles para personas hipoacúsicas y los equipos especiales no pueden ser superiores a los de los equipos habituales.

Si la sordera es especialmente aguda, puede afectar considerablemente en la forma en que la persona sorda se relaciona con su entorno humano, al encontrarse con una seria limitación en su capacidad de encontrar una vía de comunicación por el canal auditivo, es decir, con el lenguaje oral. Sin embargo, el modo en que se entienden las consecuencias de esa incapacidad puede variar considerablemente, de manera que dos perspectivas fundamentales acerca del modo de entender la sordera.

Estudios recientes (a partir de los trabajos de William C. Stokoe en 1960, fundamentalmente) proponen abordar la sordera desde un punto de vista antropológico. Un colectivo de personas sordas que se comunican entre sí por medio de una lengua de signos puede ser considerado una comunidad lingüística minoritaria, con una cultura propia. La literatura especializada hace muchas veces la distinción entre Sordera, con una mayúscula inicial, para referirse a la antropológica, y sordera, para la definida clínicamente.

Dependiendo de los casos, una persona sorda normalmente puede desarrollar una idiosincrasia con las personas que se comunican por el canal visual, es decir, con la lengua de signos (LS), considerándose como una colectividad cultural y social propia diferenciada, normalmente con la definición de Comunidad Sorda. El vínculo social entre los sordos signantes suele ser muy fuerte debido, sobre todo, al aislamiento social con respecto a los oyentes, provocado por el escaso conocimiento de su problemática común, o estilo de vida, así como la escasa relación social por motivos de entendimiento lingüístico o también por ideas preconcebidas que las personas tienen acerca de los sordos las cuales pueden ir cambiando mediante la completa interacción dentro de su cultura.

De hecho, en esta colectividad se definen a sí mismos como "personas sordas signantes", y suelen clasificar su entorno social entre "oyentes" a las personas que no tienen sordera (entre los que puede haber algún oyente signante, si conoce una LS), y al resto de las personas sordas que, dependiendo del país, pueden formar parte también de la Comunidad Sorda. Entre las personas sordas, además, los sordos signantes se distinguen de las "personas sordas oralistas", es decir, quienes no utilicen habitualmente una LS o usan una comunicación bimodal (léxico de una LS con estructura gramatical de una lengua oral). Por último, están los "sordos implantados", es decir, quienes llevan un implante coclear en vez de un audífono, que pueden ser signantes u oralistas.

En cambio, las personas sordas oralistas, es decir, aquellas personas sordas que han recibido una intensa reeducación del lenguaje oral en su infancia y que no usan una lengua de signos como lengua vehicular (a menudo como consecuencia de una prohibición expresa de los educadores), suelen adoptar una actitud de invisibilidad social respecto a su condición de persona sorda, a veces incluso no reconociéndose como tales (recurriendo a otras definiciones como discapacitado auditivo, hipoacúsico, medio oyente, etc.). Asimismo, este grupo suele asociar como personas sordas únicamente aquellos que son signantes, o bien diferenciándose de ellos definiéndolos como "personas sordomudas", especialmente a aquellos no hablan un lenguaje oral correctamente en el aspecto gramatical.

Esta última definición, la de sordomudez, por otra parte, es considerada peyorativa por los sordos signantes, pues consideran que "hablan" (por canal visual, en LS). Asimismo, consideran que un alto analfabetismo de la lengua oral entre las personas sordas no tiene ninguna relación con la mudez, sino a un fracaso del método oralista en el sistema educativo en su infancia y juventud. De hecho, llamar "sordomudo" a una persona sorda por no hablar en lengua oral correctamente, equivaldría llamar "manco" a una persona por no escribir con la grafía correcta, o "ciego y manco" por no saber leer y escribir. Por último, en el sentido estricto, la "sordomudez" solo sería aplicable a aquellos que padezcan sordera y, además, son incapaces de generar sonidos humanos por la ausencia o el daño en las cuerdas vocales, siendo aspectos independientes entre sí.

Cada año se celebra el Día Internacional de las Personas Sordas la última semana del mes de septiembre, concordando con el último domingo. Aunque en España, suele coincidir con el último viernes o sábado del mismo mes.

En este día se reivindican los derechos y demandas de las personas con discapacidad auditiva, y se hacen visible ante la sociedad la riqueza cultural de las comunidades sordas de todo el mundo.

¿Por qué este día? La Fundación Mundial de Personas Sordas (WFD) eligió esta fecha para conmemorar el primer Congreso Mundial de la WFD que tuvo lugar en septiembre de 1951.

A lo largo de la historia encontramos muchos personajes que a pesar de su manifiesta pérdida auditiva han conseguido grandes logros en sus carreras. Encontramos entre otros a:





</doc>
<doc id="29644" url="https://es.wikipedia.org/wiki?curid=29644" title="Aeropuerto George F. L. Charles">
Aeropuerto George F. L. Charles

El Aeropuerto George F. L. Charles (código IATA: SLU, código OACI: TLPC) está ubicado en Castries, capital de Santa Lucía. Originalmente era llamado "Aeropuerto Vigie" pero desde el 4 de agosto de 1997 cambió de nombre al actual en honor a George Frederick Lawrence Charles, quien fuese el primer Ministro de Educación y Asuntos Sociales del país. Dado que el país tiene dos aeropuertos, el George F. L. Charles se especializa en los vuelos regionales mientras que al sur, el Hewanorra se encarga de los vuelos transatlánticos o continentales. 

El aeropuerto había programado previamente el servicio de jet de pasajeros volado por Caribair (Puerto Rico) que en 1968 operaba los aviones McDonnell Douglas DC-9-30 diariamente con vuelos sin escalas a Antigua y Barbados con servicio de jet directo a Puerto España, Trinidad, St. Maarten, San Juan, St. Croix y St. Thomas. Otro operador de jet anterior fue BWIA West Indies Airways (que operaba como BWIA International en ese momento) que en 1996 operaba los aviones McDonnell Douglas MD-80 entre Castries y Miami dos veces por semana y también dos veces por semana entre el aeropuerto y el aeropuerto John F. Kennedy de Nueva York Con estos dos vuelos directos, ningún cambio de avión realiza una parada intermedia en Antigua, así como el servicio MD-80 varios días a la semana sin escalas a Barbados con estos vuelos que luego continúan hacia Puerto España. 

El 8 de noviembre de 2015, un Beechcraft Modelo 99, registrado N7994H, se desvió de la pista hacia un área cubierta de hierba en el Aeropuerto George F. L. Charles después de que el tren de aterrizaje derecho del avión no funcionara correctamente. La aeronave contaba con un solo tripulante, el cual no resultó lesionado. Tras el incidente, Hummingbird Air suspendió todas las operaciones, y la Autoridad de Aviación Civil del Caribe Oriental inició una investigación.



</doc>
<doc id="29646" url="https://es.wikipedia.org/wiki?curid=29646" title="Ur">
Ur

Ur fue una antigua ciudad del sur de Mesopotamia. Originalmente, estaba localizada cerca de Eridu y de la desembocadura del río Éufrates en el golfo Pérsico. Hoy en día, sus ruinas se encuentran a 24km al suroeste de Nasiriya, en el actual Irak.

En julio de 2016, la Unesco eligió el sitio arqueológico de Ur como Patrimonio Mixto de la Humanidad, como «parte de los vestigios arqueológicos de asentamientos sumerios en la Baja Mesopotamia, que florecieron entre el tercer y cuarto milenio antes de Cristo en el delta pantanoso formado por los ríos Éufrates y Tigris».

Los restos de Ur forman una colina de ruinas de 12m de altitud en mitad del desierto de Irak, a unos 24km al suroeste de Nasiriya. Las ruinas eran llamadas por los habitantes locales "Tell al-Muqayyar" (montículo de brea).

La primera investigación en la zona fue llevada a cabo por el cónsul británico en Basora J. E. Taylor en 1854 por sugerencia del Museo Británico. Ya entonces se encontraron tablillas que indicaban que los restos pertenecían a la Ur bíblica; sin embargo, esto no fue suficiente para que se realizasen investigaciones de importancia y poco después se abandonó el lugar, produciéndose saqueos. Miles de tablillas cuneiformes terminaron en los mercados de Bagdad y, desde allí, en colecciones privadas.

Tras la Primera Guerra Mundial, Irak pasó a formar parte del Imperio británico. Esta situación fue aprovechada por el Museo Británico, que consiguió establecer excavaciones en Ur, Eridu y El Obeid entre 1918 y 1919. En 1920, arqueólogos de la Universidad de Pensilvania al mando de Leonard Woolley tomaron el relevo de los ingleses. En las excavaciones, que duraron hasta 1934, se encontraron numerosos objetos de valor, entre los que destacó el contenido de las llamadas "Tumbas Reales".

En la década de 1970, el gobierno de Saddam Hussein emprendió la restauración del zigurat de Ur-Nammu, que se convirtió en uno de los monumentos más importantes de Irak.

Los primeros restos de Ur pertenecen al período de El Obeid (), en el cual se produjeron los primeros asentamientos urbanos en la zona. Ur es, por tanto, una de las ciudades más antiguas de Sumeria. También, fue la ciudad cuna del profeta y patriarca hebreo Abraham, que es llamada en la Biblia como Ur de los caldeos o Ur Kaśdim.

Durante el (período de Uruk) la gran cantidad de cerámica encontrada parece indicar que Ur pudo haber sido un centro importante de producción. Esta situación se prolongó hasta el período Yemdet-Nasr, hacia el 3000 a. C. En algún momento del milenio siguiente se produjo una inundación de carácter local que dejó una importante capa de lodo en los estratos.

La información de las capas pertenecientes al período Dinástico Arcaico es reducida, ya que unos 500 años después se derribó gran parte de las antiguas estructuras para construir otras más monumentales. Sin embargo, la historia de la ciudad puede reconstruirse sobre la base de inscripciones en otras ciudades.

En algunos textos de Lagash, ciertos monarcas de esa ciudad se atribuyen haber conquistado Ur, si bien no indican los nombres de los reyes derrotados. Tampoco en la lista Real Sumeria se menciona a esos conquistadores, sino que hace referencia a una cesión de la realeza desde Uruk, al monarca de Ur, Mesannepada. En los sellos de este rey se encuentra que se titulaba rey de Kish, título que podría hacer referencia no tanto a la ciudad acadia como a todo el territorio de la Mesopotamia central, lo cual podría estar apoyado por el uso que, posteriormente, Sargón de Acad dio a este título. Esto indicaría una posible hegemonía de Ur en la zona a mediados del Dinástico Arcaico, lo cual estaría respaldado por algunos restos, que muestran el incendio de la ciudad de Shuruppak y la destrucción del palacio de Kish.

Se conocen algunos datos de la familia de Mesanepada. Así, una tablilla de fundación encontrada en un templo cerca de tell Obeid nombra a un tal Aanepada, hijo de Mesannepada. El hijo de Aanepada se llamaba Meskiaga-nuna, y fue él quien sucedió a su abuelo en el trono. De este rey se conoce su existencia por una tablilla que le dedicó su esposa a su muerte. La lista real sumeria menciona a estos dos reyes y a dos más, en la que denomina dinastía I de Ur. De estos dos últimos reyes destaca que sus nombres no son sumerios sino acadios.

Los nombres de los monarcas de la dinastía II de Ur aparecen muy deteriorados en la lista Real; sin embargo, se conocen bien los acontecimientos de este período, marcado por la rivalidad entre las distintas ciudades. Hacia el el rey de Umma Lugalzagesi conquista las ciudades del sur de Mesopotamia, incluida Ur, formando una hegemonía local y declarándose rey de Kish, al igual que habían hecho los monarcas de la dinastía I de Ur.

El dominio de Lugalzagesi no duró mucho ya que hacia el 2335 a. C. Sargón I de Acad fundó Agadé y comenzó sus conquistas, venciendo primero a Lugalzagesi y después a todas las ciudades sumerias, incluida Ur, a la que derribó sus murallas. Tras esto Ur y las demás ciudades sumerias quedaron incorporadas en el Imperio acadio. Tras la muerte de Sargón todas ellas se sublevaron, siendo reprimidas por su sucesor.

Durante el reinado del nieto de Sargón, Naram-Sin, la ciudad seguía formando parte del Imperio acadio, si bien se produjeron rebeliones. A esta época pertenece un texto escrito por Enheduanna, una sacerdotisa "en" y escriba en el templo de Nannar en Ur. La historia narra en primera persona el sufrimiento de la sacerdotisa que ha sido expulsada de Ur por el "lugal" local, Lugal-ane. La historicidad de los personajes parece estar demostrada; en el caso de Lugal-ane, por inscripciones en las que Naram-Sin le nombra como uno de los cabecillas de las revueltas de las ciudades del sur y, en el caso de Enheduanna, por un relieve en la que se le dibuja sentada junto al dios Nannar.

Los motivos de la expulsión de Enheduanna no están claros; el texto la menciona como "hija de Sargón", lo cual podría indicar una filiación simbólica más que una relación familiar. De hecho, según su sello, fue nombrada sacerdotisa por el conquistador acadio. Así, es posible que esta designación hubiese incomodado al lugal de Ur, siendo este el motivo de la expulsión.

La historia es representada como un conflicto entre el dios Nannar, que representa a Ur, e Innana, que representa a Agadé y al poder imperial; el árbitro del conflicto es el dios del cielo An de Uruk. Según la historia, An falla en favor de Inanna y Enheduanna recupera su posición. No se conoce cuál fue la historia real que inspiró esta alegoría, si bien se sabe que las revueltas de Ur y las demás ciudades fueron sofocadas por Naram-Sin.

A finales del siglo, durante el reinado de Sharkalisharri, hijo de Naram-Sin, el imperio se vio superado por las numerosas revueltas y los ataques de los pueblos vecinos. Así consiguió su independencia Ur.

Pocos años después de la caída del imperio, el norte fue invadido por los nómadas gutis, si bien parece que no llegaron a afectar al área del sur, donde se encontraba Ur. En esta etapa destacó la ciudad de Lagash que según parece mantuvo algún tipo de dominio sobre Ur.

Hacia el , Utu-hegal de Uruk expulsó a los gutis del norte consiguiendo la hegemonía en Sumeria. A su muerte fue su hermano Ur-Nammu, que posiblemente gobernaba hasta entonces en Ur, quien le sucedió en su imperio. En todo caso, el nuevo rey escogió a Ur como capital de su reino, fundando la que se ha llamado dinastía III de Ur o Ur III, que durante casi un siglo mantuvo la hegemonía sobre un territorio que abarcaba la totalidad de la cuenca mesopotámica y Elam.

En esta situación la ciudad de Ur quedó convertida en una gran capital, llegando a alcanzar los 200 mil habitantes. Es en este período cuando se destruyeron los anteriores edificios y se levantaron los que se pueden contemplar aún actualmente. Entre estas construcciones destaca el enorme zigurat de Ur, construido durante los reinados de Ur-Nammu (2113-2094 a. C.) y su sucesor Shulgi (2094-2047 a. C.) y que aún se mantiene en pie, tras su restauración parcial en los años 1970. No se conoce la altura que llegó a alcanzar ya que, si bien las ruinas actuales miden 15 metros, a lo largo de 4000 años la edificación ha debido sufrir una gran erosión. También en esta etapa se construyó el Gipar, un templo consagrado a Ningal. La tercera dinastía de Ur se caracterizó también por desarrollar un sistema de impuestos que, si bien resultaba eficaz, suponía una carga muy pesada para las clases populares.

La caída de la hegemonía de Ur estuvo marcada por la llegada de oleadas de nómadas procedentes de las regiones desérticas occidentales: los amorreos. Los recién llegados se fueron estableciendo en el curso medio del Éufrates, en la zona de Babilonia, consiguiendo cada vez más influencia. Tras la pérdida de las regiones periféricas del imperio, Shu-Sin (2037-2027 a. C.) dirigió la construcción de una muralla de 270km con el objetivo de frenar a los nómadas. Su sucesor Ibbi-Sin (2026-2004 a. C.) tuvo que enfrentar además los intentos de independencia de las demás ciudades. En esta situación, un antiguo gobernante de Mari e influyente funcionario llamado Ishbi-Erra se asoció a los distintos enemigos de Ur dándole el golpe final, causando la disolución del imperio. Tras esto, Ishbi-Erra fundó una dinastía en Isin.

Hacia finales del siglo XXI a. C. los elamitas, dirigidos por el rey de Simash y que hasta entonces habían estado sometidos a Ur, ocuparon la ciudad, que fue arrasada. Los templos fueron saqueados y las viviendas destruidas, su monarca Ibbi-Sin fue hecho prisionero y llevado a Elam, y los campos fueron incendiados. Tras el saqueo, la ciudad cayó bajo la influencia de Ishbi-Erra.

En este contexto se desarrollan las llamadas "Lamentaciones de Ur", un texto sumerio en el cual se atribuye la caída de Ur a la pérdida del favor de los dioses, tras lo cual se narran una serie de proyectos y deseos para que la ciudad recupere su estado anterior. Las lamentaciones se han interpretado como un texto de carácter político donde, tras la caída en desgracia de la ciudad, Ishbi-Erra, el nuevo gobernante, procederá a su reconstrucción con el beneplácito de los dioses.

En los años siguientes, el dominio de Ur y el del resto de la región se alternó entre Isin y Larsa. Tras las conquistas de Hammurabi, durante el Imperio paleobabilónico (siglos XVIII y ), la ciudad jugó un papel muy importante como centro de culto. Mil años después, Nabucodonosor II llevó a cabo una ambiciosa reconstrucción de los templos de Ur, que aún era un importante centro urbano. El declive de la ciudad solo se produjo tras el final de los reinos mesopotámicos, con la conquista de la región por parte del Imperio persa.

Debido a su tamaño, el montículo formado por las ruinas de Ur destacó durante siglos después de su abandono. Entre las edificaciones de las que quedan restos destacan el "Gipar" y el zigurat, construidos durante los primeros reinados de la dinastía III. No se conservan los templos del Imperio acadio, ya que fueron destruidos al construir los templos posteriores. Del período Dinástico Arcaico solo se conservan algunos restos en los que se aprecia una edificación a base de ladrillos plano convexos.

Los dos edificios religiosos que se conservan estaban situados en un segmento de la ciudad rodeado por una muralla de 8 metros, cuya pared exterior estaba inclinada 45º. La sección noroeste de este recinto sagrado estaba dedicada al dios Nannar.

El zigurat de Ur-Nammu, cuyo nombre en sumerio era "é-temen-ní-gùr-ru" (casa de cimientos revestidos de terror) fue construido durante la primera mitad del y estaba rodeado por su propia muralla. La estructura aún se conserva y fue parcialmente reparada a finales de los años 1970. Tiene planta rectangular de y 15 metros de altura, si bien es probable que en su época tuviese bastante más metros de altura, perdidos debido a la erosión. El interior del zigurat no es hueco, sino que está completamente formado por ladrillos de barro. Las paredes exteriores están recubiertas por una capa de 2,4 metros de grosor de ladrillo cocido y betún y cada una de ellas está orientada a un punto cardinal. Es posible que en la cima albergase un templo. El acceso a las plantas superiores se realizaba a través de tres escaleras exteriores.

El Gippar era un recinto sagrado consagrado a Ningal situado en el sureste del recinto. Si bien fue remodelado por completo durante la dinastía III de Ur, es muy posible que su construcción se remontase al período Dinástico Arcaico. El interior del edificio estaba dividido en dos partes por un pasillo y contenía numerosas habitaciones que se situaban alrededor de patios. El Gippar funcionaba como residencia de la sacerdotisa "en" y su séquito. Además, la diosa Ningal tenía varias habitaciones reservadas a su uso.

En cuanto a la arquitectura residencial, la vivienda del Ur del estaba organizada en torno a un espacio central y generalmente tenía dos plantas. El espacio central ha sido interpretado en ocasiones como un patio, si bien es probable que se encontrase cubierto. En la ciudad se ha encontrado otro tipo de edificaciones de peor calidad, formadas simplemente como un agrupamiento en línea de unas pocas habitaciones. Se ha especulado sobre la posibilidad de que se tratase de comercios o talleres, si bien también es posible que fuese un tipo más humilde de vivienda.

Uno de los hallazgos más sorprendentes de la expedición de Leonard Wooley en Ur fue una serie de 16 sepulturas a las que se denominó las "Tumbas Reales de Ur". Pertenecían al período Dinástico Arcaico y estaban construidas por paredes de ladrillo o piedra coronadas por una bóveda. Se encontraban en un cementerio mayor, destinado a todo tipo de personas y que contenía más de 2500 tumbas. Cada una de las tumbas reales contenía un cuerpo principal y un cierto número de acompañantes, así como numerosas riquezas.

De todas las sepulturas, destacaba la de una reina identificada gracias a su sello cilíndrico como Puabi. En su interior, además de la reina, se encontraban los cuerpos de cinco hombres armados y diez mujeres acompañadas por la magnífica Arpa de Ur rematada por la cabeza de un toro en oro. La cámara contenía incluso un carro y los esqueletos de dos bueyes. El cuerpo de la reina estaba envuelto en joyas y mantos con incrustaciones. Sobre la cabeza llevaba un tocado a base de hojas y una peineta rematada por estrellas de cinco puntas. Cerca de su mano tenía una copa de oro. Debajo de un baúl había un pasadizo que comunicaba con otra cámara funeraria; en ella se encontraba el rey A-kalam-dug de Ur, cuya tumba había sido parcialmente saqueada.

Otra de las tumbas reales pertenecía al "lugal" Meskalamdug. En otra de las fosas, cuyo dueño no se conoce, se encontraron 74 cuerpos, la mayoría de mujeres, lujosamente ataviados. Es en esta última tumba donde se encontró el Estandarte de Ur, una de las piezas más célebres de las halladas en Ur. El estandarte, está dividido en distintas franjas que contienen escenas cotidianas y de guerra, en la que destaca la representación de carros de guerra.

Se ha interpretado de diferentes formas el hecho de que las tumbas reales contuviesen cuerpos de sus sirvientes; para algunos autores, se trataba de enterramientos rituales, en los que el monarca era acompañado por estos hacia el más allá. Sin embargo esto no ha sido demostrado y también se han barajado otras opciones, como que la tumba real fuese escogida por las élites como lugar ilustre de enterramiento, siendo sus cuerpos desplazados allí una vez construida.



</doc>
<doc id="29653" url="https://es.wikipedia.org/wiki?curid=29653" title="Estocolmo">
Estocolmo

Estocolmo ( ) es la capital y ciudad más grande de Suecia, en la que residen 972 647 personas y 2.4 millones en su área metropolitana. La ciudad está compuesta por 14 islas donde el lago Mälar desemboca en el mar Báltico. En el este de la ciudad, y a lo largo de la costa, se encuentra la cadena de islas conocida como el archipiélago de Estocolmo. Estocolmo ha sido poblada por humanos desde la Edad de Piedra, en el sexto milenio antes de Cristo, y fue fundada como ciudad en 1252 por el "jarl" sueco Birger Magnusson. Es también la sede del Condado de Estocolmo.

La ciudad de Estocolmo es administrativamente un municipio de la provincia homónima, incluye el área metropolitana. Ha sido nombrada por la GaWC como una ciudad global de «clase alfa» en el índice global de las ciudades 2008, ocupa el puesto 24 en el mundo, el número 8 en Europa y el número 1 de toda Escandinavia.

Estocolmo es el centro cultural, mediático, político y económico de Suecia. La región de Estocolmo, por sí sola, acumula más de un tercio del PIB del país y está entre las 10 regiones de Europa con un PIB per cápita más alto. Es considerada una ciudad global y el principal centro de sedes corporativas en los países nórdicos. 

La ciudad forma parte del grupo de ciudades conocidas vulgarmente como las «Venecias del Norte» y fue premiada con el título Capital Verde Europea 2010.

En la ciudad se localizan importantes universidades, como el Instituto Karolinska, la Universidad de Estocolmo y el Real Instituto de Tecnología. Así mismo, alberga la ceremonia anual de los Premios Nobel y el banquete en Estocolmo Concert Hall y en el Ayuntamiento. Uno de los museos más premiados de la ciudad, el Museo Vasa, es el museo no relacionado con el arte más visitado de Escandinavia. El metro de Estocolmo, abierto en 1950, es conocido por la decoración de sus estaciones, se le ha llamado por ello, la galería de arte más grande del mundo. También fue sede de los Juegos Olímpicos de 1912.

Como capital del Estado, Estocolmo es la sede del Gobierno sueco y de la mayoría de sus instituciones, como el Tribunal Supremo de Justicia (Hōgsta Domstolen). El edificio Rosenbad es la sede del Gobierno sueco, el Palacio del Parlamento es sede del Parlamento sueco (Riksdag) y la residencia del primer ministro es adyacente a Sager House. El Palacio Real de Estocolmo es la residencia oficial y principal centro de trabajo del rey sueco, Carlos XVI Gustavo de Suecia, mientras que el Palacio de Drottningholm, a las afueras de Estocolmo, es la residencia privada de la Familia Real Sueca. 

El nombre de la ciudad se debe en primer lugar a que en la época de su fundación los nativos tenían que defender sus tierras, para ello colocaban troncos amontonados a las orillas del lago Mälar (Sigtuna) para evitar el tránsito de barcos enemigos, de ahí sacaron la palabra "Stack" que es lo mismo que apilar en español y que debido a la pronunciación se queda en "Stock". La otra sílaba correspondiente al nombre de una de las antiguas islas de la ciudad vieja, Stadsholmen, y que luego la situación expansiva y territorial terminó por obligar a añadir la misma monosílaba para cambiarla por la antes conocida como "Stock", que debido a la unificación de las islas ya al final pasa a conocerse como ""Stock"" "holm" y definitivamente Stockholm.

La primera mención de la ciudad de Estocolmo data de 1252. La ciudad se reducía entonces a la pequeña isla llamada Gamla Stan. Fue fundada por Birger Jarl, con el objetivo de proteger a Suecia de las invasiones de flotas extranjeras y para poder poner fin a los pillajes de los cuales eran víctimas ciudades como Sigtuna, situada sobre el lago Mälaren. El primer edificio construido fue un fuerte que controlaba el tránsito marítimo entre el mar Báltico y el lago Mälaren. Bajo la influencia de Magnus Ladulás, prospera gracias a sus relaciones comerciales con Lübeck. Forma entonces parte de la Liga Hanseática. En 1270, se describe en documentos como una verdadera ciudad, y en 1289 es ya la mayor ciudad de Suecia. La primera estimación fiable de su población se remonta al siglo XV. Se reunían entonces aproximadamente un millar de cabezas de familia, que equivalía a unos cinco a seis mil habitantes. En 1350, la ciudad conoce un episodio de Peste Negra con devastadoras consecuencias.

No es hasta finales de 1436 que Estocolmo se proclama capital de Suecia. Su posición estratégica, así como su peso económico, la convierten en una plaza importante en las relaciones entre los reinos daneses de la Unión de Kalmar y el movimiento de independencia sueco durante el siglo XV. Hubo numerosas batallas, sobre todo la batalla de Brunkeberg ganada en 1471 por Sten Sture el Viejo contra el rey de Dinamarca Cristián I, y el Baño de sangre de Estocolmo que tendrá lugar en 1520 ordenado por Cristián II de Dinamarca, que pondría fin a la Unión de Kalmar.

En 1521, Gustavo Vasa hace su entrada en Estocolmo y señala el comienzo de una nueva era para Suecia. La ciudad crece y se extiende más allá de Stadsholmen sobre Södermalm y Norrmalm. En 1600, ya cuenta con unos doce mil habitantes.

En el siglo XVII Estocolmo es ya una ciudad europea de envergadura. Entre 1610 y 1680 su población se multiplica por seis. Ladugårdslandet, actualmente nombrada Östermalm, así como la isla de Kungsholmen, son en aquellos momentos absorbidos por la ciudad. En 1628, durante el reinado de Gustavo II Adolfo, el navío de guerra Vasa zozobró en el archipiélago de Estocolmo. El mismo año son instauradas las reglas que dan a Estocolmo un monopolio sobre los intercambios entre los negociantes extranjeros y los territorios escandinavos. En esta época se construyen numerosos castillos y palacios para los nobles, entre los cuales se encuentran la Casa de la Nobleza (Riddarhuset) y en el siglo XVIII el Palacio real.
Después de la Gran Guerra del Norte, que conllevaría la destrucción parcial de la ciudad, Estocolmo ve cómo su crecimiento comienza a declinar. Conserva, sin embargo, su papel de capital política de Suecia, y bajo el reinado de Gustavo III de Suecia afirma su superioridad cultural. La Ópera Real es un buen ejemplo de la arquitectura de esta época.

Al comienzo del siglo XIX, Estocolmo pierde cada vez más su influencia económica. Norrköping es entonces la principal ciudad industrial del país, y Gotemburgo un puerto ineludible gracias a su localización en el mar del Norte. En la segunda parte del siglo, Estocolmo consigue recuperar su papel de líder en el aspecto económico con la aparición de las nuevas industrias, y la convierte en un centro importante del comercio y de los servicios, así como la principal puerta de entrada de Suecia. Su población creció entonces de manera muy importante gracias a una fuerte inmigración. Al final del siglo, solamente el 40 % de los habitantes de la ciudad habían nacido en ella. Se comienzan entonces a desarrollar barrios más allá de los límites de Estocolmo, en el campo y sobre las costas. Es también en esta época que la ciudad aumenta su papel central en la educación y la cultura, con la apertura de numerosas universidades, como el Instituto Karolinska.

Durante el siglo XX, Estocolmo rehabilita una gran parte de su centro, que tenía calles estrechas y curvadas que planteaban problemas a medida que la circulación automovilística aumentaba. Las autoridades municipales prohibieron aquí la renovación de los edificios, sobre todo los que comprenden la zona próxima a la Estación Central, de Hötorget a Sergels torg, durante la primera mitad del siglo. De 1945 a 1967 la zona es demolida y después reconstruida, con amplios paseos de viandantes, así como edificios de oficinas o viviendas de alturas elevadas. Al final del siglo, Estocolmo es una ciudad moderna, cosmopolita y muy avanzada en el ámbito tecnológico. En 1923 el gobierno del municipio cambia de sede y pasa al Ayuntamiento nuevo, obra del arquitecto Ragnar Östberg. El Metro de Estocolmo se construye a partir de 1950, y el distrito de Kista se ha convertido en un importante centro para las nuevas tecnologías.

En 1986 el primer ministro Olof Palme muere tras ser abatido en plena calle, y en 2003 la ministra de Asuntos Exteriores Anna Lindh es asesinada en el gran almacén Nordiska Kompaniet.

Estocolmo fue fundada en la pequeña isla de Stadsholmen, lugar hoy conocido como Gamla Stan (ciudad vieja), situada exactamente entre el lago Mälaren y el mar Báltico. Limita al norte con Norrmalm y Östermalm, y al sur con Södermalm. En total, se sitúa sobre 14 islas, siendo el agua un elemento omnipresente. La ciudad cuenta con 57 puentes que permiten circular entre los diferentes barrios. Por eso es llamada también la "Venecia del Norte". Uno de estos puentes une la ciudad con Lidingö, en el estrecho de Lilla Värtan (puente de Lidingö). Como todas las ciudades escandinavas, se encuentra muy bien comunicada por vía marítima con las otras ciudades importantes de su entorno, como Helsinki, Copenhague y las ciudades costeras de Alemania, Polonia y los Estados bálticos. También está comunicada con Gotemburgo, a través de lagos interiores y el canal Göta kanal.

La temperatura media anual de Estocolmo es de alrededor de 7 °C. La temperatura media en enero y febrero es de –3 °C, en julio +17,5 °C. La mayor parte de las precipitaciones cae en verano, siendo julio el mes más lluvioso (72mm) y marzo el más seco (26mm). En verano las precipitaciones pueden ser torrenciales, en otoño en cambio se reparten más igualitariamente entre los meses. La temperatura más baja registrada en la ciudad fue –31 °C el 20 de enero de 1814 y la máxima +36 °C el 3 de julio de 1811.

Estocolmo es uno de los 290 municipios de Suecia, y es la capital de la provincia de Estocolmo, además de ser la capital de Suecia. El municipio está dividido desde 1998 en 18 distritos, que tienen la responsabilidad de las escuelas primarias, asistencia social y los medios culturales locales. El actual alcalde de Estocolmo es "Sten Nordin" (desde 2008), del Partido Moderado (Suecia). La elección por democracia representativa del consejo municipal se realiza cada cuatro años. La última fue en 2010, quedando representados:

Estocolmo es conocida por sus tres zonas: Innerstaden (centro), Söderort (sur) y Västerort (oeste). A su vez la ciudad está integrada en 14 distritos:

Como capital de Suecia, Estocolmo cuenta con una gran parte de los principales lugares culturales de Suecia, con teatros, museos u óperas. En Estocolmo y sus alrededores se encuentran inscritos en la lista del Patrimonio de la Humanidad en Suecia de la UNESCO varios edificios como el Palacio de Drottningholm o el cementerio de Skogskyrkogården. Por otra parte, Estocolmo fue designada Capital Europea de la Cultura en 1998.

Estocolmo cuenta con un centenar de museos, algunos de los cuales son gratuitos. Los más célebres son:


Otros museos famosos de la ciudad son el Museo Histórico, el Museo Nacional de Bellas Artes, el Museo de Ciencias e Historia Natural y el Museo Nórdico.

La parte principal de Estocolmo está compuesta de diversos barrios que constituyen interesantes y hermosos lugares para visitar.


Otros barrios notables son Norrmalm, Östermalm, Kungsholmen o Skeppsholmen.

El domicilio social de más del 40% de las empresas suecas se sitúan en Estocolmo, que es el centro económico y financiero de Suecia. Aquí están ubicadas algunas empresas de alta tecnología, como Ericsson, Electrolux o AstraZeneca, en el distrito de Kista, que es uno de los centros europeos más dinámicos en lo referente a las tecnologías de la información y de la comunicación. El turismo se ha convertido desde hace unos años en una actividad muy importante para la ciudad de Estocolmo, con siete millones de visitantes anuales.

El área de Estocolmo representa el 22% de la población total de Suecia, por lo tanto su presencia en la economía de Suecia es la mayor de todas las regiones, con un 29% del producto interior bruto del país. En los siglos XVIII y XIX solo abarcaba el centro de la ciudad actual, una quinta parte de la misma. En el siglo XX se incorporaron varios municipios como Brännkyrka (1913) o Spånga (1949), entre otros. Un 49% de la población son hombres y un 51% mujeres. Su tasa de inmigración está en aumento, y en el último recuento había un 26% de población extranjera. La población total del área urbana en 2010 era de 1 372 565.

En Estocolmo hay gran cantidad de suburbios diferentes de mayoría extranjera, por ello se hablan multitud de lenguas. Las principales son el sueco y el finés (Finlandia), y las minoritarias el bosnio, siríaco, árabe, turco, kurdo, persa, holandés, español, serbio y croata.

Estocolmo es el punto central del sistema de ferrocarril sueco. La ciudad posee un aeropuerto internacional, Arlanda, así como dos aeropuertos menos importantes: Skavsta y Aeropuerto de Estocolmo-Bromma. Estocolmo es igualmente un puerto importante, con relaciones sobre todo con las ciudades de Helsinki, San Petersburgo, Turku y Tallin. Los transportes públicos son de una gran variedad. El Metro de Estocolmo ("Tunnelbanan") para los puntos principales de la ciudad e implica tres líneas de trenes de cercanía ("pendeltågen", "Roslagsbanan" y "Saltsjöbanan") que permiten unir el centro con las afueras, situadas a gran distancia (un centenar de kilómetros de norte a sur). También hay tranvías ("Tvärbanan", "Nockebybanan", "Lidingöbanan" y "Djurgårdslinjen"), así como una vasta red de bus. Los equipamientos son propiedad de la sociedad "Storstockholms Lokaltrafik" (SL, propietaria de la diputación provincial de Estocolmo), y son explotadas y mantenidas por subcontratistas. El transporte con barco también se utiliza entre las islas del archipiélago.

El 3 de enero de 2006 se instauró un sistema de peaje urbano semejante al de Londres, por un periodo de prueba de seis meses. Al final de este periodo, por medio de referéndum, se decidió instaurarlo de forma permanente, comenzando el 1 de agosto de 2007. Los objetivos principales son de reducir los atascos y la contaminación generada por el tránsito de los vehículos. Las líneas de autobús se han reforzado como ayuda a estas medidas.

Academias reales que igualmente se encuentran en Estocolmo, son la Academia Real de las Ciencias de Suecia y la Academia Sueca que deciden todos los años quien van a recibir los Premios Nobel.

Los Juegos Olímpicos de Verano de 1912 se celebraron en la capital sueca. Existe el Estadio Olímpico de Estocolmo, el Globen y el estadio de Friends Arena para la práctica de fútbol, del hockey sobre hielo o incluso del bandy. La presencia de gran cantidad de agua embalsada, que se hiela fácilmente en el invierno, facilita la práctica del patinaje sobre hielo en el período invernal.







</doc>
<doc id="29657" url="https://es.wikipedia.org/wiki?curid=29657" title="Moscú">
Moscú

Moscú (, transliterado como "Moskvá") es la capital y la más poblada de Rusia. La ciudad es un importante centro político, económico, cultural y científico de Rusia y del continente. Moscú es la megaciudad más septentrional de la Tierra, la segunda ciudad de Europa en población después de Estambul, y la sexta del mundo. Su población es de 12 108 257 habitantes. En virtud de su expansión territorial al suroeste del óblast de Moscú, el 1 de julio de 2012 la capital aumentó su área en 2,5 veces, desde unos 1000 km² hasta 2500 km², y ganó una población adicional de 230 000 habitantes.

Moscú está situada a orillas del río Moscova, en el Distrito Federal Central de la Rusia europea. En el curso de su historia, la ciudad ha sido capital de una sucesión de estados, desde el Gran Ducado de Moscú de la Edad Media, el Zarato ruso y la Unión Soviética, exceptuando el período del Imperio ruso. En Moscú se encuentra el Kremlin de Moscú, una antigua fortaleza donde se halla hoy el lugar de trabajo del presidente de Rusia. El Kremlin también es uno de los varios sitios que son Patrimonio de la Humanidad en la ciudad. Ambas cámaras del Parlamento ruso (la Duma Estatal y el Consejo de la Federación) también tienen su sede en Moscú.

La ciudad posee una amplia red de transporte que incluye tres aeropuertos internacionales, nueve estaciones de ferrocarril y uno de los más profundos sistemas de metro del mundo, el metro de Moscú, solo superado por el de Tokio en número de pasajeros. Su suburbano es reconocido como uno de los más ricos y variados arquitectónicamente en sus 215 estaciones, repartidas por la ciudad. Según la publicación "Forbes 2011", Moscú es la segunda ciudad del mundo en número de multimillonarios.

El nombre de la ciudad procede del río que la atraviesa, denominado Moscova ("Moskvá"; en ruso antiguo: "град Москов [Grad Moskov]", literalmente "la ciudad del río Moskvá"). 

El origen del nombre es desconocido, aunque existen varias teorías. Una teoría sugiere que el origen del nombre procede de una antigua lengua finesa, en la que significa «oscuro» y «turbio». La primera referencia rusa de Moscú data de 1147, cuando Yuri Dolgoruki exhortó al príncipe de la República de Nóvgorod: «Venid a mí, hermano, a Moscú» ("") ["Pridi ko mne, brate, v moskov"].

La primera referencia rusa de Moscú data de 1147 con Yuri Dolgoruki￼￼ ("Юрий Долгорукий"). Nueve años más tarde, en 1156, el príncipe Yuri Dolgoruki de Rostov ordenó la construcción de una empalizada que rodeara la ciudadela, que tuvo que ser reconstruida varias veces. Tras el saqueo de 1237-1238, en que los mongoles quemaron la ciudad y mataron a sus habitantes, Moscú se recuperó y se convirtió en la capital de un principado independiente, el Principado de Moscú, en 1327. Su posición favorable a la cabecera del río Volga contribuyó a su expansión constante. Moscú fue un país estable y un próspero principado durante muchos años y atrajo a un gran número de refugiados procedentes de toda Rusia.

Iván I sustituyó la ciudad de Tver como centro político del Principado de Vladímir-Súzdal y pasó a ser el único recaudador de impuestos para los gobernantes tártaro-mongoles, tras la invasión mongola de Rusia. Iván ganó una importante concesión del kan mediante el pago de importantes tributos. A diferencia de otros principados, Moscú no fue dividido entre sus hijos, lo que lo mantuvo intacto. Sin embargo, creció la oposición de Moscú contra la dominación extranjera. En 1380, el príncipe de Moscú Dmitri Donskói dirigió el ejército ruso en una importante victoria sobre los tártaros en la batalla de Kulikovo que, no obstante, no fue decisiva. Solo dos años más tarde Moscú fue saqueada por el kan Toqtamish. En 1480, Iván III acabó finalmente con el dominio tártaro, lo que permitió a Moscú convertirse en el centro del poder en Rusia. Durante el reinado de Iván III, la ciudad pasó a ser la capital de un imperio que finalmente abarcaría toda la actual Rusia y otras tierras.

En 1571, los tártaros de Crimea atacaron y saquearon Moscú, quemando todo, excepto el Kremlin.

En 1609, el ejército sueco, dirigido por el conde Jacob de la Gardie y Evert Horn, comenzó su marcha de Veliki Nóvgorod hacia Moscú para ayudar al zar Basilio IV de Rusia; entraron en Moscú en 1610 y reprimieron la rebelión contra el zar, pero dejaron la ciudad a principios del año 1611, tras lo cual fue invadida por el ejército de la Mancomunidad Polaco-Lituana.

El siglo XVII fue rico en levantamientos populares, tales como el que llevó a la liberación de Moscú de la Mancomunidad Polaco-Lituana (1612), la Revuelta de la Sal (1648), la Revuelta del Cobre (1662) o la Revuelta de Moscú en 1682. De 1654 a 1656 la peste mató a la mitad de la población de Moscú. La ciudad dejó de ser la capital de Rusia en 1712, después de la fundación de San Petersburgo por Pedro el Grande en la costa del mar Báltico en 1703.

El 14 de septiembre de 1812, cuando las fuerzas invasoras de Napoleón se aproximaban a Moscú, los moscovitas incendiaron y evacuaron la ciudad. El ejército de Napoleón, azotado por el hambre, el frío y las malas líneas de suministro, se vio obligado a retirarse y fue casi completamente aniquilado por el devastador invierno ruso y los esporádicos ataques de las fuerzas militares rusas. 

En enero de 1905, se creó oficialmente en Moscú la institución de gobernador de la ciudad, o alcalde; el primer alcalde de Moscú fue Aleksandr Adriánov. El 12 de marzo de 1918, tras la Revolución Rusa de 1917, Moscú se convirtió en la capital de la República Socialista Federativa Soviética de Rusia y cinco años más tarde, de la Unión Soviética. 

Durante la Gran Guerra Patria (denominación que recibe en Rusia el combate en el frente oriental de la Segunda Guerra Mundial, después de la invasión alemana de la URSS), el Comité de Defensa del Estado Soviético y el Estado Mayor del Ejército Rojo tenían su sede en Moscú. 

En 1941, se formaron entre los moscovitas dieciséis de las divisiones nacionales de voluntarios (más de 160.000 personas), veinticinco batallones (18.500 personas) y cuatro regimientos de ingeniería. En noviembre, el Grupo de Ejércitos Centro alemán fue detenido en las afueras de la ciudad y, a continuación, expulsado hacia fuera en el transcurso de la Batalla de Moscú. Se evacuaron muchas fábricas, junto con gran parte del gobierno, y a partir del 20 de octubre, la ciudad fue declarada en estado de sitio. El resto de sus habitantes construyeron defensas antitanque, mientras que la ciudad era bombardeada desde el aire. Es de señalar que Stalin se negó a abandonar la ciudad, lo que significó que el personal general y el Consejo de Comisarios del Pueblo permanecieran en la ciudad. A pesar del asedio y los bombardeos, la construcción del metro de Moscú continuó durante la guerra y al finalizar esta se abrieron nuevas líneas de metro. 

El 1 de mayo de 1944, se instituyó la "Medalla por la Defensa de Moscú" y en 1947 otra medalla "En memoria del 800 aniversario de la fundación de Moscú". El 8 de mayo de 1965, en conmemoración del 20 aniversario de la victoria en la Segunda Guerra Mundial, Moscú fue una de las doce ciudades soviéticas galardonadas con el título de Ciudad Heroica. En 1980, acogió los Juegos Olímpicos de Verano, que fueron boicoteados por los Estados Unidos y otros países occidentales debido a la Guerra de Afganistán. 

En 1991, Moscú fue el escenario de un intento de golpe de Estado por miembros del gobierno y del KGB opuestos a las reformas de Mijaíl Gorbachov. Tras la disolución de la URSS acontecida ese mismo año, Moscú continuó siendo la capital de la Federación de Rusia. 

Desde entonces, el surgimiento de una economía de mercado en el país ha producido una explosión de estilo de vida occidental, la venta al por menor y servicios. En 1998, se organizaron los primeros Juegos Mundiales de la Juventud.

En 2018 Moscú y otras 10 ciudades rusas albergaron el Campeonato Mundial de Fútbol. Para este evento en la ciudad se construyeron importantes instalaciones deportivas e infraestructurales.

La ciudad de Moscú está gobernada por un . El actual alcalde es Serguéi Sobianin (elegido en 2013). Moscú se divide en 12 distritos administrativos ("ókrugs", es una organización muy similar a la de Viena), 125 distritos y 21 asentamientos. Todos los "ókrugs" poseen su escudo y bandera. Casi todos ellos disponen de su propia estación de televisión.

Moscú es el centro del poder político ruso. El Kremlin se encuentra en el corazón de la ciudad, en el "ókrug" central. Allí se encuentra la residencia oficial del presidente de Rusia y numerosos cuarteles militares. En Moscú se encuentran las embajadas de los países extranjeros.

Moscú se encuentra a las orillas del río Moskvá, que fluye por poco más de 500 kilómetros a través de la llanura europea oriental en el centro de Rusia. Cuarenta y nueve puentes atraviesan el río y sus canales dentro de los límites de la ciudad. La altitud de Moscú, en el Centro de Exposiciones de Rusia (VDNJ), donde se encuentra la principal estación meteorológica de la ciudad, es de 156 metros. Las tierras altas de Tioply Stan son el punto más alto de la ciudad, a 255 metros. La ciudad de Moscú (sin incluir el anillo de circunvalación MKAD) mide 39,7 km de este a oeste y 51,8 km de norte a sur.

Moscú es el punto de referencia para la zona horaria utilizada en la mayor parte del centro de Rusia, incluido San Petersburgo. La hora estándar Moscú (MSK, мск), es UTC + 3 o GMT + 3. El horario de verano ya no se usa.

Moscú posee un clima continental húmedo con rigurosos y largos inviernos, y suaves y breves veranos. Los días nubosos y cubiertos son frecuentes a lo largo del año, por lo que en invierno las horas medias de insolación pocas veces superan 15 minutos diarios. Las temperaturas en invierno rara vez superan los 0 °C, con abundantes días de nieve. La nieve puede permanecer en las calles de la ciudad medio año perfectamente, desde finales de octubre hasta principios de abril. En las fuertes olas de frío, las temperaturas pueden descender hasta los –40 °C, o a temperaturas incluso inferiores, que se han registrado en varias ocasiones hasta la fecha. La primavera es fría durante las primeras semanas, aunque por lo general es suave a partir de abril-mayo, cuando comienzan a ser frecuentes los días de lluvia. En verano, las temperaturas ascienden en casos aislados hasta los 35 °C con olas de calor, aunque oscilan normalmente entre los 10 °C y los 30 °C. Las tormentas también suelen ser habituales. El máximo pluviométrico se da en esta estación, coincidiendo con la época de temperaturas más elevadas. Durante el otoño vuelve la época de transición del calor al frío, y las nevadas vuelven a ser normales a partir de finales de octubre.

El gentilicio de Moscú es moscovita. La población de Moscú es una mezcla de muchas nacionalidades. Los rusos son el grupo étnico más grande de Moscú con diferencia. Otras nacionalidades incluyen ucranianos, tártaros, bielorrusos y armenios, aunque no existen barrios étnicos separados. El gobierno ha procurado limitar el número de personas que vive en la ciudad.

Evolución demográfica de Moscú entre 1400 y 2002:

El cristianismo ortodoxo es la confesión predominante en la ciudad, de la que la Iglesia Ortodoxa Rusa es la que cuenta con mayor número de fieles. Moscú es la capital de Rusia del cristianismo ortodoxo. Ha sido la religión tradicional del país y se considera parte del "patrimonio histórico" de Rusia en una ley aprobada en 1997. Otras religiones practicadas en Moscú incluyen el islam, el protestantismo, el catolicismo, los viejos creyentes, el budismo y el judaísmo.

El Patriarca de Moscú es el jefe de la iglesia y reside en el monasterio de Danílov. Moscú fue llamada la "ciudad de 40 veces 40 iglesias" ("город сорока сороков церквей", "Górod Soroká Sorokov Tserkvey") antes de 1917. En 1918, Rusia se convirtió en un estado laico y la religión perdió su posición en la sociedad. Desde la desintegración de la Unión Soviética muchas de las iglesias destruidas antes de 1991 han sido restauradas y las religiones tradicionales, una vez más han ganado popularidad.

Mientras que la población musulmana de la ciudad se estima en 1,2-1,5 millones (de un total de 10,5 millones de habitantes), sólo había cuatro mezquitas de la ciudad a partir de 2010. A pesar de que se aprobó una mezquita adicional en el sureste, los activistas anti-mezquitas han bloqueado la construcción.

A partir de una encuesta oficial en 2012, el 52,8% de la población de Moscú se adhiere a la Iglesia Ortodoxa Rusa, el 3% declara ser cristiano genérico (excluyendo católicos y protestantes), el 2% sigue a otras Iglesias ortodoxas, 1% son eslavistas y otro 1% viejos creyentes. Los musulmanes constituyen el 4% de la población. El 5,2% sigue otras religiones o no dan una respuesta a la encuesta. Además, el 19% de la población declara ser "espiritual pero no religioso" y un 12% ser ateísta.

Desde la crisis financiera de Rusia de 1998, varios sectores han experimentado un importante crecimiento. Las industrias primarias en Moscú incluyen productos químicos, la metalurgia, el alimento, el textil, los muebles, la producción energética, desarrollo de software y las industrias de maquinaria. La actividad industrial de la capital rusa es muy importante, prueba de ello es que ocupa una sexta parte del volumen total de la industria de los países de la Comunidad de Estados Independientes.

Un reciente estudio elaborado por "Mercer Human Resources Consulting" reveló que la ciudad moscovita es la más cara del mundo, superando a Tokio. Para este estudio, la consultora evaluó 144 urbes de todo el mundo siguiendo criterios en función del transporte, comida, vestimenta, bienes domésticos y costos de entretenimiento. Una de las causas fundamentales que han provocado esta situación son los excesivos precios que se están pagando por las viviendas, no solo en Moscú, sino en toda Rusia, ya que los altos precios del petróleo hicieron subir la inflación a niveles del 5% en el primer trimestre de 2006. Tras Moscú, las ciudades más caras son Seúl, Tokio, Hong Kong y Londres.
Una importante pieza de la economía rusa y sus beneficios es Moscú. En ella se concentran grandes compañías multinacionales y sucursales de todo tipo. Las lujosas oficinas y el estilo de vida de empleado de las compañías moscovitas la hacen indistinguible de cualquier ciudad de la Europa Occidental, aunque, eso sí, los salarios medios son algo más bajos. La capital rusa ostenta otro récord producto de un estudio, esta vez de la prestigiosa revista "Forbes". Según se indica en esta publicación, Moscú es la ciudad donde residen más personas dueñas de fortunas superiores a 1.000 millones de dólares. Al frente de la lista de "millonarios" con residencia en Moscú figura Román Abrámovich, magnate del petróleo y el aluminio y dueño de los clubes de fútbol Chelsea FC y CSKA de Moscú, con 12.000 millones de dólares. En esta ocasión, Moscú superó a Nueva York, que cuenta con 31 multimillonarios por los 33 de la ciudad rusa.

La fábrica de helicópteros Mil de Moscú es una de las líderes en producción, en Rusia y en el mundo, de helicópteros civiles y militares. La planta de Jrúnichev produce material aeronáutico y espacial para las estaciones MIR, Saliut y la ISS, así como cohetes Protón y misiles balísticos intercontinentales. Las plantas de automóviles ZIL, AZLK y Voitóvich están situadas en Moscú, mientras que la planta de vagones de metro Metrovagonmash se encuentra en los límites de la ciudad.

La destilería Kristall es la más antigua de Rusia y produce varios tipos de vodka, entre ellos el "Stolichnaya". Mientras que una amplia gama de vinos se producen en varias plantas moscovitas, incluida la Moscow Interrepublican Vinery. La Moscow Jewelry Factory y la Jewellerprom son las productoras de joyas más importantes de Rusia. Jewellerprom solía producir la Orden de la Victoria, la más alta condecoración militar que otorgaba la Unión Soviética en la Segunda Guerra Mundial. El mercado de la informática tiene oficinas y sucursales en Moscú de varias compañías de software, como Kaspersky, 1C Company, ABBYY y la desarrolladora de videojuegos Akella.

En el año 2018 en Moscú se jugarán 12 partidos del Campeonato Mundial de Fútbol. La realización del torneo será un estímulo adicional para el desarrollo de la economía de la ciudad, la infraestructura deportiva y turística, al igual que para el embellecimiento de su territorio.

Durante la época soviética los apartamentos eran prestados por el gobierno a las personas de acuerdo con una norma de metros cuadrados por persona (algunos grupos, como los artistas, héroes, científicos destacados tenían primas en función de su historial). La propiedad privada de los apartamentos se limitó hasta la década de 1990, cuando la gente se permitía a los derechos de propiedad seguros a los lugares que habitan. Desde la época soviética, los propietarios tenían que pagar el cargo por el servicio de sus residencias, una cantidad fija basada en las personas que viven por la zona. Debido a la situación económica actual, el precio de los bienes e inmuebles en Moscú sigue en aumento. Hoy en día, uno puede pagar 4000 dólares EE.UU de promedio por metro cuadrado en las afueras de la ciudad, y entre 6000 y 7000 dólares por metro cuadrado en un barrio prestigioso. El precio de un piso a veces puede superar los 40 000 dólares americanos por metro cuadrado. Un piso típico de una habitación es de unos treinta y cinco metros cuadrados, un apartamento típico de dos habitaciones es de cuarenta y cinco metros cuadrados, y un piso típico de tres habitaciones es de setenta metros cuadrados. Algunos residentes de la ciudad han tratado de hacer frente al coste de la vida de sus apartamentos de alquiler y al mismo tiempo mantenerse en sus dachas (casa de campo) fuera de la ciudad. 

Entre 2006 y 2008, Moscú fue nombrada como la ciudad más cara del mundo. Sin embargo en 2012 ocupó el cuarto puesto de Rusía.

En 2006, había 8,47 millones de moscovitas en edad de trabajar. 1,73 millones están empleados por el Estado, 4,42 millones están empleados por empresas privadas, y 1,99 millones están empleados por pequeñas empresas. Hay 74 400 desempleados registrados oficialmente en edad de trabajar, de los cuales 34 400 son elegibles para beneficios de desempleo.

En Moscú conviven multitud de estilos arquitectónicos, desde edificios renacentistas hasta barrocos y arquitectura moderna.

En el centro histórico de Moscú predominan los edificios prerrevolucionarios, cuya construcción data finales de siglo XIX y principios del XX, antes de la revolución de octubre de 1917. Destacan también en la ciudad los edificios del período estaliniano, estilo comprendido entre los años 1930 y 1950. Estas edificaciones suelen localizarse en las calles y avenidas más importantes de la ciudad, como la calle Tverskaya, y las avenidas Kutúzovski, Léninski y Leningradski.

Las Siete Hermanas son los siete rascacielos que existen en Moscú, llamados también rascacielos estalinistas. Tres de ellos están destinados a viviendas, pero los otros cuatro incluyen dos hoteles, la Universidad Estatal de Moscú y el Ministerio de Asuntos Exteriores. Por último se encuentra la arquitectura post-estaliniana, edificios más pequeños que los del período estalinista y construidos entre 1960 y 1970.

Otros tipos de edificación más moderna son los edificios ministeriales, que se construyeron entre los años 1970 y 1980 destinados, principalmente, a los empleados de los ministerios soviéticos y altos mandos del partido comunista.

Últimamente, las edificaciones en Moscú tratan de adecuarse a los tiempos modernos. Prueba de ello es el plan que está llevando a cabo el prestigioso arquitecto británico Norman Foster al construir una zona dedicada a los negocios conocida como Moscow City, a las orillas del río Moscova.

Entre los lugares más famosos de Moscú se encuentran el Kremlin, la fortaleza de los zares, en él se encuentran varios palacios como el Gran Palacio del Kremlin o el Palacio de las Facetas; además de varios templos como la Catedral de la Dormición, la Catedral de la Anunciación o el Campanario de Iván el Grande de Iván III de Rusia, también conocido como Iván el Grande. Rodeando a todos los edificios están la Muralla, que incluye las torres del Kremlin. Desde 1990 el Kremlin fue incluido, junto con el conjunto de la Plaza Roja, en la lista de Patrimonio de la Humanidad de Unesco.

Junto al Kremlin está la Plaza Roja, con la famosa Catedral de San Basilio, finalizada en 1561 y mundialmente conocida por sus cúpulas de colores. En esta plaza también está el Museo Nacional de Historia, el Mausoleo de Lenin y el GUM, uno de los centros comerciales más grandes del mundo, construido en época soviética y después privatizado, pasando a ocuparse por las más elitistas marcas, el edificio que cuenta con un puente e innovadoras bóvedas de metal y cristal. Su arquitecto Vladímir Shújov fue responsable de construir varias de las señales de identidad de Moscú durante la época soviética; entre otras la Torre de Shújov, sólo una de muchas torres hiperboloide diseñadas por Shújov. Por su parte, el Museo Nacional de Historia fue mandado construir por el emperador Alejandro II en 1872. Las salas en que se dividen el museo están fielmente recreadas y decoradas con motivos de los distintos períodos a los que representa, que son desde la antigüedad hasta comienzos del siglo XX. Está considerado como el tesoro nacional de Rusia.
Las iglesias y monasterios de Moscú son muy numerosos, pese a que se han perdido muchos debido a las demoliciones de los soviéticos como el monasterio de los Milagros y el monasterio de la Ascensión de Cristo en el Kremlin. Sin embargo aún se conservan gran cantidad de edificios religiosos de indudable valor histórico. El monasterio de San Daniel es uno de los que mejor estado presentan, cuya edificación data de 1282. En él fue enterrado Daniel, el hijo de Aleksandr Nevski. Posteriormente fue transferido al Kremlin y restaurado por Iván el Terrible. El Alto Monasterio de San Pedro fue fundado por el hijo de Daniel, Iván I el Kalitá, pese a que debe su nombre a Pedro el Grande.

Otro monasterio notable de la ciudad es el Nuevo Monasterio de Nuestro Salvador, que salvaguarda a Moscú desde la orilla. Su construcción data del siglo XV y en él descansan los restos de la familia Románov. Otro monasterio notable es Monasterio Novodévichy, proclamado Patrimonio de la Humanidad en 2004. Por último y no menos importantes son los monasterios en los alrededores de Moscú: el Monasterio de la Trinidad y San Sergio en Sérguiev Posad (considerado como "La Meca" de la Iglesia Ortodoxa Rusa y declarado Patrimonio de la Humanidad por la Unesco), el monasterio de Volokolamsk de San José y el Monasterio de la Nueva Jerusalén cerca de la ciudad de Istra.

La ciudad posee, además de algunas joyas arquitectónicas de la antigüedad, ejemplos de arquitectura soviética como los rascacielos soviético-estalinistas denominados «las siete hermanas» ya que son siete edificios de similar diseño; entre ellos destaca la Universidad Estatal de Moscú y el hotel Ucrania. Otra obra de época soviética es el Metro de Moscú, una red de metro suntuosamente decorada y denominada por Stalin como «los palacios del pueblo». El Parque Gorki ofrece sus jardines para el descanso y la recreación.

Recientemente, tras la disolución de la URSS fue reconstruida la Catedral del Cristo Salvador con base en los planos originales de la catedral, demolida tras la revolución de 1917. También destaca la reconvertida Exposición del Progreso de la Economía Nacional y las hacienda-museo de Tsarítsino, Kolómenskoe y Kuskovo.

El Parque Zaryadye se encuentra en una zona céntrica, a pasos de la Catedral de San Basilio, la Plaza Roja y el Kremlin, y fue construido en 2017.

En Moscú están los frescos e iconos más importantes de Rusia. Precisamente, en el segundo tercio del siglo XIV apareció en la ciudad la escuela de Moscú de la pintura de icono, impulsada por artistas como Prójor de Gorodéts, Daniel el Negro y Andréi Rubliov. Son varios los edificios históricos que contienen iconos de estos artistas, como en las catedrales del Kremlin, la catedral de la Asunción o la iglesia de la Aparición de la Virgen. Los frescos suelen encontrarse en muchas iglesias moscovitas. Los frescos más notables están en iglesias como la de Santa Trinidad en Nikítniki, la catedral del monasterio Srétenski o la iglesia de San Juan el Guerrero.

La galería nacional de arte Tretyakóv es el más importante de Moscú, gracias, en parte, a su colección de arte pre-revolucionario ruso. La galería, y por supuesto su nombre, está estrechamente relacionada con la vida de un mecenas local, Pável Tretiakov, hombre dedicado a la colección de obras de arte y que impulsó la creación de la galería. De hecho, en 1872 la colección artística de Tretiakov contaba con cerca de 500 obras. Tras su muerte, la dirección de la construcción de la galería la dirigió el propio gobernador de Moscú.

Otro de los edificios culturales fundamentales de Moscú es el Museo de Bellas Artes Púshkin, abierto al público el 31 de mayo de 1912. Destaca sobremanera la colección de obras impresionistas y post-impresionistas y algunos de los artistas que pueden encontrarse en el museo incluyen a Pablo Picasso, Claude Monet, Auguste Renoir, Vincent Van Gogh o Auguste Rodin. Entre todas las obras sobresalen dos Rembrandt, dos Rubens, tres retratos de Van Dyck, un cuadro de El Greco y otro de Sandro Botticelli.

Moscú es considerada una de las mayores capitales culturales del mundo. Su Teatro Bolshói (en español "Gran Teatro") es quizá el emblema teatral de la ciudad, sede de espectáculos de ópera y ballet ruso, donde se representan las obras de los compositores rusos como Glinka o Rimski-Kórsakov, y es la sede de la Compañía de Ballet del Bolshói, que contó con la actuación de Maya Plisétskaya y Mijaíl Barýshnikov. El edificio es una de las obras más importantes del clasicismo ruso, cuya construcción corrió a cargo de Joseph Bové, arquitecto ruso, en 1824, siendo restaurado en 1854 tras un incendio. Está situada en uno de los emplazamientos clave de la ciudad, la plaza Teatral.

El Teatro Maly (en español "Teatro Pequeño") ofrece obras de teatro clásicas, el conservatorio con sus salas grande y pequeña ofrece música clásica, coral o de órgano. Muchas otras salas aportan diversidad cultural a Moscú. Otros teatros importantes de Moscú son el teatro Taganka de Yuri Lubímov, el teatro musical de Stanislavski y de Nemiróvich-Dánchenko o la Sala de conciertos de Chaikovski, un gran auditorio frecuentado por músicos y bailarines.

Uno de los centros culturales más importantes con respecto al teatro moscovita es el Museo de Teatro Bajrushin, que originariamente era una finca que Alekséi Bajrushin convirtió en 1894 en el primer museo del arte teatral. En 1913 lo cedió a la ciudad de Moscú. Actualmente el museo Bajrushin alberga una colección que contiene 1,5 millones de todo tipo de objetos auténticos relacionados con cada uno de los períodos de la historia del teatro. Está situado cerca de la estación de metro Pavelétskaya.

Hay 1696 escuelas secundarias en Moscú, así como 91 centros de estudios superiores. Además de estos, hay 222 instituciones que ofrecen educación superior en Moscú, incluyendo 60 universidades estatales, y la Universidad Estatal de Moscú M.V. Lomonósov, que fue fundada en 1755. El edificio más importante de la universidad principal está situado en la Colina de los Gorriones (Vorobiovy Gory), tiene una altura de 240 metros y una vez terminado, fue el edificio más alto fuera de los Estados Unidos. La universidad cuenta con más de 30.000 estudiantes de pregrado y más de 7000 estudiantes de posgrado, que tienen una opción de veinte y nueve facultades y 450 departamentos de estudio. Además, aproximadamente 10.000 estudiantes de secundaria toman cursos en la universidad, en la que trabajan más de dos mil investigadores. La Biblioteca de la Universidad Estatal de Moscú contiene más de nueve millones de libros, por lo que es una de las mayores bibliotecas de toda Rusia. Su fama en toda la comunidad académica internacional ha significado que más de 11.000 estudiantes extranjeros se han graduado de la universidad, con muchos que vienen a Moscú a aprender el idioma ruso. 

La Universidad Técnica Estatal Bauman de Moscú, fundada en 1830, está situada en el centro de Moscú y ofrece más de 18.000 plazas de estudiantes de pregrado y 1.000 de postgrado con una educación en ciencia e ingeniería que ofrece una amplia gama de grados técnicos. Desde que abrió la matrícula a los estudiantes de fuera del país en 1991, esta universidad ha aumentado su matrícula internacional para un máximo 200 alumnos extranjeros.

El Conservatorio de Moscú, fundada en 1866 es una destacada escuela de música de Rusia, cuyos graduados incluyen a Serguéi Rajmáninov, Aleksandr Skriabin, Aram Jachaturián, Mstislav Rostropóvich, y Alfred Schnittke.

El Instituto Estatal de Cinematografía Panruso, abreviado como VGIK, es la institución educativa en cinematografía más antigua del mundo, fundada por Vladímir Gardin en 1919. Serguéi Eisenstein, Vsévolod Pudovkin y Alekséi Batálov figuran entre sus más distinguidos profesores y Mijaíl Vartánov, Serguéi Parajanov, Andréi Tarkovski, Nikita Mijalkov, Eldar Riazánov, Aleksandr Sokúrov, Yuri Norstein, Aleksandr Petrov, Vasili Shukshín y Konrad Wolf, entre los alumnos más distinguidos. 

La Universidad de Relaciones Internacionales del Ministerio de Asuntos Exteriores de Rusia, abreviada como MGIMO, fue fundada en 1944, en Rusia sigue siendo la escuela más conocida de relaciones internacionales y diplomacia, con seis escuelas diferentes que se centran en las relaciones internacionales. Alrededor de 4500 estudiantes conforman el cuerpo estudiantil de la universidad y más de 700.000 rusos y extranjeros han pasado por sus aulas. Se pueden encontrar más de 20.000 libros raros en la biblioteca de esta universidad. 

Entre otras instituciones destacadas están el Instituto de Física y Tecnología de Moscú, también conocido como Phystech, el Instituto de Aviación de Moscú y el Instituto de Ingeniería Física de Moscú. El Instituto de Física y Tecnología ha impartido clase a numerosos ganadores del Premio Nobel, incluidos Piotr Kapitsa, Nikolái Semiónov, Lev Landáu y Aleksandr Prójorov, mientras que el Instituto de Ingeniería Física es conocido por su investigación en física nuclear. Otras instituciones, como la Academia Financiera, la Universidad Estatal de Gestión, la Academia de Economía Plejánov y la Escuela Superior de Economía ofrecen títulos en la gestión y la teoría económica.

Aunque Moscú desde la era soviética tiene una serie de famosas instituciones de educación superior, la mayoría de las cuales se orientan más hacia la ingeniería o la ciencia fundamental, en los últimos años Moscú ha visto un crecimiento significativo en el número de instituciones privadas o comerciales que ofrecen clases de negocios y administración de empresas. Las instituciones de Moscú, así como el resto de la Rusia post-soviética, han comenzado a ofrecer nuevos certificados internacionales y posgrados, entre ellos el "Master in Business Administration (MBA)". El intercambio de estudiantes con diferentes programas (sobre todo de Europa) también se ha multiplicado en las universidades de Moscú, mientras que en muchas escuelas de la capital de Rusia también se ofrecen seminarios, conferencias y cursos para empleados y empresarios. 

Moscú es conocida como uno de los más importantes centros científicos de Rusia. La sede de la Academia rusa de las Ciencias se encuentra en Moscú, así como numerosas investigaciones y ciencia aplicada. 

El Instituto Kurchátov, es el líder de investigación y desarrollo institucional en el ámbito de la energía nuclear en Rusia. Fue donde se construyó el primer reactor nuclear en Europa. El Instituto Landau de Física Teórica del Instituto de Teórica y Física Experimental, el Instituto Kapitsa de Problemas de Física y el Instituto Steklov de Matemáticas, están todos situados en Moscú. 

Existen 452 bibliotecas en la ciudad, entre ellas 168 para niños. La Biblioteca del Estado de Rusia fundada en 1862, es el hogar de más de 275 kilómetros de estanterías y cuarenta y dos millones de artículos, con inclusión de más de diecisiete millones de libros y volúmenes de serie, trece millones de revistas, 350.000 partituras musicales y grabaciones sonoras y 150.000 mapas, por lo que es la biblioteca más grande de Rusia y una de las más grandes del mundo. Los artículos en 247 idiomas diferentes componen aproximadamente el veinte y nueve por ciento de la colección.

La Biblioteca Histórica Pública, fundada en 1863, es la mayor biblioteca especializada en la historia rusa. Su colección contiene cuatro millones de artículos en 112 idiomas (incluidos 47 idiomas de la antigua Unión Soviética), en su mayoría en ruso. También trata la historia mundial, heráldica, numismática y la historia de la ciencia.

Moscú siempre ha estado ligada al deporte. Desde los tiempos de la URSS, donde dominaron varias modalidades deportivas, hasta ahora, la capital rusa cuenta con instalaciones del más alto nivel como el estadio Olímpico Luzhnikí (estadio elite por la UEFA) que han llevado a la ciudad a ser sede de los Juegos Olímpicos de 1980, a presentarse sin éxito finalmente a los de 2012 y albergar eventos internacionales tales como la final de la UEFA Champions League de 2008. La capital fue una de las sedes de la Copa Mundial de Fútbol de 2018.

En Moscú hay multitud de deportes que gozan de un gran éxito entre la población. El fútbol, evidentemente, es uno de ellos, a la vez que el baloncesto, el hockey sobre hielo o el ajedrez. En todas las modalidades, tanto clubes como deportistas individuales han ganado trofeos internacionales. También hay siete pistas de carreras de caballos, siendo el Hipódromo Central de Moscú, fundada en 1834, el más grande.

En el fútbol, la tradición es larga y la rivalidad es máxima, ya que los clubes moscovitas estaban todos ligados a un sector de la sociedad y política soviética (excepto el Spartak). El club más laureado es el Spartak de Moscú con 12 Ligas de la URSS y 9 ligas de la Rusia actual, aunque no cuenta con ningún título internacional. Disputa sus partidos en el Otkrytie Arena.

Le siguen el FC Dinamo Moscú con 11 ligas soviéticas por ninguna del formato ruso actual. Su mayor hito internacional fue el , ya que perdió por 3-2 ante el Glasgow Rangers en la final disputada en el Camp Nou. El Dinamo, que era el equipo del ministerio de interior soviético, disputa sus partidos locales en el viejo Estadio Dinamo. Otro de los clubes moscovitas es el PFC CSKA Moscú, equipo que tiene 7 campeonatos soviéticos, 3 rusos y una Copa de la UEFA, lograda en 2005 ante el Sporting de Portugal. El club, cuyo dueño es el multimillonario Román Abramóvich, disputa también sus partidos en el Estadio Dinamo a la espera de la construcción de uno nuevo propio. El CSKA era el equipo del ejército soviético. Por su parte, el Lokomotiv de Moscú era el equipo ligado a los trabajadores del ferrocarril y cuenta con dos ligas rusas. Juega como local en el moderno Estadio Lokomotiv. El último equipo moscovita de la Liga Premier es el FC Moscú, el más joven de ellos. Fue fundado en 1997 y no tiene ningún título oficial en sus vitrinas. Disputa sus partidos como local en el Estadio Torpedo. Por último, cierra la lista de equipos moscovitas el histórico Torpedo de Moscú, que actualmente vive sumido en una crisis deportiva que le mantiene en la Segunda división rusa. El Torpedo representa a los trabajadores del sector del automóvil, cuenta con 3 ligas soviéticas y juega como local también en Luzhniki.

Otro rasgo característico de algunos clubes moscovitas es que son equipos multideportivos. Los dos más importantes son el CSKA Moscú y el Dinamo de Moscú. El primero tiene secciones de fútbol, baloncesto y hockey sobre hielo, mientras que el Dinamo cuenta con fútbol, baloncesto, vóleibol, hockey sobre hielo y bandy.

Moscú acoge el Palacio de gimnasia rítmica Irina Viner-Usmanova, situado en el Complejo Olímpico Luzhniki. La construcción de la instalación deportiva empezó en 2017, mientras la inauguración se celebró el 18 de junio 2019. El inversor del proyecto es el multimillionario Alisher Usmanov, marido de la ex gimnasta y entrenedora de gimnasia rítmica Irina Viner-Usmanova. La superficie total del edificio es de 23500m², que comprenden 3 gimnasios, vestuarios, salas para los árbitros y los entrenedores, saunas, una cantina y una cafetería, 2 salónes de baile, un Centro medico, un espacio dedicado a los periodistas, un espacio museístico y un albergue para los atletas. 

En baloncesto, los dos grandes son el PBC CSKA Moscú y el MBC Dinamo Moscú. El CSKA es, sin duda, el equipo más laureado del básquet ruso y uno de los más importantes de Europa, ya que posee en sus vitrinas 6 Euroligas, 25 ligas de la URSS y 14 ligas rusas. El Dinamo, por su parte, cuenta con una Copa ULEB y dos ligas soviéticas.

Mientras tanto, en hockey sobre hielo destacan nuevamente los dos equipos anteriores. La sección de hockey del CSKA es el HC CSKA Moscú y es el equipo más importante. Otros clubes moscovitas son el HC Dinamo Moscú, el Krylia Sovétov Moscú y el HC Spartak Moscú. El estadio más importante de hockey sobre hielo es el moderno Jodynka Arena, que fue sede del Campeonato Mundial de Hockey sobre Hielo Masculino de 2007. Sin embargo, es un estadio multiusos ya que alberga combates de boxeo y partidos de la Euroliga de baloncesto.

Otro de los deportes más populares de Moscú, y en general de toda Rusia, es el tenis. La ciudad siempre ha dado grandes talentos de este deporte, tanto en su versión masculina como femenina. Destacados tenistas moscovitas son, entre otros, Marat Safin, María Kirilenko, Anna Kúrnikova, Anna Chakvetadze, Ígor Andréiev, Mijaíl Yuzhny, Yelena Deméntieva o Vera Zvonariova. Uno de los torneos de la ATP y la WTA es el Torneo de Moscú, también conocido como "Copa Kremlin", que se disputa en el Estadio Olimpiski.

Se está terminando, además, la reconstrucción de otros estadios, como Dinamo, Streltsov. En agosto de 2014 fue puesto en explotación el estadio Otkritie Arena, en agosto de 2016 – Arena CSKA , y en 2017, Luzhnikí. Junto a estos estadios, acogerá los juegos de la  Primera Liga  otro estadio moscovita, Lokomotiv. En 2018 Luzhnikí y Otkrítie Arena acogerán los juegos del Campeonato Mundial de Fútbol 2018. Luzhnikí albergará, además, los partidos de la apertura y de la final del torneo.

Dos estadios de la ciudad albergaron los partidos del Campeonato Mundial de Fútbol: Luzhnikí y Otkritie Arena.

En la capital se jugaron 12 partidos del Campeonato. 11 regiones municipales ya están preparando bases de entrenamiento para los deportistas.

Luzhnikí es el estadio más grande de Rusia. Su capacidad llega a 81 000 de espectadores. Tiene 102 palcos para los espectadores VIP, 2000 asientos para los periodistas y 300 asientos para las personas con discapacidad. En la reconstrucción del estadio se invirtieron 24.000 millones de rublos. En 2017 la arena fue reconocida como el mejor estadio, según el portal prestigioso stadiumdb.com. En la comisión determinante del vencedor entraban cinco arquitectos que evaluaban las obras por tres criterios claves: valor arquitectónico, funcionalidad e innovación.

Otkritie Arena es capaz de acoger a 45.360 espectadores. El estadio es la sede del club deportivo «Spartak» (Moscú). Conforme a las exigencias de la UEFA y la FIFA durante los juegos oficiales en los torneos internacionales entre los clubes y selecciones el estadio no puede llevar el «nombre de patrocinador» y va a llamarse de un modo neutral «Estadio Spartak».

El estadio Otkrítie Arena es el centro deportivo del proyecto urbanístico de habilitación del antiguo aeropuerto en la región de Túshino. Aquí se encuentran viviendas, comercios,oficinas y edificios públicos, centros de salud y cultura física, canchas de tenis, palacios de deportes acuáticos y de hielo, guarderás y escuelas, centros de salud y puntos de trabajo creativo.

En un principio se planeó habilitar las zonas adyacentes a la Universidad Estatal de Moscú (MGU) para la organización del Festival de Hinchas de la FIFA. Esta iniciativa fue criticada duramente por numerosos estudiantes y profesores de la universidad motivándolo con que no podrían prepararse para los exámenes durante el Mundial. Al final, las partes llegaron a un compromiso: el recinto del festival estará cerca de la MGU pero sin llegar a ocupar el territorio del campus, y el escenario principal del evento se hallará a la distancia de 310 metros del edificio central de la MGU. También se decidió reducir la capacidad del recinto de festival de 40 a 25 mil personas. Las demás instalaciones para el Mundial-2018 previstos en el territorio de la universidad tampoco estarían allí.

El Campeonato Mundial 2018 representará un estímulo importante para el desarrollo del movimiento deportivo de la ciudad. En 11 campos de entrenamiento, que aparecieron en Moscú con motivo de la celebración del Mundial, ya están entrenándose más de 10 mil deportistas jóvenes. «Cerca de cada uno de estos campos se construyeron escuelas deportivas juveniles. El Campeonato Mundial pasará, pero su legado queda, ya hoy en día en estos campos pueden entrenarse más de 10 mil niños».

En los días del Campeonato Mundial para el desplazamiento de los espectadores por Moscú serán utilizados unos shuttle-bus gratuitos. 


Hay tres principales aeropuertos comerciales al servicio de Moscú: el Aeropuerto Internacional de Moscú-Sheremétievo, el Aeropuerto Internacional de Moscú-Domodédovo y el Aeropuerto Internacional de Moscú-Vnúkovo. El Aeropuerto de Moscú-Sheremétievo es uno de los más importantes de la ciudad y el segundo que mayor volumen de pasajeros tiene de Moscú, con más de 12 millones de usuarios al año, y controla el 70% de todos los vuelos internacionales. El Aeropuerto Internacional de Moscú-Domodédovo es el principal aeropuerto de Rusia en términos de rendimiento de pasajeros, más de 16 millones al año, y es la principal puerta de entrada a los vuelos de larga distancia nacionales y de la Comunidad de Estados Independientes. Se encuentra a unos 34 kilómetros al sur de Moscú y consta de dos terminales. Es uno de los aeropuertos más modernos del país y la previsión del propio aeropuerto para los próximos años es muy positiva. La EAST LINE Group, compañía que gestiona el aeropuerto, reveló que en 2010 recibirán a 22 millones pasajeros y en 2015 a cerca de 30 millones, el doble volumen que en la actualidad.

Los otros aeropuertos en particular ofrecen vuelos dentro de Rusia y hacia y desde los estados de la antigua Unión Soviética. Los aeropuertos de Moscú varían en distancia del cinturón MKAD: Domodédovo está a 22 kilómetros, Vnúkovo a 11 kilómetros, Sheremétievo a 10 kilómetros, y Ostáfievo, el más cercano, a unos 8 kilómetros.

También hay varios aeropuertos más pequeños cerca de Moscú, como el aeropuerto de Myachkovo, destinados a los aviones privados, helicópteros y chárteres.

Moscú cuenta con nueve estaciones de tren, que se encuentran cerca del centro de la ciudad, pero cada una de ellas maneja los trenes de diferentes partes de Europa y Asia. Las estaciones ferroviarias de Moscú son: Belorussky, Kazansky, Kíyevsky, Kursky, Leningradsky, Paveletsky, Rizhsky, Savyólovsky y Yaroslavsky. También hay muchas pequeñas estaciones de ferrocarril. Como los trenes son relativamente baratos, son el modo preferido de los moscovitas para desplazarse, sobre todo cuando salen a San Petersburgo. Moscú es también la terminal occidental del Transiberiano, que atraviesa casi 9.300 kilómetros de territorio ruso hasta Vladivostok, en la costa del Pacífico. Por otra parte, existe una terminal de autobuses de larga distancia y otras de autobuses interurbanos de pasajeros con un volumen de usuarios diario de 25 mil pasajeros.

Moscú tiene dos terminales de barcos de pasajeros sobre el río, y rutas regulares de buques y cruceros a lo largo del Moscova y el Oká, que se utilizan principalmente para entretenimiento. La terminal norte del río, construida en 1937, es también el principal centro de larga distancia en las rutas fluviales. La ciudad posee tres puertos de carga.

El transporte local incluye el metro de Moscú, famoso por sus obras de arte, murales, mosaicos, lámparas y decoración en general. Cuando se inauguró en 1935, el sistema tenía una sola línea pero hoy cuenta con doce, en su mayoría subterráneas con un total de 203 estaciones. Es uno de los sistemas de metro más profundos del mundo, por ejemplo, la estación Park Pobedy, terminada en 2003, se encuentra a 84 metros bajo tierra y tiene las escaleras mecánicas más largas de Europa. El metro de Moscú es uno de los más activos del mundo, transportando más de 7 millones de pasajeros al día. Hay también una línea de monorraíl, operado por la misma empresa. Existen planes para expandir el metro, para mitigar los grandes problemas de transporte que sufre Moscú.

El metro posee un gran desarrollo geográfico con algunas estaciones ubicadas fuera del centro de la ciudad a distancias de hasta cuatro kilómetros, desde las estaciones de metro una extensa red de autobuses transporta los pasajeros a las zonas residenciales. Cada gran avenida de la ciudad es atendida por lo menos por una ruta de autobús. También hay gran cantidad de redes de tranvías y trolebuses. Hay un servicío de monorriel en la parte oriental de la ciudad.

La página web "Rutas de Moscú" facilita la búsqueda de medios y rutas de transporte a través de la ciudad. Por esta circulan a diario más de 2,6 millones de automóviles. Los últimos años se ha observado un marcado crecimiento del número de vehículos, lo que causa numerosos atascos de tráfico.

El futuro Cuarto Anillo, es sólo una de las tres autopistas que se construyen dentro de los límites de la ciudad de Moscú. Estos complementarán a otros sistemas de carretera existentes que forman círculos concéntricos en torno a la ciudad.

El ayuntamiento de Moscú ofrece diferentes servicios de comparticíon de vehículos por medio de empresas privadas. Hay el servicio de comparticíon de coches, llamado "Karshering", con la posibilidad de buscar sur la aplicación del servicio un coche para conducirlo en la ciudad, estacionandolo al final del viaje. En el año 2018 el alcalde de Moscú dijo que el servicio de comparticíon de coches ciudadano llegó a ser el "car sharing" con más vehículos disponibles en Europa. Hay también la posibilidad de alquilar una bicicleta con el "Velobike", el servicio de comparticíon de bicicletas tradicionales y eléctricas, con más de 3000 bici y 380 puntos de alquiler. Para alquiler una bici, hay que pagar con tarjeta de crédito o con Troika Card, la tarjeta polivalente de los transportes de Moscú. Existe un servicio de comparticíon de los patinetes eléctricos. Hay servicios privados de comparticíon de vehículos cerca de los grandes parques ciudadanos.

El 26 de noviembre de 2018, el alcalde de la capital rusa Sobianin inauguró oficialmente el complejo de Telecabina de Moscú , operativo sobre el río Moscova. La infraestructura conecta el Complejo Olímpico Luzhnikí a la Colina de los Gorriones y a la calle Kosyguin.

La ruta tiene una duración de 5 minutos, mientras utilizando el coche el tiempo empleado para terminar el recorrido es de 20 minutos, así que la telecabina es más conveniente.

El horario de funcionamiento del servicio de transporte público empieza a las 11.00 y termina a las 23.00. El complejo tiene 3 estaciones: "Vorobyovy Gory" (Colina de los Gorriones), "Nóvaya Liga" y "Luzhnikí".

El cable tiene una longitud de 720 metros y es capaz de transportar un máximo de 1.600 pasajeros cada hora en cualquier condición meteorológica. Las 35 cabinas están equipadas con pantallas audiovisuales, iluminación LED, ganchos para las bicicletas, esquís y tablas de snowboard así como audioguías disponibles en ruso, inglés, alemán y chino.

En 2016, se introdujo el sistema de navegación único de los transportes de Moscú, que consiste en un diseño de señalización único, que ayuda a los pasajeros y peatones a navegar por la ciudad. El sistema fue diseñado para ser implementado en el Metro, en el transporte terrestre, en las áreas peatonales, en la red Velobike y en los espacios de tránsito. Cada elemento del sistema tiene un papel específico en la orientación y construcción de la ruta. Algunos elementos de navegación incluyen un mapa que indica al usuario en el espacio y los objetivos que se pueden alcanzar a pie en 5 minutos. Los materiales utilizados para la realización de los elementos de señalización permiten minimizar el vandalismo y las diversas piezas se pueden reemplazar fácilmente.

Los nuevos letreros se colocan donde los pasajeros y peatones necesitan encontrar su camino. El proyecto requiere un análisis de modelos peatonales, consideración del contexto y características específicas de cada lugar y una lista de preguntas para las cuales el usuario puede obtener una respuesta en ese lugar específico. El Departamento de Transporte y Desarrollo de Infraestructura Vial de Moscú desarrolló este sistema uniforme de señalización de navegación en un equipo que incluye diseñadores gráficos, diseñadores industriales, cartógrafos, analistas, editores y gerentes. Expertos rusos y mundiales trabajan constantemente en el proyecto. 

La criminalidad en la capital rusa ha tenido un largo historial de la mano de la mafia. Sin embargo, en el los principios de los años 2000 otros sectores de la población han sustituido a la propia mafia (pandillas juveniles) en cuanto a crímenes perpetrados en Moscú.

Un informe de "Mercer Human Resource Consulting" en 2003 reveló que Moscú era la ciudad más peligrosa de Europa. Los datos de los sucesos en Moscú durante el año 2002 revelaron que se registraron 163.418 delitos, un 29,7% más que en 2001, un 20,4% más en cuanto al número de delitos relacionados con el tráfico de drogas y las agresiones a personas, incluidos los asesinatos, aumentaron en un 7,3% con respecto al año anterior.

El aumento de los crímenes de jóvenes menores de edad coincidía con el alto número de niños que vivían en las calles. Se calcula que en torno a 2.000 niños vivían en las calles de la capital y otros 5.000 estaban alojados en instituciones gubernamentales. En total, unos 300.000 vagabundos y "sin techo" deambulaban por Moscú y sólo un 10% era natural de la ciudad. El resto provenía de otros lugares de Rusia o de antiguas repúblicas soviéticas.

The Economist ha publicado el índice Safe Cities (Ciudades Seguras) de este 2019, un 'ranking' elaborado cada dos años por la prestigiosa revista para evaluar la seguridad en 60 urbes de todo el mundo. El índice se deriva del promedio entre las notas asignadas a cada ciudad conforme a cuatro rubros: seguridad digital, seguridad de infraestructura, seguridad de salud y seguridad personal. Con 65,8 puntos, Moscú se ha colocado en el puesto 37 de entre las ciudades evaluadas, con lo que sube cuatro puestos respecto a la versión 2017 del 'ranking'. Más de 200,000 cámaras con sistema de reconocimiento facial están activas en el transporte, en las calles y edificios de la ciudad. 

Los servicios de ambulancia de la ciudad han sido considerados los segundos mejores del mundo, según los analistas de PricewaterhouseCoopers (PwC), se dice en la web de la empresa. De hecho, en todas las categorías, Moscú se sitúa justo detrás de Berlín y por delante de Nueva York. Incluso aparece en primer lugar en algunos criterios, como el tiempo medio de respuesta de los operadores (4 segundos), el intervalo entre la llamada y la salida de la ambulancia (2 minutos y 30 segundos) y el tiempo de espera antes de la llegada de la ayuda (14 minutos 34 segundos, pero 9 minutos 24 segundos en caso de emergencia).

Como en todo el país, los números de emergencia son 101 para el servicio de bomberos, 102 para la policía, 103 para emergencias de salud, 104 para el servicio de emergencia de gas y 112 como número único de emergencia.

Moscú colabora activamente en el plan de hermanamiento de ciudades. Para ello está asociada con las siguientes urbes:




</doc>
<doc id="29660" url="https://es.wikipedia.org/wiki?curid=29660" title="Idioma okinawense">
Idioma okinawense

El okinawense o uchināguchi (en japonés: "kanji" 沖縄語, "kana" おきなわご y "rōmaji" "Okinawa-go"; en okinawense 沖縄口, うちなーぐち y "Uchinaaguchi", respectivamente) es un idioma que se habla en Okinawa y en las islas: Kerama, Kumejima, Tonaki, Aguni y otras islas cercanas, en la parte más meridional de Japón. Es el único idioma ryukyuense que ha podido prosperar con la modernización de la isla; se estima que lo hablan 900 mil personas. Dentro de Japón, a este idioma se le llama 沖縄方言(Okinawa hōgen, o en español, 'dialecto de Okinawa').

El idioma okinawense pertenece a la familia ryukyuense, la cual forma con el japonés y sus dialectos la familia japónica.

El idioma okinawense usa una mezcla entre kanji y hiragana. Se cree que el hiragana se introdujo desde Japón hasta las islas Okinawa. El kanji se introdujo poco a poco debido a la gran influencia de Japón y las similitudes entre los idiomas okinawense y japonés.

Sigue la fórmula del idioma japonés Sujeto + Objeto + Verbo y hace uso de partículas como el japonés. Los dialectos de este idioma tienen un sinfín de características del japonés clásico —tales como la distinción entre la forma terminal y la atributiva—. La función genitiva de が "ga" (desusada en el dialecto "Shuri"), el uso nominativo de ぬ "nu" (en japonés sería la partícula の "no").

Algunas partículas

Primero se dirá en okinawense y luego su equivalente en japonés.



Cópula

Primero se dirá en okinawense y luego su equivalente en japonés.


Todas estas formas tienen forma en pasado.

Preguntas

Primero se dirá en okinawense, luego en japonés y por último en español.








El okinawense se habla en Okinawa, en las islas circundantes: Kerama, Kume-jima, Tonaki, Aguni y en numerosas islas más pequeñas que se encuentran al este de la isla principal de Okinawa.

Se divide en tres dialectos principales: el norteño (Kunigami), el central o estándar (Shuri-Naha) y el sureño. El dialecto shuri fue estandarizado en la era del reino ryukyuense durante el reinado de Sho Shin (1477-1526). Fue el idioma oficial, empleado por la realeza y la aristocracia. La totalidad de las canciones y los poemas escritos en este idioma en dicha era empleaban el dialecto Shuri.

Hubo importantes cambios en los sonidos de dicha lengua con el tiempo. Respecto del japonés estándar, el dialecto shuri presenta las siguientes diferencias, que pueden ser vocálicas (caso de los cambios "e" -> "i", y también "o" -> "u") o silábicas:

Los valores del silabario japonés son distintos en el idioma okiwanense. La siguiente lista da una muestra de los cambios en la pronunciación de sílabas; el primer carácter es la escritura hiragana, seguido de la pronunciación en okiwanense y luego la pronunciación Japonesa:

De los anteriores ejemplos se desprende que algunas sílabas han variado consistentemente, como el cambio de A por E, y de E por I.




</doc>
<doc id="29664" url="https://es.wikipedia.org/wiki?curid=29664" title="The Truman Show">
The Truman Show

The Truman Show "(en España, El show de Truman (una vida en directo); en Hispanoamérica, The Truman Show: Historia de una vida") es una película estadounidense de 1998 de comedia dramática y ciencia ficción dirigida por Peter Weir, escrita por Andrew Niccol y producida por Scott Rudin, Niccol, Edward S. Feldman y Adam Schroeder. El filme está protagonizado por Jim Carrey en el papel de Truman Burbank, adoptado y criado por una corporación dentro de un show televisivo de realidad simulada que se centra en su vida, hasta que lo descubre y decide escapar; el resto del reparto está compuesto por Laura Linney, Noah Emmerich, Natascha McElhone y Ed Harris.

"The Truman Show" se originó a partir de un guion de Niccol inspirado en un episodio de "The Twilight Zone" titulado «Special Service». A diferencia de la película terminada, se trataba de un "thriller" de ciencia ficción ambientado en la ciudad de Nueva York. Scott Rudin compró el guion para producirlo con Paramount Pictures como distribuidora. La cinta iba a ser dirigida por Brian De Palma pero finalmente Weir fue contratado como director, realizando el filme con sesenta millones de dólares, veinte millones menos de lo que se estimó en un principio. Niccol reescribió el guion mientras que la producción esperaba que Carrey se uniera al proyecto. La mayor parte de la filmación se llevó a cabo en Seaside, una comunidad planificada ubicada en el mango de Florida.

El filme fue un éxito en la taquilla, recibió elogios por parte de la crítica cinematográfica y fue nominado a los premios , , y Saturn. "The Truman Show" ha sido estudiada como tesis en cristianismo, metafilosofía, realidad simulada, existencialismo y telerrealidad.

Gira en torno al programa de telerrealidad «The Truman Show». Su protagonista, Truman Burbank (Jim Carrey), está frente a las cámaras aún antes de nacer, aunque no era consciente de este hecho. La vida de Truman es filmada a través de miles de cámaras ocultas, las 24 horas del día y es transmitida en vivo a todo el mundo, permitiendo al productor ejecutivo Christof captar la emoción real de Truman y el comportamiento humano cuando se pone en determinadas situaciones. La ciudad natal de Truman, Seahaven, es un decorado construido bajo una cúpula tan grande que incluye un sol, firmamento y mar artificiales siendo por su tamaño visible desde el espacio; este set está poblado por actores de la serie y del equipo, lo que permite a Christof controlar cada aspecto de la vida de Truman, incluso el clima o la duración de los días. 

Para evitar que Truman descubra su falsa realidad, Christof ha utilizado todos los medios para anular su sentido de exploración y deseo de salir de Seahaven (incluyendo fingir la muerte de su padre en un viaje de pesca para infundir en él miedo al agua). Sin embargo, pese al control de Christof, Truman ha sabido comportarse de maneras inesperadas, especialmente enamorándose en su adolescencia de un extra, Sylvia, en vez de Meryl, la actriz destinada a ser su novia. Aunque Sylvia es retirada de la serie rápidamente, explicándole a Truman que su familia debía mudarse a Fiji, él todavía la recuerda. Además, Sylvia ha iniciado el «"Free Truman"», una campaña que lucha para que Truman sea liberado de la serie. Tras esto Truman, como había sido planificado por Christof, inicia una relación con Meryl y finalmente acabó siendo su esposa, a pesar de lo cual él jamás ha olvidado a Sylvia, teniendo como costumbre comprar revistas femeninas para recortar de las fotografías los rasgos que le recuerdan a la muchacha.

Durante el trigésimo año de emisión del programa Truman descubre hechos que parecen fuera de lugar, como un foco usado para simular el firmamento nocturno que casi lo golpea al desprenderse (tras lo que rápidamente anuncian por la radio local que fue una luz de aterrizaje que se había desprendido de un avión), una conversación acerca de un «Truman Show» por parte del equipo de filmación en su radio del coche, que describe su viaje matutino al trabajo, el hecho que descubre un patrón repetitivo en el modo en que la gente (los extras) se mueven por la calle y la perturbadora coincidencia que hace que cada vez que algo despierta las sospechas de Truman algún medio de comunicación comunica al instante una noticia que explica «racionalmente» el fenómeno. 

Estos acontecimientos hacen que Truman comience a cuestionar su vida, dándose cuenta que gran parte de la ciudad parece girar en torno a él. Truman trata de escapar de Seahaven, pero se lo impiden una serie de convenientes acontecimientos como la falta de vuelos, averías del autobús, embotellamientos de tráfico y una aparente fuga en una central nuclear. Meryl no puede aguantar el estrés y renuncia al programa y Christof trae de vuelta al padre de Truman, esperando que su presencia hará que Truman deje de intentar escapar por lo que la versión para Truman es que sobrevivió al accidente y estando amnésico vagó por décadas hasta recordar quién era y tras recuperar la memoria regresó a Seaheaven. 

Aunque por un tiempo la vida de Truman parece haber vuelto a la normalidad tras reencontrarse con su padre y conociendo a una nueva colega de trabajo que Christof ha introducido para que sea su nueva novia. Sin embargo, sólo resulta ser un alivio temporal: Truman pronto se aísla y empieza a quedarse solo en su sótano. Una noche, Truman consigue escapar del sótano sin ser detectado a través de un túnel secreto, lo que obliga a Christof a suspender la emisión del show por primera vez en la historia. Esto provoca un aumento de la audiencia, con muchos espectadores, incluyendo a Sylvia, que desean que Truman logre fugarse.

Por orden de Christof, cada actor y miembro del reparto inicia una búsqueda en toda la ciudad; al no tener resultados por falta de luz, decide adelantar el «día» activando el sol artificial de la ciudad. Meditando un poco, deciden buscar en el mar y descubren que Truman trata de huir en barco y restauran la emisión, pero Christof decide provocar una gran tormenta para tratar de volcar la embarcación y explotar la fobia de Truman, sin embargo este se inspira en un retrato de Sylvia que ha hecho con sus recortes de revistas y se ata al barco para evitar ceder ante el miedo. La determinación de Truman lleva a Christof a poner fin a la tormenta; paralelamente el público en su totalidad se pone de parte de Truman y reprueba los actos con que Christof intenta obligarlo a quedarse. Mientras Truman se recupera, el barco llega al borde de la cúpula, chocando la proa con el cielo pintado en la pared. Truman, aterrorizado, por un momento cree que no existe una salida, pero descubre la existencia de un tramo de escaleras cercano, el cual conduce a una puerta en la que existe un rótulo «Salida».

Al ver que escapará de su mundo, Christof le habla directamente a través de un sistema de sonido de gran alcance, tratando de convencerlo de quedarse, demostrando en su actitud que se siente parte padre de Truman y parte Dios; argumentando que no hay más verdad en el mundo real que la que existe en su propio mundo artificial y que debe quedarse allí ya que está libre de los peligros del mundo y su deber es alegrar a la gente que lo sintoniza; impaciente por el silencio de Truman, Christof se exalta y le exige que hable ya que está en vivo para todo el mundo. Truman, después de pensarlo un momento, dice su famosa frase: «"Y por si no nos vemos, ¡buenos días, buenas tardes y buenas noches!"» y haciendo una reverencia frente a la cámara atraviesa la puerta hacia el mundo real. Los telespectadores celebran la fuga de Truman, mientras Sylvia rápidamente sale de su apartamento para reunirse con él. 

El equipo ejecutivo ordena el corte de la emisión. Como el espectáculo ha terminado, se muestra a la audiencia del show buscando otro programa que ver.


Andrew Niccol completó un tratamiento de una página titulado "The Malcolm Show" en mayo de 1991. El borrador original tenía el tono de un "thriller" de ciencia ficción y la trama estaba ambientada en la ciudad de Nueva York. Niccol declaró: «Pienso que todo el mundo cuestiona la autenticidad de su vida en cierto punto. Es cómo cuando los niños preguntan si fueron adoptados». Durante el otoño de 1993, el productor Scott Rudin compró el guion por un poco más de un millón de dólares. Simultáneamente, Paramount Pictures estuvo de acuerdo con distribuir el filme. Parte del trato indicaba que el proyecto iba a ser dirigido por Niccol —su debut como director—, pero Paramount sintió que la suma de ochenta millones de dólares aproximados del presupuesto iba a ser demasiado alta para él. Paramount quería contar con un director reconocido y le pagó a Niccol dinero extra para que «diera un paso al costado». Brian De Palma estuvo en negociaciones para dirigir antes de que abandonara la United Talent Agency en marzo de 1994. Algunos de los directores que fueron tenidos en cuenta después de De Palma fueron Tim Burton, Terry Gilliam y Barry Sonnenfeld, antes de que Peter Weir fuese contratado a principios de 1995, después de ser recomendado por Niccol. Bryan Singer quería dirigir pero Paramount se decidió por el más experimentado Weir.

Weir quería que el filme fuese más gracioso, sentía que el guion de Niccol era demasiado oscuro, declaró: «Donde él [Niccol] lo hacía deprimente, yo lo puedo hacer ligero. Podría convencer al público de que puede ver un "show" de estas dimensiones todo el tiempo». Niccol escribió dieciséis borradores del guion antes de que Weir lo consideró pronto para filmarse. Más tarde en 1995, Jim Carrey firmó como protagonista, pero debido a compromisos con "The Cable Guy" y "Liar Liar", no iba a estar disponible para empezar a filmar hasta por lo menos otro año. Weir consideró que Carrey era perfecto para el papel y prefirió esperar un año en vez de seleccionar a otro actor. Niccol reescribió el guion doce veces, mientras que Weir creó un libro ficticio sobre la historia del "show". Imaginó historias de fondo para los personajes y animó a los actores a que hiciera lo mismo.

Weir recorrió locaciones en el este de Florida pero los paisajes no lo convencieron. Se reservaron escenarios en Universal Studios para la historia ambientada en Seahaven antes de que la esposa de Weir le mostrara a éste Seaside, una «comunidad planificada» ubicada en el mango de Florida. Inmediatamente se instalaron oficinas de preproducción en Seaside, donde se llevó a cabo la mayor parte del rodaje. Otras escenas se filmaron en Paramount Studios en Los Angeles, California. Se utilizaron pinturas de Norman Rockwell y postales de los años 1960 para inspirar el diseño de la película. Weir, Peter Biziou y Dennis Gassner estudiaron técnicas de vigilancia para crear ciertos planos.

La estética general estuvo influenciada por imágenes televisivas, particularmente comerciales: muchos planos presentan personajes acercándose a la cámara con los ojos totalmente abiertos, y las escenas interiores están muy iluminadas, porque Weir quería recordarle al público que «en este mundo, todo está a la venta». A quienes estuvieron encargados del trabajo de efectos visuales la película les resultó difícil de hacer, porque 1997 fue el año en que muchas compañía de efectos visuales estaban tratando de pasarse a la imagen generada por computadora (CGI). Se usó CGI para crear las partes superiores de algunos edificios grandes del set del centro de la ciudad. Craig Barron, uno de los supervisores de efectos, dijo que esos modelos digitales no tenían que verse tan detallados y envejecidos como normalmente se verían en una película debido al aspecto artificial de todo el pueblo, aunque sí imitaron ligeras manchas presentes en las edificaciones reales.

El "soundtrack" fue compuesto por Burkhard Dallwitz, quien fue contratado después de que Peter Weir recibió una cinta de su trabajo mientras se encontraba en Australia durante la posproducción. Algunas partes del "soundtrack" fueron compuestas por Philip Glass, incluyendo composiciones presentes en trabajos previos ("Powaqqatsi", "Anima Mundi" y "Mishima"). Glass y Dallwitz ganaron el . El sitio AllMusic calificó el álbum de banda sonora con cuatro estrellas sobre cinco y lo llamó «uno de los "soundtracks" más intrigantes de 1998».

El filme también incluye «Romance-Larghetto» de Frédéric Chopin de su primer concierto para piano interpretada por Arthur Rubinstein, «Rondo Alla Turca» de Wolfgang Amadeus Mozart de su sonata para piano n.º 11 interpretada por Wilhelm Kempff, «Father Kolbe's Preaching» de Wojciech Kilar interpretada por la Orquesta Filarmónica Nacional de Polonia y «20th Century Boy» interpretada por la banda "rockabilly" The Big Six.

El escritor Benson Y. Parkinson de la Association for Mormon Letters notó que Christof representaba a Jesús como un «"off-Christ"» («Christ-"off"») o Anticristo, comparando su personaje de megalómano productor de Hollywood con Lucifer. La conversación entre Truman y Marlon en el puente puede ser comparada con la de Moisés y Dios en el "Éxodo".

En "C.S. Lewis and Narnia for Dummies" de Rich Wagner, Christof es comparado con Escrutopo, el personaje de "Cartas del diablo a su sobrino" de C. S. Lewis.

En 2008, la revista "Popular Mechanics" nombró a "The Truman Show" una de las diez películas de ciencia ficción más proféticas. El periodista Erik Sofge argumentó que la trama refleja la falsedad de los "reality shows". «Truman simplemente vive, y la popularidad del "show" es su voyeurismo directo. Y, como "Big Brother", "Survivor" y cualquier otro "reality show" al aire, ninguno de sus ambientes es real». Consideró una coincidencia inquietante que "Big Brother" haya debutado un año después del estreno de la película, y también comparó el filme con el programa de 2003 "The Joe Schmo Show": «A diferencia de Truman, Matt Gould podía ver las cámaras, pero el resto de los participantes eran actores pagados, interpretando papeles de varios estereotipos de "reality-show". Mientras que Matt al final se llevó todos los premios en el concurso arreglado, la broma constante del "show" estaba en el mismo rango existencial que "The Truman Show"». Weir declaró: «Siempre ha existido esta pregunta: ¿El público se esta volviendo más tonto? ¿O los cineastas los estamos tratando con condescendencia? ¿Es eso lo que quieren? ¿O es eso lo que les estamos dando? Pero el público asistió a mi película en grandes números. Y eso tiene que ser alentador».

Ronald Bishop de "Journal of Communication Enquiry" sugirió que "The Truman Show" exhibió el poderío de los medio de comunicación de masas. La vida de Truman sirve de inspiración a telespectadores en todo el mundo, sus vidas son controladas por la de él. Bishop comentó: «Al final, el poder de los medios se afirma en vez de ser desafiado. Estas películas y programas de televisión se apropian de nuestro encanto (y desencanto) con los medios y nos lo venden de vuelta».

Un ensayo publicado por "The International Journal of Psychoanalysis" describió a Truman como: 

Se pueden trazar paralelos con el libro de Thomas More de 1516, "Utopía", en donde More describe una isla que tiene solamente una entrada y una salida. Solo aquellos que pertenecen a la isla saben cómo trasladarse sanos y salvos a través de las engañosas puertas. Esta situación es similar en "The Truman Show" porque hay entradas limitadas hacia el mundo que conoce Truman. Truman no pertenece a esa utopía a la cual ha sido implantado y un trauma de la niñez lo mantiene asustado ante la posibilidad de abandonar esa pequeña comunidad. Modelos utópicos del pasado tienden a incluir individuos parecidos entre sí y con mucho en común, comparables a "Utopía" de More y grupos de la vida real como los Shakers o la Comunidad de Oneida. Está claro que la gente en el mundo de Truman es similar entre sí con respecto a su esfuerzo en común para mantenerlo ajeno a la realidad. La apariencia suburbana «"picket fence"» del "set" del programa es remite al «sueño americano» de los años 1950. El concepto de «sueño americano» en el mundo de Truman es un intento de mantenerlo feliz e ignorante.

En un principio, el estreno en los cines estaba programado para el 8 de agosto de 1997, pero Paramount Pictures lo postergó para el 14 de noviembre del mismo año. La fecha se volvió a cambiar para principios de 1998 y más tarde para el verano del mismo año. NBC compró los derechos de radiodifusión en diciembre de 1997, alrededor de ocho meses antes del estreno del filme. En marzo de 2000, Turner Broadcasting System compró los derechos para transmitir la película a través de TBS.

"The Truman Show" recibió elogios por parte de la crítica cinematográfica. En Rotten Tomatoes, el filme tiene un puntaje positivo del 94 %, basado en 125 reseñas, con un puntaje promedio de 8,4 sobre 10. El consenso del sitio dice: «Un filme gracioso, sensible, que da para reflexionar, "The Truman Show" es digno de tener en cuenta por su notable visión profética de la cultura desmedida de la celebridad y de una nación con una insaciable sed por los detalles privados de vidas ordinarias». En Metacritic, el filme tiene un puntaje de 90 sobre 100, basado en 30 reseñas, siendo catalogada como una película «aclamada universalmente».

Calificando a la película con cuatro estrellas sobre cuatro, Roger Ebert la comparó con "Forrest Gump", afirmando que tenía un balance justo entre comedia y drama. También le impresionó la actuación dramática de Jim Carrey. Kenneth Turan de "Los Angeles Times" escribió: «"The Truman Show" es atractiva emocionalmente sin perder la habilidad de emitir agudas preguntas satíricas como también lograr numerosas risas. Extraño filme que es inquietante a pesar de funcionar a la perfección dentro de las normas estándar de la industria». La nombró la mejor película de 1998. En junio de 2010, "Entertainment Weekly" nombró a Truman uno de los cien mejores personajes de los últimos veinte años.

James Berardinelli comentó que la película no es «el típico "blockbuster" con efectos especiales de verano» y le gustó comparó la «carismática, sutil y efectiva» actuación de Carrey con las de Tom Hanks y James Stewart. Jonathan Rosenbaum del "Chicago Reader" escribió: «Innegablemente provocativa y razonablemente entretenida, "The Truman Show" es una de esas películas de trama sencilla cuyo concepto es tanto ingenioso como tonto». Tom Meek de "Film Threat" dijo que la cinta no era lo suficientemente graciosa pero que aún encontró «algo gratificante en su apariencia extravagante».

En la , "The Truman Show" estuvo nominada a tres estatuillas pero no logró llevarse ninguna. Peter Weir recibió una nominación a mejor director, mientras que Ed Harris estuvo nominado como mejor actor de reparto y Andrew Niccol estuvo nominado en la categoría de mejor guion original. Muchos creían que Carrey iba a ser candidato a mejor actor y que la película sería candidata a mejor película, pero finalmente no sucedió. Asimismo, "The Truman Show" fue nominada a los en las categorías de mejor película dramática, mejor director y mejor guion. Jim Carrey y Ed Harris se llevaron los premios a mejor actor dramáticos y mejor actor de reparto, respectivamente, y Burkhard Dallwitz y Philip Glass consiguieron el premio a la mejor banda sonora.

En los , se premió a Weir (dirección), Niccol (guion original) y Dennis Gassner (diseño de producción). También estuvo nominada como mejor película y mejor efectos visuales. Harris fue candidato a mejor actor de reparto y Peter Biziou a mejor fotografía. "The Truman Show" fue un éxito en los Premios Saturn, donde ganó en las categorías de mejor película de fantasía y mejor guion. Carrey, Harris y Weir también estuvieron nominados. El filme ganó el . Al mismo tiempo, en 2006 el American Film Institute la nominó para ser incluida en su lista .

Joel Gold, un psiquiatra del Bellevue Hospital Center, reveló en 2008, que cinco de sus pacientes con esquizofrenia creían vivir dentro de un "show" de televisión. Gold llamó a este síndrome «Delirio de The Truman Show» y atribuyó el delirio a un mundo necesitado de publicidad.

Gold señaló que algunos pacientes eran felices con su síndrome mientras que «otros estaban atormentados». Uno de ellos viajó a Nueva York para ver si el World Trade Center había sido derribado, creyendo que los atentados del 11 de septiembre de 2001 tenían que ver con la trama de su "show". Otro escaló la Estatua de la Libertad, creyendo que como parte del show se reuniría allí con su antigua novia del instituto. En agosto de 2008, el "British Journal of Psychiatry" informó de casos similares en el Reino Unido. Este delirio también se ha llamado informalmente «Síndrome de Truman».

Después de saber sobre esta condición, Andrew Niccol, escritor de "The Truman Show", dijo: «Sabes que has triunfado cuando tienes una enfermedad que lleva tu nombre».





</doc>
<doc id="29666" url="https://es.wikipedia.org/wiki?curid=29666" title="Islas Vírgenes Británicas">
Islas Vírgenes Británicas

Las Islas Vírgenes Británicas son un territorio británico de ultramar localizado al este de Puerto Rico, en aguas del mar Caribe. Las islas forman parte del archipiélago de las Islas Vírgenes, siendo las otras islas parte de las Islas Vírgenes de los Estados Unidos y de las Islas Vírgenes Españolas pertenecientes a Puerto Rico.

El nombre oficial del Territorio es "Islas Vírgenes", y el término "Británicas" es usado generalmente para distinguirlas de otros territorios vecinos pertenecientes al archipiélago. Las publicaciones del gobierno del territorio siguen poniendo "El Territorio de las Islas Vírgenes" y en los pasaportes solo hace referencia a las "Islas Vírgenes".

El archipiélago está constituido por unas cuarenta islas, de las cuales once están habitadas. Las más grandes son Tórtola, Virgen Gorda, Anegada y Jost Van Dyke. La población del archipiélago es de 27 800 habitantes, viviendo 23 000 en la isla de Tórtola.

Las islas estuvieron habitadas inicialmente por indígenas arahuacos de Sudamérica alrededor del 100 a. C. Ellos fueron los únicos habitantes del archipiélago hasta el siglo XV, cuando tuvieron que huir de los caribes, la tribu más agresiva originaria de Sudamérica que se extendió por las Antillas Menores- y cuyo nombre dio origen al mar Caribe.

En 1493, las islas fueron descubiertas por Cristóbal Colón durante su segundo viaje. España colonizó las islas a principios del siglo XVI, utilizándolas principalmente para extraer el cobre de Virgen Gorda. Los holandeses establecieron un asentamiento permanente en la isla de Tórtola en 1648. En 1672, los ingleses llegaron a la región y se anexionaron el archipiélago, expulsando a la población holandesa de Tórtola ese mismo año, y la de Anegada y Virgen Gorda en 1680.

Los británicos introdujeron la caña de azúcar en las islas, que se convirtió en el principal cultivo, y fuente de ingresos para el comercio exterior. Los esclavos fueron traídos desde África para trabajar en estas plantaciones. Las islas prosperaron económicamente hasta el crecimiento de las cosechas de remolacha en Europa y Estados Unidos, cuando la demanda de caña de azúcar se vio considerablemente reducida.

Ingleses, neerlandeses, franceses, españoles y daneses toman el control del archipiélago durante los doscientos años siguientes; al final los británicos expulsan a los neerlandeses, ocupando definitivamente Virgen Gorda y Tórtola. 

Al final del siglo XVII Inglaterra poseía Tórtola, Virgen Gorda, Anegada y otras islas que conforman actualmente las Islas Vírgenes Británicas, España conservó Vieques y La Culebra, mientras que Dinamarca tenía el resto de las Vírgenes (Saint John, Saint Thomas y Saint Croix). Las II.VV.BB. eran algo más estratégicas para los británicos, pero fue plantado cuando las condiciones económicas eran en particular favorables.

En 1898 Estados Unidos se apodera de la Islas Vírgenes Españolas. Luego en 1917, Estados Unidos compró Saint John, Saint Thomas y Saint Croix a los daneses por diecisiete millones de dólares estadounidenses, renombrándolas por "Islas Vírgenes de los Estados Unidos" (""United States Virgin Islands""). Entonces, se comenzó a utilizar la denominación no oficial "Islas Vírgenes Británicas".

Las islas fueron administradas por diversos organismos como parte de la Colonia de las Islas de Sotavento y también dentro San Cristóbal y Nieves, con un administrador que representa el Gobierno Británico en el archipiélago. El estatus de colonia separada se consiguió en 1960 y se hicieron autónomas en 1967. Desde los años 60, las islas han diversificado la economía lejos de la agricultura tradicional hacia el turismo y servicios financieros, haciéndose una de las áreas más ricas del Caribe.

El poder ejecutivo de las Islas Vírgenes Británicas está compartido entre Isabel II del Reino Unido que es representada por un gobernador general designado directamente por ésta por consejo del gobierno británico y el Premier de las Islas Vírgenes Británicas que es elegido por la Cámara de la Asamblea. La defensa y los asuntos exteriores están bajo responsabilidad del Reino Unido.

El poder legislativo está depositado en la Cámara de la Asamblea que está compuesta por 15 miembros, 13 son elegidos directamente y dos son miembros ex officio (Fiscal General y el Presidente de la Cámara).

El actual gobernador es Augustus Jaspert (desde agosto de 2017) y el premier es Andrew Fahie (desde el 26 de febrero de 2019).

Es uno de los diecisiete territorios no autónomos bajo supervisión del Comité de Descolonización de las Naciones Unidas, con el fin de eliminar el colonialismo.

Las Islas Vírgenes Británicas comprenden aproximadamente cincuenta pequeñas islas caribeñas; alrededor de quince están habitadas. Localizadas a unos kilómetros al este de las Islas Vírgenes Estadounidenses, el océano Atlántico Norte queda al norte y el mar Caribe al sur.

Las islas más grandes del archipiélago son Tórtola, Virgen Gorda, Anegada y Jost Van Dyke. Road Town, la capital y ciudad más grande, está situada en la isla Tórtola.

La cumbre más alta es el Pico de Sage con 512 metros.

Las Islas Vírgenes Británicas tienen un clima tropical, moderado por los vientos alisios. Las temperaturas varían poco a lo largo del año. En la capital Road Town las temperaturas máximas diarias son de alrededor de 32° C en verano y 29° C en invierno. Las temperaturas mínimas diarias son de alrededor de 24° C en verano y 21° C en invierno. Las precipitaciones promedio son de 1.150 mm por año. Las precipitaciones pueden ser muy variables pero los meses más lluviosos en promedio son de septiembre a noviembre y los meses más secos en promedio son febrero y marzo. Los huracanes de vez en cuando golpean las islas. La temporada de huracanes va desde junio a noviembre.

Los bosques en la mayor parte de las islas se componen de árboles de caoba, higueras y helechos. Las islas de las Antillas Menores, son un paraíso de frutas tropicales con árboles de mango, papaya y coco que crecen en su suelo en abundancia, también contienen numerosos animales, incluyendo lagartos, monos y pequeñas lagartijas. En las islas de Tórtola y Virgen Gorda. Como cuestión de hecho, el nombre Tórtola se deriva de la 'tierra de tórtolas', el pájaro que simboliza las Islas Vírgenes Británicas. en las islas puede encontrarse palomas, garzas, garcetas, halcones y colibríes.La vida marina incluye una amplia variedad de peces que pululan alrededor de los arrecifes de coral y restos de naufragios. En el invierno, las ballenas jorobadas vienen a reproducirse en el canal de Drake.

Las plantas que solo están en Anegada son las acacia anegadensis y las "Metastelma anegadense" que están en peligro crítico debido al cambio climático. La "Pitcairnia jareckii" es una bromelia conocida solamente en la Isla de Guana, Y la "kiaerskov's lidflower" anteriormente también estaba en Puerto Rico, pero ahora solamente sobreviven en las islas Virgen Gorda y Tórtola.

El reptil más espectacular se encuentra en las Islas Vírgenes Británicas que están en peligro crítico, la iguana de anegada en la isla de Anegada dejando posiblemente menos de 200 iguanas en total que están al borde de la extinción.Tan solo 32 mm de longitud, el Gecko enano de las Islas Vírgenes es uno de los animales vertebrados más pequeño, que es único en la Islas Vírgenes Británicas: Virgen Gorda, Tórtola y la Isla Mosquito. Otros reptiles únicos en las islas incluyen "carrot rock" que se encuentra únicamente en el islote al sur de la Isla de Peter. La "anegada skink" que se encuentra únicamente en la isla de Anegada, y dos serpientes ciegas: T"yphlops naugus" y "Typhlops catapontus".

La única rana endémica es "Virgin Islands Coqui", antes vivía en las Islas Vírgenes de los Estados Unidos, pero ahora solo vive en las islas Jost Van Dyke, Tórtola, Virgen Gorda y la Isla Great Dog.

Los únicos invertebrados de las Islas Vírgenes Británicas son: el escarabajo de cuernos largos, la mariposa "Anegada calisto," la polilla "Perigea gloria", la avispa bracónida, la arañas goblin y la araña "Stenoonops tortola" que están en la islas Virgen Gorda y Tórtola; la "araña caponiida", un ciempiés en la isla de Tórtola, y varios camarones: "Pseudalpheopsis guana", "Alpheus zimmermani" y "Microprosthema jareckii "en la Isla de Guana.

La economía de las Islas Vírgenes británicas (IVB) es una de las más prósperas de entre los estados caribeños, con un PIB per cápita de alrededor de 38 500 dólares (2004 est. PBI B.V.I). Reposa sobre dos pilares principales: el registro de empresas extranjeras (servicios financieros offshore) y el turismo. 

Las IVB son uno de los principales paraísos fiscales del planeta. Se estima que están registradas en ellas unas 800.000 sociedades offshore, es decir, empresas que realizan su actividad en otros países pero se domicilian en las IVB para aprovechar su legislación más ventajosa. Las empresas offshore no pagan ningún impuesto en las IVB, salvo una pequeña tasa anual por registrarse. Se estima que un 41% de las sociedades offshore del planeta están domiciliadas en las IVB (informe de KPMG publicado en 2000). La trayectoria de las IVB como paraíso fiscal comenzó en 1984. En abril de 2013 sufrió un varapalo cuando una fuente anónima filtró 2,5 millones de documentos relativos a empresas y cuentas de las IVB al International Consortium of Investigative Journalists. A consecuencia de la fuga de información secreta, el registro de nuevas sociedades bajó un 23% en el último trimestre de 2013. 

El otro sector económico importante es el turismo, que se estima aporta el 45 % de la renta nacional. Las islas son un destino popular para ciudadanos estadounidenses, con alrededor de 350 000 turistas anuales (cifras de 1997) que acuden a las numerosas playas de arena blancas, visitan los Baños de la Virgen Gorda, hacen submarinismo en los arrecifes de coral que hay cerca de Anegada, experimentan las barras conocidas sobre el Dique de Furgoneta Jost, o fletan yates para explorar las islas menos accesibles.

La economía está estrechamente unida con la de las Islas Vírgenes estadounidenses, más grandes, que hay al oeste. La moneda de las IVB es, desde 1959, el dólar estadounidense.

Por último, también tuvo importancia históricamente la emisión de sellos postales para coleccionistas.

Hay 113 kilómetros de carreteras. El principal aeropuerto, el Aeropuerto Internacional Terrance B. Lettsome, también conocido como el Aeropuerto de Beef Island, está situado en Beef Island, que se encuentra en el extremo oriental de Tórtola y es accesible por el puente de la Reina Isabel II. Cape Air y Air Sunshine están entre las aerolíneas que ofrecen servicio regular. Virgen Gorda y Anegada tienen sus propios aeropuertos más pequeños. Los servicios privados de flete aéreo operados por Island Birds Air Charter vuelan directamente a las tres islas desde cualquier aeropuerto importante del Caribe. Se utilizan helicópteros para llegar a las islas que carecen de pistas de aterrizaje; Antilles Helicopter Services es el único servicio de helicópteros con base en el país.

El puerto principal está en Road Town. También hay transbordadores que operan dentro de las Islas Vírgenes Británicas y a las vecinas Islas Vírgenes de los Estados Unidos. Al igual que en el Reino Unido y en las Islas Vírgenes de los Estados Unidos, los automóviles de las Islas Vírgenes Británicas se conducen por la izquierda; sin embargo, casi todos los automóviles tienen el volante a la izquierda, ya que son importados de los Estados Unidos. Las carreteras suelen ser bastante empinadas, estrechas y sinuosas, y los surcos pueden ser un problema cuando llueve.

La población de las islas es de alrededor de 21 730 habitantes en 2003. La mayoría de la población (el 83 %) es negra, descendiente de los esclavos traídos a las islas por los británicos. Otros grupos étnicos mayoritarios son los de origen británico y europeo (datos de 2003).

Según el censo del año 1999 la población está estructurada de la siguiente manera:

Según estadísticas del año 2003 la población asciende a unas 21.730 personas, las cuales en su mayoría son de raza negra (83%).

Como en la mayoría de los países del Caribe, el cristianismo es la religión dominante en las Islas Vírgenes, más del 90% de la población que indicó una afiliación religiosa en el censo de 2010 era cristiano y las mayores denominaciones cristianas individuales eran la metodista (18,1%), anglicana (9,8%) (la Comunión Anglicana (Iglesia de la provincia de las Indias Occidentales), Iglesia de Dios (10,6%) y la Iglesia católica (9,1%). Los hindúes y los musulmanes constituyen cada uno aproximadamente el 1,2% de la población según la Base de Datos sobre Religión Mundial de 2005.

En un reflejo de territorio danés de la herencia colonial, el protestantismo esta muy extendido. La fuerte presencia católica se explica a la gran población hispana, así como la influencia irlandesa durante la época de la colonia danesa.

El idioma principal es el inglés británico, aunque hay un dialecto local. El español es hablado por puertorriqueños, dominicanos y otros inmigrantes hispanos. El criollo de las Islas Vírgenes, o inglés criollo de las Islas Vírgenes, es un criollo basado en el inglés que consta de varias variedades que se hablan en las Islas Vírgenes y en las cercanas islas SSS de Saba, San Martín y San Eustaquio, donde se conoce como inglés de Saba, inglés de San Martín e inglés de San Eustaquio, respectivamente.

En las Islas Vírgenes Británicas opera un sistema escolar idéntico al que se utiliza en el resto del Reino Unido, donde las escuelas públicas y privadas coexisten. Además existe una universidad comunitaria, H. Lavity Stoutt Community College, que se localiza en la punta este de Tórtola. Esta universidad fue nombrada así en honor al H. Lavity Stoutt (ministro principal). El índice de alfabetización de las Islas Vírgenes Británicas es alto, ya que más del 98% de la población mayor a los diez años sabe leer y escribir.

La música tradicional de las Islas Vírgenes Británicas se llama quimbombó, y es tocada por unos instrumentos llamados hongos. El sonido especial de los hongos se debe a una fusión única entre la música local, africana y europea. Funciona como un medio de la historia y el folclore local y por tanto es una forma de expresión cultural, muy querida por sus habitantes, que forma parte del plan de estudios en las escuelas de las Islas Vírgenes Británicas. Las bandas de hongos, también llamadas "bandas de rasca y gana", que emplean instrumentos que van desde el uso de calabaza, tabla de lavar, bongos y el ukelele, a los instrumentos occidentales más tradicionales como el teclado, banjo, guitarra, bajo, triángulo y el saxofón. Además de ser una forma de música de baile festivo, los hongos a menudo contiene humorísticos comentarios sociales, así como la historia oral de las Islas Vírgenes Británicas. El popular cantante Iyaz es de las Islas Vírgenes Británicas. En el vídeo musical para su canción de "Replay", tenía la bandera de las Islas Vírgenes Británicas al fondo.

Debido a su clima y localización, las Islas Vírgenes Británicas han sido un destino constante para los practicantes de la vela. De hecho, los atletas del archipiélago suelen tener buenos resultados en competiciones internacionales. Esto se debe a que las aguas calmas y la brisa constante proveen uno de los mejores escenarios en el Caribe para la vela. Muchos campeonatos de vela se celebran cada año en las aguas del país, siendo el Spring Regatta el más importante de todos ellos. Como parte del legado británico, deportes como el fútbol y el críquet son ampliamente practicados. De hecho, en el archipiélago se organiza un campeonato de fútbol anualmente.



</doc>
<doc id="29671" url="https://es.wikipedia.org/wiki?curid=29671" title="Praga">
Praga

Praga ( ) es la capital de la República Checa y de la región de Bohemia. Situada a orillas del río Moldava, tiene aproximadamente 1,2 millones de habitantes, lo que la convierte en la y la . El área metropolitana de Praga cuenta con una población de 1,9 millones de habitantes. Su belleza y patrimonio histórico la convierten en una de las veinte ciudades más visitadas del mundo.

Se ha desarrollado desde el , convirtiéndose en una de las capitales más importantes de Europa en los siglos y . Antigua capital del Reino de Bohemia y de Checoslovaquia, en el sufrió las dos guerras mundiales y la dictadura nazi. Tras la segunda guerra, quedó dentro de la esfera de influencia soviética. Tras la Revolución de terciopelo y la caída del Muro de Berlín la ciudad se ha ido adaptando a la economía de mercado. Praga, cuyo casco histórico fue declarado Patrimonio de la Humanidad en 1992, es considerada una ciudad global de «clase Beta+», a la altura de Roma, Atenas o Berlín.

Según las leyendas de la princesa Libuše vio muchas profecías de su castillo Libušín, que se encuentra en el centro de Bohemia. En una profecía, se le dijo, ella preveía la gloria de Praga. Un día ella tuvo una visión: 

Otros historiadores afirman que el nombre de la capital tiene su origen en la palabra eslava "Prga", que significa «harina tostada», debido a la aridez del lugar elegido para construir el Castillo de Praga.

Por último algunos opinan que el origen es la palabra checa "Prahy", que significa «rápidos», por los rápidos del río Moldava, a cuyas orillas se asienta la ciudad.

Los primeros vestigios en el lugar que hoy ocupa Praga datan del Paleolítico. El primer asentamiento estable se considera que fue el de la tribu celta, hacia el a. C. se estableció al sur de la actual Praga. La población se denominaba Závist. Posteriormente esta tribu fue reemplazada por el pueblo germánico y más adelante por los eslavos, que permanecieron desde el , aunque durante un período fueron dominados por los ávaros.

Fue fundada en la última parte del con la construcción del castillo en la ribera derecha del río Moldava. Según la leyenda por Libuše, quien se casó posteriormente con Přemysl e inició la dinastía Přemyslida. Este castillo es conocido como Vyšehrad y no es el actual Castillo de Praga, que se erigió en el lado opuesto del río.

Praga muy pronto se convirtió en el asentamiento de los reyes de Bohemia, algunos de los cuales reinaron como emperadores del Sacro Imperio Romano. En el el rey Otakar II funda el barrio de Malá Strana (lado pequeño), que se asienta en el lado opuesto del río.

La ciudad floreció durante el bajo el reinado de Carlos IV, quien ordenó la construcción de la Nueva ciudad, unió los núcleos urbanos en ambas márgenes del río a través del famoso Puente Carlos (que sustituye a un puente anterior del que se derrumbó en 1342) y propició la construcción de la primera Universidad de Europa central. En el , debido a agitaciones políticas y religiosas entre Jan Hus y el rey Segismundo se desarrollaron las guerras husitas.

Bohemia entró a formar parte de los dominios de los Habsburgo en 1526, por lo que Praga fue capital de una provincia austríaca. A principios del la elección de Fernando II, católico, como rey de Bohemia causó ira entre los nobles bohemios, de confesión protestante. Cuando Fernando II envió dos concejales católicos para preparar su llegada a Praga, estos fueron secuestrados y arrojados por una ventana del castillo, lo que se conoce como la Defenestración de Praga. Estos hechos desembocaron en la guerra de los Treinta Años, cuya consecuencia principal fue la soberanía de las provincias alemanas, a pesar de permanecer bajo el Imperio Germánico.

En el y primera mitad del , Praga gozó de un gran crecimiento económico, que atrajo a mercaderes y nobles de toda Europa. La ciudad se desarrolló rápidamente y se construyeron iglesias y palacios, muchos según el nuevo estilo del Barroco.

Durante la dominación austrohúngara, en el se convirtió en el centro del nacionalismo checo y su actividad cultural e intelectual fue brillante, construyéndose el Museo Nacional, el Teatro Estatal y el Rudolfinum.

En 1918, como consecuencia de la Primera Guerra Mundial, se fundó Checoslovaquia, y el nuevo presidente de la república Tomáš Masaryk hizo de Praga la sede de su gobierno y capital del Estado checo. Entre 1939 y 1945 el ejército de Hitler ocupó Praga. La ciudad, hasta el momento de carácter multiétnico, asistió a la persecución nazi del pueblo judío. Muchos fueron capturados y enviados a campos de concentración, donde la mayoría fueron exterminados. En 1945 el ejército estadounidense bombardeó la ciudad al confundirla con Dresde, causando la furia de los checos. Pocos días más tarde el ejército soviético liberó la ciudad y los ciudadanos checos se tomaron la venganza por su mano, causando la muerte de ciudadanos de origen alemán.

Tras la Segunda Guerra Mundial, Checoslovaquia pasó a formar parte del bloque comunista, bajo la protección de la Unión Soviética. En 1968 estalló la Primavera de Praga, un movimiento que pretendía reformar el inmovilista socialismo soviético, que fue duramente reprimida con la invasión de los ejércitos del Pacto de Varsovia.En diciembre de 1988, el primer ministro soviético Mijaíl Gorbachov anunció la llamada Doctrina Sinatra, la cual establecía que la Doctrina Brezhnev sería abandonada y que los países de la Europa del Este podrían hacer lo que consideraran conveniente. A finales de 1989, con la caída del muro de Berlín, Praga abandonó el socialismo. Praga fue el centro de la Revolución de Terciopelo que propició la caída del comunismo en el país. La vigencia de la doctrina Sinatra contribuyó a acelerar los cambios que terminaron por hacer colapsar a los gobiernos socialistas prosoviéticos de la Europa del Este a finales de 1989 e inicios de 1990. Los ahora nuevos gobiernos prooccidentales de la Europa oriental fueron fervientes partidarios de la pronta disolución del Pacto de Varsovia y de la Comecon. La disolución oficial del Pacto de Varsovia terminó siendo aceptada de forma pacífica por la Unión Soviética y se formalizó en la reunión de Praga el 1 de julio de 1991.

Dos años después, en 1993, se decidió pacíficamente la disolución de Checoslovaquia y su división en la República Checa y Eslovaquia. Praga pasó a ser la capital de la República Checa, tal y como recoge la Constitución del país. En septiembre de 2000 las protestas de Praga contra la globalización durante la cumbre del Fondo Monetario Internacional y el Banco Mundial derivaron en una guerrilla urbana contra la policía. Más de manifestantes participaron en una de las mayores protestas de la historia contra la globalización y el capitalismo. En agosto de 2002 el río Moldava se desbordó con un caudal superior a los 5100 m³/s causando graves daños en la ciudad, que necesitó dos años para recuperarse.

Praga se encuentra en el corazón de Europa, a menos de 500 kilómetros de los mares Báltico, del Norte y Adriático. Sus coordenadas son 50 grados norte de latitud y 14 grados este de longitud. Pertenece a la Bohemia Central y ocupa principalmente la región de "Poberounská soustava" y en su extremo noreste una pequeña parte del área de "Česká tabule".

Dentro de la República Checa, la ciudad de Praga está ligeramente desplazada al noroeste del centro geográfico del país. La erosión y los procesos de sedimentación en ambos márgenes del río Moldava son los principales causantes del relieve de la ciudad. Alrededor del río y sus afluentes hay unas pendientes relativamente escarpadas que terminan en forma de planicie al alcanzar cierta altura. Las pendientes pronunciadas se producen en la ribera occidental del Moldava. La altitud máxima de la ciudad es de 399 metros, en el barrio de "Zličín", al oeste de la ciudad, y la altitud mínima es de 177 metros al borde septentrional de la ciudad, donde el río la abandona. Por tanto la diferencia máxima de altitud es de 222 metros en un área relativamente pequeña.

El río Moldava ("Vltava" en checo) atraviesa Praga, dividiendo la ciudad de sur a norte. El Moldava es un río muy caudaloso, incluso aún más que el río Elba ("Labe" en checo), a pesar de que es afluente de este último.

El clima de Praga es continental. La temperatura media anual es de 8-9 grados centígrados. El invierno es riguroso aunque no excesivamente severo y relativamente seco, siendo la temperatura media en los meses de invierno aproximadamente de cero grados.

La primavera es fresca al principio y se vuelve templada y húmeda según se acerca el verano, alcanzando los 16-17 grados en junio de media y sobrepasando los 70 mm de precipitación. El verano no es excesivamente cálido, sobrepasando ligeramente los 18 grados de media en los meses de julio y agosto, aunque es habitual superar los 25 grados. Las precipitaciones alcanzan su punto máximo en julio y empiezan a descender en el mes de agosto.

El otoño de Praga es templado y moderadamente seco, con precipitaciones similares a las del invierno y la primavera, aunque el mes de noviembre es bastante frío, con menos de tres grados centígrados de media.
A inicios del siguió el crecimiento demográfico, alcanzando los 850 000 habitantes en el año 1930. Uno de los motivos fue la independencia de la República Checa y la designación de Praga como capital del nuevo Estado, así como por la absorción de los municipios colindantes a la capital en 1922 (37 municipios que sumaban más de habitantes).
La población de Praga es de aproximadamente habitantes, lo que supone algo más del 10% de la población del país. La ciudad tuvo una explosión demográfica durante el debido a su florecimiento económico y cultural. Si incluimos los suburbios que por entonces no formaban parte de Praga, la población pasó de poco más de habitantes en 1850 a más de 500 000 en el año 1900.

La primera ampliación de la ciudad ocurrió en 1850 cuando Praga absorbió el municipio de Josefov. En 1883 y 1884 se integraron dos nuevos municipios y otro en 1901.

Se realizaron nuevas ampliaciones 1960 (3 municipios), 1968 (25 municipios que sumaban cerca de ciudadanos), 1974 (37 municipios y cerca de habitantes), y en 1987 y 1988 llegaron las dos últimas ampliaciones (Černý Most y Kamýk), que sumaron otros habitantes. Desde entonces creció de forma moderada hasta los años 1980, y a partir de dicho momento la población se ha estabilizado.

La población praguense está envejeciendo a un ritmo superior al del resto del país, debido a la inmigración de ciudadanos en edad de trabajar y el descenso del índice de natalidad. De este modo, el porcentaje de habitantes en edad de jubilación no ha crecido en los últimos años.

La prosperidad económica a inicios del con la apertura a la Unión Europea, la instalación de nuevas multinacionales y los salarios más elevados han atraído la población productiva del resto del país. También ha provocado un aumento de la inmigración, aunque el porcentaje de extranjeros es aún moderado. Según datos del año 2006 el 8.77% de la población de Praga era extranjera. Analizando el país de origen, destacan los ucranianos, que representan el 34.3% de los inmigrantes, los eslovacos, que representan el 15.7% y los rusos, que son el 9.5% de los extranjeros.

La Constitución checa, aprobada por 172 votos de los 198 totales por el Parlamento checo, el 16 de diciembre de 1992, establece que la capital del Estado checo es Praga. La ciudad fue previamente capital del reino de Bohemia en la Edad Media y desde 1918 hasta la creación del nuevo Estado checo fue la capital de Checoslovaquia.

Como capital del Estado, Praga es la residencia del presidente de la República, así como la sede de las dos cámaras del Parlamento: la Cámara de diputados y El Senado.

De acuerdo con el Acta del Consejo Nacional Checo de la Ciudad Capital de Praga del año 2000 la ciudad forma una ciudad-estado. Esto implica que varias de las competencias del Estado son transferidas a la ciudad.

El máximo dirigente de la capital es el alcalde. La ciudad está administrada por tres entidades: La Asamblea de la Ciudad de Praga, formada por 70 representantes, el Consejo de la Ciudad de Praga, compuesto por 11 miembros de la Asamblea, y el Ayuntamiento de Praga. Las elecciones se realizan cada 4 años. El Ayuntamiento de Praga, con sus correspondientes departamentos y secciones, alberga el poder ejecutivo de la ciudad.

El alcalde actual de Praga es Tomáš Hudeček, del partido TOP 09. Fue elegido por primera vez en 2013.

Desde 2001 Praga se divide administrativamente en 22 distritos, que están numerados del 1 al 22. En los rótulos donde aparece el nombre de las calles se puede ver el número de distrito al que pertenece la vía. A su vez estos 22 distritos se dividen en 57 municipalidades, de las que tan solo 4 pertenecen al Praga histórico (Staré Město, Nové Město, Malá Strana y Hradčany). Cada una de ellas tiene sus propios representantes electos y competencias:
La moneda oficial del país es la corona checa (Česká Koruna), que se divide en 100 hellers (haléř). Sus abreviaturas son Kč. y h, respectivamente.

El PIB per cápita de Praga es más del doble que el de la República Checa. Alcanzó los 38 400 dólares (PPA) en 2006, lo que supone más de un 50% por encima de la media de la Unión Europea para ese año. Esto la sitúa como la undécima región más rica de la Unión. Praga es el centro económico del país. Además de ser la capital del país, la mayoría de las instituciones financieras y de las empresas transnacionales tienen sede en la ciudad. Estos, entre otros motivos, hacen posible que Praga represente cerca del 25% del PIB checo. Sin embargo, el crecimiento del PIB en Praga es menor que el de la media nacional desde 2001.
Praga ha sido capaz de absorber la fuerza de trabajo tanto nacional como internacional atraída por la prosperidad de la ciudad. En el año 2001 Praga concentraba aproximadamente el 20% de los puestos vacantes de trabajo de la República Checa, lo cual es especialmente significativo si se compara con el poco más del 10% que representa la población de Praga sobre la población total. La demanda de trabajadores era de personas mientras la población activa de la ciudad era de tan solo . La tase de desempleo en Praga es aproximadamente la mitad que la del país. Su posición central en Europa, además de un nivel de precios más bajo que el de la Europa Occidental, contribuyen a que muchas compañías internacionales la elijan como su sede europea.

La distribución sectorial de la ciudad sigue una marcada tendencia hacia el sector terciario. En 2006 el 82% del valor añadido creado en la ciudad se debía al sector servicios. En el otro lado de la balanza está el sector industrial, cuyo peso es significativamente inferior al del resto del país. Otro sector que ha crecido en la primera década del es la construcción, con más de un 50% de incremento entre 2001 y 2006 en término de valor de las casas construidas.

Desde la Revolución de Terciopelo, la ciudad saca provecho de su belleza arquitectónica en el sector del turismo. En el año 2005, 6.4 millones de turistas visitaron la República Checa, de los que una gran mayoría pasaron por Praga. Más del 90% de las plazas hoteleras fueron ocupadas por turistas extranjeros. No obstante, solo el 41% de las plazas disponibles se hallaba en hoteles de cuatro o cinco estrellas en 2006.

Praga concentra las principales universidades del país. En 2006 contaba con diez centros universitarios y treinta y ocho facultades, en los que estaban inscritos 87 500 estudiantes, además de 11 500 cursando estudios de posgrado. También contaba con una gran cantidad de escuelas de educación secundaria, entre las que destacan las de gramática y de estudios técnicos. Entre las universidades destaca la prestigiosa Universidad Carolina, la más antigua de Centroeuropa.

Las estadísticas señalan el buen nivel de la educación praguense. En 2001 el 56% de la población ha completado los estudios secundarios, lo que sitúa a Praga entre las primeras clasificadas según este indicador. Respecto a la población que tiene estudios universitarios, el porcentaje baja hasta el 12%, por lo que Praga desciende varios puestos en la clasificación, pero se sitúa todavía por delante de la mayoría de las ciudades europeas. El 39% de la población menor de cuatro años asiste a algún centro preescolar.

Tradicionalmente Praga ha sido uno de los centros culturales más importantes de la Europa Central. Tras la caída del comunismo, recuperó su tradición y festeja centenares de eventos como festivales de cine, música o literatura. La actividad teatral y operística de la ciudad presenta una enorme oferta durante todo el año. El Teatro Negro, el Teatro Nacional de Praga y el Teatro del ballet Nacional son algunos de los más famosos atractivos culturales.

El Museo Nacional es uno de los principales museos de Praga. Ocupa un edificio neorrenacentista situado en lo alto de la plaza de Wenceslao, la principal de la ciudad. Su vestíbulo central sirve también de panteón de los grandes checos. Se fundó en 1818 como Museo Patriótico de Bohemia. En 1848 toma el nombre de Museo Checo, y de 1854 a 1919 el de Museo Real Checo. El edificio ocupado por el museo es obra del Josef Schulz, el arquitecto del Teatro Nacional de Praga, construido en la misma época (1885-1890).

El Museo Judío es en realidad un conjunto de distintos edificios dentro del antiguo gueto de Josefov. Fue fundado en 1906 por Hugo Lieben y Augustin Stein para la conservación de valiosos objetos de las sinagogas de Praga. Durante la ocupación nazi y el régimen comunista el museo estaba muy limitado en sus funciones. Desde 1994 recuperó y amplió su estatus original. Posee una de las mayores colecciones de arte judío en el mundo, que comprende unas piezas de museo y libros. El museo incluye las sinagogas Maisel, Española, Pinkas, Klaus, el Antiguo Cementerio Judío, la Sala de Ceremonias de la Sociedad Funeraria y el cementerio judío de Žižkov.

Otros museos de menor relevancia son el Museo Alfons Mucha que recoge más de 100 obras u objetos relacionados con este autor checo, el Museo Franz Kafka que alberga las primeras ediciones de sus libros, así como correspondencia personal y audiovisuales del escritor, el Museo de las Máquinas Sexuales o el Museo del Juguete, que se encuentra en el Castillo de Praga, entre otros.

Praga tiene una larga historia musical. La que fuese una de las capitales culturales de Europa en los siglos y conserva su tradición musical.

En la ciudad se organizan, durante todo el año, conciertos de música clásica. Los entornos son variados, desde antiguas iglesias a hermosos auditorios como la Sala Smetana, sede de la Orquesta Sinfónica de Praga situado, en un edificio art-nouveau en Republiky 5, el Palacio de la Cultura y la sala Dvorak (en el Rudolfinum, Plaza Jan Palach), sede de la Orquesta Filarmónica Checa ubicado en un edificio neoclásico en donde se celebra la inauguración y clausura, sobre todo, del famoso Festival Primavera de Praga entre el 12 de mayo y los primeros días de junio. Los conciertos también se celebran regularmente en la Galería Nacional del Castillo de Praga, en los jardines, al pie del Castillo y en el Museo Nacional, en la plaza de Wenceslao. Los conciertos en la Villa Bertramka en Mozartova 169, Smichov, suelen tener a Mozart y sus contemporáneos como protagonistas.

Las antiguas iglesias son el entorno en donde se puede disfrutar con el Festival de Música de Órgano que tiene lugar en agosto. Los mejores programas son los que se ofrecen en la Catedral de San Vito, en Hradcany; U Krízovníku, cerca del Puente de Carlos, la Iglesia de San Nicolás de la Malá Stupartská en la Ciudad Antigua, donde las notas del órgano fluyen en medio de hermosas estatuas barrocas. La Ópera Nacional tiene como sede el Teatro Nacional de Praga y el pequeño Teatro de los Estados. La antigua Ópera Alemana, actualmente Ópera Estatal de Praga, organiza una temporada independiente de la de la Ópera Nacional.

Un Festival de Música Judía se celebra en los meses de octubre y noviembre en el Barrio Judío.

La tradición musical checa se inicia en el de nuestra era con canciones de origen religioso. Durante la Edad Media se componen varias canciones que han perdurado en el cancionero checo, pero que es difícil concretar de donde surgieron. En el el reformista Jan Hus impulsa de modo notable la canción religiosa en la capilla de Belén de Praga, y según los textos contemporáneos es compositor de varias piezas de la época.

En el se instala el famoso órgano de la Catedral de San Vito. El emperador Fernando I funda en esta época la orquesta de la corte, que sus sucesores ampliarían posteriormente. De esta orquesta, formada por músicos de diversas nacionalidades, surgen varias canciones populares checas.<ref>


</doc>
<doc id="29674" url="https://es.wikipedia.org/wiki?curid=29674" title="OTAN">
OTAN

La Organización del Tratado del Atlántico Norte (en inglés: "North Atlantic Treaty Organization", NATO; en francés: "Organisation du Traité de l'Atlantique Nord", OTAN), también denominada la Alianza Atlántica, es una alianza militar intergubernamental que se rige por el Tratado del Atlántico Norte o Tratado de Washington, firmado el . La organización constituye un sistema de defensa colectiva, en el cual los Estados miembros acuerdan defender a cualquiera de sus miembros si son atacados por una potencia externa.

La sede de la OTAN se encuentra en Bruselas (Bélgica) y sus 30 Estados miembros abarcan Estados Unidos, Canadá y Europa. La incorporación más reciente es la de Macedonia del Norte, en marzo de 2020. Además, veintiún Estados no miembros colaboran con la OTAN dentro del programa Asociación para la Paz, con otros quince involucrados en programas de diálogo y nueve como socios globales. En 2017, el gasto militar combinado de los 29 países fue el 52 % del .

En sus primeros años, la OTAN no era mucho más que una asociación política. Sin embargo, la guerra de Corea hizo que se planteara una coalición permanente. Entonces se creó una estructura militar bajo la dirección de los comandantes de Estados Unidos. La Guerra Fría llevó a las naciones rivales a crear el Pacto de Varsovia en 1955.

Siempre se han manifestado dudas sobre la alianza europeo-norteamericana ante una invasión soviética, desacuerdos que se plasmaron con la creación por parte de Francia de la fuerza de choque nuclear y con su retirada de la estructura militar de la alianza entre 1966 y 2009.

Después de la caída del Muro de Berlín en 1989, la organización intervino dentro de la guerra de Yugoslavia, lo que se convirtió en la primera intervención conjunta de la OTAN. En lo político la organización ha mejorado sus relaciones con los antiguos miembros del bloque del Este, dando como resultado la incorporación a la OTAN de varios miembros del Pacto de Varsovia.

La única ocasión en que un país miembro invocó el artículo 5 del tratado reivindicando la ayuda en su defensa, fue Estados Unidos en 2001. Desde entonces, los miembros colaboraron con los Estados Unidos en la guerra de Afganistán. El artículo 4 del tratado prevé llamar a consulta a los miembros y ha sido convocado cuatro veces, tres de ellas por Turquía, la primera por la guerra de Irak y las dos restantes por ataques recibidos durante la guerra civil siria, la cuarta ha sido invocada por Polonia durante la crisis de Crimea de 2014, debido a la movilización de tropas rusas en la frontera polaca con Kaliningrado y las maniobras rusas en el mar Báltico.

En 2019, la OTAN celebró su 70º aniversario con una cumbre en Londres, a comienzos de diciembre, en donde firmó un documento conjunto que resaltaba sus logros y que recogía por primera vez la influencia de China, con los retos que ello supone para la organización.

En 1949, en plena Posguerra de la Segunda Guerra Mundial, en Occidente se veía con preocupación la política expansionista que estaba siguiendo la Unión Soviética. Era evidente que la Organización de las Naciones Unidas no podría ser capaz por sí sola de mantener la estabilidad en el mundo, ya que las propuestas de Estados Unidos en el Consejo de Seguridad eran rechazadas por los numerosos vetos soviéticos. La aparición de gobiernos comunistas en Europa Central y Oriental por influencia soviética aumentaban la presión en Europa Occidental. Entre 1947 y 1949, una serie de sucesos, más dramáticos por el hecho de la reciente marcha de las tropas estadounidenses y canadienses que aún se encontraban en Europa desde el fin de la Segunda Guerra Mundial, marcaron el punto más alto en la tensión que se estaba experimentando. Estos sucesos fueron amenazas a la soberanía de Noruega, Grecia, Turquía y Checoslovaquia, entre otros, donde el golpe de Praga de febrero de 1948, fue interpretado como un ataque directo a los intereses europeos. Además, el bloqueo de Berlín, entre junio de 1948 y mayo de 1949, empeoró la situación para los países antes mencionados.

La necesidad de una asociación de países cada vez era más manifiesta, de forma que en marzo de 1948, Francia, Bélgica, Países Bajos, Luxemburgo y el Reino Unido firmaron el Tratado de Bruselas, con el que creaban una alianza militar, la Unión Europea Occidental.

Ante la creciente expansión socialista, se decidió crear una alianza defensiva más amplia que la Unión Europea Occidental, por lo que se llevaron a cabo negociaciones entre Estados Unidos, Canadá y los países de la Unión Europea Occidental, a las que se decidió invitar a Dinamarca, Islandia, Italia, Noruega y Portugal. Las negociaciones giraron en torno a la creación de una alianza militar que tuviese una base en el artículo 51 de la Carta de las Naciones Unidas, y tuvieron como resultado la firma del Tratado de Washington, el 4 de abril de 1949, por la que se establecían las bases de la creación de la Organización del Tratado del Atlántico Norte.

Una de las dificultades surgidas durante las negociaciones estuvo relacionada con la integración de Estados Unidos en la organización. Los países europeos, devastados después de la guerra, estaban interesados en aliarse con Estados Unidos para así asegurarse una defensa eficaz, pero en Estados Unidos no se compartía este deseo. Sin embargo, el golpe de Praga y el bloqueo de Berlín hicieron aumentar la reivindicación por parte de los europeos, especialmente de Francia, de la creación de una alianza militar con Estados Unidos. En secreto, en Reino Unido se firmó un acuerdo, llamado "Pentagon Paper", con el que se establecía un esbozo de como debía ser una alianza en el Atlántico Norte.

El último elemento a tener en cuenta en el proceso de integración de Estados Unidos pasó por la necesidad de sortear la dificultad que suponía la prohibición por parte de la Constitución de los Estados Unidos de aliarse militarmente en tiempos de paz. El senador Vandenberg promovió la votación en el Senado de los Estados Unidos de la Resolución 239, que el 11 de junio de 1948 dio luz verde a la unión de Estados Unidos a la Alianza Atlántica. Bajo petición del Senado de Estados Unidos, se hizo constar en el tratado de constitución de la alianza (artículo 5) que las medidas a tomar en caso de agresión a algún país miembro fuesen resultado de la libre elección de cada país. El Senado quería mantener así el poder de decisión del Congreso de los Estados Unidos en materia militar.

En teoría, estaba destinada a ser una garantía de seguridad de los estados de Europa Occidental ante la Unión Soviética y sus aliados. Como le era propio a la coyuntura de la Guerra Fría, las fuerzas de la OTAN actuaron como fuerza disuasoria.

Después de la constitución de la OTAN, nuevos países fueron sumándose. En 1952, se unieron los dos primeros: el Reino de Grecia y Turquía.

En 1954, la Unión Soviética propuso su unión a la OTAN, con el objetivo de mantener la paz en Europa, pero los aliados rechazaron la propuesta. Esto, junto con la incorporación de Alemania Occidental a la organización el 9 de mayo de 1955 —descrita como «un momento decisorio en la historia de nuestro continente» por el ministro de Asuntos Exteriores de Noruega, Halvard Lange— tuvo como consecuencia inmediata la creación del Pacto de Varsovia, firmado el 14 de mayo de 1955 por la Unión Soviética y sus aliados. Este pacto se considera la respuesta formal a la OTAN, poniendo de manifiesto los dos bandos opuestos de la Guerra Fría.

La unidad de la OTAN fue puesta en evidencia ya desde sus principios. En 1958, el presidente francés Charles de Gaulle protestó por el papel hegemónico que tenían los Estados Unidos en la Organización y por lo que, a entendimiento del presidente, era una relación especial entre Estados Unidos y Reino Unido. En un memorándum enviado al presidente estadounidense Eisenhower y al primer ministro británico Macmillan el 17 de septiembre de 1958, De Gaulle argumentaba a favor de la creación de una dirección tripartida, que pusiese a Francia en igualdad de condiciones con Estados Unidos y Reino Unido, abogando también por la expansión de la OTAN en las áreas geográficas de interés para Francia, como Argelia, donde Francia intentaba eliminar a las fuerzas insurgentes y necesitaba la ayuda de la OTAN.

De Gaulle consideró las respuestas dadas como insatisfactorias, así que decidió construir una defensa independiente para su país. El 11 de marzo de 1959, retiró su flota en el Mediterráneo del comando de la OTAN; tres meses después, en junio de 1959, De Gaulle prohibió la entrada de armas nucleares en territorio francés. Esto provocó que Estados Unidos transfiriera doscientos aviones a Francia y devolviera el control, entre 1950 y 1967, de las diez mayores bases aéreas que habían operado en Francia. La última base devuelta fue la de Toul-Rosières, base de la 26ª Ala de Reconocimiento, trasladada a la base aérea de Ramstein, en Alemania Occidental.
Mientras tanto, Francia había iniciado su programa nuclear, "Force de frappe". Probó su primer arma nuclear, Gerboise Bleue, el 13 de febrero de 1960 en el desierto del Sahara.

Aunque Francia mostró solidaridad respecto al resto de la OTAN durante la crisis de los misiles en Cuba en 1962, De Gaulle continuó su propósito de constituir una defensa independiente retirando del comando la flota francesa del Atlántico y del canal de la Mancha. En 1966, las Fuerzas armadas francesas fueron retiradas del comando integrado de la OTAN, y se ordenó que todas las tropas no francesas abandonasen el territorio galo. Todo ello provocó que el 16 de octubre de 1967 se trasladase el Cuartel General Supremo de las Potencias Aliadas en Europa (SHAPE) de Rocquencourt, cerca de París, a Casteau, al norte de Mons, en Bélgica. Francia continuó en la alianza y con su ayuda a la defensa de Europa ante un posible ataque soviético con sus tropas estacionadas en Alemania Occidental.

El 30 de mayo de 1982, España firmó el Tratado de Washington, convirtiéndose en el miembro número dieciséis de la Alianza Atlántica. Sin embargo, tras las elecciones generales de 1982 se suspendió la integración española en la organización, celebrándose el 12 de marzo de 1986 un referéndum sobre la permanencia de España en la OTAN que, con un 52,54% de los votos, se mostró a favor de permanecer en la alianza pero sin participar en su estructura militar integrada.

Después de las revoluciones de 1989 y la disolución de la Unión Soviética en 1991, el Pacto de Varsovia quedó disuelto. La OTAN reformuló sus objetivos y actividades hasta apropiarse de la seguridad de todo el hemisferio norte.

En este marco, se desarrolló la primera operación de ataque por parte de la OTAN de su historia, la incursión en 1995 en la República de Bosnia y Herzegovina contra las fuerzas serbias en lo que se conoció como la Operación Fuerza Deliberada. En 1999 se llevó a cabo la Operación Fuerza Aliada, el ataque aéreo contra la República Federal de Yugoslavia, destinada a parar la limpieza étnica en Kosovo, donde se cometían crímenes contra la población civil.

En 1996, con la autorización del Congreso de los Diputados, España negoció su ingreso en la nueva estructura de mandos de la OTAN, aspirando a la «plena participación» en la estructura militar integrada, entonces en periodo de reforma. España culminaría su incorporación en la estructura militar integrada el 1 de enero de 1999.

La República Checa, Hungría y Polonia, antiguos miembros del Pacto de Varsovia, se unieron a la Alianza Atlántica el 12 de marzo de 1999.

Tras los atentados del 11 de septiembre de 2001 se llevó a cabo la Operación Libertad Duradera, la invasión de Afganistán por parte de Estados Unidos. La OTAN activó por primera vez el mecanismo de defensa mutua que prevé el artículo 5 del Tratado de Washington para apoyar los ataques de Estados Unidos a Afganistán.

En Afganistán, desde el 11 de agosto de 2003, la OTAN lideró una misión encargada por la ONU llamada Fuerza Internacional de Asistencia para la Seguridad (ISAF). El objetivo de la ISAF era ayudar al gobierno afgano a proporcionar una seguridad efectiva en todo el país y a desarrollar unas fuerzas de seguridad propias. Fue desplegada para proporcionar seguridad en torno a Kabul, a medida que la presencia de la ISAF se expandió para cubrir todo el territorio, sus tropas participaron en la lucha contra la insurgencia al tiempo que intentaban ayudar a la reconstrucción del país.

En Irak, la OTAN se ha limitado a entrenar a las fuerzas de seguridad. Las negativas de numerosos países europeos a que la OTAN actuara en el conflicto iraquí, encabezados por Alemania, disuadió a este organismo de involucrarse en una guerra iniciada por Estados Unidos y Reino Unido.

El 29 de marzo de 2004 Bulgaria, Eslovaquia, Eslovenia, Rumania y las ex repúblicas soviéticas Estonia, Letonia y Lituania firmaron el Tratado de Washington.

En febrero de 2005, Gerhard Schröder propuso crear una comisión para reformar la OTAN ante el peso creciente de la Unión Europea. El canciller alemán considera necesario reestructurar la Alianza Atlántica, dado que los desafíos estratégicos estaban fuera de su ámbito defensivo y no requerían en primera línea una respuesta militar, «ha habido malentendidos, malestares, desconfianza y hasta tensiones», indicó el canciller, como quedó reflejado tras la invasión de Irak de 2003 que dejó de manifiesto cuán profundas pueden ser las discrepancias y cuán poco preparada está la OTAN para reaccionar a tales dificultades.

En septiembre de 2006, la OTAN puso en marcha la Operación Medusa sobre el sur de Afganistán, con el objetivo de acabar con los reductos talibán en Panjwai y Zhari, en Kandahar, donde los insurgentes poseían una fuerte presencia. Se estima que en la operación fallecieron unos quinientos talibanes.

En 2008, la OTAN solicitó a Colombia tropas del Ejército y expertos antiminas y antinarcóticos para participar en la labor que se desarrollaba en la región afgana bajo la jurisdicción del Ejército español, debido a su experiencia en estos temas, mayor a la del resto de países. El 20 de febrero de 2009 fue aprobada la participación de entre 120 y 150 miembros del Ejército colombiano en la ISAF bajo bandera española. El caso de Colombia resultó particular por no pertenecer a la geografía del Atlántico Norte y, al mismo tiempo, no ser una nación en la OTAN.

Croacia y Albania se adhirieron a la Alianza Atlántica el 1 de abril de 2009.

El presidente Nicolas Sarkozy reintegró a Francia en el comando integrado coincidiendo con la cumbre del 60.º aniversario de la Alianza del 3 y 4 de abril de 2009, que se celebró entre Estrasburgo y Kehl, en la frontera franco-alemana.

El 31 de diciembre de 2014 la Alianza Atlántica puso fin a la misión de la ISAF. Desde 2015 la OTAN lleva a cabo en Afganistán la misión Apoyo Decidido, con una presencia militar más reducida, para entrenar, asesorar y ayudar a las fuerzas afganas.

En 5 de junio de 2017 se adhirió Montenegro, el vigésimo noveno miembro de la Alianza.

Por su parte, Macedonia del Norte pasó a formar parte de la alianza desde el 27 de marzo de 2020. Hasta entonces, el único obstáculo del país era superar los problemas derivados del nombre de la república, ya que en 2008 Grecia hizo bloquear la invitación al país eslavo hasta que fuera resuelta la disputa sobre su nombre, reclamado por Grecia como patrimonio helénico. Así, en junio de 2018, ambos países firmaron un acuerdo provisional por el cual la Antigua República Yugoslava de Macedonia pasaría a llamarse Macedonia del Norte, lo cual fue ratificado por los parlamentos de ambos países en enero de 2019, dando vía libre para seguir con el proceso de incorporación a la alianza, que culminaría un año después.

En el tratado se observa cómo se pretendía que Europa llevase a cabo su propia defensa militar, pues en el artículo 3 se permite que Estados Unidos ayude al desarrollo militar de Europa, a modo de Plan Marshall en el ámbito militar.

Listado de países miembros y la fecha en que ingresaron a la organización:

El artículo 10 del Tratado del Atlántico Norte, permite que nuevos estados formen parte de la OTAN:

Este artículo pone dos condiciones al ingreso de nuevos estados:



Este último criterio implica que los estados miembros pueden poner una serie de condiciones de cara al ingreso de nuevos países. Sin embargo, en la práctica la OTAN pone una serie de condiciones comunes.

Bosnia y Herzegovina es el único con un Plan de Acción para la Adhesión. Junto con Georgia, fueron nombrados «países aspirantes» a la OTAN en la reunión del Consejo del Atlántico Norte del 7 de diciembre de 2011.

Georgia celebró un referéndum el 5 de enero de 2008, en el cual fue aprobado por el 72.5 % de su población, la incorporación de este país a la Alianza Atlántica.

En agosto del 2010, una encuesta mostró que el 70% del país apoya la membresía de la OTAN, pero los resultados fueron muy diferentes en las dos entidades constitutivas de Bosnia y Herzegovina. Mientras que el 90% de la Federación de Bosnia y Herzegovina (región de mayoría bosníaca) apoyó la membresía de la OTAN, solo el 33% en la República Srpska (de mayoría serbia) lo hizo.

Ucrania comenzó su proceso de adhesión en enero de 2008, al ser presentada en Bruselas la solicitud de ingreso en la alianza, que debe ser respaldada a través de un referéndum popular, en fecha que aún no se ha establecido.

El 8 de junio de 2017, Ucrania, Verkhovna Rada, aprobó una ley que hace de la integración con la OTAN una prioridad de política exterior. En julio de 2017, Poroshenko anunció que buscaría la apertura de negociaciones sobre un Plan de Acción de Membresía con la OTAN.

Para marzo de 2018, la OTAN había reconocido a Ucrania como un país aspirante. El 20 de septiembre de 2018, el parlamento ucraniano aprobó enmiendas a la constitución que harían de la adhesión del país a la OTAN y la UE un objetivo central y el principal objetivo de la política exterior.

Aunque ni Suecia ni Finlandia forman parte de la OTAN, ambos participan en sus ejercicios y, tras la guerra en el este de Ucrania de 2014, ha habido en ambos países debates internos, invitando a reconsiderar su neutralidad y adherirse a la organización.

La OTAN está gobernada por sus treinta estados miembros, sin embargo, el Tratado del Atlántico Norte y otros acuerdos describen cómo se deben tomar las decisiones que surgen dentro de la OTAN.

Cada uno de los treinta miembros envía una delegación a la sede de la OTAN en Bruselas. Al alto miembro permanente de cada delegación se conoce como el «representante permanente», que por lo general es un alto funcionario o embajador con experiencia. Los miembros permanentes forman el Consejo del Atlántico Norte ("North Atlantic Council", NAC), que se reúne al menos una vez por semana y tiene la autoridad de gobierno efectivo y el poder de decisión en la OTAN. Las decisiones más importantes se toman en reuniones compuestas por los ministros de Asuntos Exteriores, los de Defensa o los jefes de Estado o de Gobierno.

Las reuniones del Consejo del Atlántico Norte son presididas por el secretario general de la OTAN. Todas las decisiones se toman de forma unánime y de común acuerdo. No hay ninguna votación o decisión por mayoría. Cada nación representada en la mesa del Consejo o en cualquiera de sus comités subordinados conserva la completa soberanía y la responsabilidad de sus propias decisiones.

El secretario general es el presidente del Consejo del Atlántico Norte, el más alto representante de la dirección política de la Organización del Tratado del Atlántico Norte. Desde la creación de la OTAN, los secretarios generales han sido:

Desde octubre de 2019 el vicesecretario general de la OTAN es el rumano Mircea Geoană.

La Asamblea Parlamentaria de la OTAN establece los numerosos objetivos estratégicos de la OTAN. Se reúne en la Sesión Anual, y es el órgano que interactúa directamente con las estructuras parlamentarias de los gobiernos nacionales de los estados miembros a través de sus miembros permanentes o embajadores ante la OTAN. La Asamblea Parlamentaria de la OTAN se compone de los legisladores de los países miembros de la Alianza del Atlántico Norte, así como trece miembros asociados. Tienen como objetivo discutir las políticas de seguridad en el Consejo de la OTAN.

La Asamblea es el órgano de integración política de la OTAN que elabora la agenda política del Consejo de la OTAN a través de informes de sus cinco comités:


La estructura militar de la OTAN es dirigida por el Comité Militar, que a su vez se encuentra bajo la autoridad del Consejo del Atlántico Norte. El Comité se encarga de asesorar a la Alianza en materia militar, pudiéndose reunir para ello los jefes de Estado Mayor, siendo más común la reunión a nivel de representantes militares.

El presidente del Comité Militar es el oficial militar de más alto rango de la OTAN, y el principal asesor militar del secretario general.

El Comité Militar, cumpliendo su objetivo de asesorar en materia militar, da directrices a los dos comandantes estratégicos de la organización: el comandante supremo aliado en Europa ("Supreme Allied Commander Europe", SACEUR) y el comandante supremo aliado de Transformación ("Supreme Allied Commander Transformation", SACT).

La función del SACEUR es la de preservar la paz, la seguridad y la integridad territorial de todos los países que conforman la OTAN. El SACEUR, en calidad de comandante supremo, se encargará de repeler, mediante las oportunas medidas militares, cualquier ataque que suceda o con riesgo de que suceda.

También se encarga de planificar las campañas militares, incluyendo el reclutamiento de las fuerzas militares necesarias para llevar a cabo las misiones de la OTAN, que incluyen la promoción de estabilidad, ayuda en crisis y provisión de una defensa efectiva allá donde sea necesario. Por otra parte, se encarga de hacer las pertinentes recomendaciones a las autoridades políticas y militares respecto a cualquier asunto militar que se encuentre dentro de su responsabilidad. El SACEUR también tiene acceso directo a los representantes militares de cada país, así como, si lo encuentra necesario, con las autoridades pertinentes, para facilitar el cumplimiento de las misiones.

El SACEUR tiene un perfil público muy importante, siendo el portavoz del Cuartel General Supremo de las Potencias Aliadas en Europa ("Supreme Headquarters Allied Powers in Europe", SHAPE). Mediante sus actos públicos y el personal de su gabinete, mantiene contacto regularmente con la prensa y lleva a cabo viajes por los países pertenecientes a la OTAN, así como con los que se está llevando a cabo programas de diálogo, cooperación y asociación. El SACEUR también se responsabiliza de desarrollar los contactos militares con los países pertenecientes al programa Asociación para la Paz.

El SHAPE es el centro de comando militar del SACEUR. Hasta 1967 estaba situado en Francia, en Rocquencourt, en la actual sede del "Institut National de Recherche en Informatique et en Automatique", cerca de París. Como consecuencia del retiro de Francia de la estructura militar de la OTAN, el SHAPE fue trasladado a Bélgica, en el territorio de las antiguas comunas de Casteau, Maisières y de Masnuy-Saint-Jean. Después de la fusión de las comunas, todo el territorio del SHAPE pasó a ser parte de Mons, en la provincia de Henao.

Lista de los comandantes supremos aliados en Europa:

El Mando Aliado de Operaciones esta compuesto por diversos cuarteles generales subordinados al SHAPE: A nivel operativo por dos mandos de fuerzas conjuntas, uno en Brunssum (Países Bajos) y otro en Nápoles (Italia). A nivel táctico por el Cuartel General del Mando Aéreo Aliado en Ramstein (Alemania), el Cuartel General del Mando Terrestre Aliado en Esmirna (Turquía) y el Cuartel General del Mando Marítimo Aliado en Northwood (Reino Unido).

Desde su creación en 2003, la misión del Mando Aliado de Transformación es contribuir a preservar la paz, la seguridad y la integridad territorial de los estados miembros de la Alianza liderando la transformación de las estructuras, fuerzas, capacidades y doctrinas militares. Esta misión debe permitir a la OTAN cumplir sus objetivos y sus misiones principales.

El Mando Aliado de Transformación está organizado en torno a cuatro funciones principales:


Tiene su cuartel general en Norfolk (Virginia, Estados Unidos) y tres entidades subordinadas: El Centro de Guerra Conjunta (Stavanger, Noruega), el Centro de Adiestramiento de Fuerzas Conjuntas (Bydgoszcz, Polonia) y el Centro de Análisis Conjunto y Lecciones Aprendidas (Monsanto, Portugal).

Desde el 11 de septiembre de 2018 el SACT, comandante supremo aliado de Transformación, es el general del Ejército del Aire Francés André Lanata.

En noviembre de 2002, durante la cumbre de Praga, se abrieron los Planes de Acción Individual de Asociación (IPAP) a los países que tienen la voluntad política y la suficiente capacidad para tener una mayor relación con la OTAN. Los IPAP han sido implementandos en los siguientes países:

La Asociación para la Paz (en inglés: "Partnership for Peace", PfP) es un programa originalmente iniciado en 1994 y basado en las relaciones bilaterales entre cada país asociado y la OTAN: cada país puede elegir hasta qué punto quiere participar.

Los veintiún países asociados son los siguientes:




Chipre encuentra una gran oposición a su admisión en la Asociación para la Paz por parte de Turquía a causa del conflicto entre los dos países. Por ese motivo, Chipre no participa en la Política Europea de Seguridad y Defensa.

El Consejo de Asociación Euro-Atlántico (en inglés: "Euro-Atlantic Partnership Council", EAPC), establecido el 29 de mayo de 1997, consiste en un foro destinado a la coordinación, consulta y diálogo entre los participantes del programa Asociación para la Paz y los países miembros de la OTAN.

Desde mediados de la década de los 2000, Rusia ha estado protestando por los planes de Estados Unidos de instalar diez interceptores antimisiles en Polonia y un radar en la República Checa dirigidos a prevenir hipotéticos ataques desde Irán y Corea del Norte. En opinión del Ministro de Asuntos Exteriores ruso, Serguéi Lavrov, en abril de 2007, el sistema de defensa no ofrece protección a Rumanía, Bulgaria, Grecia y Turquía, que podrían ser alcanzados por misiles provenientes de Irán, interpretando Rusia su construcción como «el avance de la infraestructura militar de la OTAN hacia nuestras fronteras». Ante los argumentos de Condoleezza Rice, la secretaria de Estado estadounidense, Lavrov no se muestra convencido, reacción que crea cierta inquietud entre algunos países de la OTAN, como Alemania y Noruega.

Sin embargo, es desde principios de marzo de 2014 cuando las relaciones entre la OTAN y la Federación Rusa se deterioran significativamente, debido a la crisis en Ucrania y a la anexión rusa de Crimea, pidiendo la OTAN a Rusia que detenga sus acciones y afirmando públicamente su apoyo a la soberanía e integridad territorial de Ucrania. Así, el 1 de abril de 2014, la OTAN emite un comunicado en el que anuncia que ha «decidido suspender toda la cooperación civil y militar entre la OTAN y Rusia. El diálogo político en el Consejo OTAN-Rusia puede continuar a nivel de embajadores para poder intercambiar puntos de vista, especialmente en lo que a esta crisis se refiere». Además, el comunicado condena «la intervención militar ilegal rusa en Ucrania y la violación de la soberanía e integridad territorial de Ucrania por parte de Rusia».

Ser socio global de la OTAN no significa ser miembro de la organización, pero tiene privilegios de cooperación por parte de los países miembros de la alianza en el área de equipamiento, formación e investigación hacia los países que conforman este grupo como si fuesen miembros.

En esta categoría de «socio global» están Afganistán, Australia, Irak, Japón, Colombia, Corea del Sur, Mongolia, Nueva Zelanda y Pakistán.

En junio de 2013 el Gobierno de Colombia suscribió un acuerdo de cooperación y acercamiento con la organización transatlántica, con el objetivo que el país sea un miembro asociado a futuro.

El 25 de mayo de 2018 el entonces presidente colombiano, Juan Manuel Santos, declaró vía Twitter que Colombia ingresará a la OTAN la semana siguiente como «socio global» ("global partner"), convirtiéndose en el primer país de Latinoamérica asociado a la organización, ingreso concretado el día 31 de mayo.

Un aliado importante no-OTAN (en inglés, "Major non-NATO ally"; o su acrónimo "MNNA") es una designación dada por el gobierno de los Estados Unidos a un grupo de países aliados que mantienen un trabajo conjunto con las Fuerzas Armadas de los Estados Unidos pero no son miembros de la organización. El ser elevado al estatus de MNNA no incluye automáticamente un pacto de defensa colectiva con Estados Unidos, pero brinda ventajas militares y financieras, por parte del país norteamericano, que no podrían obtener países que no son miembros de la OTAN.

Entre paréntesis, el año en que fueron declarados en esta categoría:

La OTAN ha recibido numerosas críticas por sus acciones y medidas militares por parte múltiples sectores tales como los ministros de Exteriores de Reino Unido y Francia, William Hague y Alain Juppé, durante las operaciones de la OTAN en Libia. La UNESCO le acusó de violar las resoluciones de Naciones Unidas y las convenciones de Ginebra que condenan la violencia contra los periodistas y los medios de comunicación al bombardear las instalaciones de la televisión estatal libia. Durante este mismo periodo, el entonces canciller de Venezuela, Nicolás Maduro, habló desde la Asamblea General de las Naciones Unidas, dejó entrever con su discurso que la OTAN apoyó a un grupo de rebeldes para que derrocaran a Muamar el Gadafi con el único objetivo de apoderarse de los valiosos recursos naturales que posee Libia.

Recibió críticas del ministro de Exteriores Ruso, Serguéi Lavrov, debido a la forma en la que afrontó la guerra en Malí, Siria y su actuación frente al programa nuclear iraní; «El modo en el que la OTAN encara los problemas no ayuda», aseguró el ministro ruso, y agregó que la comunidad internacional debería «adoptar una aproximación más global», no centrada en los intereses exclusivos de Europa y los Estados Unidos —la alianza transatlántica—, y que los riesgos son compartidos. Lavrov puso en duda la manera en la que la OTAN está afrontando los «principales problemas de la actualidad», como la primavera árabe, Siria, Malí o Irán.

La inversión en defensa estimada en 2018 de cada país es la siguiente:

En la Cumbre de Newport de 2014 los países miembros acordaron como objetivo aumentar su gasto en defensa hasta llegar a un mínimo del 2 % de su PIB en 2024.



</doc>
<doc id="29676" url="https://es.wikipedia.org/wiki?curid=29676" title="Arcangelo Corelli">
Arcangelo Corelli

Arcangelo Corelli (Fusignano; 17 de febrero de 1653-Roma; 8 de enero de 1713) fue un violinista y compositor del período barroco italiano.

Nació en Fusignano (provincia de Rávena, Italia) el 17 de febrero de 1653. Era hijo de una de las familias más importantes de esta ciudad. Desde la infancia estudiaba violín en su ciudad natal. En 1666 viaja a Bolonia, donde estudia con Giovanni Benvenuti y Leonardo Brugnoli. En 1670 pasa a ser miembro de la Academia Filarmónica de Bolonia. Cinco años más tarde se establece en Roma, donde fue adoptado y alojado por el cardenal Pietro Ottoboni, sobrino del papa Alejandro VIII. En Roma alcanzaría una extraordinaria fama como violinista, a la vez que perfeccionaba su técnica compositiva. Fue protegido por la reina Cristina de Suecia. 

En 1682 se convirtió en primer violinista de la orquesta de capilla de la iglesia de San Luis de los Franceses, la iglesia nacional de la comunidad francesa en Roma. En 1684 ingresó en la Congregazione dei Virtuosi di Santa Cecilia, el mismo año que adoptó el nombre de Arcomelo Erimanteo. Para 1700, Corelli era ya primer violinista y director de conciertos del Palacio de la Cancillería. Su fama era tal, que en 1706 ingresó en la Academia de la Arcadia, una altísima distinción en esa época, donde conoció a Domenico Scarlatti. Dos años después conocerá a Händel. 
Falleció en Roma el 8 de enero de 1713, siendo enterrado en el panteón de la iglesia de Santa María ad martires (el Panteón de Roma).

Corelli es considerado como uno de los más grandes precursores de la sonata preclásica y el representante por excelencia del concerto grosso, técnica que adquirió debido a las innovaciones de Alessandro Stradella que empleo para su (Concerto Grosso Op. 6). La música de Corelli ejerció una gran influencia en los compositores alemanes, especialmente en Bach y Händel. Después de las de Franz Joseph Haydn, las obras de Corelli fueron las más publicadas y reeditadas de su tiempo.

Su música de cámara incluye:





</doc>
<doc id="29678" url="https://es.wikipedia.org/wiki?curid=29678" title="Islas Caimán">
Islas Caimán

Las Islas Caimán () son un territorio británico de ultramar dependiente de Reino Unido y ubicado al noroeste de Jamaica, entre la isla de Cuba y la costa de Honduras, en aguas del mar Caribe. Es uno de los diecisiete territorios no autónomos bajo supervisión del Comité de Descolonización de las Naciones Unidas, con el fin de eliminar el colonialismo. Fueron colonizadas por exploradores británicos entre los siglos XVIII y XIX y comenzaron a ser administradas por el gobierno de Jamaica a partir de 1863. Cuando Jamaica consiguió su independencia en 1962, las Islas Caimán pasaron a ser administradas como territorio del Imperio británico.

Las Islas Caimán fueron descubiertas por Cristóbal Colón el 10 de mayo de 1503 durante su cuarto viaje a América. En 1586 el corsario Francis Drake atracó en las islas, siendo el primer inglés del que queda constancia que las visitara, y las bautizó como Islas Caimán. Las islas, junto con la cercana Jamaica fueron ocupadas por Inglaterra durante la guerra anglo-española de 1655-1660; España reconoció oficialmente la soberanía inglesa sobre ellas mediante el tratado de Madrid de 1670. Junto con Jamaica fueron gobernadas como una única colonia hasta 1962 cuando se convirtieron en un territorio británico de ultramar mientras que Jamaica obtenía su independencia (dentro de la Mancomunidad Británica de Naciones).

En 1788, diez barcos que regresaban a Gran Bretaña procedentes de Jamaica naufragaron en las costas de la isla mayor y fueron acogidos por los nativos. Por esta acción, el rey Jorge III del Reino Unido eximió a la colonia del pago de tributos, situación que se mantiene hasta la fecha.

Políticamente se dividen en 8 distritos: Creek, Oriental, Central, South Town, Spot Bay, Stake Bay, West End y Occidental, los cuales son administrados desde la ciudad de George Town. Desde el 23 de noviembre de 2005 el Gobernador es Stuart Jack, quien es el encargado de presidir el parlamento unicameral de la isla compuesto por 18 miembros, de los cuales 15 son elegidos y 3 son designados por un Consejo Ejecutivo.
Los partidos políticos existentes en la isla son la Alianza Democrática Popular, el Partido Independiente, el Movimiento Progresista Popular y el Partido Demócrata Unido.

Las Islas Caimán son un territorio británico de ultramar, designadas por el Comité de Descolonización de la ONU como uno de los últimos territorios no autónomos. Los quince representantes que componen la Asamblea Legislativa son elegidos por el Pueblo cada cuatro años, ellos se ocupan de administrar los asuntos internos. De los miembros electos de la Asamblea Legislativa (MLA), cinco son elegidos para servir como ministros en un gabinete encabezado por el gobernador. El jefe de Gobierno es el primer ministro.

El gobernador es nombrado por el Gobierno británico para representar al monarca. El gobernador puede ejercer la autoridad ejecutiva plena si así lo desea a través de los poderes que le están reservados por la Constitución. Se debe dar la sanción real a toda la legislación, que les permite el poder de anular cualquier ley que el legislador considere conveniente para el país. En tiempos modernos, el gobernador permite que el país sea dirigido por el Consejo de Ministros y el Servicio civil para ser ejecutados por el vicegobernador, que es el gobernador interino cuando el gobernador no puede desempeñar sus funciones habituales. El actual gobernador de las Islas Caimán es Anwar Choudhury.

El Gabinete está compuesto por dos miembros oficiales y cinco miembros elegidos, llamados ministros, uno de los cuales es designado primer ministro.

Los miembros oficiales son el vicegobernador y el fiscal general. Son nombrados por el gobernador, de conformidad con las instrucciones de Su Majestad y, aunque tienen escaños en la Asamblea Legislativa en virtud de la Constitución de 2009, no votan.

Los cinco ministros se eligen de entre los quince miembros electos de la Asamblea Legislativa. Uno de los ministros, el líder del partido político de mayoría, es nombrado primer ministro por el gobernador. Previa consulta con el primer ministro, el gobernador asigna una cartera de responsabilidades a cada miembro del Gabinete. En virtud del principio de responsabilidad colectiva, todos los ministros están obligados a apoyar en la Asamblea las medidas aprobadas por el Gabinete.

Casi 80 departamentos, secciones y unidades de llevar a cabo los asuntos de gobierno, unidos por una serie de organismos oficiales y autoridades creadas para fines específicos, tales como la Autoridad Portuaria, la Autoridad de Aviación Civil, la Junta de Inmigración, la Dirección de Recursos Hídricos, la Universidad Colegio Junta de Gobernadores, la Junta Nacional de Pensiones, y la Comisión de Seguros de Salud.

La defensa de las Islas Caimán es responsabilidad del Reino Unido. El Real Servicio Policial de las Islas Caimán presta servicios de policía en el país. El Cuerpo de Cadetes de las Islas Caimán se creó en marzo de 2001.

Desde 2000, ha habido ministros de los dos principales partidos políticos: Partido Democrático Unido (UDP) y Movimiento Popular Progresista (PPM). Si bien ha habido un cambio en los partidos políticos, muchos contendientes para un cargo público todavía funcionan como independientes.

Las Islas Caimán se localizan en la parte occidental del mar Caribe, a unos 240 kilómetros al sur de Cuba y a unos 290 km al noroeste de Jamaica. El archipiélago se compone principalmente de tres islas: Gran Caimán, Caimán Brac, y Pequeña Caimán, siendo Gran Caimán la más extensa con un área de 197 km². Las dos otras islas, Caimán Brac y Pequeño Caimán, están situadas a 145 km al este de Gran Caimán, tienen una extensión de 36 km² y 26 km² respectivamente.

No se destacan accidentes geográficos en las islas, a excepción del acantilado "The Bluff" en Cayman Brac, que se eleva a más de 40 m s. n. m., siendo el punto más alto de la isla.

Las Islas Caimán tienen más negocios registrados que personas. Las últimas estimaciones de población de las Islas Caimán son cerca de 69 000 en 2008, lo que representa una mezcla de más de 100 nacionalidades. De ese número, aproximadamente la mitad son descendientes de las Islas Caimán. Alrededor del 60 % de la población es de raza mixta (en su mayoría de africanos-europeos). Del 40 % restante, aproximadamente la mitad son de ascendencia europea y la mitad son de ascendencia africana. Las islas son casi exclusivamente cristiana, con gran número de presbiterianos y católicos. Islas Caimán disfruta del más alto nivel de vida en el Caribe. La gran mayoría de la población reside en Gran Caimán. Caimán Brac es la segunda más poblada, seguida de Pequeña Caimán.

La religión predominante en las Islas Caimán es el cristianismo (81,9%). En conjunto, los diversos grupos protestantes representan el 67,8%, mientras que la Iglesia católica representa el 14,1%. Las denominaciones que se practican incluyen la Iglesia Unida, la Iglesia de Dios, la Iglesia Anglicana, la Iglesia Bautista, la Iglesia católica, la Iglesia Adventista del Séptimo Día y la Iglesia Pentecostal. Las iglesias católicas son la Iglesia de San Ignacio, la de George Town y la de Stella Maris, en Caimán Brac. Muchos ciudadanos son profundamente religiosos, yendo regularmente a la iglesia. Los puertos están cerrados los domingos y los días festivos cristianos. Hay lugares de culto en George Town para los Testigos de Jehová y los seguidores de la fe Bahá'í. Las Islas Caimán también albergan una creciente comunidad judía.
El idioma oficial de las Islas Caimán es el inglés. Los acentos de los isleños conservan elementos transmitidos por los colonos ingleses, escoceses y galeses (entre otros) en una variedad lingüística conocida como criollo de las Islas Caimán. Los caimaneses de origen jamaicano hablan en su propia lengua vernácula (véase el criollo jamaicano y el inglés jamaicano). También es bastante común escuchar a algunos residentes conversar en español, ya que muchos ciudadanos se han trasladado de América Latina para trabajar y vivir en Gran Caimán. Las naciones latinoamericanas con mayor representación son Honduras, Cuba, Colombia, Nicaragua y la República Dominicana. Los hispanohablantes constituyen aproximadamente entre el 10 y el 12% de la población y son predominantemente del dialecto caribeño. El tagalo es hablado por alrededor del 5% de los habitantes, la mayoría de los cuales son residentes filipinos con permiso de trabajo.

Considerada un paraíso fiscal, la economía de las Caimán es una de las más sólidas del Caribe. De las casi 40 000 compañías que se encuentran registradas en la isla, 600 son bancos, los cuales manejan 500 000 millones de dólares estadounidenses en activos. El turismo también es otra importante fuente de ingresos y está orientado a viajeros de altos ingresos, principalmente del área de Norteamérica. Las Islas Caimán son un miembro asociado de la Comunidad del Caribe desde el año 2002. La emisión de sellos para colección es también otra fuente de ingresos.

Una de las principales atracciones de Gran Caimán (GCM) es Seven Mile Beach, sobre la que se extienden varios hoteles y centros turísticos. Los visitantes son atraídos por lugares históricos de GCM como el Castillo St. James en BoddenTown. Existen las islas Hermanas - Pequeño Caimán y Caimán Brac.

Las tres islas ofrecen oportunidades para el buceo. Hay varios arrecifes y lugares donde los turistas pueden nadar con rayas incluyendo Stingray City, Gran Caimán. Hay dos naufragios en las costas de Caimán Brac incluida la Tibbetts Keith MV.

Otras atracciones turísticas de Gran Caimán son el paisaje "Ironshore del Infierno", un Parque Marino de 9,3 hectáreas en la playa del contramaestre, también el hogar de la Cayman Turtle Farm, la producción de sal gourmet del mar y la Ruta de Mastic, un sendero a través de los bosques en el centro de la isla. El National Trust para las Islas Caimán ofrece visitas guiadas semanales en el Camino del Lentisco y otros lugares.

Las Islas Caimán son el quinto centro financiero internacional. Los sectores más importantes son la banca, las inversiones, los seguros y en general, las actividades corporativas.

Las Islas Caimán son el quinto centro bancario más grande del mundo, con 1500 millones de dólares estadounidenses en pasivos bancarios. Hay 279 bancos (a fecha de junio de 2008), de los cuales 19 tienen licencia para realizar actividades bancarias con los clientes que viven en Cayman y clientes internacionales, los 260 restantes están autorizadas para operar sobre una base internacional con sólo la actividad interna limitada.

Una de las razones para el éxito de las Islas Caimán como centro financiero extraterritorial ha sido la concentración de los principales proveedores de servicios de calidad. Estos incluyen las principales instituciones financieras globales (incluida la UBS y Goldman Sachs), más de 80 administradores, las prácticas de contabilidad más importantes (incluidos los Cuatro Grandes Cuentas), la legislación y las prácticas en el mar (incl. Maples & Calder y Ogier).

Desde la introducción de la Ley de Sociedades de Inversión en 1993, que ha sido copiado por las jurisdicciones de todo el mundo, las Islas Caimán se han convertido en líderes en alta mar en el mundo jurisdicción de fondos de cobertura. En junio de 2008 pasó de 10,000 inscripciones de fondos de cobertura, y durante el año finalizado en junio 2008 CIMA informó de una tasa de crecimiento neto del 12% para los fondos de cobertura.

A partir de mediados y finales de 1990 los centros financieros extraterritoriales, como las Islas Caimán, fue objeto de una creciente presión de la OCDE para sus regímenes fiscales supuestamente perjudiciales, donde la OCDE deseaba impedir que los regímenes de impuestos bajos de tener una ventaja en el mercado global. La OCDE amenazó con colocar a las Islas Caimán y otros paraísos fiscales en una lista negra y la imposición de sanciones contra ellos. Sin embargo, las Islas Caimán lograron evitar ser colocadas en dicha lista en 2000, comprometiéndose a realizar una reforma normativa para mejorar la transparencia y empezar a intercambiar información con los países miembros de la OCDE acerca de sus ciudadanos.

Las Islas Caimán había aparecido previamente en la lista negra del GAFI en 2000.

En el 2004, bajo la presión del Reino Unido, las Islas Caimán acordaron en principio aplicar la Directiva de Ahorros de la Unión Europea (EUSD), pero solo después de obtener algunos beneficios importantes para la industria de servicios financieros en las Islas Caimán. En las Islas Caimán no están sujetos a las leyes de la UE, la aplicación de la EUSD es por medio de acuerdos bilaterales entre cada Estado miembro de la UE y las Islas Caimán. El Gobierno de las Islas Caimán de acuerdo en un modelo de acuerdo, que establece cómo la EUSD se llevaría a cabo con las Islas Caimán.

Un informe presentó una carta abierta al Presidente en defensa de las islas y su "rol en las finanzas internacionales y su valor para el sistema financiero de Estados Unidos".

Las Islas Caimán cuentan con más de quinientas millas de carreteras, todas ellas pavimentadas.

Puertos: Cayman Brac, George marina mercante Población: Total: 123 barcos (1000 TRB o más) por un total de 2 402 058 toneladas de registro bruto y 3 792 094 toneladas métricas de peso muerto (TPM) por tipo de buques: 22 a granel, carga 5, quimiquero 31, envase 2, gas licuado de 1, tanque de petróleo 21, carga refrigerada 35, roll on / roll off 5, cisterna especializados 1 Nota: Algunos buques extranjeros inscribirse en las Islas Caimán como un pabellón de conveniencia; incluye los buques de 11 países entre los cuales están: Grecia 15, EE. UU. 5, Reino Unido 5, Chipre 2, Dinamarca 2, Noruega 3 (2002 est).

La Luz de East End (a veces llamado Görling Bluff luz) es un faro situado en el extremo este de la isla Gran Caimán en las Islas Caimán. El faro es el centro de East End Lighthouse Park, gestionado por la Fundación Nacional para las Islas Caimán, las ayudas a la navegación por primera vez en el sitio fue el primer faro en las Islas Caimán.

Hay tres aeropuertos en las Islas Caimán, uno en cada isla.

George Town, así como el resto de Gran Caimán, es servido por las inmediaciones del Aeropuerto Internacional Owen Roberts. Caimán Brac, es servido por Gerrard Smith International Airport y Pequeño Caimán es servida por Edward Bodden aeródromo.

Cayman Airways es la aerolínea de bandera nacional de las Islas Caimán. Con su sede central en Gran Caimán, que opera principalmente como compañía internacional y nacional de pasajeros, con servicios de carga disponible en todas las rutas y el servicio limitado que ofrece la Carta. Su funcionamiento se basa en el Aeropuerto Internacional Owen Roberts, Gran Caimán.

Island Air es una pequeña aerolínea en las Islas Caimán la prestación de servicios entre Gran Caimán, Caimán Brac y Pequeño Caimán

No hay impuestos directos en las Islas Caimán, la mayoría de los ingresos del Gobierno procede de impuestos indirectos. Se aplica un arancel del 20 % a los productos importados, aunque algunos artículos como leche para bebé, libros o cámaras están exentos. El impuesto sobre automóviles depende de la clase y modelo del vehículo, pudiendo llegar a un 40 % para los modelos de alta gama. Las instituciones financieras que operan en las islas pagan una tarifa plana de concesión de licencias por el gobierno. Además, existe un impuesto del 10 % sobre todos los alojamientos turísticos y una pequeña cuota que cada turista paga al llegar a la isla.

El Departamento de Educación opera las escuelas estatales de las Islas Caimán. Los niños de las Islas Caimán tienen derecho a la libre educación primaria y secundaria. Varias iglesias y fundaciones privadas operan varias escuelas privadas que ofrecen estudios basados en modelos estadounidenses y británicos a partir de guardería.

El Colegio de la Universidad de las Islas Caimán se encuentra en Gran Caimán, y es la única universidad ejecuta el gobierno de la isla. El Colegio está ubicado en la Universidad de George Town en Gran Caimán. El Colegio Internacional de las Islas Caimán es un colegio privado y se encuentra en Newlands, Gran Caimán, a 11 km al este de George Town. La universidad fue creada en 1970 y ofrece Asociado, Licenciatura y los programas de Posgrado. Gran Caimán es también el hogar de la Universidad de San Mateo, que incluye una escuela de medicina y una escuela de medicina veterinaria. La legislación de las Islas Caimán la Escuela, una rama de la Universidad de Liverpool en el Reino Unido, se basa también en Gran Caimán. situado en George Town, la escuela de derecho ha estado en operación desde 1982.

Las Islas Caimán, Civil Service College, una unidad de Gobierno de las Islas Caimán, organizadas en el marco de la cartera de la Función Pública, también se encuentra en Gran Caimán. Co-ubicada con el University College de las Islas Caimán en un edificio en el lado sur del campus, la intención de la CICSC es ofrecer tanto a los programas de grado y las unidades de educación continua de diversos tipos. Además, la universidad está prevista para desarrollarse como un centro de investigación del gobierno. Abrió sus puertas en otoño de 2007.

Hay dos hospitales en George Town, el gobierno ejecuta en George Town Hospital y los más pequeños, privados Chrissie Tomlinson Memorial Hospital. Además, el Hospital la Fe es una de dieciocho camas en Caimán Brac. El Gobierno mantiene una clínica satélite en Pequeño Caimán.

El seguro de salud está a cargo de las aseguradores privadas y el gobierno-empresa dirigida (Cínico). Todos los empleadores están obligados por ley a proporcionar seguro médico para sus empleados (aunque el empleado puede ser obligado a aportar el 50 % de la prima). Los empleados de tiempo completo también contribuyen con 10 $ cada mes para el «fondo de indigentes», que ayuda a cubrir la atención de las personas de edad desempleados, y otros grupos que necesitan asistencia monetaria. A partir de enero de 2010, las islas ha carecido de instalaciones para la cateterización cardiaca. Muchos sienten que la población es lo suficientemente grande como para soportar el procedimiento. Varios intentos de establecer un laboratorio de cateterización en George Town Hospital se han estancado. Sigue habiendo una urgente necesidad de cirugía de la retina en las islas. Actualmente, los residentes con severos trastornos oculares diabéticas o desprendimientos de retina se quedan ciegas, a menos que tengan los medios financieros para buscar atención inmediata en el continente. En julio de 2007 una unidad de resonancia magnética se instaló en el Chrissie Tomlinson Memorial Hospital, en sustitución de la destruida por el huracán Iván. En agosto de 2009, un nuevo stand-alone Abrir instalación RM se abrió. Este centro ofrece MRI, CT, X-Ray y DEXA (densidad ósea) de exploración. También se encuentra en este edificio es el Centro de Salud del Corazón, que proporciona Ultrasonido, Medicina Nuclear, la ecocardiografía y la prueba de esfuerzo cardíaco.

Para los buzos y otras personas que necesitan tratamiento con oxígeno hiperbárico, hay una cámara hiperbárica con capacidad para dos personas en el Hospital George Town en Gran Caimán, a cargo de Servicios Caimán hiperbárica. La misma organización ha construido una unidad hiperbárica en el Hospital la Fe en Caimán Brac.

En sus viajes marítimos, Cristóbal Colón, viniendo a través de las Islas Caimán en 1503, las llamó «islas Tortuga», debido a la abundancia de las tortugas marinas verdes encontradas allí. Durante los siglos XVII y XVIII, las Islas Caimán se convirtieron en un punto de parada para barcos en el mar Caribe que necesitan alimentos; las tortugas capturadas en las islas eran llevadas a bordo de la nave y mantenidas vivas como fuente de carne fresca.

Con el tiempo se establecieron asentamientos y ciudades en las islas Tortuga llegando a ser la tortuga su forma de supervivencia e ingresos en las Islas Caimán. Pero en el siglo XIX, la población de tortugas alrededor de las islas fue casi agotada y su comercio de la tortuga marina verde cambió a las costas de Nicaragua en Centroamérica.
La Granja de Tortugas Gran Caimán es un centro turístico y de conservación ubicado en el distrito West Bay de las Islas Caimán y fundada en 1968 por un grupo de inversionistas estadounidenses y británicos como la "Mariculture Limited", siendo la finca inicialmente una instalación utilizada para criar la tortuga verde con fines comerciales. Pero los inversionistas no podían obtener carne de tortuga para el consumo sin agotar la población silvestre de la especie.

Todavía en funcionamiento como una granja que reproduce y aumenta las tortugas con el fin de vender el producto, la granja de tortugas Caimán también se ha convertido en un atractivo turístico, centro de investigación. y un gran proyecto de conservación, así como la atracción más grande en tierra de las Islas Caimán. La granja acoge a más de 500 000 visitantes al año.

Es necesario poseer un permiso de trabajo para desempeñar actividad laboral en las islas. Esto implica pasar un chequeo de antecedentes policiales y un chequeo de salud. A un posible trabajador no se concederá una autorización si ciertas condiciones médicas están presentes, que incluyen pruebas positivas para la hepatitis o el VIH. Los permisos de trabajo no se emiten después de los 60 años de edad. El permiso puede concederse a las personas sobre el trabajo especial.

Las Islas Caimán tienen una población pequeña y por lo tanto una fuerza de trabajo limitada. Los permisos de trabajo concedidos a extranjeros por lo tanto sobre una base regular. En promedio, hay más de 40 000 extranjeros con permisos de trabajo válidos.

Para que un extranjero pueda ir a las Islas Caimán, primero debe encontrar un empleo. El permiso de trabajo es solicitado por el empleador. Todos los cargos asociados con un permiso de trabajo, son pagados por el empleador. Los permisos de trabajo no se conceden a los extranjeros que se encuentran en las Islas Caimán (a menos que sea una renovación). El Departamento de Inmigración de Las Islas Caimán exige que los extranjeros permanezcan en casa hasta que su permiso de trabajo haya sido aprobado. Los extranjeros suelen utilizar los recursos en línea para obtener ofertas de empleo.

Las Islas Caimán en la actualidad imponen una polémica política en relación con los trabajadores extranjeros que necesitan un permiso de trabajo. Las Islas Caimán sólo les permite residir y trabajar dentro del territorio un máximo de siete años (no renovable), a menos que cumplan los criterios de los empleados clave. La política ha sido objeto de cierta controversia en la prensa. Los bufetes de abogados han hecho sentir, especialmente, su molestia por las dificultades de contratación que ha causado. Otros sectores peor remunerados de empleo se han visto afectados también. Las preocupaciones por la seguridad han sido expresadas por los instructores de buceo y agentes de bienes raíces. Otros apoyan el traspaso como sea necesario para proteger la identidad de las Islas Caimán en la cara de la gran inmigración de trabajadores extranjeros.

Se ha expresado preocupación de que en el largo plazo, la política pueda dañar la preeminencia de las Islas Caimán como un centro financiero extraterritorial por lo que es difícil contratar y retener personal experimentado, de tierra, en centros financieros. Los empleados del gobierno ya no están exentos de este vuelco, política de acuerdo a este informe en un periódico local. El gobernador ha decidido usar sus poderes constitucionales, que le dan el control absoluto de la disposición de los empleados de la administración pública para determinar qué extranjeros son despedidos después de siete años de servicio y quienes no lo son.

Esta política está consagrada en la Ley de Inmigración (revisión de 2003), escrita por el gobierno de la UDP y posteriormente, aplicada por el gobierno de PPM. Ambos gobiernos están de acuerdo en los límites de los mandatos de los trabajadores extranjeros, y la mayoría de las Islas Caimán coinciden también en que esto es necesario para proteger la cultura y el patrimonio local evitando su deterioro por un gran número de extranjeros que obtengan la residencia y la ciudadanía.

La defensa de las Islas Caimán es responsabilidad del Reino Unido, contando con su propio cuerpo de policía, el Real Servicio de Policías de las Islas Caimán. No obstante, las patrulleras aduaneras que se pueden localizar en sus costas son dependientes del RCIP y Gran Caimán es una de las bases de actuación de los Guardacostas de Estados Unidos.




</doc>
<doc id="29679" url="https://es.wikipedia.org/wiki?curid=29679" title="Cadena de custodia">
Cadena de custodia

La cadena de custodia de una prueba se define como el procedimiento controlado que se aplica a los indicios materiales relacionados con el delito, desde su localización hasta su valoración por los encargados de su análisis, normalmente peritos, y que tiene fin no viciar el manejo que de ellos se haga y así evitar alteraciones, sustituciones, contaminaciones o destrucciones.

Desde la ubicación, fijación, recolección, embalaje y traslado de la evidencia en la escena del siniestro, hasta la presentación al debate, la cadena de custodia debe garantizar que el procedimiento empleado ha sido exitoso, y que la evidencia que se recolectó en la escena, es la misma que se está presentando ante el tribunal, o el analizado en el respectivo dictamen pericial.

Al recolectar las pruebas, lo importante es el significado, el valor que va a tener en el proceso de investigación y por medio de la cadena de custodia, este valor va a ser relevante, debido a que no se va a poder impugnar, al haberse acatado el procedimiento. 

El procedimiento que se debe seguir en cuanto a la evidencia en la escena, y en todo proceso de investigación, es el siguiente:


Las etapas de la cadena de la custodia son las siguientes:

La cadena de custodia implica: la extracción adecuada de la prueba, la preservación, individualización, transporte apropiado, entrega controlada.

Al recolectar las pruebas, lo importante es el significado, el valor que va a tener en el proceso de investigación y por medio de la cadena de custodia, este valor va a ser relevante, debido a que no se va a poder impugnar, al haberse acatado el procedimiento.
Consiste en el seguimiento que una empresa u organización transformadora de materias primas para la obtención de otros productos se compromete a hacer al objeto de garantizar que al menos un determinado porcentaje de aquellas materias, denominadas materias certificadas, cumplen unas ciertas características de calidad, generalmente medioambientales.
Habitualmente este seguimiento es también objeto de certificación y se denomina certificación de la cadena de custodia; como ocurre, por ejemplo, en las industrias transformadoras de madera, como pueden ser las de fabricación de muebles o las de fabricación de pasta de papel.


Tal como otro campo de la criminalística, la informática forense aplica conceptos de esta disciplina. Pero se añaden elementos particulares que son factibles en el momento de trabajar con pruebas documentales informáticas, como el digesto matemático (hash) en el momento de realizar el acta de secuestro, permitiendo así asegurar la integridad de la prueba.

El Acuerdo A/009/15, publicado en el Diario Oficial de la Federación el 12 de febrero de 2015, establece las directrices que deberán observar los servidores públicos que intervengan en materia de cadena de custodia, definiendo a ésta como el "Sistema de control y registro que se aplica al indicio o elemento material probatorio, desde su localización, descubrimiento o aportación, en el lugar de intervención, hasta que la autoridad competente ordene su conclusión"


</doc>
<doc id="29681" url="https://es.wikipedia.org/wiki?curid=29681" title="Capablanca">
Capablanca

El apellido Capablanca puede hacer referencia a las siguientes personas:


</doc>
<doc id="29682" url="https://es.wikipedia.org/wiki?curid=29682" title="Fala (valle de Jálama)">
Fala (valle de Jálama)

La fala ("fala") es una lengua romance del subgrupo galaico-portugués hablada en los municipios de San Martín de Trevejo, Eljas y Valverde del Fresno, todos ellos en el Valle de Jálama, al noroeste de la provincia española de Cáceres, junto a la frontera portuguesa y el límite provincial de Salamanca.

Es también nombrada de diversas formas: fala de xálima, xalimés, xalimegu, mañegu, a fala d'acá, a nossa fala y chapurráu (en Valverde) o, por los partidarios de la teoría sobre la relación entre la fala y el idioma gallego, gallego de Extremadura o galaico-extremeño. Nótese que es limítrofe con el altoextremeño circundante, incluyendo el habla de El Rebollar en Salamanca, a menudo considerada leonés.

En cada pueblo se dan particularidades dialectales (por lo que etimológicamente resultaría más correcta la denominación más genérica de "as falas"), si bien son lo suficientemente inteligibles entre sí y respecto al gallego o el portugués. Estas variantes suelen denominarse "lagarteiru" (Eljas), "manhegu/mañegu" (San Martín de Trevejo) y "valverdeiru" (Valverde del Fresno).

Contando a todos los habitantes de los tres pueblos se estiman unos 6000 hablantes, aunque otras fuentes elevan la cifra hasta los 10 000, por los nativos que trabajan fuera de la localidad pero muchos de los cuales vuelven al valle en verano.

Una de las primeras referencias escritas acerca de "A fala" se halla en el Diccionario de Madoz (1844):

El diccionario de Madoz recoge dos valoraciones de esta habla muy interesantes. Así, acerca del mañego, en 1848, escribe:

En la voz Eljas del mismo diccionario se lee:

Federico de Onís visitó la zona durante el año 1909 cuando buscaba materiales para completar sus estudios sobre los fueros leoneses. Los resultados de su investigación los publicó en 1930 y respecto a estos hablares opinó que:

No pasan desapercibidos ciertos rasgos comunes con el altoextremeño hablado en zonas circundantes debido a la influencia de éste, como el cierre de las vocales postónicas -e y -o en -i y -u respectivamente excepto en interjecciones y vocativos (excepción compartida con las hablas altoextremeñas), la conversión de la r final en l, la neutralización de l y r trabantes de sílaba (en decadencia, pero claramente manifiesta y sistemática en las encuestas del ALPI para Eljas/as Ellas en los años 30) o la conversión de la z sonora medieval en un sonido de d interdental en las variantes lagarteira y mañega, rasgo que se conserva en esos dos pueblos con más vitalidad y sistematicidad que en las hablas altoextremeñas serragatinas circundantes.

La opinión de los lingüistas es variada: muchos de ellos postulan que la fala es bien un habla de transición entre el portugués y el asturleonés o bien un dialecto galaico-portugués con claras influencias del asturleonés. Estas dos posturas tienden a ser las más aceptadas (respaldadas por ser de origen portugués o gallego-portugués la mayor parte del vocabulario), pero en cualquier caso existen posiciones variadas:










Además, existe otra lengua conocida como "fala" en La Alamedilla (Salamanca), pueblo también fronterizo con Portugal, que al parecer guarda una gran semejanza respecto al habla del Valle de Jálama.
Según algunos filólogos, existiría una fuerte relación entre estas dos "falas" y el habla portuguesa de Concelho do Sabugal.

En 1992, una encuesta hecha por José Enrique Gargallo Gil (profesor de la Universidad de Barcelona) a escolares ofreció los siguientes datos respecto al uso del español en la conversación familiar:

En septiembre/diciembre de 1993 se publicó una encuesta en el número 30 de la "Revista Alcántara", realizada por José Luis Martín Galindo, que mostraba los siguientes porcentajes de autoidentificación en San Martín de Trevejo:
Debe reseñarse que en dicha la encuesta participaron solo veinte personas (sobre 960 vecinos), no existiendo la alternativa de contestar gallego o variedad del gallego. Se sostiene que la ausencia de esta opción era lógica, pues las teorías acerca de la posible relación de la "fala" con el gallego apenas eran conocidas.

En 1994, un nuevo estudio pone de manifiesto que el 80 % de los encuestados aprendió a hablar español en el colegio, siendo el porcentaje de uso de la "fala" en el entorno familiar como sigue:

En la Edad Media se encuentran variantes mixtas portugués-leonesas a lo largo de la frontera entre León y Portugal, representadas en textos como el "Foro de Castelo Rodrigo" (Fuero de Castel-Rodrigo, siglo XIII); y aunque no existe documentación relativa a la colonización y repoblación en esta zona en el siglo XIII, se maneja la hipótesis de súbditos gallegos enviados a defender las fronteras bajo dominación musulmana como castigo impuesto por el rey leonés, o bien a la entrega de territorios a diversas órdenes militares por parte de Fernando II y Alfonso IX.

Por lo general, los filólogos partidarios de la teoría gallega se basan en la conjetura de que el valle era una región aislada y, por tanto, los colonizadores gallegos mantuvieron casi "pura" su forma de hablar al no existir influencias externas (de Portugal). Esta tesis suele ser refutada al contrastarla con otros datos históricos:

El 3 de agosto de 1992 se fundó la asociación "Fala i Cultura", entre cuyos propósitos se encuentra la elaboración de una gramática común (con base gallega), así como la conmemoración de "u día da nosa fala" ("día de nuestra habla") celebrado una vez al año desde 1992 en Eljas, 1993 en Valverde y 1994 en San Martín.

No sería hasta seis años después, en 1998, cuando llegará a publicarse la primera obra literaria en "a fala", "Seis sainetes valverdeiros", escrita por Isabel López Lajas, y editada en 1998 por Edicións Positivas de Santiago de Compostela. Fue en esa fecha cuando el Gabinete de Iniciativas Transfronterizas comenzó a preocuparse seriamente por a fala y a fomentar su estudio, publicando en 1999 una decena de tratados científicos; y celebrando en el mes de mayo un "Congreso sobre "a fala"".

Habrían de transcurrir varios años más hasta que el 14 de junio de 2000, la Consejería de Cultura de la Junta de Extremadura reconociese el habla del Valle de Jálama como Bien de Interés Cultural, a fin de protegerlo y conservarlo; siendo, en 2001, declarado por la Junta de Extremadura como Bien de Interés Cultural.

En la actualidad, si bien los habitantes del Valle de Jálama pueden hablar castellano (siendo ésta la lengua de uso común en los colegios y entornos administrativos), y presumiendo a menudo de hacerlo de forma más "correcta" que sus vecinos cacereños o salmantinos, la gran mayoría son bilingües, dado que en el ámbito casero, así como también en diversas actividades extraescolares, sigue empleándose la lengua local.

Sin embargo, es tal el empuje y auge de la lengua castellana en los últimos años (constatado por la pérdida de numerosos localismos, reemplazados por la versión castellanizada), que algunos filólogos consideran que esta lengua podría evolucionar hasta convertirse en una suerte de dialecto castellano-extremeño plagado de abundantes términos gallegos y/o portugueses.

Basándose en el estudio de algunos lingüistas que postulan que la "fala" es un dialecto del gallego, en los últimos años ha surgido una polémica por parte de ciertos sectores gallegos, siendo esta sobre todo de carácter político. Dicha postura ha recibido fuertes críticas por parte de la Junta de Extremadura, que rechaza oficialmente el deseo de algunas instituciones gallegas por implantar medidas lingüísticas respecto a esta lengua extremeña.

También ha sido criticada la intención (y puesta en práctica) de los ayuntamientos locales por desvincular toda relación posible de la lengua portuguesa con la "fala", reemplazando por la ortografía gallega algunas palabras hasta desvirtuar su pronunciación original (ej.: emplear la ""x"" para representar la ""j"" prepalatal fricativa sonora ("Ajuntamentu-Axuntamentu"), como en el portugués).

La postura galleguizante es respaldada por algunos hablantes de "fala", como Domingo Frades Gaspar, poeta en la "fala" y presidente de Fala i cultura (entidad que busca la normalización de la "fala" usando la normativa gallega) y miembro correspondiente de la Real Academia Gallega.

Por el otro lado, los defensores de la no relación de la "fala" con el idioma gallego afirman que la mayor parte de las características de esta pueden entenderse como una conjunción de los citados dominios lingüísticos, sin necesidad de acudir a hipótesis galleguizantes.



</doc>
<doc id="29684" url="https://es.wikipedia.org/wiki?curid=29684" title="Epífisis">
Epífisis

Se llama epífisis a cada uno de los extremos de un hueso largo. Es la zona en la que se sitúan las articulaciones. La epífisis suele ser más ancha que la porción central del hueso o diáfisis.

La epífisis está formada por un tejido esponjoso en el centro y por una capa delgada de tejido compacto en su periferia y se encuentra separada de la parte central del hueso por una región llamada metáfisis que es donde se encuentra el cartílago de crecimiento.

Está cubierta en su parte externa por el periostio y en su parte interna se encuentra la médula ósea roja que es donde se forman los glóbulos rojos y otras células sanguíneas. En la zona que forma la articulación la recubre un tejido cartilaginoso que se llama cartílago articular.

Los principales huesos que poseen epífisis son los huesos largos de las extremidades. En el ser humano tienen epífisis el fémur, la tibia, el peroné, los metatarsianos y las falanges en las extremidades inferiores, y el húmero, el cúbito, el radio, los metacarpianos y las falanges en las extremidades superiores.

Los huesos largos de las extremidades tienen dos epífisis, la que está más próxima a la raíz del miembro se llama epífisis proximal y la que está más alejada epífisis distal.

El término "epífisis" puede también hacer referencia a una glándula situada en el interior del cráneo que se conoce más comúnmente como glándula pineal.



</doc>
<doc id="29687" url="https://es.wikipedia.org/wiki?curid=29687" title="Emanuel Lasker">
Emanuel Lasker

Emanuel Lasker (Berlinchen, Prusia, 24 de diciembre de 1868 - Nueva York, 11 de enero de 1941) fue un ajedrecista, matemático y filósofo alemán, campeón del mundo de ajedrez de 1894 a 1921.

Obtuvo el título a los 25 años tras derrotar a Wilhelm Steinitz y es el campeón del mundo de ajedrez que más tiempo lo ha retenido, 27 años consecutivos, hasta que en 1921 perdió el "match" de la Habana frente al gran maestro cubano José Raúl Capablanca. Fue pionero entre sus contemporáneos en la explotación de los aspectos psicológicos del juego, sacando partido con enorme habilidad de las deficiencias particulares de cada uno de sus oponentes.

Emanuel Lasker nació el 24 de diciembre de 1868 en Berlinchen (actualmente Barlinek en Polonia), una pequeña localidad alemana cercana a la frontera ruso-prusiana entonces perteneciente a Brandeburgo. Procedente de una humilde familia judía su madre se llamaba Rosalie Israelssohn y su padre, Adolf Lasker, era cantor en la sinagoga local. A los doce años ya había evidenciado talento para las matemáticas y fue enviado a una escuela en Berlín al cuidado de su hermano mayor Berthold, quien le enseñó a jugar al ajedrez y le llevó a cafés donde pronto empezó a ganar dinero apostando en sus partidas.

Terminada la escuela secundaria ingresó en 1888 en la facultad de matemáticas de la Universidad de Berlín a la vez que seguía progresando en el ajedrez. Ganó un torneo en el Café Kaiserhof con el 100% de la puntuación y también un torneo secundario del Sexto Congreso de la Unión Alemana de Ajedrez, celebrado en Breslau en 1889. Este último triunfo le valió el título de maestro y la posibilidad de ser invitado a torneos internacionales, debutando en el de Ámsterdam de 1889 en el que se clasificó en segunda posición, con 6 puntos de 8 partidas, por detrás de Amos Burn y superando a jugadores de la talla de James Mason o Isidor Gunsberg. Ya en este su primer torneo importante realizó, en su partida contra Bauer, una combinación basada en el sacrificio de ambos alfiles que se convertiría en un modelo clásico.
Al volver de Ámsterdam derrotó en un encuentro individual a Curt von Bardeleben (+2 -1 =1) y mucho más claramente a Jacques Mieses (+5 =3). En 1890 viajó a Londres donde también derrotó entre otros al maestro inglés Henry Bird. Londres seguía conservando fama de capital ajedrecística, así que Lasker regresó allí para instalarse y en 1892 volvió a medirse con Bird al que aplastó (+5 =0), como también lo hizo con el ilustre Joseph Blackburne (+6 =4).

Ya pensando en la posibilidad de convertirse en campeón mundial, desafió a uno de los máximos aspirantes Siegbert Tarrasch, pero este declinó, respondiendo que primero debía vencer en un torneo importante, y prefirió disputar un "match" con Mijail Chigorín que finalizó empatado.

Así las cosas, decidió dar un paso audaz: viajar a Estados Unidos, donde vivía el campeón, Wilhelm Steinitz. Allí disputó enfrentamientos con varios maestros, entre ellos contra Jackson Showalter (+6 -2 =2), además de ganar el torneo de Nueva York (1893) con 13 puntos de 13 posibles por delante de Albin, Showalter y un joven Harry Nelson Pillsbury. Finalmente en agosto de 1893 desafió a Steinitz y el campeón aceptó el reto. 

El "match" se celebró entre el 15 de marzo y el 26 de mayo de 1894 entre Nueva York, Filadelfia y Montreal y, aunque discurrió igualado en sus inicios, la victoria de Lasker en una complicadísima séptima partida pareció quebrar la resistencia de Steinitz, que a su avanzada edad unía algunos problemas de salud. El resultado final fue un claro triunfo de Lasker (+10 -5 =4).

Al año siguiente se jugó en Hastings el que, atendiendo a la nómina de participantes, fue el torneo más fuerte del siglo XIX. Estaban presentes los cuatro principales aspirantes al título: Lasker, Steinitz, Tarrasch y Chigorín, pero la victoria le correspondió a la nueva estrella, el norteamericano Pillsbury. Aunque Lasker lo derrotó en su encuentro particular, finalmente solo pudo terminar tercero por detrás de Chigorín a un punto de Pillsbury. 

La revancha con Pillsbury llegaría en el match-torneo cuadrangular a seis vueltas celebrado poco después en San Petersburgo (1895/96). Pillsbury derrotó a Lasker en sus dos primeros enfrentamientos y parecía destinado a la victoria y a ganarse el derecho a la disputa del título, pero en su cuarto encuentro Lasker derrotó a Pillsbury con negras en una sensacional partida que él mismo acabó considerando la mejor de su carrera. Finalmente Lasker ganó el torneo y Pillsbury, quizás enfermo, se derrumbó y solo pudo ser tercero. Acto seguido Lasker también se impuso en el supertorneo de Núremberg (1896) por delante de Géza Maróczy, Pillsbury y Tarrasch. 

Poco después disputó en Moscú (1896/97) el primer "match" revancha de la historia contra Steinitz, que con sesenta años ya no estaba en su mejor forma y acumulaba problemas de salud. Lasker revalidó el título con el resultado de +10 -2 =5.

Una vez confirmado el título Lasker abandonó la competición para continuar sus estudios de filosofía y matemáticas regresando triunfalmente en 1899, cuando se impuso de forma aplastante en el torneo de Londres (con 23,5 de 27, por delante de Maróczy, Pillsbury y Janowski) y después en París (14,5 de 16, por delante de Pillsbury, Maróczy y Marshall). 

En 1899 Janowski fue el primero en proponer a Lasker un desafío por el título de campeón que no llegó a disputarse debido a las exigencias de Lasker tanto en el formato de la competición como en la bolsa de premios. En aquel entonces no había una reglamentación establecida y era el vigente campeón el que imponía sus condiciones; Lasker alegaba que estas, principalmente las económicas, debían ser elevadas y dignificarían el ajedrez profesional (el mismo Steinitz había fallecido en apuradas circunstancias), aunque esto también daba al poseedor del título la posibilidad de evitar a oponentes que considerara peligrosos en un momento determinado.

En estos años Lasker estaba más dedicado a sus estudios académicos en filosofía y matemáticas que al ajedrez. Dirigido por el matemático alemán Max Noether presentó en 1900 su tesis doctoral, que fue publicada en "Philosophical Transactions", la revista de la Royal Society. Hasta el año 1907 únicamente jugó en 1904 el torneo de Cambridge Springs, Pensilvania, en el que compartió el segundo puesto con Janowski, por detrás de Marshall.

Finalmente fue Marshall el que en 1907 disputó el título a Lasker gracias a que los aficionados norteamericanos consiguieron reunir el dinero necesario. Aunque Marshall era un jugador brillante y derrotó a Janowski en 1905 (+8 -5 =4), ese mismo año fue aplastado por Tarrasch (+8 -1 =8) y tampoco pudo oponer la menor resistencia al campeón, perdiendo el "match" por un contundente +8 =7. Después del torneo de Cambridge Springs Lasker se quedó en Estados Unidos, donde publicó durante cuatro años la revista "Lasker's Chess Magazine" que finalmente debió cerrar a causa de dificultades económicas en 1908, año en el que regresó a Alemania. 

En 1908 puso finalmente el título en juego contra el rival que era unánimemente considerado el aspirante más peligroso, Siegbert Tarrasch. El encuentro por el campeonato, que se llevaría el primero en ganar ocho partidas, comenzó el 17 de agosto en Düsseldorf y siguió a partir de la quinta partida en Múnich. Lasker se impuso por un claro +8 =5 -3 ante un Tarrasch que, a sus 46 años quizás había dejado atrás la cúspide de su fuerza.
Al año siguiente Lasker compartió el primer puesto en el torneo de San Petersburgo con Akiba Rubinstein. Ambos aventajaron en 3,5 puntos a sus inmediatos seguidores, pero Lasker perdió con Rubinstein un final de torres que hoy en día es clásico y su rival se convirtió en el principal aspirante al título mundial. Sin embargo Rubinstein no consiguió financiación para organizar el "match" en sus mejores años y, además de que Lasker utilizaba su potestad de campeón para evitar a sus rivales más peligrosos, posteriormente perdió la oportunidad debido el estallido de la Primera Guerra Mundial a cuyo término José Raúl Capablanca ya había realizado enormes progresos.

En 1910, en Viena y Berlín, disputó un nuevo "match" por el campeonato con el jugador austriaco Carl Schlechter. Aunque Schlechter era un fuerte ajedrecista resultó una sorpresa que Lasker estuviese a punto de perder el título: después de ser derrotado en la quinta partida tras desperdiciar su ventaja solo pudo igualar el marcador (+1 -1 =8) tras ganar, después de enormes complicaciones, la décima y última partida (en la que se jugó una variante de la defensa eslava a la que el propio Schlechter dio nombre). El "match" fue inusualmente corto ya que, previsto inicialmente a treinta partidas, se recortó a diez debido a problemas financieros y además su reglamentación, que no fue hecha pública totalmente en su día, ha sido muy discutida por los historiadores del ajedrez ya que en condiciones normales Schlechter podría haber jugado a tablas la décima partida. Sin embargo estaba estipulado que el aspirante tenía que lograr una ventaja de dos puntos para proclamarse campeón. 

A finales de ese mismo año retuvo el título con mucha más facilidad ante Janowski. El "match" se jugó en Berlín y Lasker no concedió la más mínima oportunidad a su rival: ganó ocho partidas sin sufrir ni una sola derrota (+8 =3). 

Más peligroso hubiera sido un "match" contra Capablanca, que venía de obtener una sensacional victoria en el torneo de San Sebastián (1911), su primera aparición en la elite mundial. Capablanca aceptó la bolsa de premios de 10 000 dólares que se le exigió, pero rechazó el resto de las condiciones: el ganador sería el primero en vencer seis partidas, aunque llegados al límite de treinta el campeón solo perdería el título en caso de perder por un margen no inferior a dos puntos. Esta última norma, similar a la utilizada en el "match" con Schlechter, fue la que Capablanca consideró más injusta.

Afirmado su título no disputó torneos importantes hasta 1914 y mientras tanto se casó en 1911 con Martha Cohen, procedente de una acomodada familia judía. El matrimonio se instaló en Berlín. 
En los años previos a la guerra el acontecimiento ajedrecístico más importante fue sin duda el supertorneo de San Petersburgo de 1914. Se había decidido que el ganador, de no ser el propio Lasker, adquiriría el derecho a disputar un encuentro por el título mundial. El torneo constaba de una primera fase por sistema liga entre once jugadores que clasificaría a los cinco primeros a una final a doble vuelta. La liga preliminar la ganó Capablanca con punto y medio de ventaja sobre Lasker, seguidos de Tarrasch, Alekhine y Marshall. Sin embargo en la fase final Lasker jugó de forma sensacional anotándose 7 puntos en 8 partidas y aventajando en el cómputo total a Capablanca por medio punto. La partida decisiva fue la de la segunda ronda entre los líderes, una apertura española con la variante del cambio, en la que Lasker consiguió una de las victorias más famosas de su carrera.

Durante la guerra la actividad ajedrecística disminuyó y también se aplazaron las negociaciones con Rubinstein y Capablanca para una posible disputa del campeonato mundial. En 1916 Lasker jugó en Berlín contra Tarrasch un encuentro amistoso con el resultado de +5 =0 -1 y, ya en 1918, un torneo a doble vuelta contra Rubinstein, Schlechter y Tarrasch que ganó con 4,5 puntos de 6.

Finalizada la guerra Lasker estaba prácticamente arruinado y en 1920 reanudó las negociaciones con Capablanca para la organización del mundial. Se reunieron en La Haya donde Lasker, que prefería jugar en Holanda y Estados Unidos, debió aceptar que el "match" se disputará el año siguiente en La Habana ya que los seguidores cubanos de Capablanca habían reunido un fondo nada despreciable de 20 000 dólares. Por primera vez se jugó al mejor de 24 partidas, norma que posteriormente se convirtió en habitual.

Por lo tanto el encuentro se jugó en el país del aspirante y bajo un clima que causó grandes problemas de adaptación a Lasker, que con 52 años, veinte más que su rival, además no se encontraba bien de salud. Aunque Lasker no lo puso en ningún momento en aprietos el juego fue más disputado de lo que parece indicar el abultado marcador final de 9 a 5 favorable a Capablanca (+4 =10), momento en el que Lasker abandonó el encuentro por recomendación de su médico. 

Después de perder el título Lasker siguió jugando a un gran nivel enfrentándose con éxito tanto a los maestros de su generación como a los jóvenes ajedrecistas cargados de nuevas ideas. Así ganó el torneo internacional de Mährisch-Ostrau (1923) terminando invicto con 10,5 puntos de 13 por delante de Richard Réti y Ernst Grünfeld. 

Aún más importante fue su claro triunfo en el torneo de Nueva York (1924), jugado a doble vuelta con la presencia de casi toda la elite del ajedrez mundial. Lasker se impuso con 16 puntos de 20 superando por punto y medio a Capablanca y por cuatro a Alekhine. Meses después terminó segundo tras Bogoljubov en el primer torneo internacional de Moscú (1925) aventajando en medio punto a Capablanca.

Después de Moscú Lasker abandonó el ajedrez y dedicó esos años a sus actividades académicas y también a otras aficiones como el bridge y el go. Sin embargo tanto él como su esposa Martha eran judíos y la llegada de Hitler al poder hizo que tuvieran que abandonar Alemania y perdieran sus propiedades, lo que los dejó en una muy difícil situación económica.

Reapareció en 1934 para tomar parte en el torneo de Zúrich donde comenzó brillantemente derrotando a Max Euwe (que el año siguiente arrebataría a Alekhine el título de campeón del mundo) aunque finalmente solo pudo clasificarse quinto. Los Lasker se habían mudado a Inglaterra durante unos meses, pero recibieron la invitación de trasladarse a la Unión Soviética, en plena era estalinista, cuando el ajedrez estaba dominado por el todopoderoso Nikolái Krylenko. Allí consiguió el último éxito de su larga carrera al conseguir el tercer puesto, invicto y precediendo a Capablanca, en el segundo torneo internacional de Moscú (1935), cuyos ganadores fueron Mijaíl Botvínnik y Salo Flohr.

En 1936 también participó en Moscú, aunque con menos éxito, clasificándose sexto con 8 puntos de 18. Su último gran torneo fue el de Nottingham (1936) donde hizo tablas con Botvinnik y Capablanca y derrotó al vigente campeón, Max Euwe.

En 1937 los Lasker abandonaron la URSS, lo que tal vez tuviera algo que ver con la progresiva caída en desgracia de Krylenko (que sería ejecutado al año siguiente) y se marcharon a los Estados Unidos. Emanuel Lasker murió a los 72 años, el 11 de enero 1941 en Nueva York.

Lasker, al contrario que muchos de sus contemporáneos, no realizó grandes aportaciones en el terreno de la teoría de aperturas, dada su tremenda fuerza en parte se contentaba con conseguir posiciones jugables. Aun así dio nombre a varios sistemas, como la defensa Lasker del gambito de dama y el plan contra el gambito Evans que de hecho casi lo erradicó de la práctica magistral, así como un esquema contra la apertura Reti que todavía sigue en vigor. Otro ejemplo sería su tratamiento de la variante del cambio de la apertura española, tradicionalmente considerada con tendencia a las tablas, como en su partida contra Capablanca en San Petersburgo. Posteriormente Fischer incorporó la variante en su repertorio de aperturas con un enfoque similar, particularmente evidente en su partida contra Unzicker en la Olimpíada de Siegen en 1970. 

En cuanto al final de partida, evidentemente Lasker tenía una técnica sobresaliente, aunque quizá no conserve la misma fama por sus logros en esta fase de la partida que algunos de sus contemporáneos, especialmente Capablanca y Rubinstein. De todas formas muchos de sus finales son hoy clásicos, como el final de torres que le ganó al mismo Rubinstein en San Petersburgo. Lasker es también autor de un famoso final compuesto, de torre y peón contra torre y peón, que aparece en la mayoría de los manuales debido a que la maniobra es de considerable valor práctico. 
Sin embargo siempre se ha considerado que la principal característica de su estilo es el aspecto «psicológico». Así lo recalcaron ya sus contemporáneos:
Según suele decirse, en sus partidas, muchas veces optaba por jugadas que no eran necesariamente las mejores sino las que más complicaban la partida al adversario con el que se enfrentaba, como si buscara en cada encuentro la manera de imponerse utilizando los puntos débiles de cada uno de sus rivales. Pero lo cierto es que Lasker contaba con una impresionante fuerza combinativa y una brillante técnica en los finales, armas suficientes para derrotar a la mayoría de sus coetáneos. Cuando se enfrentaba a contendientes de alto nivel, su técnica no era exactamente «psicológica» sino que buscaba complicaciones de alto riesgo, exigiendo del contrincante enorme capacidad de cálculo, y tratando de romper los estereotipos sobre estrategia general de juego vigentes en su tiempo. En esos casos, trataba de alterar el juego equilibrado, sólido y seguro que, en sus días, se creía irrebatible. En cierto modo, Lasker se adelantó por décadas al estilo de juego de su momento y tal vez haya que buscar en estas rupturas el motivo de que fuera el mejor jugador del mundo durante muchos años.

Lasker fue un más que destacado matemático. Se graduó en Landsberg an der Warthe (hoy Gorzów Wielkopolski en Polonia) y continuó posteriormente sus estudios de filosofía y matemáticas en las universidades de Berlín, Gotinga y Heidelberg.

En 1895 ya había publicado dos artículos en la prestigiosa revista "Nature", y finalmente en 1900 se doctoró en la Universidad de Erlangen-Núremberg donde bajo la supervisión de Max Noether presentó su tesis doctoral, que fue publicada en "Philosophical Transactions", la revista de la Royal Society.

Sus principales aportaciones se centraron en un campo, entonces en activo desarrollo, del álgebra abstracta que estaba siendo sistematizado por el grupo de Gotinga del eminente matemático alemán David Hilbert. Su trabajo más importante lo publicó en 1905 en la revista "Mathematische Annalen", cuyo resultado generalizado más tarde por Emmy Noether, hoy se conoce como el teorema de Lasker-Noether y es una parte fundamental de la teoría de ideales.

El trabajo de los matemáticos de Gotinga, sobre todo el del propio David Hilbert, tuvo aplicaciones en muchos campos, entre ellos la teoría de la relatividad de Albert Einstein, cuyo desarrollo Lasker siguió con particular interés. Lasker y Einstein se habían conocido en Alemania en la casa del escritor Alexander Moszkowski y mantuvieron una amistad que Einstein recordó más tarde en el prólogo de una biografía póstuma de Lasker:

Lasker también se interesó por diversos juegos de estrategia, proclives al análisis matemático. Así inventó un juego de tablero denominado lasca (o "Laskers", el término procede de su nombre) cuyas reglas publicó en 1911, y propuso variaciones en las reglas del nim. Fue también un destacado jugador de go y sobre todo de bridge.

Lasker publicó un libro titulado "Kampf" ("Lucha") en Nueva York, editado en 1907 por la Lasker Publishing Company, y que se traduciría al inglés con el título de "Struggle". En dicha publicación, que fue un desastre comercial, ahonda sobre el concepto de la lucha, la estrategia y la confrontación desde un punto de vista filosófico.




</doc>
<doc id="29689" url="https://es.wikipedia.org/wiki?curid=29689" title="Mars Global Surveyor">
Mars Global Surveyor

Mars Global Surveyor es una misión de la NASA y el NASA Jet Propulsion Laboratory destinada a estudiar el planeta Marte a fondo mediante varios instrumentos científicos de alta utilidad. La misión ha sido la primera en 20 años en llegar con éxito al planeta rojo. Durante su primer año y medio se dedicó a la fase de aerofrenado consistente en ir adquiriendo la órbita definitiva a base de pasar por las capas superiores de la atmósfera marciana y así ir frenando su velocidad hasta conseguir una órbita adecuada. Este período fue más largo de lo previsto, para no dañar los paneles solares en exceso. Ahora, en la actualidad, sigue una órbita polar cercana a la superficie y desde allí envía las fotos a la tierra con mayor resolución de la exploración de Marte y ha mandado más datos que todas las misiones anteriores juntas.

La nave tiene forma de caja de 1,7 x 1,17 x 1,17 metros, con dos partes bien diferenciadas; una para los instrumentos y otra para la propulsión. Los paneles solares tienen una envergadura de 3,5 x 1,9 m y proporcionan 980 W de potencia para los instrumentos. La parabólica tiene un diámetro de 15 dm y un brazo extensible de dos metros.

La misión fue financiada por la NASA y controlada desde el Jet Propulsion Laboratory (JPL) y por Lockheed Martin Astronautics.

La nave espacial es una caja rectangular de 1,17 m x 1,17 m x 1,7 m de tamaño, con dos módulos, uno de equipo y otro de propulsión. Sus experimentos excepto el magnetómetro (montado en el panel solar) se montan en el módulo de equipamiento de de superficie. El módulo de propulsión mide 3,5 x 1,9 m, contiene 380 kg de combustible con hidracina, un principal de N2O4 y dos propulsores 596 N Wil para correcciones e inserción orbital. Las comunicaciones se realizan mediante una antena parabólica de 1,5 m de diámetro con un asta de 2 m, dos antenas de baja ganancia con transmisor x de 21,33 kbit/s, 2 kbit/s de datos de ingeniería y 10 bit/s auxiliar. La electricidad es obtenida mediante dos paneles solares de 3,5 x 1,9 m de tamaño, montados en los lados opuestos de la nave; proporcionan 980 W de potencia en la nave, almacenada en dos baterías de hidrógeno y níquel.


Esta sonda observó el sistema Tierra-Luna en 2003, cuando mostró que nuestro planeta también presenta fases, al igual que Mercurio, Venus y la Luna. La fase lunar es un efecto puramente geométrico que es debido a la posición relativa del elemento que ilumina (el Sol), el iluminado y el observador.

El Global Surveyor también detectó manchas oscuras en las dunas debajo de la capa de hielo del polo sur de Marte, entre las latitudes 60°-80°. La peculiaridad de estas manchas, es que el 70 % de ellas recurre anualmente en el mismo lugar del año anterior. Las manchas de las dunas aparecen al principio de cada primavera y desaparecen al principio de cada invierno, por lo que un equipo de científicos de Budapest, han propuesto que estas manchas podrían ser de origen biológico y de carácter extremófilo. La agencia espacial ESA también está analizando el fenómeno de estas manchas mediante el Mars Express.

Por su parte, los diseñadores de la cámara a bordo del 'Mars Global Surveyor', quienes obtuvieron las imágenes, estiman que las manchas simplemente podrían ser causadas por la evaporación y congelamiento de áreas en el hielo que contienen principalmente dióxido de carbono ( CO).

El 5 de noviembre de 2006 se recibió la última señal de la sonda, tras comunicar a los controladores de tierra que tenía problemas con uno de los paneles solares. Durante semanas, la NASA intentó sin éxito recuperar contacto con la sonda, tanto desde tierra como desde su sonda Mars Reconnaissance Orbiter y también con sus todoterrenos Spirit y Opportunity. El 22 de noviembre de 2006, la sonda se dio por perdida, finalizando así una de las más exitosas misiones de la NASA a Marte. Sin embargo, los esfuerzos por recuperar contacto no se han dado por finalizados; la ESA, a través de su sonda Mars Express, podría haber fotografiado la sonda el 9 de diciembre que, según parece, estaría girando fuera de control. La Mars Express intentó de nuevo detectar la Mars Global Surveyor el 21 de diciembre.

Los resultados preliminares de la investigación indican que en junio de 2006 se envió a la sonda un software que contenía un error en dos direcciones de memoria (que ya estaban ocupadas por otros procesos, por lo que fueron reescritas). Cuando en noviembre los paneles solares se bloquearon, el radiador de la batería, que debía evitar que se calentara demasiado, fue orientado por error hacia el Sol, lo que produjo un sobrecalentamiento de la batería que la hizo inservible. La sonda habría entrado en modo de seguridad pero, sin batería, no estaría en condiciones de contactar con la Tierra.




</doc>
<doc id="29694" url="https://es.wikipedia.org/wiki?curid=29694" title="Oswald Avery">
Oswald Avery

Oswald Theodore Avery, (Halifax, 21 de octubre del 1877- 2 de febrero del 1955). Médico e investigador canadiense, estudió en la Universidad de Columbia y casi todo su trabajo lo realizó en el hospital del Instituto Rockefeller en Nueva York, Estados Unidos. Fue uno de los primeros biólogos moleculares y un pionero en el campo de la inmunoquímica, aunque es mejor conocido por su descubrimiento en 1944, junto con su colaborador Maclyn McCarty, de que el ADN (ácido desoxirribonucleico) es el material del que los genes y los cromosomas están formados y de cómo estos definen la sexualidad del ser humano. Anteriormente se creía que las proteínas eran las portadoras de los genes.

Fue una continuación de los trabajos de Frederick Griffith en 1928. A su vez, Alfred Hershey y Martha Chase, continuaron este trabajo en 1952 con el experimento Hershey-Chase.

Avery, Colin MacLeod y Maclyn McCarty hicieron una serie de experimentos usando cepas de la bacteria neumococo, la cual causa neumonía. Los neumococos crecen en el cuerpo huésped, pero, como otros tipos de bacterias, también pueden crecer en superficies sólidas o líquidas.

Los neumococos son bacterias que cuando no tienen cápsula, crecen en el laboratorio, formando colonias con superficie rugosa; si tienen esa envoltura su apariencia se torna lisa. La diferencia pudiera parecer menudencia estética, pero no. Según datos emanados del laboratorio de Avery, precisamente la cápsula es causante de la virulencia.

[[Frederick Griffith| descubrió que al inyectar a ratones con pequeñas dosis de neumococos no virulentos junto con grandes cantidades de neumococos patógenos pero «muertos» por calentamiento, los animales no solo mueren de neumonía sino que muestran en su sangre bacterias encapsuladas vivas. Es decir, en estas condiciones experimentales el neumococo no virulento adquiere la información para sintetizar la cápsula (se transforma, diría Griffith) en el cuerpo del ratón y, con ella, la capacidad de producir enfermedad.

Griffith concluyó que había algún «principio» que transformó las cepas rugosas (R) en lisas (S) con una cubierta de azúcares.
Cuando Avery leyó los resultados de Griffith se interesó en identificar este «principio transformador», Avery y su equipo comenzaron a experimentar usando un tubo de ensayo en vez de un ratón. Usaron detergente para descomponer las células lisas muertas por calor creando una [[lisis]] a partir de ellas. Entonces usaron esta lisis para los ensayos de transformación. Los tubos funcionaron bien y mostraron que la lisis de S muerta por calor podían cambiar (R) Rugosa a (S) Lisa. El principio transformador estaba en algún lugar de la lisis.

Probaron cada uno de los componentes de la lisis para la actividad transformadora. Primero incubaron la lisis de cepa lisa muerta por calor con una enzima, SIII, que consume completamente la cubierta de azúcar. La lisis de cepa lisa sin cubierta seguía siendo útil para transformar. Esto les reveló que las cepas R no creaban una nueva capa a partir de las partes de la cubierta de cepa lisa. Luego incubaron la lisis de cepa lisa sin cubierta [[enzima]]s que digieren proteínas ([[tripsina]] y [[quimotripsina]]) y después probaron la habilidad de esta lisis para transformar. Esta lisis sin proteínas seguía trasformando, así que el principio trasformador no era proteína.

Cuando querían probar y purificar la lisis, precipitaron los [[ácidos nucleicos]] – [[ADN]] y [[ARN]] - con alcohol. Fueron los primeros en aislar los ácidos nucleicos de un neumococo.
Cuando vieron que el «principio» transformador no estaba en la cubierta de azúcar, ni en la [[proteína]] sospecharon que tal vez estaría en uno de los ácidos nucleicos.

Disolvieron la mezcla con alcohol en agua, primero destruyeron el ARN con la enzima RNasa, probaron la capacidad trasformadora de esta solución, la solución todavía tenía capacidad para transformar, de tal manera que el ARN no podía ser el «principio» transformador. Cuando habían dejado virtualmente ADN puro, como una prueba final, incubaron la solución con la enzima digestora de ADN, Dnasa.
Probaron la capacidad trasformadora de esta solución, la cual fue incapaz de transformar.
Avery y su equipo concluyeron que el [[ADN]] era el principio transformador y publicaron sus resultados en [[1944]].




[[Categoría:Miembros extranjeros de la Royal Society]]
[[Categoría:Médicos de Canadá]]
[[Categoría:Biólogos moleculares]]
[[Categoría:Neoyorquinos]]
[[Categoría:Premio Lasker]]
[[Categoría:Medalla Copley]]
[[Categoría:Microbiólogos de Canadá]]
[[Categoría:Genetistas de Canadá]]
[[Categoría:Doctores honorarios de la Universidad de Chicago]]
[[Categoría:Miembros de la Real Academia de Medicina de Bélgica]]
[[Categoría:Candidatos al Premio Nobel de Química]]
[[Categoría:Candidatos al Premio Nobel de Fisiología o Medicina]]
[[Categoría:Fallecidos por cáncer de páncreas]]
[[Categoría:Nacidos en Halifax]]

</doc>
<doc id="29697" url="https://es.wikipedia.org/wiki?curid=29697" title="Stephen Jay Gould">
Stephen Jay Gould

Stephen Jay Gould (10 de septiembre de 1941-20 de mayo de 2002) fue un paleontólogo estadounidense, geólogo, biólogo evolutivo, historiador de la ciencia y uno de los más influyentes y leídos divulgadores científicos de su generación.

Gould pasó la mayor parte de su carrera docente en la Universidad de Harvard y trabajando en el Museo Americano de Historia Natural de Nueva York. En los últimos años de su vida, impartió clases de biología y evolución en la Universidad de Nueva York, cercana a su residencia en el SoHo. 

La mayor contribución de Gould a la ciencia fue la teoría del equilibrio puntuado que desarrolló con Niles Eldredge en 1972. La teoría propone que la mayoría de los procesos evolutivos están compuestos por largos períodos de estabilidad, interrumpidos por episodios cortos y poco frecuentes de bifurcación evolutiva. La teoría contrasta con el gradualismo filogenético, la idea generalizada de que el cambio evolutivo se caracteriza por un patrón homogéneo y continuo.

La mayor parte de la investigación empírica de Gould se basó en los géneros de caracoles terrestres "Poecilozonites" y "Cerion" y además contribuyó a la biología evolutiva del desarrollo. En su teoría evolutiva se opuso al seleccionismo estricto, la sociobiología aplicada a seres humanos y la psicología evolucionista. Hizo campaña contra el creacionismo y propuso que la ciencia y la religión sean considerados dos ámbitos distintos, o «magisterios», cuyas autoridades no se superponen ("Non overlapping magisteria" en el original).

Muchos de los ensayos de Gould para la revista "Natural History" fueron reimpresos en libros entre los que sobresalen "Desde Darwin" y "El pulgar del panda". Sus tratados más populares incluyen libros como "La falsa medida del hombre", "La vida maravillosa" y "La grandeza de la vida". Poco tiempo antes de su muerte, Gould publicó un largo tratado recapitulando su versión de la teoría evolutiva moderna llamado "La estructura de la teoría de la evolución" (2002).

Gould nació y se crio en la comunidad de Bayside, un tranquilo barrio ubicado en Queens, Nueva York. Su padre Leonard trabajaba de taquígrafo en los juzgados, y su madre Eleanor era artista. Cuando Gould tenía cinco años de edad, su padre lo llevó a la sala de los dinosaurios del Museo Americano de Historia Natural, donde se encontró por primera vez ante un "Tyrannosaurus rex". «No tenía idea de que hubiera tales cosas; estaba asombrado», recordaba Gould. En ese momento decidió convertirse en paleontólogo.

Criado en un hogar secular judío, Gould no practicó ninguna religión y prefería ser considerado agnóstico. A pesar de que «había sido criado por un padre marxista», afirmó que las ideas políticas de su padre eran «muy diferentes» de las suyas. Con respecto a sus opiniones políticas dijo que «tendían a ser de centro-izquierda». Según Gould, los libros de política más influyentes que leyó fueron "La élite del poder" de C. Wright Mills y los escritos políticos de Noam Chomsky.

Cuando asistía al Antioch College en la década de 1960, Gould participó en el movimiento de derechos civiles y con frecuencia hizo campaña por la justicia social. En la Universidad de Leeds, siendo estudiante visitante, organizó manifestaciones semanales contra un salón de baile de Bradford que se negaba a admitir a negros. Gould continuó estas manifestaciones hasta que esa política fue revocada. A lo largo de su carrera y en sus escritos denunció la opresión cultural en todas sus formas, sobre todo lo que él veía como pseudociencia usada al servicio del racismo y el sexismo.

Gould se casó dos veces. Su primer matrimonio fue con la artista Deborah Lee, en 1965, a quien conoció cuando ambos estudiaban en el Antioch College, y con la que tuvo dos hijos, Jesse y Ethan. Su segundo matrimonio fue en 1995 con la artista y escultora Rhonda Roland Shearer, que es la madre de Jade y London Allen, hijastros de Gould.

En julio de 1982, Gould fue diagnosticado con mesotelioma peritoneal, una forma mortal de cáncer que afecta el revestimiento abdominal y que se encuentra con frecuencia en personas que han estado expuestas al amianto. Después de dos años de difícil recuperación, Gould publicó una columna para la revista "Discover," titulada «The median isn't the message» («La mediana no es el mensaje»), que habla de su reacción al descubrir que los pacientes con mesotelioma tenían una esperanza de vida mediana de sólo ocho meses después del diagnóstico. A continuación, describe el verdadero significado detrás de este número y su alivio al darse cuenta de que los promedios estadísticos son útiles abstracciones y no abarcan toda la gama de variación.

La mediana es el punto medio en estadística lo que significa que el 50 % de los pacientes mueren antes de los 8 meses, pero la otra mitad vivirá posiblemente mucho más tiempo. Necesitaba determinar dónde se localizaban sus características personales dentro de este conjunto de posibilidades. Teniendo en cuenta que su cáncer fue detectado a tiempo, y el hecho de que era joven, optimista y tuvo los mejores tratamientos disponibles, Gould imaginó que debía estar en la mitad favorable del rango superior estadístico. Después de un tratamiento experimental de radiación, quimioterapia y cirugía, Gould consiguió una recuperación completa y su columna se convirtió en fuente de inspiración para muchos pacientes de cáncer.

Gould fue también un defensor de la marihuana medicinal. Durante su lucha contra el cáncer, fumó esta droga para aliviar las náuseas asociadas a sus tratamientos médicos. Según Gould, el uso de la marihuana tuvo un «efecto muy importante» en su eventual recuperación. En 1998 fue testigo en el caso de Jim Wakeford, un usuario y activista de la marihuana medicinal canadiense.

Sus ensayos científicos para "Natural History" aluden con frecuencia a sus pasatiempos e intereses no científicos. Cuando era niño coleccionaba cromos de béisbol y siguió siendo un ferviente seguidor de este deporte durante toda su vida. De adulto le gustaban las películas de ciencia ficción, pero a menudo se lamentaba por su mediocridad (no sólo en su presentación de la ciencia, sino por sus argumentos). Otras aficiones incluían cantar en el coro Boston Cecilia, y era un gran fanático de las operetas de Gilbert y Sullivan. Coleccionaba libros raros y antiguos. A menudo viajaba a Europa y hablaba francés, alemán, ruso e italiano y admiraba la arquitectura renacentista. Cuando hablaba de la tradición judeocristiana, se refería a ella simplemente como «Moisés» y solía aludir con pesar sobre su tendencia a engordar.

Gould murió el 20 de mayo de 2002 de una metástasis de adenocarcinoma de pulmón, una forma de cáncer que se había extendido a su cerebro. Sin embargo, no estaba relacionado con su cáncer abdominal, del que se había recuperado plenamente veinte años antes. Murió en su casa, «en una cama puesta en la biblioteca de su "loft" del SoHo, rodeado por su esposa Rhonda, su madre Eleanor y los muchos libros que amaba».

Gould comenzó sus estudios superiores en el Antioch College, donde se graduó con doble especialidad en 1963 en geología y filosofía. Durante ese tiempo, también estudió fuera de su país, en la Universidad de Leeds del Reino Unido. Después de completar sus estudios de posgrado en la Universidad de Columbia en 1967 bajo la dirección de Norman Newell, fue contratado de inmediato por la Universidad de Harvard, donde trabajó hasta el final de su vida (1967-2002). En 1973, Harvard le ascendió a profesor de geología y conservador de paleontología de invertebrados en el Museo de Zoología Comparada de Harvard, cargos en los que permaneció hasta su fallecimiento en 2002.

En 1982, la Universidad de Harvard le otorgó el título honorífico de profesor Alexander Agassiz de zoología. Al año siguiente, en 1983, le fue otorgada una beca de posgrado de la Asociación Americana para el Avance de la Ciencia, («AAAS» son sus siglas en inglés) donde más tarde desempeñó el cargo de presidente (1999-2001). El comunicado de prensa de la AAAS señalaba sus «numerosas contribuciones tanto a los avances científicos como a la comprensión pública de la ciencia». También ocupó el cargo de presidente de la Sociedad de Paleontología (1985-1986) y el de vicepresidente en la Sociedad para el Estudio de la Evolución (1990-1991).

En 1989, Gould fue aceptado como miembro de la Academia Nacional de Ciencias. Entre 1996 y 2002 fue profesor investigador visitante Vincent Astor de biología en la Universidad de Nueva York y en 2001, la Asociación Humanista Americana lo nombró humanista del año por su trabajo. En 2008, se le concedió póstumamente la medalla Darwin-Wallace, además de a otros doce científicos. Hasta 2008, esta medalla era otorgada cada cincuenta años por la Sociedad Linneana de Londres.

Al principio de su carrera Gould y Niles Eldredge desarrollaron la teoría del equilibrio puntuado, que propone que los cambios evolutivos se producen con relativa rapidez, alternando con períodos más largos de relativa estabilidad, como parece deducirse de la escasez de formas intermedias encontradas en el registro fósil. Según Gould, el equilibrio puntuado modifica un pilar fundamental «en la lógica central de la teoría darwiniana». Algunos biólogos evolutivos han argumentado que, si bien el equilibrio puntuado fue «de gran interés para la biología», se limitaba a modificar el neo-darwinismo de una manera que era plenamente compatible con lo que se conocía anteriormente. Otros, sin embargo, resaltaron su novedad teórica y argumentaron que el estancamiento evolutivo había sido «inesperado por la mayoría de los biólogos evolucionistas» y «tuvo un gran impacto en la paleontología y la biología evolutiva».

Existieron también críticos que, en tono de broma, calificaron la teoría como «evolución a tropezones», lo que llevó a Gould a describir el gradualismo como «evolución por arrastre».

Gould hizo importantes contribuciones a la biología evolutiva del desarrollo, especialmente en su obra "Ontogenia y filogenia." En este libro hizo hincapié en el proceso de heterocronía, que comprende dos procesos distintivos: pedomorfosis y adiciones terminales. Pedomorfosis es el proceso donde la ontogenia se ralentiza y el organismo no alcanza el final de su desarrollo, mientras que la adición terminal es el proceso por el cual un organismo se desarrolla acelerando y acortando etapas tempranas del proceso de desarrollo. La influencia de Gould en este campo sigue viva en áreas de investigación como la evolución de las plumas.

Gould defendió las restricciones biológicas, como las limitaciones de las vías del desarrollo en los resultados evolutivos, así como otras fuerzas no selectivas de la evolución. Por ejemplo, consideraba muchas de las funciones superiores del cerebro humano como consecuencias secundarias imprevistas o subproductos de la selección natural, en lugar de adaptaciones directas. Para describir tales características acuñó, junto a Elisabeth Vrba el término «exaptación». Gould pensaba que esta interpretación socava una premisa esencial de la sociobiología humana (el determinismo biológico) y la psicología evolucionista.

En 1975, E. O. Wilson presentó su análisis de la conducta humana desde el punto de vista de la sociobiología. En respuesta, Gould, Richard Lewontin y otros científicos de Boston escribieron una carta que tuvo gran repercusión posteriormente, al "New York Review of Books" titulada "Contra la «sociobiología»". Esta carta abierta criticaba la «visión determinista de la sociedad y acción humanas» de Wilson.

Gould sin embargo no descartó las explicaciones sociobiológicas para muchos aspectos del comportamiento animal; así escribió: «Los sociobiólogos han ampliado su gama de explicaciones por selección mediante la invocación de los conceptos de eficacia biológica inclusiva y la selección de parentesco para resolver (con éxito creo) el molesto problema del altruismo —anteriormente el mayor obstáculo para una teoría darwiniana de la conducta social. [...] Aquí la sociobiología ha tenido y seguirá teniendo éxito. Y aquí le deseo lo mejor, ya que representa una extensión del darwinismo básico en un ámbito donde debe aplicarse».

Con Richard Lewontin, Gould escribió un influyente trabajo en 1979 titulado "Las enjutas de San Marcos y el paradigma panglossiano", que introdujo el término de arquitectura «enjuta» en la biología evolutiva. En arquitectura, una enjuta es una zona curva de mampostería que existe entre los arcos de apoyo de una cúpula. Las enjutas, también llamadas pechinas en este contexto, se encuentran sobre todo en iglesias góticas.

Cuando visitaba Venecia en 1978, Gould se dio cuenta que las enjutas de la Basílica de San Marcos, aunque eran muy hermosas, no eran espacios proyectados por el arquitecto. Más bien los espacios surgieron como «subproductos arquitectónicos inevitables al montar una cúpula sobre arcos de medio punto». Por eso Gould y Lewontin definieron «enjutas» en el ámbito de la biología evolutiva como cualquier característica biológica de un organismo que surge como una consecuencia secundaria e inevitable de otras características; es decir, que no es directamente producto de la selección natural. Algunos ejemplos incluirían los «genitales masculinizados de las hienas hembra, el uso exaptativo de un ombligo como cámara de incubación por los caracoles, la joroba del ciervo gigante irlandés y varias características clave de la mentalidad humana».

En "Cándido" de Voltaire, el Dr. Pangloss es retratado como un sabio despistado que a pesar de las pruebas dice que «todo está mejor en este, que es el mejor de los mundos posibles». Gould y Lewontin afirmaron que es panglossiano que los biólogos evolucionistas vean todos los rasgos como cosas atomizadas que han sido seleccionadas de forma natural y criticaron a los biólogos por no conceder espacio teórico a otras causas, tales como restricciones filogenéticas y del desarrollo. La frecuencia relativa de las enjutas así definidas, frente a las características adaptadas por la naturaleza, sigue siendo un tema polémico en biología evolutiva. Un ejemplo ilustrativo del enfoque de Gould se puede encontrar en un estudio de Elisabeth Lloyd que considera el orgasmo femenino como un subproducto de compartir vías de desarrollo. Gould también escribió sobre este tema en su ensayo "Pezones masculinos y ondas clitorídeas", impulsado por el trabajo anterior de Lloyd.

Gould fue partidario de que la evolución no tiene una tendencia inherente hacia el progreso a largo plazo. A menudo hay comentarios que presentan la evolución como una escalera de progreso que conduce hacia organismos más grandes, más rápidos y más inteligentes, en el supuesto de que la evolución impulsa de algún modo a los organismos a ser más complejos y en última instancia más parecidos a la especie humana. Gould argumenta que el camino de la evolución no fue hacia la complejidad, sino hacia la diversificación. Como la vida estaba obligada a comenzar desde un punto de partida simple, cualquier diversidad resultante en este paseo aleatorio sería percibida en la dirección de mayor complejidad. Pero la vida, argumenta Gould, se puede adaptar fácilmente a la simplificación, como suele ser en el caso en los parásitos.

En una reseña de "La grandeza de la vida", Richard Dawkins aprobó el argumento general de Gould, pero propuso que había visto pruebas de «una tendencia en los linajes a mejorar de forma acumulativa su eficacia adaptativa a su particular forma de vida, aumentando el número de características que se combinan en adaptaciones complejas. [...] Según esta definición, la evolución por adaptación no es progresiva por casualidad, sino que es profunda, recalcitrante e imprescindiblemente progresiva».

Gould nunca abrazó la cladística como método de investigación de líneas y procesos evolutivos, posiblemente porque le preocupaba que esas investigaciones le llevasen a descuidar detalles de la biología histórica, que consideraba de suma importancia. A principios de la década de 1990 esto le llevó a un debate con Derek Briggs, que había comenzado a aplicar técnicas cuantitativas cladísticas a los fósiles del yacimiento conocido como Esquisto de Burgess, acerca de los métodos que se debían utilizar en la interpretación de esos fósiles. Por esa época la cladística se convirtió rápidamente en el método predominante de clasificación en la biología evolutiva. Ordenadores personales baratos pero cada vez más potentes hicieron posible procesar grandes cantidades de datos acerca de los organismos y sus características. Casi al mismo tiempo el desarrollo de técnicas efectivas de reacción en cadena de la polimerasa también hizo posible la aplicación de métodos de análisis cladístico a los rasgos bioquímicos.

La mayor parte de la investigación empírica de Gould está relacionada con los caracoles terrestres. Centró sus primeros trabajos en el género "Poecilozonites" de las Bermudas y posteriormente en el género "Cerion" del Caribe. Según Gould «el "Cerion" es el caracol terrestre con mayor diversidad de forma de todo el mundo. Hay 600 especies descritas de este género. De hecho no son realmente especies ya que todos ellos se cruzan, pero los nombres existen para expresar esta diversidad morfológica increíble. Algunos tienen forma de pelotas de golf, otros de lápices. [...] Ahora bien, mi interés principal es la evolución de la forma y el problema de cómo puede alcanzarse esa diversidad con tan pocas diferencias genéticas es muy interesante. Y si pudiésemos resolver esto aprenderíamos algo general sobre la evolución de la forma».

Dada la extensa diversidad geográfica del "Cerion", Gould posteriormente lamentó que si Cristóbal Colón hubiese catalogado un único "Cerion" se habría terminado el debate académico sobre cuál fue la isla en la que Colón puso el pie en América por vez primera.

La interpretación de Gould sobre los fósiles del período Cámbrico hallados en las lutitas de Burgess que figura en su libro "La vida maravillosa" enfatiza la chocante disparidad morfológica (o «rareza») de dicha fauna; y el rol del azar, que determina cuántos miembros de ella sobrevivirán y cuántos desaparecerán. El científico utilizó las formas de vida de este período como ejemplo del papel que juegan las circunstancias en el amplio patrón de la evolución. Tras una serie de estudios basados en la comparación entre trilobites y moluscos modernos, Gould y Eldredge elaboraron la alternativa al gradualismo, el «saltacionismo», que indica que las especies se transforman rápidamente para luego permanecer invariables durante largo tiempo. Estos estudios permitieron a Gould comprender que «la evolución [...] es la adaptación a los ambientes cambiantes, no progreso».

Este punto de vista fue criticado por Simon Conway Morris en su libro de 1998 "El crisol de la creación", cuyo título original en lengua inglesa es "The Crucible of Creation". También promovió la teoría de la evolución convergente en cuanto mecanismo que produce formas similares en circunstancias ambientales similares, y en un libro posterior sostuvo que la aparición de animales semejantes al hombre es probable. Los paleontólogos Derek Briggs y Richard Fortey han discutido también que una importante parte de la fauna cámbrica puede considerarse como grupos madre de los taxones existentes, aunque esto todavía es materia de investigación y debate, y la relación entre varios taxones cámbricos y los "phyla" modernos no se estableció aún a los ojos de muchos otros paleontólogos.

Fortey ha notado que antes de la publicación de "La vida maravillosa", Conway Morris compartía muchas de las opiniones de Gould, sin embargo, tras su lanzamiento, este último revisó su postura y adoptó una posición más progresista en el contexto de la historia de la vida.

A diferencia de la teoría del equilibrio puntuado, la teoría jerárquica tiene un alcance causal, no sólo fenomenológico. La teoría jerárquica de la evolución generaliza la teoría de la selección natural a unidades evolutivas distintas del organismo: la selección de linajes celulares, la clásica selección organísmica, la selección de grupos o demes, de especies e incluso de clados. En este sentido, Gould sostiene que la teoría jerárquica no trata de reemplazar sino de extender la teoría de Darwin. Según la teoría jerárquica, la evolución es el resultado de la interacción simultánea de distintos niveles que pueden coincidir pero también entrar en conflicto.

Para que un objeto biológico sea una unidad de selección ha de tener cinco propiedades fundamentales: puntos de nacimiento y de muerte, estabilidad suficiente a lo largo de su existencia, reproducción y herencia de rasgos parentales por descendencia. Las tres primeras propiedades son necesarias para distinguir a las unidades dentro de un continuo, mientras que las dos últimas son necesarias para que sean consideradas agentes de la selección natural, definida como éxito reproductivo diferencial.

La consideración de un individuo como individuo evolutivo es relativa, dependiendo del nivel de análisis en el que nos situemos en cada caso, según lo que expresó Gould en "La estructura de la teoría de la evolución":

Gould es uno de los científicos más citados en el campo de la teoría evolutiva. Su trabajo de 1979 sobre las «enjutas» ha sido citado más de 3000 veces. En "Palaeobiology", la revista insignia de su propia especialidad, sólo Charles Darwin y G. G. Simpson han sido citados más cantidad de veces. Gould fue también un historiador de la ciencia bastante respetado. El historiador Ronald Numbers ha afirmado: «No puedo decir mucho acerca de los puntos fuertes de Gould como científico, pero durante mucho tiempo lo he considerado el segundo historiador de la ciencia más influyente (junto a Thomas Kuhn)».

Gould se hizo conocido a través de sus ensayos de divulgación científica en la revista "Natural History" y fue autor de varios libros sobre la evolución. Sus tratados más populares incluyen libros como "La falsa medida del hombre", "El pulgar del panda", "La vida maravillosa" y "Desde Darwin". Su tratado «A Biological Homage to Mickey Mouse» explica cómo los dibujantes aprovechan la fisionomía de los bebés, cabeza y ojos grandes, que les dan un aspecto que incitan a la ternura y afecto, observaciones ya realizadas por Konrad Lorenz.

Fue un apasionado defensor de la teoría evolutiva escribiendo prolíficamente sobre el tema y tratando de comunicar su comprensión de la biología evolutiva contemporánea a un público amplio. Un tema recurrente en sus escritos es la historia y el desarrollo de la evolución y el pensamiento antes de la formulación de la teoría evolutiva. También fue un aficionado entusiasta del béisbol; hizo a menudo referencias a este deporte en sus ensayos. Muchos de sus ensayos de béisbol fueron recopilados y publicados póstumamente en "Triumph and Tragedy in Mudville" (2003).

Aunque un orgulloso darwinista, era menos reduccionista que la mayoría de los neo-darwinistas. Se opuso firmemente a muchos aspectos de la sociobiología y su descendiente intelectual, la psicología evolucionista. Dedicó mucho tiempo a la lucha contra el creacionismo y conceptos relacionados como la ciencia de la creación y la teoría del diseño inteligente. Gould desarrolló más tarde el término «magisterios no superpuestos» ("non-overlapping magisteria", NOMA, en inglés) para describir como, en su opinión, la ciencia y la religión no pueden hacer comentarios sobre el ámbito del otro. Gould llegó a desarrollar esta idea detalladamente en el libro "Ciencia versus religión, un falso conflicto" (1999) y "Érase una vez el erizo y el zorro" (2003). En un ensayo de 1982 de "Natural History" Gould escribió:

Gould se convirtió en un rostro conocido de la ciencia para el público general, apareciendo a menudo en series y programas de televisión como "NOVA", el documental "Baseball" de Ken Burns, "Evolution" (programas de la cadena de televisión PBS), "Crossfire" (de la CNN) y otros. En 1997 prestó su voz a una versión de sí mismo en un episodio de la serie de dibujos animados "Los Simpson".

Gould recibió muchos elogios por su trabajo académico y exposición popular de la historia natural, pero no fue inmune a las críticas por parte de aquellos en la comunidad científica que sentían que sus presentaciones públicas, por varias razones, estaban alejadas de la teoría evolutiva dominante. Los debates públicos entre los partidarios y detractores de Gould han sido tan belicosos que varios comentaristas los han llamado «las guerras de Darwin».

John Maynard Smith, un eminente biólogo evolutivo británico, estaba entre los más duros críticos de Gould. Maynard Smith pensaba que este no tenía en cuenta lo suficiente la función vital que la adaptación desempeña en biología, y también criticaba la aceptación de Gould de la selección al nivel de especies como un componente importante de la evolución biológica. En una reseña del libro de Daniel Dennet, "La peligrosa idea de Darwin", Maynard Smith escribió que Gould «está dando a los no biólogos en gran parte una falsa imagen del estado de la teoría evolutiva». Pese a estas discrepancias, escribió en una reseña del "El pulgar del panda" que «Stephen Gould es el mejor escritor de ciencia popular [...] A menudo me enfurece, pero espero que vuelva al ataque con la redacción de ensayos como éstos». El británico estaba también entre aquellos que agradecieron los trabajos de paleontología de Gould.

Una de las razones de esas críticas era que Gould parecía presentar sus ideas como una forma revolucionaria de comprender la evolución y argumentaba sobre por la importancia de otros mecanismos aparte de la selección natural, mecanismos que pensaba habían sido ignorados por muchos evolucionistas profesionales. El resultado fue que muchas personas no especialistas a veces dedujeron por sus primeros escritos que se había demostrado que las explicaciones de Darwin no eran científicas (algo que Gould nunca intentó dar a entender). Junto a muchos otros investigadores de ese campo las obras de Gould a veces son deliberadamente puestas fuera de contexto por los creacionistas que pretenden demostrar que los científicos ya no entienden cómo evolucionaron los organismos. Gould corrigió algunas de estas malas interpretaciones y tergiversaciones de sus escritos en obras posteriores.

Gould y Dawkins también expresaron su desacuerdo sobre la importancia de la selección genética en la evolución. Dawkins argumenta que la evolución se entiende mejor como competencia entre genes (o replicadores), mientras que Gould defendió la importancia de varios niveles de competencia, incluyendo la selección entre los genes, linajes celulares, organismos, grupos ("demes"), especies y clados. Críticas a Gould y su teoría del equilibrio puntuado se pueden encontrar en el capítulo 9 de "El relojero ciego" de Dawkins y el capítulo 10 de "La peligrosa idea de Darwin" de Dennett. Dawkins posteriormente hizo una concesión a través de una nota al final en una nueva edición de su libro "El gen egoísta", donde dice:

Gould también tuvo una enemistad pública durante largo tiempo con E. O. Wilson y otros biólogos evolucionistas sobre la sociobiología humana y su descendiente posterior la psicología evolucionista (a la que Gould, Lewontin y Maynard Smith se oponían, pero que Richard Dawkins, Daniel Dennett y Steven Pinker defendían. Estos debates alcanzaron su clímax en la década de 1970, e incluyeron una fuerte oposición de instituciones como el Grupo de estudio de la sociobiología y Ciencia para el Pueblo. Pinker acusó a Gould, Lewontin y otros oponentes de la psicología evolucionista de ser «científicos radicales», cuya postura sobre la naturaleza humana está influenciada por la política en lugar de la ciencia. Gould afirmó que «no vio ningún motivo detrás de Wilson o cualquier otro», pero advirtió que todos los seres humanos son influenciados, sobre todo inconscientemente, por sus expectativas y prejuicios personales. Escribió:

La crítica principal de Gould era que las explicaciones sociobiológicas sobre el hombre carecían de pruebas, y argumentaba que las conductas adaptativas se suponen genéticas frecuentemente por ninguna otra razón que su pretendida universalidad o naturaleza adaptativa. Gould hizo hincapié en que las conductas adaptativas pueden ser transmitidas también a través de la cultura, y ambas hipótesis son igualmente plausibles. Gould no negaba la importancia de la biología en la naturaleza humana, pero reformuló el debate como «potencialidad biológica contra determinismo biológico». Afirmaba también que el cerebro humano permite una amplia gama de comportamientos. Su flexibilidad «nos permite ser agresivos o tranquilos, dominantes o sumisos, rencorosos o generosos [...] La violencia, el sexismo y la maldad generalizada "son" biológicos, ya que representan un subconjunto de un posible rango de comportamientos. Pero la paz, la igualdad y la bondad son igual de biológicos —y podríamos ver aumentada su influencia si podemos crear estructuras sociales que les permitan prosperar».

Gould es el autor de "La falsa medida del hombre" (1981), una investigación histórica sobre la psicometría y los tests de inteligencia. Gould investigó los métodos de la craneometría del siglo XIX, como así la historia de los tests psicológicos. El paleontólogo afirmaba que estas dos teorías se desarrollaron sobre la base de una infundada creencia en el determinismo biológico, en el punto de vista por el cual «las diferencias económicas y sociales entre los grupos humanos (razas, clases sociales y sexos) [se dan por] distinciones heredadas e innatas y la sociedad, en ese sentido, es una exacta réplica de la biología».

El libro ha sido reeditado en 1996 y se le añadió un prólogo y una reseña de "The Bell Curve". "La falsa medida del hombre" ha sido tal vez el libro más controvertido de Gould. Ha recibido alabanzas, y una extensa serie de críticas negativas de un amplio número de psicólogos, incluso varios reclamos de malinterpretación.

En su libro de 1999, "Ciencia versus religión, un falso conflicto", Gould presenta lo que describe como una «benditamente simple y puramente convencional resolución [al] supuesto conflicto entre ciencia y religión». Define el término «magisterio» como «un dominio en el que una forma de enseñanza mantiene las herramientas adecuadas para elaborar un discurso significativo y [llegar a una] solución». El principio de magisterios no superpuestos ("Non overlapping magisteria" en el original) establece, por lo tanto, que el magisterio de la ciencia cubre «la esfera de lo empírico: de qué está formado el Universo (hecho) y por qué funciona de determinada manera (teoría). El magisterio de la religión se extiende sobre preguntas acerca del sentido último y asuntos morales. Estos dos magisterios no se superponen, ni abarcan todo lo que puede conocerse». Sugiere además que «[este principio] goza de un fuerte y explícito apoyo, aún de los estereotipos culturales más primarios del más estricto tradicionalismo» y que «es una sólida posición que merece el consenso general, instaurado tras una larga lucha entre gente de buena voluntad de ambos magisterios». Sin embargo, esta postura no estuvo exenta de críticas. En su libro "El espejismo de Dios" ("The God Delusion"), Richard Dawkins opina que esta división no es tan sencilla como parece, ya que muchas religiones están basadas en milagros que inciden en el magisterio científico.

Durante su vida, las publicaciones de Gould han sido numerosas. Una recapitulación de sus publicaciones desde el año 1965 al 2000 menciona 479 documentos, 22 libros, 300 ensayos y 101 reseñas de libros escritas por él mismo.

La siguiente es una lista de los libros escritos y/o editados por Gould, incluyéndose los publicados en forma póstuma en 2002. Aunque varias de estas obras fueron reimpresas posteriormente por muchas editoriales, esta enumeración mantiene la editorial y la fecha de publicación iniciales y los títulos en el idioma original.







</doc>
<doc id="29698" url="https://es.wikipedia.org/wiki?curid=29698" title="Islas Turcas y Caicos">
Islas Turcas y Caicos

Las Islas Turcas y Caicos (en inglés "Turks and Caicos Islands") son un territorio británico de ultramar dependiente del Reino Unido. Fueron administradas por Jamaica hasta el año de 1962 y luego pasaron a la administración de las islas Bahamas hasta 1973. En 1982 el Reino Unido le concedió la independencia a las islas, sin embargo, las islas prefirieron seguir como un territorio dependiente de la corona británica. Las islas son uno de los dieciséis territorios no autónomos bajo supervisión del Comité de Descolonización de las Naciones Unidas.

En nombre de las islas Turcas proviene de un cactus autóctono, la cabeza del turco (Melocactus intortus), cuya parte superior escarlata se asemeja a un fez. El nombre Caicos puede derivar de "caya hico", una frase que significa "cadena de islas" en el idioma del pueblo indígena Lucayan (arahuaco).

Las Islas Turcas y Caicos están al sureste de Mayaguana en las Bahamas y al norte de la isla Española, donde se encuentran Haití y la República Dominicana, en aguas del océano Atlántico. Cockburn Town, la capital, está a unos 1.042 kilómetros (647 millas) al este-sureste de Miami en los Estados Unidos. Las islas tienen una superficie total de 948 km . 

Desde el 17 de octubre de 2016 el Gobernador de las islas designado por la reina es John Freeman, quien actúa como jefe de Estado y supervisa las labores de un Consejo Legislativo compuesto por 17 miembros (15 elegidos por sufragio universal). El consejo legisla desde la ciudad de Cockburn Town, la capital del territorio. La jefa del gobierno actual es Sharlene Cartwright-Robinson. Su partido, el Partido Nacional Progresista obtuvo 13 de 15 escaños en las últimas elecciones. 

La economía de las islas se basa en el turismo, los servicios financieros offshore y la pesca. La mayoría de los bienes de capital y alimentos para el consumo interno son importados. Estados Unidos es la principal fuente de turistas, representando más de tres cuartas partes de los más de 1 millón de visitantes que llegan anualmente. Tres cuartas partes de los visitantes vienen en barco. Las principales fuentes de ingresos del gobierno también incluyen las tasas procedentes de actividades financieras extraterritoriales y los ingresos aduaneros. La moneda de curso legal es el dólar estadounidense y desde el 2 de julio de 1991 las Islas Turcas y Caicos son miembro asociado de la Comunidad del Caribe.

Las islas Turcas y Caicos estaban habitadas por el pueblo indígena Lucayan Taino, de la familia arahuaca. El diario de Cristóbal Colón indica que llegó a las islas en 1492. Luego, las islas fueron documentadas por el conquistador español Juan Ponce de León, en 1512. Durante los siglos XVI, XVII y XVIII, las islas pasaron del dominio español al francés y al británico, pero ninguna de las tres coronas hizo asentamientos.

Recolectores de sal de las Bermudas se asentaron en las Islas Turcas alrededor de 1680, mientras las islas estaban bajo ocupación francesa. A principios del siglo XVIII, las islas sirvieron de refugio a piratas. Después de la Revolución estadounidense (1775-1783), muchos británicos leales a la corona inglesa huyeron y se convirtieron en 1783 en los primeros pobladores europeos permanentes en las Islas Caicos. 

En 1799, tanto las Turcas como el grupo de islas Caicos fueron anexadas por Gran Bretaña como parte de las Bahamas.

En 1841, el "Trouvadore", un barco español usado para el tráfico ilegal esclavos, naufragó en la costa de Caicos del Este. Ciento noventa y dos africanos en cautividad sobrevivieron al hundimiento y llegaron a tierra, donde, como colonia británica, el comercio de esclavos era también ilegal desde 1834. Estos supervivientes fueron liberados e incorporados como aprendices de oficios durante un año y se instalaron principalmente en la Isla Gran Turco. En 2004, arqueólogos marinos redescubrieron un naufragio, llamado "Black Rock Ship" (Nave Roca Negra), que la investigación posterior ha sugerido que puede ser el "Trouvadore". Esta propuesta fue respaldada en una expedición de arqueología submarina financiado por la NOAA en noviembre de 2008, que confirmó que los restos de artefactos que comprende el estilo y la fecha de fabricación da apoyo a la asociación de estos restos con el del "Trouvadore". Los restos, sin embargo, no han sido identificados con certeza absoluta.

Desde 1873 las islas pertenecieron a la colonia de Jamaica. Cuando Jamaica obtuvo su independencia del Reino Unido en agosto de 1962, pasaron a depender de la colonia de Bahamas. Cuando las Bahamas obtuvieron la independencia en 1973, Turcos y Caicos recibieron su propio gobernador convirtiéndose en colonia y, más tarde, en Dependencia y Territorio Británico de Ultramar hasta la actualidad. En 1917 el primer ministro de Canadá, Robert Borden, sugirió la posibilidad de que Turcos y Caicos se incorporasen como una nueva provincia canadiense, pero la propuesta fue rechazada por los británicos. En 1980 los británicos llegaron a un acuerdo con el Movimiento Democrático Popular de Turcos y Caicos para concederles la independencia en 1982. Pero en las elecciones subsiguientes, que constituyeron al mismo tiempo un referéndum, ganó el Partido Nacional Progresivo, contrario a la misma, que rechazó el ofrecimiento.

Las islas están divididas en seis distritos administrativos (dos en las Islas Turcas y cuatro en las Islas Caicos), encabezados por los comisionados de distrito. Para la Cámara de la Asamblea, el archipiélago se divide en 15 distritos electorales (cuatro en las Islas Turcas y once en las Islas Caicos).

Los dos grupos de islas se ubican en el océano Atlántico Norte, al sudeste de las Bahamas, al norte de Isla Española, y cerca de 1.000 kilómetros de Miami en los Estados Unidos. El territorio es geográficamente contiguo a las Bahamas y comprende el archipiélago de las Lucayas, pero políticamente son entidades separadas. 

Las ocho islas principales y más de veinte islas más pequeñas tienen una superficie total de 948 kilómetros cuadrados, principalmente de bajos, de piedra caliza plana con extensos pantanos y manglares y 332 kilómetros cuadrados de frente a la playa. El clima es soleado y seco, pero sufre frecuentes huracanes. Las islas han limitado los recursos naturales de agua dulce; cisternas privadas recogen el agua de lluvia para beber. Los principales recursos naturales son la langosta, conchas y otros mariscos.

Los dos grupos de islas que conforman Turcas y Caicos están separados por el pasaje de los Turcos.

Las Islas Caicos son el grupo más grande, con casi el 96 % de la superficie terrestre 227.6 y el 82 % de la población (26.584 de un total de 33.302 en 2006). Las Islas Caicos, se encuentran alrededor del Banco Caicos, como un atolón, con las seis grandes islas en el oeste, norte y este, y algunos pequeños arrecifes y cayos en el sur. El banco Caicos es de piedra caliza y tiene una superficie de 6.140 km² / 2.370 millas cuadradas. La zona se compone de arena, los arrecifes de coral mixta de coral, las algas, y otros hábitats, normalmente a profundidades de 1 a 5 metros.



Las islas Turcas están divididas por dos distritos:

La población de las islas es de 55,926 personas (julio de 2020). 

La composición étnica es la siguiente:

· Afrodescendientes 87,6 %

· Blancos 7,9 %

· Mestizos 2,5 %

· Indios orientales 1,3 %

· Otros 0,7 % (2006)

El idioma oficial de las islas es el inglés, pero la población también habla el criollo de las Islas Turcas y Caicos, que es similar al criollo de las Bahamas. Debido a su proximidad a Cuba y a la Hispaniola, se han establecido en el territorio grandes comunidades criollas haitianas y de habla hispana, que llegaron tanto legal como ilegalmente, desde Haití llegó el criollo haitiano y desde Cuba y República Dominicana el idioma español.

El 86% de la población de las Islas Turcas y Caicos se considera cristiano (Bautistas 35,8%, Iglesia de Dios 11,7%, Católicos Romanos 11,4%, Anglicanos 10%, Metodistas 9,3%, Adventistas del Séptimo Día 6%, Testigos de Jehová 1,8%), otras creencias constituyen el 14% restante.

Los católicos son atendidos por la Misión "Sui Iuris" para las Islas Turcas y Caicos, que fue erigida en 1984 con territorio tomado de la entonces Diócesis de Nassau.

Turcas y Caicos es un territorio de ultramar británico, una posesión del Reino Unido, actualmente gobernada directamente por un gobernador británico designado. Su sistema de gobierno autónomo, fue suspendido en agosto de 2009. El Comité de Descolonización de las Naciones Unidas incluye el territorio en la lista de las Naciones Unidas de territorios no autónomos.

Con la primera elección popular en el del territorio, el Ministro Principal de las islas, JAGS McCartney, adoptó una constitución (1976), este día se celebra el 30 de agosto. de 1976, La Constitución fue suspendida en 1986, pero restaurada y revisada el 5 de marzo de 1988. Una nueva Constitución entró en vigor el 9 de agosto de 2006, pero fue suspendida en algunos apartes y modificada en el 2009. El sistema jurídico del territorio se basa en el derecho consuetudinario inglés, con un pequeño número de leyes aprobadas de Jamaica y las Bahamas. El sufragio es universal para los mayores de 18 años de edad. El inglés es el idioma oficial. Gran Turco, es la capital administrativa y política de las Islas Turcas y Caicos y Cockburn Town ha sido la sede de gobierno desde 1766.

Como un territorio británico, la reina Isabel II del Reino Unido es la soberana, representada por un gobernador nombrado por la monarca, con el asesoramiento de la Oficina de Asuntos Exteriores.

La asamblea tiene 19 escaños; 15 miembros en circunscripciones multiplaza y una sola circunscripción de todas las islas elegidas directamente por mayoría simple, 1 miembro designado por el primer ministro y nombrado por el gobernador, 1 nombrado por el líder del partido opositor y nombrado por el gobernador, y 2 de la Sociedad Cívica de las Islas Turcas y Caicos nombrados directamente por el gobernador.

La rama judicial del gobierno está a cargo de un Tribunal Supremo y las apelaciones son oídas por el tribunal de apelaciones y el recursos final por el Comité Judicial del Consejo Privado del Reino Unido. Las islas también tienen un Tribunal de Apelaciones, con un Presidente y al menos dos jueces de apelación.

Turcas y Caicos participan en el Banco de Desarrollo del Caribe, es un asociado de la CARICOM, y mantiene una suboficina de la Interpol. La defensa es la responsabilidad del Reino Unido. 

El partido ganador de las primeras elecciones generales de las Islas Turcas y Caicos en 1976, el Movimiento Popular Democrático (MPD) en virtud de McCartney, trató de establecer un marco y una infraestructura de acompañamiento en la búsqueda de la plena independencia para las islas. Sin embargo, con la trágica muerte de McCartney, la confianza en el liderazgo del país disminuyó. En 1980, el PDM de acuerdo con el gobierno británico de que la independencia se concedió en 1982 si el PDM fue reelegido en las elecciones de ese año. Esa elección fue efectivamente un referéndum sobre la cuestión de la independencia y fue ganado por el pro-dependencia Partido Nacional Progresista (PNP), que proclamó la victoria de nuevo cuatro años más tarde. Con estos acontecimientos, la cuestión de la independencia en gran parte desapareció de la escena política.

Sin embargo, a mediados de la década de 2000, la cuestión de la independencia de las islas se planteó de nuevo. En abril de 2006, el PNP primer ministro Michael Misick reafirmó que su partido se vio la independencia de Gran Bretaña como el "objetivo final" para las islas, pero no en el momento actual.

En 2008, los opositores de Misick lo acusaron de avanzar hacia la independencia de las islas con el fin de eludir una comisión de investigación, que examinó los informes de corrupción por la Administración Misick.

Un gran número de turistas que visitan Turcas y Caicos son canadienses. Debido a esto, la situación de las islas como una colonia británica, y los vínculos comerciales históricos, algunos políticos tanto en Canadá como en Turcas y Caicos han sugerido que alguna forma de unión debería existir entre Canadá y el territorio británico. En 1971, el primer ministro de Canadá, Robert Borden, fue el primero en sugerir que se anexaran a Canadá las Islas Turcas y Caicos. En 1974, Max Saltsman, diputado del distrito electoral de Waterloo-Cambridge en la Cámara de los Comunes por el Nuevo Partido Democrático de Canadá, presentó un intento fallido de la consolidación de las islas. La idea se planteó de nuevo en 1986 por iniciativa del diputado conservador Dan McKenzie, pero fue rechazada por el comité de su partido en los asuntos exteriores en 1987. El comité, presidido por el diputado David Daubney, consideró a la inmigración, la banca, la atención de la salud y cuestiones de turismo en la toma de su decisión. [cita requerida] Para que las islas puedan unirse a Canadá como una provincia, ello requeriría una enmienda a la Constitución de Canadá, ratificado por siete legislaturas provinciales que representen al menos la mitad de la población nacional. Debido a que esa medida podría atraer la atención a las provincias para exigir otros cambios a la Constitución, a cambio de ese apoyo, este es visto como una opción políticamente inviable. Teniendo en cuenta que los cambios menos radicales a la Constitución no requieren el consentimiento unánime. La última provincia admitida a la Confederación, Terranova y Labrador, fue anexada al país en 1949 por un acto del Parlamento británico. Participar como un territorio sería más fácil, pues estos pueden ser creados por una ley del Parlamento. Además, una población de alrededor de 30.000 personas se considera insuficiente para la categoría de provincia - la provincia más pequeña, Isla del Príncipe Eduardo, tiene cerca de 130.000 personas. Sin embargo, esta actitud podría cambiar si el territorio de Yukón, los Territorios del Noroeste y Nunavut, con cerca de 30.000 personas cada uno, fueran convertidos en provincias.

En 2004, el diputado conservador por Alberta Peter Goldring, visitó Turcas y Caicos para explorar la posibilidad una vez más. Se redactó una moción solicitando al Gobierno canadiense para examinar la cuestión, pero su partido se negó, citando a la inmigración, el turismo y las cuestiones económicas. Sin embargo, el gobierno canadiense no descarta la posibilidad de una futura unión.

La provincia de Nueva Escocia votó a favor de invitar a Turcas y Caicos a unirse a la provincia en 2004, las islas nunca deben convertirse en parte de Canadá. Esto evitaría la admisión de los problemas con Turcas y Caicos como una provincia separada.

El 2 de marzo de 2009, el Ottawa Citizen publicó un artículo en su sitio en línea de información de interés del gobierno de Canadá para abrir un puerto de aguas profundas en el Caribe que se abriría "un nuevo mercado para productos canadienses en el Caribe".

En apoyo de la década de 1990 para la integración en la provincia de Canadá como un "11" estaba en 90 %, mientras que en 2003 el apoyo a la integración se mantuvo en alrededor del 60 % en las Islas Turcas y Caicos. Goldring, un diputado conservador de Edmonton, ha defendido la causa de la integración de Turcas y Caicos como un territorio de Canadá para las prestaciones de seguridad, así como la creciente influencia de Canadá en América Central y América del Sur en cuanto a lucha contra el terrorismo, el comercio y la lucha contra la invasión influencia china en varias islas pequeñas del Caribe, como Santa Lucía.

En 2008, después de que miembros del parlamento británico realizaran una revisión rutinaria de la administración tras haber recibido varias denuncias de corrupción de alto nivel de funcionarios en Turcas y Caicos, el entonces gobernador Richard Tauwhare, anunció el nombramiento de una Comisión de Investigación sobre la corrupción. El mismo año, el primer ministro Michael Misick se convirtió en el foco de una investigación criminal después de una mujer identificada por los medios de noticias como un ciudadano estadounidense residente en Puerto Rico acusado de abusar sexualmente de ella, aunque él niega tajantemente la acusación.

El lunes 16 de marzo de 2009, el Reino Unido amenazó con suspender el autogobierno en las islas y la transferencia de poder al nuevo gobernador, Gordon Wetherell, sobre la corrupción sistémica.

El 18 de marzo de 2009, con el asesoramiento de sus ministros del Reino Unido, la Reina Isabel II emitió una Orden en Consejo da al Gobernador la facultad de suspender las partes de la Constitución de 2006 regula que con el gobierno ministerial y la Cámara de la Asamblea, y para ejercer los poderes del gobierno mismo. La orden, que podría también establecer un Consejo Consultivo y el Foro Consultivo, en lugar de la Asamblea, entrará en vigor en una fecha que será anunciada por el gobernador, y permanecerá en vigor durante dos años salvo que sea prorrogada o revocada.

El 23 de marzo de 2009, después de la investigación encontró pruebas de "alta probabilidad de que la corrupción sistémica o de otras formas graves de falta de honradez", Misick dimitió como primer ministro para dar paso a un nuevo gobierno unificado. Los políticos fueron acusados de la venta de tierras de la Corona para beneficio personal y mal uso de fondos públicos. Al día siguiente, Galmo Williams fue juramentado en su reemplazo. Misick negó todos los cargos, y se refirió al debate, el gobierno británico sobre la posibilidad de eliminar la soberanía del territorio como "equivalente a siendo re-colonizado. Es un paso hacia atrás completamente contrario a todo el movimiento de la historia.

El Aeropuerto Internacional de Providenciales es la principal puerta de entrada a Turcas y Caicos. Además, hay siete aeropuertos más, ubicados en cada una de las otras islas. Para ellos se cuentan con cinco pistas asfaltadas (tres de las cuales tienen aproximadamente 2.000 metros de longitud y una de ellas aproximadamente 1.000 metros), si bien hay otras dos más que están sin asfaltar (una de las cuales mide aproximadamente 1.000 metros de largo y la otra es significativamente más corta).

La isla tiene 121 kilómetros de vías rápidas, 24 km asfaltados y 97 km sin asfaltar.

Los principales puertos y embarcaderos del país están en Gran Turco y Providenciales.

Las islas no cuentan con un sistema ferroviario digno de mención.

La educación es gratuita y obligatoria para niños de cinco a dieciséis años. La educación primaria dura seis años y la enseñanza secundaria tiene una duración de cinco años. En la década de 1990, la isla puso en marcha la primaria en el servicio docente Education Project (PINSTEP) en un esfuerzo para aumentar las habilidades de sus maestros de escuela primaria, casi una cuarta parte de los cuales no estaban calificados. Turcas y Caicos ha trabajado para renovar las escuelas primarias, reducir los costos de libros de texto y equipo y suministros de aumento dado a las escuelas. Por ejemplo, en septiembre de 1993, a cada escuela primaria se le dieron suficientes libros para permitir a los maestros establecer en las bibliotecas de clases. En 2001, la relación alumno-profesor en la enseñanza primaria fue de aproximadamente 15 alumnos para un profesor. Las Islas Turcas y Caicos Community College ofrece educación superior a estudiantes que han completado con éxito su educación secundaria. El colegio de la comunidad también supervisa un programa de alfabetización de adultos. El Ministerio de Asuntos de Salud, Educación, Juventud, Deportes y de la Mujer supervisa la educación en Turcas y Caicos.


Turcas y Caicos aparece en la novela de Frederick Forsyth "Icon", que fue traducida en lengua castellana con el nombre de "El Manifiesto Negro". En la novela, el protagonista Jason Monk, antiguo espía de la CIA que había trabajado en la División Soviética, ya retirado, vive en estas islas como patrón de la embarcación "Foxy Lady".

El escritor Peter Benchley, más conocido por ser el autor del best-seller "Tiburón", además de co-guionista de la película del mismo nombre dirigida por Steven Spielberg en 1975, escribió en 1979 la novela "The Island", publicada en español con el título de "Isla", también ambientada en las islas Turcas y Caicos, sobre la misteriosa desaparición, basada en hechos reales, de una serie de personas y embarcaciones en la zona.

La saga #The Worricker trilogy, escrita y dirigida por #David Hare, consta de tres películas, el segundo de los cuales lleva por título, precisamente "Turks and Caicos". Fue filmada entre 2011 y 2014 y narra las andanzas de Johnny Worricker, un exagente del MI5.





</doc>
<doc id="29701" url="https://es.wikipedia.org/wiki?curid=29701" title="Edward Sapir">
Edward Sapir

Edward Sapir (1884-1939) fue un antropólogo-lingüista estadounidense. Es una de las figuras de referencia de la lingüística estructural, y uno de los creadores de la hipótesis de Sapir-Whorf. 

Nació en Lauenburg, Alemania en el seno de una familia judía ortodoxa que emigraría a EE. UU. a finales del siglo XIX. Fue discípulo del antropólogo Franz Boas, precursor del relativismo cultural, y profesor de Benjamin Whorf. Es considerado uno de los lingüistas más importantes de Estados Unidos, y ha influido sobre otros lingüistas importantes como Noam Chomsky.

Sapir fue profesor de la Universidad de Chicago y, más tarde, de la Universidad de Yale (1931-1939). Entre sus alumnos se destacan Li Fang-kuei, Benjamin Whorf, Mary Haas y Harry Hoijer; ejerció influencia en la Escuela de Chicago de sociología, fue amigo del psicólogo Harry Stack Sullivan, y consideró al semitólogo Zellig Harris su heredero intelectual. 

Estudioso de los lenguajes amerindios, fue uno de los primeros en investigar las relaciones entre el lenguaje y la antropología. En 1921 afirmó que el lenguaje determina el pensamiento, de forma que cada lengua lleva aparejada una forma de pensar. La idea de Sapir fue adoptada y desarrollada durante los años 1940 por Benjamin Whorf, y finalmente se convirtió en la hipótesis de Sapir-Whorf. 

Sapir dijo que la lengua es un método de comunicación de ideas, emociones y deseos por medio de símbolos producidos voluntaria y no instintivamente, por lo que el lenguaje es resultado de un proceso meramente cultural y social. 

Murió el 4 de febrero de 1939, de una afección cardíaca.





</doc>
<doc id="29702" url="https://es.wikipedia.org/wiki?curid=29702" title="Sionismo">
Sionismo

El sionismo es una ideología y un movimiento político nacionalista que propuso desde sus inicios el establecimiento de un Estado para el pueblo judío, preferentemente en la antigua Tierra de Israel ("Eretz Israel)". Dicho movimiento fue el promotor y responsable en gran medida de la fundación del Estado de Israel.

El sionismo apareció en Europa central y oriental a finales del siglo XIX. Su fundador en tanto que movimiento organizado fue el periodista austro-húngaro de origen judío Theodor Herzl como respuesta a la ola antisemita que recorrió Europa en esos años, uno de cuyos exponentes fue el affaire Dreyfus. El movimiento tuvo como objetivo fomentar la emigración judía a Palestina y alcanzó su objetivo principal con la fundación del Estado de Israel en 1948. 

El sionismo constituye una rama del fenómeno más amplio del nacionalismo moderno. Descrito como un "nacionalismo en la diáspora", el sionismo se autodefine como un movimiento de liberación nacional, cuyo objetivo es la libre autodeterminación del pueblo judío.

El término "sionismo" deriva de la palabra Sion (del , uno de los nombres bíblicos de Jerusalén). Este nombre se refiere inicialmente al Monte Sion, una montaña cerca de Jerusalén, y a la fortaleza de Sion en ella. Más tarde, durante el reinado del Rey David, el término "Sion" se convirtió en una sinécdoque para referirse a toda la ciudad de Jerusalén y a la Tierra de Israel. En muchos versículos bíblicos, los israelitas fueron llamados "el pueblo", "hijos o hijas de Sion".

"Sionismo" fue acuñado como término por el editor austriaco de origen judío Nathan Birnbaum, fundador del movimiento estudiantil judío "Kadima", en su diario "Selbstemanzipation" (Autoemancipación) en 1890.

Según los historiadores Walter Laqueur, Howard Sachar y Jack Fischel, entre otros, la etiqueta de "sionista" también se usa como un eufemismo para los judíos, en general, por apologistas del antisemitismo.

Desde el siglo I d. C. los judíos han vivido en el exilio, si bien ha habido una presencia constante de judíos en la Tierra de Israel («"Eretz Israel"»). De acuerdo con el judaísmo y la Biblia, la Tierra de Israel o Sion es la Tierra Prometida por Dios para los judíos. Tras la rebelión de Bar Kojba (132–135), los judíos fueron expulsados de la Tierra de Israel formando la Diáspora judía. Durante siglos existió entre los judíos de la Diáspora una gran nostalgia de origen religioso de retornar a la patria histórica del pueblo judío –«el año próximo, en Jerusalén...»– que, a mediados del siglo XIX, comenzó a secularizarse al entrar en contacto con las grandes corrientes ideológicas europeas de la época (liberalismo, socialismo, nacionalismo).

El nacimiento del sionismo está ligado a la eclosión de los nacionalismos en el siglo XIX europeo, que tuvieron como bandera común la idea «un pueblo, un Estado» y que está en el origen del concepto de Estado-nación. Al calor de esa idea se formaron distintos Estados europeos, surgidos del desmembramiento de los imperios o bien a través de la unificación de Estados con similar cultura y lengua (como Italia y Alemania). En paralelo a ese desarrollo nacionalista, atravesándolo en muchas ocasiones, se desarrolló el moderno antisemitismo.

El sionismo sostenía que los judíos eran primordialmente un grupo nacional (como los polacos o los alemanes) y no un grupo religioso (como los musulmanes o los católicos) y que, como tal, tenía derecho a crear su propio Estado en su territorio histórico. La formulación clásica de la idea es la que hizo Theodor Herzl en su opúsculo "Der Judenstaat" ("El Estado Judío", publicado en Berlín y Viena en 1896), que tiene como precedentes doctrinales la obra de Moses Hess "Roma y Jerusalén" (1860) y la del médico judío ruso Leo Pinsker "Autoemancipación" (1882), que contiene ya la consigna «Ayudaos, que Dios os ayudará». 

El sionismo se puso como objetivo primario la creación de un Estado judío moderno, considerando que con ello devolvía al pueblo judío su estatus de nación y pondría fin a dos milenios de vida en el exilio. Ante las grandes dificultades a las que se enfrentaron los judíos para establecerse en la antigua Tierra de Israel, se barajaron algunas alternativas temporales, sin intención de establecer un Estado nacional, solo como refugio ante la ola de pogromos y persecuciones en la Rusia zarista, como la Argentina —en la que se crearon numerosas colonias de inmigrantes judíos europeos—, y aún en una porción del África Oriental Británica (el conocido como Plan Uganda), ofrecida por el gobierno de Londres; estas fueron estudiadas (el propio Herzl estudia en su obra las ventajas comparativas de Argentina y Palestina) y al fin rechazadas por la dirigencia sionista, y se prefirió el establecimiento del futuro Estado en Palestina, una región en manos del Imperio Turco y que no se correspondía a ninguna división administrativa, por lo que sus límites no estaban establecidos. 

En paralelo a estas ideas, se fueron produciendo sucesivas oleadas migratorias (llamadas "Aliyá") de muchos jóvenes obreros y estudiantes, que escapaban en su mayoría del antisemita ambiente ruso y dispuestos a levantar la antigua patria judía basándose en dos ejes: el trabajo agrícola y la resurrección de la lengua hebrea, que dejó de hablarse alrededor del siglo I a.C., aunque siguió utilizándose en la literatura y, sobre todo, en la liturgia y con propósitos académicos.

La inmigración judía a Eretz Israel se inició en 1882. La denominada Primera Aliyá vio la llegada de alrededor de 35.000 judíos en el término de unos veinte años. La mayoría de los inmigrantes procedían de Rusia, donde el antisemitismo era rampante. Ellos fundaron una serie de asentamientos agrícolas con el apoyo financiero de filántropos judíos de la Europa occidental. 

La Segunda Aliyá comenzó en 1904. Otras Aliyot, cada vez con más inmigrantes, se sucedieron entre las dos guerras mundiales, impulsadas en la década de 1930 por la persecución nazi. Siguen llegando inmigrantes a Israel, en especial desde la antigua Unión Soviética.

La Declaración Balfour de 1917 apoyó la creación de una Patria Judía en el Mandato Británico de Palestina. En 1922, la Sociedad de Naciones hizo suya la declaración formulada en el mandato que dio a Gran Bretaña:

A lo largo del siglo XX el sionismo fue ganando adeptos gradualmente, y después del Holocausto se transformó en el movimiento predominante dentro del mundo judío. Por otra parte, la aparición de un proyecto de nación territorializada inicialmente similar, puesto en marcha desde 1928 por el régimen soviético, la República Autónoma Hebrea, que devino en fracaso a mediados de la década de 1930, resultó no presentar el suficiente atractivo como para provocar una emigración masiva o estable.

El proyecto de un nuevo Israel en Palestina fructificó por varios factores:


Los objetivos del sionismo fueron puestos en práctica por la Organización Sionista Mundial (órgano político del Movimiento Sionista), fundada en 1897 en Basilea por Theodor Herzl, considerado el padre del sionismo en general y de la rama política en particular (otras ramas son: el sionismo socialista, el sionismo revisionista, el sionismo religioso, etc.).

Hasta el Holocausto, la idea sionista compitió con otra corriente igual de extendida, sobre todo en Estados Unidos y la Europa occidental, que no consideraba a los judíos como pueblo, sino como minoría religiosa que debía integrarse y luchar por su plena igualdad en las sociedades en las que vivían. Una forma extrema de esta última idea preconizaba incluso la renuncia a la religión judía.

El sionismo conjuga dos elementos: independencia y soberanía, por un lado, y la centralidad de Israel en la identidad judía, por el otro. Los objetivos del movimiento sionista están delineados en el Programa de Jerusalén, cuya última versión, del 2004, dice:

Los objetivos del sionismo son:

A través de los años una variedad de escuelas de pensamiento ha evolucionado con diferentes escuelas que predominaron en diferentes momentos. Además los sionistas procedían de una amplia variedad de orígenes y, en ocasiones, diferentes grupos nacionales -como judíos rusos, alemanes, polacos, británicos o estadounidenses- han ejercido gran influencia.

El sionismo tiene varias ideologías y algunas de ellas hoy conforman partidos políticos en el Estado de Israel:

Alrededor de 1900 el principal rival para el sionismo entre los jóvenes judíos en la Europa oriental fue el Movimiento Socialista. Muchos judíos fueron abandonando el judaísmo en favor del comunismo o en apoyo al Bundismo, un movimiento judío socialista que pedía la autonomía judía en la Europa oriental y que el yidis sea promovido como lengua oficial judía.

Muchos sionistas socialistas eran originarios de Rusia. Tras siglos de ser oprimidos por sociedades antisemitas, los judíos habían sido reducidos a la obediencia, vulnerables, con desesperada existencia en la que se les invitaba a seguir el antisemitismo. Sostenían que los judíos podían escapar de su situación convirtiéndose en agricultores, trabajadores y soldados de su propio país. La mayoría de los socialistas se rehusaron a perpetuar la religión como una "mentalidad de la diáspora" entre el pueblo judío y establecieron las comunas rurales en Israel llamadas kibutz (voz con plural invariable). Los principales teóricos del sionismo socialista incluyen a Moisés Hess, Nahum Syrkin, Dov Ber Borojov y Aarón David Gordon, y entre las figuras destacadas del movimiento se cuentan David Ben-Gurión y Berl Katznelson.

La mayoría de los sionistas socialistas consideraron el yidis como la lengua del exilio, adoptando el hebreo como lengua común entre los judíos en Israel. El socialismo y el sionismo laborista eran ardientemente secularistas con muchos sionistas ateos que se oponían a la religión. En consecuencia, el movimiento a menudo tenía una relación antagónica con el judaísmo ortodoxo.

El sionismo socialista buscaba establecer un Estado Judío en el que considerara al judaísmo como una nacionalidad, y que las bases del Estado estuvieran identificadas con el socialismo, es decir, el trabajo comunal. Tuvieron gran fuerza desde la Segunda Aliyá, hoy en día conforman el partido Avodá o Partido Laborista.

El sionismo socialista se convirtió en la fuerza dominante en la vida política y económica del Yishuv durante el Mandato Británico - en parte como consecuencia de su papel en la organización de la vida económica judía a través de la Histadrut - y fue la ideología dominante de la clase política en Israel hasta las elecciones de 1977, cuando el partido Avodá fue derrotado.

Fundamental en la expansión del sionismo socialista fue la labor de movimientos juveniles en la Diáspora, que educaron a la juventud judía en los valores del movimiento, capacitándolos también para su futura vida en kibutz en Israel. Algunos de estos movimientos continúan existiendo hasta el día de hoy, como Hashomer Hatzair y Habonim Dror.

El "Sionismo revisionista" fue un grupo fundado y dirigido por Zeev Jabotinsky. Durante el Mandato Británico presionó a Gran Bretaña para permitir la inmigración judía en masa y para la formación de un ejército judío en el Mandato.

El revisionismo fue muy popular en Polonia pero carecía del apoyo necesario desde la Tierra de Israel. En 1935 la izquierda del revisionismo y la Organización Sionista formaron una alternativa, la "Nueva Organización Sionista". Se reincorporó a la Organización Sionista Mundial en 1946.

El "Sionismo revisionista" cree en el asentamiento de la tierra y se opone al "Sionismo socialista". Los revisionistas buscan mantener el tradicionalismo judío firme, como orgullo de la nación. Apoyan mayormente la idea de volver a los límites geográficos del Estado judío de la antigüedad. Hoy en día está representado por el Partido Likud, que es el partido más fuerte de Israel siendo etiquetado como la derecha israelí.

"Sionismo general" fue inicialmente el término empleado por los miembros de la Organización Sionista Mundial que no se habían unido a una facción determinada o algún partido específico, perteneciendo solamente a las organizaciones sionistas de sus respectivos países. A medida que transcurrieron los años, los sionistas generales también crearon instituciones ideológicas y formaron la Organización de Sionistas Generales, establecida en 1922 como el partido de centro del movimiento sionista. Los preceptos de los sionistas generales incluían un sionismo al estilo de Basilea, exento de posicionamientos ideológicos, es decir, con primacía del sionismo sobre cualquier interés clasista, partidista o personal.

Impulsado principalmente por el Rabino Kalisher, une los ideales de crear un Estado judío con la religión. Su ideología se resume en una frase: "El Pueblo de Israel, en la Tierra de Israel, según la Torá de Israel". Es decir, regir un Estado con bases religiosas y asentarse en toda la Tierra de Israel, como lo marcaba el reinado de Salomón. Hoy en día representan al Partido Nacional Religioso (Mafdal).

El sionismo cristiano, es un movimiento surgido en el seno del cristianismo principalmente evangélico, pero no circunscrito únicamente a esta denominación, que apoya la idea de un hogar nacional para los judíos desde antes de 1948 y continúa apoyando la existencia del Estado de Israel hasta la fecha (Ice, 1997).

El sionismo cristiano es la creencia entre algunos cristianos de que el retorno del pueblo judío a Tierra Santa, y el establecimiento del Estado de Israel en 1948 fueron el cumplimiento de la profecía bíblica. El término empezó a ser usado a mediados del siglo XX, reemplazando al restauracionismo cristiano.

El catolicismo tradicionalmente no prestó mucha atención al sionismo, pero el apoyo cristiano a dicho movimiento creció entre la comunidad protestante.

Algunos cristianos sionistas creen que el regreso de los judíos a la Tierra de Israel, es un pre-requisito para la segunda venida de Jesús. La idea es habitual entre los protestantes, desde los tiempos de la reforma, los cristianos han apoyado activamente el regreso de los judíos a la Tierra de Israel, junto con la idea de que los judíos deben de convertirse al cristianismo para dar cumplimiento a la profecía bíblica.





El sionismo no recibió en sus comienzos –finales de siglo XIX– el apoyo mayoritario de los judíos. En particular, no contó con las simpatías de la mayoría de los judíos de Europa occidental, que creyeron poder considerarse a sí mismos como ciudadanos con plenos derechos en sus respectivos países, tras los aires de emancipación y tolerancia que trajo consigo la Ilustración y el estado liberal decimonónico clásico. La forma más exacerbada de oposición a las ideas sionistas se conoció como "integracionismo" (también llamado «asimilacionismo»), y afirmaba que el sionismo era análogo al antisemitismo, en la medida en que ambos niegan la condición de nacionales de un determinado país a los judíos. Una manifestación extrema de integracionismo es la conversión a la fe cristiana. Un ejemplo célebre de antisionismo fue el de Edwin Samuel Montagu, ministro judío del Gobierno británico que puso muchas trabas a la redacción de la Declaración Balfour tachándola de antisemita. La oposición al sionismo existía también entre el movimiento Bundista en Europa oriental, que buscaba la autonomía cultural de los judíos en los países donde vivían; así como de la mayoría de los judíos ortodoxos. Entre estos últimos sigue habiendo algo de ambigüedad e inclusive hostilidad al sionismo. El caso Dreyfus fue determinante para inspirar a Herzl, al considerar al sionismo como única solución plausible y efectiva contra el antisemitismo europeo. El impacto emocional del Holocausto convenció definitivamente a numerosos judíos asimilados, socialistas y ortodoxos, refractarios con el sionismo, que quedaban en Europa.

En Europa oriental, lugar donde el sionismo se hizo fuerte debido en muy gran medida a las incesantes persecuciones a que eran sometidos los judíos por el zarismo, la idea de un Estado judío, o incluso de pueblo judío, fue rechazada por numerosos judíos miembros de organizaciones revolucionarias, marxistas o anarquistas, quienes consideraban que la condición de judío derivaba de la religión y que, una vez erradicada esta, la distinción entre judíos y no judíos desaparecería. Estas ideas no impidieron, sin embargo, que hubiese también un importante movimiento sionista socialista.

La población árabe de Palestina, apoyada por la Liga Árabe, se opuso al sionismo, negándose a aceptar la partición del Mandato Británico de Palestina en dos Estados, uno judío y otro árabe, según la recomendación de las Naciones Unidas del 29 de noviembre de 1947. Los conflictos armados entre judíos y árabes preexistentes en la región antes de la partición de Palestina desembocaron finalmente en la Guerra árabe-israelí de 1948 ante la proclama de independencia por parte del Estado de Israel, y la negativa de los países árabes vecinos a reconocerlo oponiéndose a la partición de Palestina. La guerra tuvo diferentes lecturas dependiendo de los distintos análisis, por ejemplo Jacques Pirenne consideró que el Ejército de Liberación apostó por la destrucción del naciente Estado judío, desencadenando «una guerra de exterminio» siendo derrotado junto al Yarmuk, resultando vencedores los judíos y desencadenando la Liga Árabe una potente ofensiva que fue rechazada en casi todos los frentes. Por su parte, Ilan Pappé, profesor de la Universidad de Haifa, sostiene que la guerra de 1948 fue una limpieza étnica cometida por Israel contra el pueblo palestino, razón por la cual lo que los israelíes conocen como «Guerra de la Independencia» para los palestinos es la «Nakba», la «catástrofe». La guerra se saldó con el exilio de cientos de miles de árabes palestinos, la ampliación de Israel más allá de los límites previstos en el plan de partición de la ONU y la ocupación por parte de Egipto y Transjordania de la parte asignada al Estado árabe y la zona internacional de Jerusalén. El conflicto se hizo sentir más allá de las fronteras palestinas, ya que las comunidades judías que habitaban en países árabes (muchas desde antes de ser arabizados e islamizados), se vieron obligadas a emigrar en las décadas siguientes, víctimas potenciales del «antisionismo». El fenómeno tuvo características diferentes según los países, aunque en general liquidó prácticamente las comunidades hebreas en países árabes. Algunas fuentes hacen hincapié en la judeofobia y en las migraciones inmediatamente posteriores a la guerra de 1948, mientras que otras señalan un proceso de emigración mucho más largo y debido a una multiplicidad de factores, entre los cuales siguen estando los ecos del conflicto árabe-israelí, a los que se añaden económicos, culturales y otros. He aquí un cuadro que lo resume:

Gran Bretaña, a pesar de haber dado el primer paso hacia la creación de un Estado judío con la Declaración Balfour, dificultó por todos los medios la inmigración de judíos al Mandato Británico de Palestina, incluso durante la Segunda Guerra Mundial (véase Libro Blanco de 1939) y vaciló en el momento de facilitar su aplicación. Incluso llegó a plantearse su derogación para no perjudicar sus intereses geoestratégicos en Oriente Medio ni dañar sus alianzas con los países árabes. Los graves conflictos que los planes sionistas generaban entre la población árabe de Palestina también aconsejaban a los británicos mantener el "statu quo" previo a la guerra (que se concretaba en el llamado Libro Blanco de 1939) y esperar a que la ONU redefiniese el Mandato de la extinta Sociedad de Naciones.

Aunque cada vez más minoritarios, en el seno del judaísmo sigue habiendo antisionistas por razones religiosas y con diversos grados de oposición, como los jaredíes o algunos grupos ultraortodoxos y relativamente minoritarios como Neturei Karta, estos últimos contrarios al actual Estado de Israel, además de grupos laicos de judíos seculares disidentes y opositores al Estado de Israel, e intelectuales judíos independientes como Noam Chomsky, Norman Finkelstein, Shlomo Sand; o personalidades gentiles opositoras al sionismo, como la periodista estadounidense Helen Thomas.

En 1975, en plena Guerra Fría, la Asamblea General de la ONU adoptó, por impulso de los países árabes, y con el apoyo del bloque soviético y del no alineado, la resolución 3379, de carácter declarativo y no vinculante, que asociaba al sionismo con el racismo (72 votos a favor, 35 en contra y 32 abstenciones). El entonces embajador israelí y futuro Presidente de Israel, Jaim Herzog, rompió el documento en pedazos delante de la Asamblea. En 1991 Israel puso la anulación de la resolución 3379 como condición para su participación en la Conferencia de Madrid, lo que llevó a que fuera derogada al aprobarse la resolución 4686 (111 a favor, 25 en contra y 11 abstenciones).

Egipto fue el primer Estado árabe que reconoció al Estado de Israel, y los demás lo harían después de que la propia OLP reconociera el Estado judío en 1988. En la actualidad hay organizaciones palestinas que reconocen el derecho a la existencia de Israel, aunque los dos partidos mayoritarios, Hamás y Fatah, niegan a Israel ese derecho. Entre quienes siguen sosteniendo posiciones antisionistas están las autoridades de Irán.

El sionismo fue establecido sobre la base de la asociación entre el pueblo judío y la Tierra de Israel. La aliyá a la Tierra de Israel es un tema recurrente en las oraciones judías. Los sionistas consideran a los judíos que viven fuera de Israel como exiliados. El rechazo a la vida en la diáspora es central en el sionismo. Subrayando esta actitud se encuentra el sentimiento de que la diáspora judía restringe el crecimiento total de la vida individual y nacional judía. Los sionistas generalmente prefieren hablar en hebreo, una lengua semítica que se desarrolló en condiciones de libertad en la antigua Judá, modernizada y adaptada a la vida cotidiana. A veces rechazan hablar en yídis, una lengua que consideran afectada por la persecución cristiana. Una vez que emigran a Israel, muchos sionistas rechazan hablar su lengua materna y toman nombres hebreos. Los principales aspectos de la idea sionista se ven representados en la Declaración de Independencia de Israel:

Según Eliezer Shweid la negación de la vida en la diáspora es idea en todas las corrientes del sionismo. Subrayando esta actitud estaba el sentimiento de que la Diáspora restringía el crecimiento completo de la vida nacional judía.

Los sionistas prefieren hablar hebreo, una lengua semítica que se desarrolló bajo condiciones de libertad en el antiguo Reino de Judá y que dejó de hablarse alrededor del siglo I a. C, modernizándolo y adaptándolo a la vida diaria. El responsable principal de la resurrección del hebreo como lengua hablada a partir de su estado previo de lengua litúrgica fue precisamente un sionista, Eliezer Ben-Yehuda. Los sionistas a veces rechazan hablar yidis, una lengua derivada del alto alemán medio que consideran afectada por la persecución cristiana. Una vez que emigran a Israel, muchos sionistas rechazan hablar su lengua materna en la diáspora y se ponen nuevos nombres hebreos.

En este tema, el historiador Zeev Sternhell distingue dos escuelas de pensamiento en el sionismo. Una es la escuela liberal o utilitaria de Herzl y Nordau. Especialmente después del caso Dreyfus dicen que el antisemitismo nunca desaparecerá, y ven el sionismo como una solución racional para los judíos. La otra es la escuela nacionalista racional. Prevalece entre los judíos de Palestina, y ve el sionismo como un proyecto para rescatar a la nación judía y no como un proyecto para rescatar a los judíos. El sionismo fue un problema en el "Renacimiento de la Nación".

En Francia, personalidades como Xavier Vallat, ministro de asuntos judíos del Regímen de Vichy, o Lucien Rebatet, escritor colaboracionista que reclamaba durante la guerra el exterminio de los judíos, apoyaron a Israel contra los palestinos. Los dirigentes del apartheid en Sudáfrica, a menudo antisemitas, apoyaron a Israel.




</doc>
<doc id="29706" url="https://es.wikipedia.org/wiki?curid=29706" title="Proceso de Poisson">
Proceso de Poisson

En estadística y simulación, un proceso de Poisson, también conocido como ley de los sucesos raros, es un proceso estocástico de tiempo continuo que consiste en "contar" eventos "raros" (de ahí el nombre "sucesos raros") que ocurren a lo largo del tiempo. El tiempo entre cada par de eventos consecutivos tiene una distribución exponencial con parámetro λ; cada uno de tales tiempos es independiente del resto. Es llamado así por el matemático Siméon Denis Poisson (1781–1840). 

Un proceso Poisson con intensidad (o tasa) formula_1 es un proceso de contar en tiempo continuo formula_2, donde formula_3 es una colección de variables aleatorias con las siguientes propiedades: 

1. formula_4.

2. Si formula_5, entonces formula_6.

3. Para todo formula_7 y formula_8, las variables aleatorias formula_9 son independientes.

4. Para toda formula_10 y formula_11 y formula_12 tienen la misma distribución.

5. formula_13.

6. formula_14.

Donde o(h) es una función tal que:

formula_15

formula_3 es el número de eventos que se han producido desde el instante cero hasta el instante formula_17. Como en cualquier proceso estocástico, en el instante cero es una variable aleatoria; sin embargo, después del instante formula_17 es un dato.

A partir de la definición, es posible demostrar que:


Una importante aplicación del proceso Poisson se encuentra en la probabilidad de ruina de una compañía aseguradora. El problema fue tratado formalmente por Filip Lundberg en su tesis doctoral en 1903. Posteriormente, Harald Cramér desarrolla las ideas de Lundberg y da lugar a lo que hoy se conoce como el o .

A menudo son más realistas los modelos basados en procesos de Poisson no homogéneos, en los que la tasa de llegadas es una función del parámetro de tiempo, λ(t). Formalmente esto significa que un proceso de Poisson no homogéneo es un proceso de contar que satisface:

1. formula_27

2. Los incrementos en intervalos ajenos son independientes. 

3. formula_28

4. formula_29

Los tres métodos más conocidos de generación de un proceso de Poisson no homogéneo de este tipo se basan en la modificación de la escala de tiempo, en el condicionamiento y en una adaptación del método de rechazo.

Para procesos homogéneos hay una densidad media formula_23. Eso significa que la media de los sucesos en un intervalo de tiempo formula_17 es formula_32.

El tiempo entre dos sucesos de un proceso de Poisson con intensidad media formula_23 es una variable aleatoria de distribución exponencial con parámetro formula_23.

Se pueden modelar muchos fenómenos como un proceso de Poisson. El número de sucesos en un intervalo de tiempo dado es una variable aleatoria de distribución de Poisson donde formula_23 es la media de números de sucesos en este intervalo. El tiempo hasta que ocurre el suceso número formula_36 en un proceso de Poisson de intensidad formula_23 es una variable aleatoria con distribución gamma o (lo mismo) con distribución de Erlang con formula_38.

El ejemplo clásico de fenómenos muy bien descritos matemáticamente a través de un proceso Poisson fue el de los fallecimientos a causa de la patada de un caballo en el ejército de Prusia, según lo demostrado por Ladislaus Bortkiewicz en 1898. Este economista y estadístico polaco también analizó los datos de los suicidios infantiles conforme a este modelo. 

El proceso de Poisson también se ha aplicado para los siguientes ejemplos:


Un proceso de Poisson compuesto es un proceso estocástico que combina un proceso de Poisson con otra variable aleatoria independiente, de tal manera que para cada salto discontinuo del proceso de Poisson la otra variable asume un valor real. El modelo es muy usado para modelizar, por ejemplo, una cartera de seguros, en este modelizado las reclamaciones por daños a la aseguradora sigue un proceso de Poisson ordinario, pero la cuantía de la reclamación es una variable aleatoria adicional, de tal manera que el monto de las reclamaciones es un proceso de Poisson compuesto de la forma:


</doc>
<doc id="29707" url="https://es.wikipedia.org/wiki?curid=29707" title="Martinica">
Martinica

Martinica es una isla con estatus de región y departamento de ultramar de Francia, que forma parte integrante de la República francesa y de la Unión Europea como región ultraperiférica. En tanto colectividad territorial única, tan solo consta de una asamblea que reagrupa las competencias del Consejo regional y departamental. Está ubicada al norte de Santa Lucía, en aguas del mar Caribe. El idioma oficial es el francés, pero la población habla también el criollo martiniqués ("créole martiniquais"). 

Debe su nombre a Cristóbal Colón, quien la conquistó a partir de 1502. Se llamaba "Jouanacaëra-Matinino" y estaba habitada por los caribes. Esta isla pertenece al grupo de las llamadas Antillas Menores y dentro de este pertenece al grupo de las llamadas Islas de Barlovento. Pertenece a Francia desde 1635. Es montañosa y de origen volcánico.

Tiene una superficie total de 1100 km², el tercer lugar después de Trinidad y Guadalupe en la cadena de islas que componen las Antillas Menores. Martinica tiene 65 km de largo por 27 km de ancho. Para efectos comparativos su superficie es similar a la de Hong Kong.

Al igual que el resto de las Antillas Menores, Martinica está sujeta a riesgo sísmico. El 29 de noviembre de 2007 un terremoto de magnitud 7,3 en la escala de Richter se produjo fuera de la isla.

Martinica se divide en dos zonas. Por un lado, una situada al norte del eje entre Fort-de-France y Le Robert, área que es el resultado de la erosión a causa de las fuertes lluvias causadas por el mar hechas por evaporación de los vientos alisios, las altas de los volcanes recién formada al norte de la isla, como el Monte Pelée (con 1.397 m s. n. m.) y los picos de Carbet (1.196 m s. n. m.), al sur las cimas redondeadas y laderas escarpadas desde una altitud de entre cien y trescientos metros. En el sur, hay una zona con menor alivio, vegetación menos abundante, clima más seco, que incluye a la mayoría de las instalaciones turísticas de la isla.

El terreno es montañoso en esta isla de origen volcánico. Las zonas más antiguas corresponden a las zonas volcánicas en el extremo sur de la isla y hacia la península de La Caravelle hacia el este. La isla se ha desarrollado en los últimos 20 millones de años según una secuencia de movimientos y erupciones de la actividad volcánica hacia el norte. Un volcán, aún activo, es el Monte Pelée, situado al norte de la isla hoy y con una altura 1.397 m s. n. m., la última erupción se remonta al 8 de mayo de 1902, cobrando la vida de 29.000 personas en 2 minutos. La montaña Vauclin es el punto más alto al sur de la isla con 504 m. En el centro se localizan las llanuras y las zonas costeras, separadas por cuestas empinadas.

La costa este, costa del viento o de las Islas, ha sido llamada en el Caribe cabesterre. El término cabesterre en Martinica designa más específicamente la zona de la La Caravelle. Esta costa de barlovento, bordeada por el océano Atlántico, está directamente expuesta a los vientos alisios y el mar de fondo. La parte norte del Gran Río en Sainte-Marie está básicamente rodeada de acantilados con muy pocos puntos de amarre y el acceso a la navegación marítima se limita a la pesca de bajura con pequeñas embarcaciones tradicionales de Martinica

La isla tiene una red hidrográfica pequeña, debido a sus características geográficas y morfológicas tiene ríos cortos y torrentosos. Los principales son: El Lézarde, de 30 km de largo, el más largo de la isla. Al Norte están: Galion, Lorrain, Hood, Blanco, Baja Pointe, Río Hackaert, Macouba, La Grande, Prêcheur, Roxelane, Río Padre, río Carbet. Al Centro: el río Monsieur, Madame, Longvilliers. Al sur: el Río Salado, Vauclin, Paquemar, Simon, y La Nau.

Cristóbal Colón fue el primer navegante europeo en llegar a la isla el 15 de junio de 1502. Pero, salvo en tres ocasiones en que ha sido ocupada brevemente por otras naciones, la isla ha permanecido bajo dominio francés desde su colonización, en 1635. Martinica está dominada por el volcán Monte Pelée, de 1397 m s. n. m., que hizo erupción el 8 de mayo de 1902 durante el llamado Cataclismo de Martinica, destruyendo completamente Saint Pierre, el primer asentamiento europeo en la isla, y matando a más de 30.000 personas. La población se ha venido recuperando desde entonces, y actualmente la isla es una de las más pobladas de la región.

Marie-Josèphe Rose Tascher de la Pagerie, conocida como Josefina de Beauharnais, quien se casó en segundas nupcias con Napoleón I Bonaparte, nació en una plantación martiniqueña de esclavos, propiedad de su familia, llamada La Pagerie. Su segundo esposo fue el responsable de restablecer la esclavitud en la isla a partir de 1794, razón por la cual su imagen en la isla continúa siendo muy negativa. La estatua que la representa en Fort-de-France, ha sido decapitada en varias ocasiones.

Durante las primeras décadas de la ocupación francesa, la fuente principal de la isla era la producción de alimentos, como tabaco, añil, cacao. La crisis del tabaco de la segunda mitad del siglo XVII, hizo que las primeras plantaciones se arruinasen, así como la producción de azúcar.

El monocultivo de la caña de azúcar formó parte del paisaje y de la cultura criolla que dominaron la economía hasta la segunda mitad del siglo XX. Este cultivo lo realizaban los esclavos africanos que eran traídos a la isla, y tenían el apodo de los “treinta y tres”. El cultivo de la caña dio lugar al desarrollo de comercio triangular (Europa, África, América), por lo cual provocó una rápida afluencia de población de esclavos africanos a las posesiones francesas en América.

El imperio neerlandés fue rápidamente expulsado en la segunda mitad del siglo XVII, por la lucha entre británicos y franceses. Todos los grandes conflictos europeos fueron por dominar el Caribe. Martinica fue una posesión británica, pero durante períodos relativamente cortos. Pasa a ser territorio francés después de 1816.

El padre Labat describe en sus libros a los esclavos del siglo XVII. Una vez liberados, poseen parcelas de tierra. Los ricos plantadores buscan nueva mano de obra más barata, sobre la base del éxito de los plantadores de azúcar de Barbados. El cultivo de azúcar en Martinica se intentó desde el 1640, inspirado por el éxito de Barbados, pero sin éxito. La explosión en el número de esclavos refleja una de las mayores decisiones adoptadas en Versalles por Luis XIV entre 1671 y 1674 para fomentar el cultivo de azúcar, en detrimento del tabaco. Entre 1674 y 1680, el número de esclavos en Martinica se duplicó y entre 1673 y 1700, llegó a sextuplicarse.

El número de esclavos en 1664 había disminuido en las dos islas. Guadalupe autorizó a partir de la década de 1640 la llegada de más esclavos siempre que fueron controlados por un único propietario; hacia 1660 había unos 6000 esclavos dos veces superior al de Martinica que tenía cerca de 3.000 y que trabajaban el doble.
En Francia, en 1788, en vísperas de la Revolución francesa, Brissot fundó la Sociedad de amigos de los negros, pero a pesar de los esfuerzos de sus miembros más prominentes, como el abad Grégoire, o Condorcet, no puede obtener la abolición de la esclavitud con la Constitución. Solo el 4 de febrero de 1794 la Convención suprime la esclavitud, medida no se aplica -ni mucho menos- en todas las posesiones francesas de la época.

En Martinica seguirá siendo letra muerta, en contra de Guadalupe debido a que un grupo de colonos monárquicos bajo la dirección de Paul-Louis Dubuc, tuvo un pacto con los británicos, que ocuparon la isla desde 1794 hasta 1802.

El 19 de febrero, dos semanas después de la abolición, las grandes plantaciones de la isla, los británicos firmaron el Tratado de Whitehall: a cambio de la dominación de la isla y el lucrativo impuesto sobre el azúcar, se comprometen a mantener la esclavitud. En 1848, la población de Martinica, era de 121.130 habitantes, repartidos de la siguiente manera: 9.542 blancos, 38.729 liberados y 72.859 esclavos.

El Monte Pelée (en francés: 'Monte Pelado') es un estratovolcán de 1.397 m s. n. m. que se encuentra en estado de actividad al norte de la isla. Está compuesto de un magma extremadamente viscoso y de restos de ceniza volcánica y lava solidificada. Se caracteriza por haber tenido una actividad poco frecuente, pero con erupciones sumamente violentas, ya que la andesita presente en la lava de las profundidades del volcán tiene un alto contenido en sílice, muy viscosa. En 1902, una de estas erupciones destruyó completamente la ciudad de St. Pierre.

El 8 de mayo de 1902 se vio luz en la cima del volcán y luego una densa nube de humo negro, seguida por otra en forma de hongo visible a 100 km de distancia. La velocidad inicial de las dos nubes fue calculada más tarde en 670 km/h. Un flujo de lava bajó por las laderas del volcán a una velocidad considerable, llegando a la ciudad en un minuto, e incendiándola. Por último se produjo una precipitación con torrentes de barro, que la destruyeron por completo. Durante horas se cortó toda comunicación.

El 20 de mayo el Monte Pelée volvió a estallar aún más violentamente, pero esta vez sin dejar más víctimas.

Las consecuencias desde el punto de vista social, político y económico en Martinica fueron considerables. La erupción causó cerca de 30.000 muertes y destruyó por completo la ciudad y su puerto. Se acordó que la ciudad de Fort-de-France sustituiría a Saint Pierre como capital. Muchos niños se convirtieron en huérfanos, de ahí la creación del orfanato de la Esperanza en Fort-de-France. Por último, parte de la población afectada fue reubicada en otras ciudades de la Martinica en el Atlántico norte y el sur de la isla. Otros se fueron a Guadalupe, Santa Lucía, la Guayana Francesa, Panamá y Venezuela.

Pasarían al menos cuatro días para que llegara ayuda de Fort-de-France por vía marítima, solo para rescatar a las cuatro personas supervivientes (dos de las cuales morirían poco después) y abandonar las ruinas de la ciudad-osario. El cono del volcán había desaparecido en al menos 300 m quedando solo una forma geológica de forma de columna de huso de unos 300 m de altura que más tarde se derrumbó tras la segunda erupción.

En 1932 aterriza el primer avión, en el recorrido entre Cayena y Antigua. En 1946 se convierte en un Departamento de ultramar. En 1950 se inaugura el aeropuerto de Fort-de-France.

En 1962 se produce la primera reivindicación independista en torno al Manifiesto de la O.J.A.M (Organización de la Juventud Anticolonialista de la Martinica). Los autores son dieciocho estudiantes, que son encerrados por razones políticas y liberados solo tras dos años de proceso.

En 1983 se convierte en una región, y Aimé Césaire es elegido Presidente del Consejo Regional. En 1997 es elegido diputado de Martinica el independentista Alfred Marie-Jeanne, quien en 1998 accede al cargo de Presidente del Consejo Regional.

En 2005 mueren en la caída de un avión de la compañía colombiana West Caribbean 160 pasajeros, en su mayoría martiniqueses. En 2008 fallece Aimé Césaire a la edad de 94 años.

En 2013 se confirma la existencia del virus chikunguña en la isla.

Martinica está administrada por el Gobierno de Francia. Desde 2016, está dotada del estatuto de colectividad territorial única. Dos órganos rigen la vida política: una asamblea representativa viene acompañada de un Consejo ejecutivo, resultando así separados los dos poderes. La asamblea consta de 51 concejales, elegidos por sufragio universal cada seis años.

Martinica está dividida en cuatro municipios, treinta y cuatro comunas, y cuarenta y cinco cantones. Con Guadalupe y Guayana Francesa es uno de los Departamentos Franceses de América (DFA). Es asimismo una región ultraperiférica de la Unión Europea.

Los cuatro distritos de la isla, con sus respectivas localidades, son los siguientes





Las comunas son las ciudades, y los cantones se usan para las elecciones, son una subdivisión de las comunas y son es para elegir a alguien que representan tres o más barrios.

Su PIB por habitante equivale a la mitad del de Francia metropolitana. Por ello, Martinica se beneficia de los fondos estructurales que otorga la Unión Europea a las zonas económicas menos favorecidas.

Sus principales intercambios comerciales son con la Unión Europea.

La moneda de curso legal en Martinica es el euro, aunque, antes del año 2002, circulaban conjuntamente el franco de Martinica y el franco francés.

Como todas las islas del Caribe, es ideal para el turismo, ya sea que se busquen las playas de arenas blancas del sur, las arenas negras de las playas del norte, caletas para bucear, natación, pesca, visitar arrecifes, conocer los manantiales de aguas termales, recorrer terrenos montañosos, observar desfiladeros, entrar en lugares selváticos, apreciar las bellísimas flores como lilas, orquídeas, o degustar frutas tropicales, como cocos, piñas o papayas.
Jóvenes emprendedores franceses han divulgado la isla de La Martinica de un modo internacional por sus negocios algunos de los cuales, como Sacha Passy de Thellier, han sido premiados con el premio Jóvenes Empresarios Emprendedores de La Martinique por el Consulado de varios países, entre ellos España.

Se distinguen en la isla dos estaciones marcadas, relacionadas con la lluvia: la estación lluviosa ("hivernage" en francés) de mayo a noviembre y la estación seca (llamada "carême" en Martinica) de febrero a abril. La estación seca es normalmente un período de fuertes calores y de sequía. Como suele comenzar después de Carnaval, es llamada "carême" que quiere decir cuaresma. Durante la estación de lluvia la humedad es muy elevada y las ondas y depresiones tropicales son muy frecuentes, provocando semanas enteras de lluvia.

En la práctica, las fluctuaciones en las fechas de cada periodo son muy frecuentes. La estación húmeda es más larga o más corta dependiendo del año. A veces se producen estaciones húmedas tardías o precoces, o "carêmes" muy secas o muy lluviosas. La precipitación media es de 80 mm de lluvia en marzo en Lamentin (llanura central situado en la bahía de Fort-de-France) y de 260 mm de lluvia en octubre. Los vientos alisios soplan desde el noreste al este durante la mayor parte del año, aumentando durante la mañana y disminuyendo durante la tarde, decayendo durante las noches.

El clima de Martinica está regulado según la posición del anticiclón de las Azores, que dirige los vientos alisios del noreste, y también según la posición del área de bajas presiones ecuatoriales, donde los vientos del hemisferio norte se encuentran con los del hemisferio sur a lo largo de la zona de convergencia intertropical.

La temperatura media anual es de 26 ° C. Los meses más calurosos son marzo, abril, mayo, mientras que los meses más fríos son diciembre y enero. La temperatura más cálida se registró en “Saint Pierre” con 37° C en abril y mayo de 1986 y las más bajas alrededor de 12° C en Saint-Denis en marzo de 1965. Hay un promedio de seis días por año con una temperatura inferior a 18° C en Lamentin.

La zona occidental está sujeta al desarrollo de los ciclones. De acuerdo a la época del año, estas depresiones del Atlántico (Cabo Verde) o el centro del mar Caribe y golfo de México (al oeste del meridiano 80°). Martinica queda en la trayectoria de los huracanes.
El Huracán Dean causó daños importantes en las plantaciones de banano y caña de azúcar en agosto de 2007, alcanzando las ráfagas de viento hasta 167 km/h. Las lluvias torrenciales causaron numerosas inundaciones, sobre todo en la localidad de Riviere-Pilote. Un tercio de la población total de Martinica permaneció sin servicio eléctrico. Las autoridades reportaron el fallecimiento de un hombre de 90 años de edad a causa de un infarto, pero no se confirmó que esto haya tenido relación con el impacto de la tormenta.

Más de 5.000 casas fueron destruidas y solo las subvenciones públicas permitieron la reconstrucción. Los residentes de Martinica recibieron ayuda financiera de la metrópoli y el apoyo nacional, sobre todo por parte de representantes franceses como el o el Ministro de Territorios de Ultramar.

Por otra parte, el Gran Huracán tuvo lugar entre el 10 y el 16 de octubre de 1780. Se estima que entonces murieron unas 22.000 personas, muchas de ellas en alta mar. Este azotó las islas de Martinica, San Eustaquio y Barbados. Las fatalidades en este huracán exceden por mucho las de cualquier otro ocurrido en el Atlántico. De hecho, las cifras de superan a las acumuladas en cualquier año y en todas las otras décadas.

La selva está compuesta por helechos y árboles de caoba o courbaril, que se utilizan para la fabricación de muebles. Algunos pueden llegar a 45 metros de altura. El bosque seco tropical se compone de plantas tolerantes a la sequía adaptados a ésta. En su forma degradados o en condiciones extremas de la vegetación del suelo puede adoptar la forma de la sabana.

La isla cuenta con muy pocos animales endémicos. Los animales salvajes siguen siendo: el manikous (familia zarigüeyas), las tarántulas" matoutous falaise", iguanas verdes, mangostas y serpientes trigonocephale.

Martinica es la tierra de "garzas guardia bueyes" así como de los colibrí, de los que cuenta cuatro especies: el madère, el huppé, el verde y el azul.

La costa atlántica está bordeada de forma casi ininterrumpida por arrecifes de coral, gran refugio de vida silvestre y peces sedentarios. El medio acuático se ha deteriorado en los últimos veinte años. La contaminación industrial, incluyendo las destilerías, la falta de tratamiento de aguas residuales, intensivo de llenado de las zonas de manglares, viveros de muchas especies de peces y la pesca intensiva son la causa de considerable regresión de la superficie de los arrecifes y una disminución en el número y la variedad de pescado.

En tierra, las mangostas importadas para reducir la población de serpientes de cascabel mudas van en aumento. Las consecuencias son graves pues atacan a su vez muchas especies de aves, constituyendo una grave amenaza para la conservación de la avifauna.

La capital Fort-de-France cuenta con 100.080 habitantes. Otras ciudades importantes son: Le Lamentin (30.028 habitantes). Le Robert (25.400 habitantes) y Sainte Marie (19.682 habitantes).

El 90% de los habitantes son negros, culí (indios) o mulatos. El 85% de la población es católica.

En tanto que departamento y región de ultramar francesa, el francés constituye la lengua oficial de la isla así como es el caso en el resto del país. El criollo martiniqués, llamado "créole", es la lengua antillesa más extendida. Alrededor del 5% de los alumnos siguen la enseñanza del criollo en la escuela. El estatus de esta lengua ha cambiado desde la creación en el año 2000 de un CAPES (examen de aptitud para ejercer como profesor en Francia) sobre lengua y cultura regional, con la opción de criollo. En 2007 el escritor Raphaël Confiant publica en las ediciones Ibis rouge, el primer diccionario entre el criollo martiniqués y el francés.

El criollo martiniqués es muy próximo de los criollos de Guadalupe, Guyana y Haití, así como de los criollos de otras islas que fueron francófonas en el pasado (Dominica y Santa Lucía). A veces el criollo de Guadalupe, de Dominica, de Martinica y de Santa Lucía son considerados como una sola lengua: el criollo antillés.

Son originarios de la isla varios escritores e intelectuales franceses tales como Aimé Césaire, René Ménil y varios más, pero estos dos son los más importantes.

Aimé Césaire creó el movimiento cultural, ideológico y literario denominado negritud, a través del cual Césaire expresó su deseo de reivindicación de la cultura y la identidad negra frente a la cultura francesa, vista como dominante y opresora. Césaire fue una importantísima personalidad en Martinica por la relevancia de su actividad intelectual, por el peso de su actividad política y, sobre todo, por la calidad de su obra poética, celebrada por grandes intelectuales y artistas entre los que valdría la pena señalar a André Breton y Jean-Paul Sartre.

Los acontecimientos de la novela Corazón salvaje (de la escritora mexicana Caridad Bravo Adams) se ubican en la Martinica, concretamente antes, durante y después de la erupción del Monte Pelée.

En la música tradicional se hace una distinción entre la música de los montes (llamados "mornes" en Martinica, como en "Morne Rouge") y las músicas de baile. La música de los montes incluye el "Bèlè" (mezcla de música, danza y narración, acompañada de tambores), el "Chouval bwa" (música que acompañaba los tiovivos) y el "Damnié" (música asociada a la lucha). Entre las músicas de baile se cuentan la "Mazouk" (mazurca criolla) y la "Mazouk pitjé" (mazurca criolla picada), además de la "Quadrille" (o "Haute-taille"). Otros géneros tradicionales son la música del carnaval martiniqués y la "Biguine". Entre los géneros modernos destacan el "Zouk", el "Kadans", el "Dancehall" y el "Ragga", estos dos últimos provenientes de las antillas inglesas.

El 96.5% de los martiquineses profesan alguna forma de cristianismo, destacando la religión católica como la mayoritaria. 2.3% se declaran ateos o sin religión. 0.2% profesan el Islam y el mismo porcentaje es hinduista. 

La gastronomía de Martinica es variada y atestigua la historia de la isla y de sus habitantes, con influencias caribes (como el "Poulet boucané"), europeas, africanas (aportadas por los esclavos) e indias (a partir de la inmigración india), adaptadas a los productos de la isla. Los ingredientes que las componen pueden ser variedades foráneas cultivadas localmente (caña de azúcar, cacao, numerosos frutos), especies locales (como el lambí y los cangrejos) así como alimentos importados desde los tiempos de la época colonial para alimentar la población (arroz, bacalao).

La bebida más característica de Martinica es el Ron de Martinica, que tiene dos variedades principales: el ron agrícola AOC (por ejemplo, el "Rhum Clément") y el ron tradicional o industrial (por ejemplo, el "Rhum Negrita"). Otras bebidas alcohólicas son preparadas con el ron, como el "Ti Punch" (compuesto de sirope de azúcar de caña, limón y ron) y el "Planteur" (que también contiene zumo de fruta). En el mercado de Fort de France pueden comprarse licores artesanales a base de hierbas y frutas locales. Los jardines de la isla producen hierbas que son tomadas en infusión, conocidas como "Thé pays" y muy apreciadas por sus virtudes medicinales (citronela, "atoumo", etc). Los zumos de frutas tropicales también son muy populares (mango, guanábana, caña de azúcar, guayaba, tamarindo, ciruela del Pacífico, etc.) El "Mabi" (maceración a base de cortezas) es una preparación heredada de los indios caribes.

Entre los primeros platos y acompañamientos se encuentran especialidades como el "Féroce d'avocat" (una bola de aguacate al bacalao envuelta en harina de manioca), los "Accras de morue" (especie de buñuelos de bacalao) o de "tiriris" (peces de pequeño tamaño), el "Dombré" (bola de harina y agua con especias), el "Boudin Créole" (especie de morcilla), el "Kalalou" (sopa verde que contiene hojas de distintas especies de plantas y okras, común en el Caribe) y el "Pâté en pot" (sopa a base de verduras, de vísceras de cordero, de vino blanco y de alcaparras). Los gratinados también son muy variados y pueden componerse de chayotas, de plátano, de papaya, de fruta del árbol del pan, etc.

En cuanto a los productos del mar, el "Chatrou" es un plato a base de pulpo cocido acompañado de arroz y judías rojas, las brochetas de Lambí se cocinan a partir de grandes conchas, y en la estación adecuada pueden comerse los erizos de mar. Los cangrejos son capturados en la orilla del mar o de los cursos de agua dulce y constituyen la base de platos como el "Matoutou" (con arroz y especias). El "Macadam" es un plato de bacalao en salazón con arroz. Antiguamente, el "Ti-nan lanmori" (plato a base de plátanos verdes con bacalao) se consumía como desayuno. En el norte de la isla, el "Trempage" se prepara a partir de un caldo de bacalao, verduras, pan y carne o pescado, sumergidos en una salsa. 

Las carnes son preparadas de diversas maneras, por ejemplo en salsa con el "Colombo" de cerdo o de pollo (recetas que contienen especias como el curry, originario de la India) o al carbón como en el "Poulet boucané" (pollo cocinado lentamente bajo una capa de carbón). El "Chélou" se hace a base de vísceras de buey y de cordero con arroz.

El "Pain au beurre chocolat", igualmente llamado Chocolate de primera comunión (bebida chocolateada agrementada de especias y servida con un "Pain au beurre" martiniqués) es servido en esa ocasión. El "Blanc manger-coco" es un postre popular.





</doc>
<doc id="29708" url="https://es.wikipedia.org/wiki?curid=29708" title="Arthur (película de 1981)">
Arthur (película de 1981)

Arthur (también conocida como "Arthur, el soltero de oro" o "Arturo, el millonario seductor") es una película de 1981, dirigida por Steve Gordon.

Arthur (Dudley Moore) es el hijo de una familia muy rica, inversionistas ingleses y banqueros que trabajan en el distrito financiero de Nueva York, que no hace nada más que divertirse y beber. Vive en un apartamento lujoso con su mayordomo (John Gielgud) y sale con su Rolls-Royce y su chófer, a visitar bares nocturnos, beber y emborracharse en las noches de "Nueva York". 

El padre de Arthur quiere que siente cabeza de una vez y le busca una novia, para negociar un matrimonio por interés con la hija de un millonario local, perteneciente a una familia importante, unir a las familias y hacer nuevos negocios entre ellos, y amenaza a su hijo con desheredar una fortuna que le corresponde, si es que no acepta casarse con la novia designada por él, y así ayudar a prosperar los negocios de la familia en el país, con la familia del millonario. 

Aunque al principio Arthur no tiene inconveniente en casarse por el interés del dinero, conoce una mujer de clase más humilde (Liza Minnelli) en las calles de "Nueva York", con la que establece una relación cada vez más estrecha y finalmente se enamora, se arrepiente de casarse con el matrimonio arreglado por su padre, abandona la ceremonia de la boda y finalmente vive feliz junto al verdadero amor, sin importar su herencia, el dinero y la fortuna que le pertenece y estaba acostumbrado a vivir, aunque al final, su familia al verlo tan feliz enamorado verdaderamente de una mujer humilde, decide anular el acuerdo y retira la amenaza de quitarle su herencia.



</doc>
<doc id="29709" url="https://es.wikipedia.org/wiki?curid=29709" title="Basic Instinct">
Basic Instinct

Basic Instinct (titulada en España como Instinto básico y en Hispanoamérica como Bajos instintos) es una película estadounidense de , dirigida por Paul Verhoeven y protagonizada por Sharon Stone y Michael Douglas.

Al comienzo de la película, Johnny Boz, una estrella del rock retirada, está haciendo el amor con una rubia, y ella lo mata, inmovilizándole primero (le ata las manos a la cabecera de la cama con una bufanda blanca) y clavándole repetidamente un picahielo. Nick Curran y Gus Moran, del Departamento de Policía de San Francisco, se hacen cargo de la investigación

Nick y Gus visitan a la novia del roquero, la escritora Catherine Tramell. La encuentran en una lujosa casa junto a la playa. Hay una breve entrevista. Ellos solo se acostaban juntos nada más, les dice Catherine. No hay pruebas para detenerla y los agentes se van.

Luego observan que en una novela de Tramell, la protagonista comete un crimen de similares características. Hay dos probabilidades: que la autora sea la asesina o que algún lector lo hiciera para incriminarla. La recogen en la casa de la playa para interrogarla. En el coche, ella cuenta la trama de su próxima novela: un policía se enamora de la protagonista, que acaba matándolo. En el interrogatorio, muestra un elevado interés hacia Nick. Tramell niega haber matado a Boz y el detector de mentiras le da la razón, aunque Nick cree lo contrario.

Más tarde descubren que un tutor de Tramell, en la universidad, fue asesinado de manera similar a Boz. También Catherine es muy amiga de una mujer que asesinó a su marido e hijos en un ataque de locura: según Catherine, la mujer la ayudó mucho a comprender el instinto asesino. También se sospecha que Catherine mató a sus padres por la herencia, ya que fue un accidente sospechoso y ella había escrito un libro sobre un niño que mataba a sus padres.

Nick hace otra visita a Catherine. Comprende que ella sabe demasiado sobre él y sospecha que ha visto su ficha. Beth Garner, su psicóloga, (y psicóloga del departamento de policía) —Nick había matado accidentalmente a dos turistas—, le pasó su ficha a Nilsen, de Asuntos Internos, y este se la vendió a Tramell, según averigua Nick. Este agrede a Nilsen, que poco después aparece asesinado, haciendo que dejen a Nick suspendido. 

Nick se encuentra con Tramell y hacen el amor. Mientras están en ello, parece que Catherine fuera a matarlo, ya que le ata las manos a la cabecera de la cama con una bufanda blanca, como había aparecido Boz cuando encontraron su cuerpo. Roxy, una amante de Catherine, se siente celosa de Nick e intenta matarlo con el auto. Sin embargo, ella acaba muriendo en un accidente del que los policías creen que Nick ha sido el responsable.

Catherine parece realmente afectada por la muerte de su "amiga" (mucho más que por la muerte de Boz) y vuelve a mantener relaciones con Nick, y le confiesa que en la universidad una chica llamada Lisa la había estado acosando. Luego este descubre que Roxy había matado a sus dos hermanos con la cuchilla de afeitar de su padre. Nick va a preguntar sobre la tal Lisa, pero en el registro de la universidad no consta el nombre de la chica. 

Más tarde Catherine le deletrea el apellido, se da cuenta de que se ha equivocado y vuelve a buscar. Descubre entonces que Lisa es en realidad Beth, pero cuando le pregunta ella afirma que en realidad era Catherine la que estaba obsesionada con ella, y no al revés. Luego la misma Catherine le dice que no es cierto, e insiste que fue ella la denunciante del hecho. Nick va a buscar la denuncia, pero Nilsen la había sacado un año antes. Nick duda ahora de la culpabilidad de Catherine. Cree que fue Beth la que mató a Boz fingiendo ser Catherine para que la inculparan, y que mató luego a Nilsen porque él sabía de su anterior obsesión por la sospechosa. Va a buscar al marido de Beth, ya que ella afirma que se cambió el nombre porque se casó, y descubre que murió asesinado. Pregunta al sheriff y él le dice que no hubo sospechosos, que hubo rumores de una amante por parte de Beth y que hacía cosa de un año Nilsen había ido allí a preguntar lo mismo.

Catherine termina el libro basado en Nick, y corta su relación sentimental con él.

Una chica de la residencia universitaria de Catherine y Beth llama a Gus, diciendo que sabe toda la historia sobre ambas. Van los dos, pero solo sube Gus porque Nick está suspendido. Sin embargo, resulta ser una emboscada. Cuando Gus sale del ascensor del piso es asesinado por una figura encapuchada con un punzón de hielo. Nick sube sospechando la suerte de su amigo y encuentra a Gus moribundo. Oye unos ruidos, se levanta y ve a Beth, que dice que le han enviado un mensaje para que se reuniera con Gus allí. Ella va avanzando con la mano en el bolsillo y él, asustado, le dispara. Ella le confiesa que le quería y muere.

Los policías ven en las escaleras una capa ensangrentada (que lleva las iniciales del departamento de policía), una peluca rubia y el punzón, y registran la casa de Beth. Allí encuentran una pistola como la que mató a Nilsen, y en un cajón, libros y recortes de periódico de Catherine. Piensan que la culpable de los crímenes es Beth y cierran el caso.

Cuando Nick llega a su casa, Catherine está allí. Le dice que cortó con él porque no quería perderlo, dado que todos los que le importaron murieron. Vuelven a hacer el amor, y cuando ella le pregunta qué va a pasar después, él le dice que van a ser como conejos y van ser felices para siempre, Catherine mete la mano bajo la cama como si buscara algo y le dice a Nick que no quiere tener hijos, él le responde que entonces iban a hacer el amor y ser felices, entonces Catherine besa a Nick y empiezan a hacer el amor de nuevo. La cámara desciende bajo la cama hasta mostrar lo que Catherine estaba buscando: un picahielos, lo que sugiere que podría ser la auténtica asesina y también quiere matar a Nick.


La producción cinematográfica fue filmada completamente en 1991. Se rodó en California en muchas localidades diferentes. También se rodó en San Francisco, donde activistas homosexuales trataron de sabotear sin éxito el rodaje, porque lo consideraban homófobo y misógino. Finalmente el director tuvo que cortar 40 veces la película para no obtener la calificación de ser una película porno.

La película fue un gran éxito comercial y es catalogada como uno de los mejores thrillers de los noventa con un buen número de secuencias de gran intensidad e inolvidables. Fue la primera película de la actriz Jeanne Tripplehorn y además fue la película que convirtió a la actriz Sharon Stone en una estrella internacional.




</doc>
<doc id="29711" url="https://es.wikipedia.org/wiki?curid=29711" title="Método electroanalítico">
Método electroanalítico

Los métodos electroanalíticos son una clase de técnicas en química analítica, que estudian un analito mediante la medida del potencial eléctrico (voltios) y/o la corriente eléctrica (amperios) en una celda electroquímica, que contiene el analito. Estos métodos se pueden dividir en varias categorías dependiendo de qué aspectos de la célula son controlados y cuáles se miden. Las tres principales categorías son: potenciometría (se miden la diferencia de potenciales en el electrodo), coulombimetría (se mide la corriente de las celdas con el tiempo), y voltamperometría (se mide la corriente de las celdas mientras se altera activamente el potencial de las celdas).

La potenciometría mide pasivamente el potencial de una solución entre dos electrodos, afectando muy poco a la solución en el proceso. El potencial se relaciona entonces con la concentración de uno o varios analitos. La estructura de la celda utilizada se designa a menudo como un electrodo a pesar de que en realidad contiene "dos" electrodos: un "electrodo indicador" y un "electrodo de referencia" (distinto del electrodo de referencia utilizado en el sistema de tres electrodos). La Potenciometría generalmente utiliza electrodos construidos "selectivamente" sensibles a los iones de interés, tales como un electrodo selectivo de fluoruro. El electrodo potenciométrico más común es el electrodo de membrana de vidrio utilizado en un pH-metro.

La coulombimetría utiliza la corriente aplicada o el potencial para convertir completamente un analito (mediante oxidación o reducción electródica) de un estado de oxidación a otro. En estos experimentos, la corriente total que pasa se mide directamente o indirectamente. El conocimiento del número de electrones que han pasado nos puede indicar la concentración del analito o, cuando la concentración se conoce, el número de electrones transferidos en la reacción redox. Las formas comunes de coulombimetría incluyen la "coulombimetría potenciostática"o"coulombimetria a potencial controlado"y la "coulombimetría a intensidad constante", así como una variedad de titulaciones coulombimétricas.

La voltamperometría aplica un potencial constante y/o variable en la superficie de un electrodo y las medidas de la corriente resultante con un sistema de tres electrodos. Este método puede revelar el potencial de reducción de un analito y su reactividad electroquímica. Este método, en la práctica es un método no destructivo ya que sólo una muy pequeña cantidad del analito se consume en la superficie de bidimensional de los electrodos de trabajo y auxiliar. En la práctica, las soluciones del analito normalmente se eliminan, ya que es difícil separar el analito del electrolito soporte y el experimento requiere sólo una pequeña cantidad de analito. Un experimento normal puede implicar entre 1-10 mL con una concentración de analitos entre 1-10 mM. 

La polarometría es una subclase de voltamperometría que utiliza un electrodo de gota de mercurio como electrodo de trabajo. El electrodo auxiliar es a menudo la cubeta de mercurio resultante. La preocupación por la toxicidad del mercurio ha causado que el uso de los electrodos de mercurio se reduzca en gran medida. Materiales de electrodo alternativos, tales como los metales nobles y el carbono cristalino, son asequibles, inertes, y de fácil limpieza.

La mayoría de la amperometría es ahora una subclase de la voltamperometría en la que el electrodo se mantiene a potenciales constantes durante varios periodos de tiempo. La distinción entre "amperometría" y "voltamperometría" es principalmente histórica. Hubo un tiempo en que era difícil cambiar entre "mantener" y "escanear" un potencial. Esta función es trivial para los modernos potenciostatos, y hoy en día hay poca diferencia entre las técnicas que, o bien "mantienen", "escanean", o realizan ambas cosas en un solo experimento. Sin embargo, la terminología todavía resulta confusa, por ejemplo, la voltamperometría de pulso diferencial es también conocida como "amperometría de pulso diferencial". Este experimento puede ser visto como la combinación de la voltamperometría de barrido lineal y la cronoamperometría de ahí la confusión acerca de en que categoría debería ser nombrado. 

Una ventaja que distingue la amperometría de otras formas de voltamperometría es que en la amperometría, las corrientes medidas son un promedio (o una suma) en el tiempo. En la mayoría de las voltamperometrías, las corrientes medidas deben considerarse de forma independiente en intervalos de tiempo individuales. El promedio utilizado en amperometría da a estos métodos una mayor precisión que a la meyoría de medidas individuales de (otras) técnicas voltamperométricas. 

No todos los experimentos que históricamente fueron amperometría se incluyen ahora en el ámbito de la voltamperometría. En una valoración amperométrica, se mide la corriente, pero esto no sería considerada voltamperometría dado que la solución entera se transforma durante el experimento. Las valoraciones amperométricas son, en cambio una forma de coulombimetría.



</doc>
<doc id="29714" url="https://es.wikipedia.org/wiki?curid=29714" title="Técnica voltamétrica">
Técnica voltamétrica

La técnica voltamétrica es una técnica electroquímica en las que se aplica un determinado potencial eléctrico a un electrodo (denominado electrodo de trabajo) sumergido en una disolución que contiene una especie electroactiva y se mide la intensidad eléctrica que circula por este electrodo. La intensidad medida es función del potencial aplicado y de la concentración de la especie electroactiva presente.

Las técnicas voltamétricas tienen su origen en el año 1922, cuando el químico Jaroslav Heyrovsky desarrolló la polarografía, una técnica voltamétrica. Por esto recibió el en 1959. Se sigue empleando el término de polarografía para la voltametría que emplea electrodos de mercurio.

Según el tipo de barrido que se realice se distinguen varias técnicas. Las más usuales son las siguientes:

Cuando se emplea un potencial constante y se mide la intensidad, la técnica se denomina amperometría. Si se mide la intensidad respecto al tiempo se habla de cronoamperometría.


</doc>
<doc id="29717" url="https://es.wikipedia.org/wiki?curid=29717" title="Función gamma">
Función gamma

En matemáticas, la función gamma (denotada como formula_1, donde formula_2 es la letra griega "gamma" en mayúscula), es una aplicación que extiende el concepto de factorial a los números reales y complejos. La notación fue propuesta por Adrien-Marie Legendre. Si la parte real del número complejo formula_3 es positiva, entonces la integral

formula_4

converge absolutamente; esta integral puede ser extendida a todo el plano complejo, excepto a los enteros negativos y al cero. Si formula_5 entonces

formula_6

lo que nos muestra la relación de esta función con el factorial. De hecho, la función gamma extiende el concepto de factorial a cualquier valor complejo de formula_3. La función gamma aparece en varias funciones de distribución de probabilidad, por lo que es bastante usada tanto en probabilidad y estadística como en combinatoria.

Si la parte real del número complejo formula_3 es positiva formula_9, entonces la integral

converge absolutamente. Usando integración por partes, se obtiene la siguiente propiedad:

Esta ecuación funcional generaliza la relación formula_12 del factorial. Se puede evaluar formula_13 analíticamente:

Combinando estos dos resultados se deduce que el factorial es un caso particular de la función gamma:

para los enteros no negativos formula_16.

La función Gamma es una función meromorfa de formula_17 con polos simples en formula_18 y residuos formula_19. Estas propiedades pueden ser usadas para extender formula_20 desde su definición inicial a todo el plano complejo (exceptuando los puntos en los cuales es singular) por continuación analítica.

Las siguientes definiciones de la función gamma mediante productos infinitos, debidas a Euler y Weierstrass respectivamente, son vigentes en todo el plano complejo formula_3, excepto para valores enteros negativos: 

donde formula_24 es la constante de Euler-Mascheroni.

Es sencillo comprobar que la definición de Euler satisface la ecuación funcional, dada arriba, como sigue. Sea formula_25

También puede obtenerse la siguiente representación integral:

Encontrar formula_13 es algo fácil:

formula_29

Luego se obtiene una fórmula para formula_30 como una función de formula_31:

formula_32

Usamos integración por partes para resolver la integral:

formula_33

En el límite inferior se obtiene directamente formula_34. 

En el infinito, usando la regla de L'Hôpital formula_16 veces:

formula_36.

Por lo que se anula el primer término, formula_37, lo que nos da el siguiente resultado:

formula_38

La parte derecha de la ecuación es exactamente formula_39, con lo que hemos obtenido una relación de recurrencia:

Apliquemos la fórmula a unos pocos valores:

De la representación integral se obtiene:

Otras ecuaciones funcionales importantes de la función Gamma son la fórmula de reflexión de Euler
y la fórmula de duplicación

La fórmula de duplicación es un caso especial del teorema de multiplicación

Una propiedad básica y muy útil de la función Gamma , que puede obtenerse a partir de la definición mediante productos infinitos de Euler es:

Varios límites útiles para aproximaciones asintóticas:
\lim_{n\to\infty} \frac{\Gamma(n-\alpha)\Gamma(n+\alpha)}{\Gamma(n-\beta)\Gamma(n+\beta)} = 1;
\qquad \alpha,\beta\in\R</math>
Quizá el valor más conocido de la función Gamma con argumento no entero es:

La cual puede obtenerse haciendo formula_45 en la fórmula de reflexión o en la fórmula de duplicación, usando la relación de la función Gamma con la función beta dada más abajo con formula_46 o haciendo la sustitución formula_47 en la definición integral de la función Gamma, con lo que se obtiene una integral Gaussiana. En general, para valores impares de formula_16 se tiene:
</math>    (formula_16 impar)
donde formula_16!! denota al doble factorial. Las derivadas de la función Gamma vienen dadas por la función poligamma. Por ejemplo:

A partir de la representación integral de la función Gamma, se obtiene que su derivada formula_16-ésima es:
\,\Gamma(x) = \int_0^\infty t^{x-1} e^{-t} \ln^{n} t\,dt.</math>
La función Gamma tiene un polo de orden 1 en formula_52 para todo número entero no negativo . El residuo en cada polo es:

El teorema de Bohr-Mollerup dice que, entre todas las funciones que generalizan el factorial de los números naturales a los reales, sólo la función Gamma es logarítmicamente convexa, esto es, el logaritmo natural de la función Gamma es una función convexa.

El desarrollo en Serie de Laurent de formula_20 para valores 0 < formula_3 < 1 es:

Donde formula_55 es la función zeta de Riemann.

Gauss introdujo una notación alternativa de la función Gamma denominada función Pi, que en términos de la función Gamma es:

Así, la relación de esta función Pi con el factorial es bastante más natural que en el caso de la función Gamma:

La fórmula de la reflexión toma la siguiente forma:

Donde sinc es la función sinc normalizada, el teorema de la multiplicación se escribe así:

A veces se encuentra la siguiente definición

donde formula_61 es una función entera, definida para todo número complejo, pues no tiene polos. La razón de ello es que la función Gamma y, por tanto, la función Pi, no tienen ceros.





Fórmula válida sólo si formula_73. También aparece en la ecuación funcional de formula_71:

La función Gamma se puede calcular numéricamente con precisión arbitraria usando la fórmula de Stirling, la aproximación de Lanczos o la aproximación de Spouge.

Para argumentos que sean múltiplos enteros de 1/24, la función Gamma puede ser evaluada rápidamente usando iteraciones de medias aritmético geométricas (véase Valores de la función Gamma).

Debido a que tanto la función Gamma como el factorial crecen muy rápidamente para argumentos moderadamente grandes, muchos programas de computación incluyen funciones que devuelven el logaritmo de la función Gamma. Este crece más lentamente, y en cálculos combinatorios es muy útil, pues se pasa de multiplicar y dividir grandes valores a sumar o restar sus logaritmos.

La formula_16-ésima derivada de formula_78 (donde n es un número natural) se puede ver de la siguiente manera: 

como formula_80 entonces 

donde formula_16 puede ser cualquier número donde gamma esté definido o se pueda definir mediante límites. De esta manera se puede calcular por ejemplo, la 1/2 derivada de formula_83, de formula_84 e inclusive de una constante formula_85:





</doc>
<doc id="29725" url="https://es.wikipedia.org/wiki?curid=29725" title="Potenciometría">
Potenciometría

La potenciometría es una técnica electroanalítica con la que se puede determinar la concentración de una especie electroactiva en una disolución empleando un electrodo de referencia (un electrodo con un potencial conocido y constante con el tiempo) y un electrodo de trabajo (un electrodo sensible a la especie electroactiva) y un potenciómetro.

Existen electrodos de trabajo de distinto tipo útiles para distintos cationes o aniones. Cada vez son más usados los electrodos selectivos de iones (ESI) o electrodos de membrana. Uno de los más empleados, que se comenzó a utilizar a principios del siglo XX, es el electrodo de pH (un electrodo de vidrio). 
Tipos de electrodos:


También existen diferentes tipos de electrodos Indicadores:

- Electrodos de Membrana:

- Electrodos Indicadores Metálicos:
También se emplea la potenciometría en distintas aplicaciones como en sondas sensibles a gases o líquidos, para valoraciones potenciométricas.


</doc>
<doc id="29728" url="https://es.wikipedia.org/wiki?curid=29728" title="Α">
Α

Alfa (Α α) es la primera letra del alfabeto griego. En griego antiguo su nombre era "alpha" , nombre que deriva de la antigua letra fenicia ʾalp 'buey'. Su origen gráfico es una cabeza de buey invertida, Las letras que surgieron de Alfa incluyen a la A latina y la A cirílica.

Como es la primera letra del alfabeto, el alfa era usado para denotar el principio de algo, como opuesto de omega, que simbolizaba el fin. Por ejemplo, «Yo soy el Alfa y la Omega, el primero y el último, el que es, era y ha de venir" ("Apocalipsis" 22.13).



</doc>
<doc id="29731" url="https://es.wikipedia.org/wiki?curid=29731" title="Brújula (constelación)">
Brújula (constelación)

Pyxis o la Brújula es una constelación pequeña y débil del hemisferio celeste meridional. Fue introducida en el siglo XVIII por Nicolas-Louis de Lacaille con el nombre de Pyxis Nautica y se cuenta entre las ochenta y ocho constelaciones modernas. Está situada cerca de la antigua constelación de Argo Navis; en el siglo XIX John Herschel sugirió renombrarla Malus, el Mástil, pero la sugerencia no prosperó. La Brújula es completamente visible en latitudes más al sur de los 53 grados norte, siendo febrero y marzo los meses de mejor visibilidad.

Esta constelación está atravesada por la Vía Láctea. Sus tres estrellas más brillantes, Alfa, Beta y Gamma Pyxidis, están dispuestas aproximadamente en línea. Con una luminosidad 22 000 veces mayor que la del Sol y 3,68 de magnitud visual, Alfa Pyxidis es una estrella blancoazulada, la más brillante de la constelación. Cerca de Alfa se encuentra T Pyxidis, una nova recurrente que aumenta su brillo hasta la magnitud 7 cada pocas décadas. Tres estrellas tienen sistemas planetarios, todos ellos descubiertos por espectroscopia Doppler.

El astrónomo francés Lacaille describió por primera vez la constelación como "La Boussole" (la Brújula) en 1752  después de haber observado y catalogado cerca de 10 000 estrellas de los cielos australes durante su estancia en el cabo de Buena Esperanza. Ideó catorce nuevas constelaciones en las regiones inexploradas del hemisferio sur celeste que no son visibles desde Europa y les dio nombres de instrumentos científicos que simbolizaban la Ilustración. Lacaille latinizó el nombre como Pixis ("sic") Nautica en las cartas que publicó en 1763. Los antiguos griegos habían identificado las cuatro estrellas principales de la Brújula como el mástil de la nave Argos.

El astrónomo alemán Johann Bode estableció en 1801 la constelación Lochium Funis (la Corredera) alrededor de la constelación de la Brújula, pero la descripción no prosperó. En 1844, John Herschel trató de resucitar la configuración clásica de Argo Navis renombrándola Malus, el Mástil, una sugerencia seguida por Francis Baily. Más tarde Benjamin Gould restauró la nomenclatura de Lacaille.

En la Antigua China las estrellas Alfa, Beta y Gamma Pyxidis, junto con estrellas de la vecina Antlia, formaban parte de Tianmiao, un templo celeste en honor de los antepasados del emperador.

La Brújula ocupa la posición 65 al cubrir 220,8 grados cuadrados de cielo nocturno (un 0,535 %). Dada su posición en el hemisferio celeste sur, toda la constelación es visible hasta la latitud , aunque partes se pueden ver hasta la latitud . Está limitada al norte por Hidra, al oeste por la Popa, al sur por las Velas y las este por la Bomba Neumática. La abreviatura de tres letras adoptada por la Unión Astronómica Internacional en 1922 es «Pyx». Los límites oficiales fueron establecidos por Eugène Delporte en 1930 y delimitan un polígono de ocho lados. En el sistema de coordenadas celestes, las ascensiones rectas están comprendidas entre 8 h 27,7 m y 9 h 27,6 m, mientras que las declinaciones límite son y .






</doc>
<doc id="29735" url="https://es.wikipedia.org/wiki?curid=29735" title="Β">
Β

Beta (Β, β ϐ) es la segunda letra del alfabeto griego. En griego antiguo se pronunciaba [b], en griego moderno se pronuncia [v].

En el alfabeto fonético internacional, es la letra que representa la fricativa bilabial sonora.

No se debe confundir la beta con la letra ß ("eszett"), una ligadura de las letras ese y zeta, ſs o ss del idioma alemán.




</doc>
<doc id="29737" url="https://es.wikipedia.org/wiki?curid=29737" title="Γ">
Γ

Gamma (Γ γ) es la tercera letra del alfabeto griego. Deriva de la letra gaml () del fenicio. En griego moderno, se pronuncia como fricativa velar sonora [ɣ] o como fricativa palatal sonora [ʝ]. En griego antiguo, se pronunciaba como oclusiva velar sonora [ɡ]. Tanto en griego antiguo como en griego moderno, gamma se pronuncia [ŋ] delante de velares (incluyendo la propia gamma, así, una doble gamma se pronuncia [ŋɡ] en ambas variantes del idioma: "άγγελος", ['angelos] "mensajero").

En la numeración griega, gamma tiene el valor de tres (ɣʹ). 

Las letras romanas C y G y las cirílicas Ge Г y Ghe Ґ provienen de la griega gamma.

Gamma se usa a menudo para denotar una variable en matemáticas y física.
En algunos campos tiene significados específicos.


La letra mayúscula Γ se utiliza como símbolo de:

La letra minúscula γ se utiliza como símbolo de:




</doc>
<doc id="29739" url="https://es.wikipedia.org/wiki?curid=29739" title="Ε">
Ε

Épsilon (Ε ε ϵ) (literalmente "e breve") es la quinta letra del alfabeto griego.









</doc>
<doc id="29740" url="https://es.wikipedia.org/wiki?curid=29740" title="Ϝ">
Ϝ

Digamma (Ϝ, ϝ) es una letra obsoleta del alfabeto griego y tiene un valor numérico de 6 (ϝʹ). 

Cuando se usa como un numeral, digamma es escrita usando la marca stigma (ς). Cuando se la usa como letra, tiene la forma de una F, que parece como la solapación de dos letras gamma mayúsculas (Γ) (de ahí el nombre "digamma"= doble gamma) y tiene el valor fonético /w/. 

Deriva de la letra del alfabeto fenicio wau, que originalmente significaba "gancho". El signo de la digamma griega dio origen a la F latina y luego románica.

Existe una variante gráfica de digamma en pánfilo: .

El "pánfilo" es un dialecto poco documentado y aislado del griego antiguo (600 a 330 a. de C.) que se habló en la costa sur de Asia Menor.

Esta variante está codificada en Unicode como "Letra griega pánfila mayúscula "digamma"" U+0376 (Ͷ) y "Letra griega pánfila minúscula "digamma"" U+0377 (ͷ).

No debe confundirse con la letra cirílica (usada entre otros por el ruso) de forma casi idéntica: И , y que tiene el valor fonético /i/ o /ɪ/.



</doc>
<doc id="29741" url="https://es.wikipedia.org/wiki?curid=29741" title="Ζ">
Ζ

La dseta o zeta (Ζ, ζ) es la sexta letra del alfabeto griego y tiene un valor de 7 en el sistema de numeración griega. Su pronunciación exacta en griego antiguo sigue siendo motivo de discusión pues hay quienes insisten que era [zd], mientras que otros más arguyen que era [dz] (y hay evidencia en favor de las dos posiciones), lo que está claro es que en su evolución terminó teniendo el sonido [z] que es el que actualmente tiene en griego moderno.

Fue adoptada tal cual por los romanos para transliterar palabras que la llevaran en el original griego. Lo hicieron, por ejemplo, con "Zephyrus", "zeugma" o "zeta" (que son en castellano: céfiro, zeugma, zeta). 

Hasta su diccionario de 1992 la Real Academia Española denominaba "zeta" a ζ y "theta" a θ; y en la edición de 2001, como "dseda" (Ζ, ζ) y "zeta" (Θ, θ). En la actualidad, los nombres normativos son "dseta" (Ζ, ζ) y "zeta" (Θ, θ).



</doc>
<doc id="29742" url="https://es.wikipedia.org/wiki?curid=29742" title="Kokopelli">
Kokopelli

Kokopelli es conocido como dios de la fertilidad en la mitología de los americanos nativos del suroeste de Estados Unidos. Además, Kokopelli es un antiguo dios de la fertilidad y la comunicación poderosa de los hopi.

Travieso, curandero y cuenta cuentos, Kokopelli ha sido fuente de asombro durante siglos y en diferentes países. Kokopelli encarna el auténtico suroeste norteamericano, el cual data de hace alrededor de 3000 años de antigüedad, cuando se tallaron los primeros petroglifos.

Aunque sus auténticos orígenes son desconocidos, este flautista y viajero «casanova» es una figura sagrada para muchos nativos del sudoeste norteamericano. Su figura jorobada ha sido encontrada en pinturas y grabados en muros de roca y cantos rodados por todo el sudoeste norteamericano.

Hay muchos mitos sobre el famoso Kokopelli. Uno de ellos es que viajaba de aldea en aldea trayendo el cambio de invierno a primavera, derritiendo las nieves y trayendo la lluvia para propiciar las cosechas. Se dice también que la joroba de su espalda representaba los sacos de semillas y las canciones que portaba.

La leyenda también cuenta que el sonido de su flauta simbolizaba el paso del invierno a la primavera. Se decía que la flauta de Kokopelli se podía escuchar en la brisa de primavera, mientras entraba la calidez estival.

También se cuenta que Kokopelli era la fuente de la concepción humana. La leyenda dice que todo el mundo bailaría y cantaría durante toda la noche cuando escucharan la flauta de Kokopelli.

A la mañana siguiente incluso las doncellas estarían embarazadas. (Nota del traductor: en inglés "maiden" se refiere tanto a mujeres vírgenes como a solteras).

Cualquiera que sea el verdadero significado de Kokopelli, ha sido fuente de inspiración musical y danza, y ha repartido alegría a los que le rodeaban. Incluso hoy, Kokopelli, con su joroba y su flauta, es siempre bienvenido en las casas de los nativos americanos.

También hay versiones más antiguas o locales, narradas así por nativos locales, que Kokopelli era femenina, una Diosa, que no conocía el amor, y un día en uno de sus viajes se encontró con Trully,( la palabra significa "lobo solitario")que era un viajero, al toparse con Kokopelli hubo un momento mágico, la historia dice, pero Kokopelli no debía estar mucho tiempo alejada de los cielos, un día tuvo que marcharse, dejando a Trully, desde entonces es vista la silueta de un lobo en los riscos ahuyando a la luna, donde se dice que vive Kokopelli, la diosa le contesta con el hermoso sonido de su flauta, que representa el viento de la primavera.




</doc>
<doc id="29745" url="https://es.wikipedia.org/wiki?curid=29745" title="Η">
Η

Eta (Η η) es la séptima letra del alfabeto griego. Su pronunciación en griego antiguo era como e larga [e:], mientras que en griego moderno se pronuncia [i] y se llama ita (Ήτα). Su origen es la letra del alfabeto fenicio ḥēt, y es a su vez el origen de la letra H latina y la letra cirilica И.



</doc>
<doc id="29746" url="https://es.wikipedia.org/wiki?curid=29746" title="Parálisis de Erb-Duchenne">
Parálisis de Erb-Duchenne

La parálisis de Erb Duchenne consiste en una parálisis de los nervios periféricos cervicales V y VI (C5 y C6), que forman parte del plexo braquial superior (monoparesia braquial).

Su manifestación principal es una pérdida de la movilidad del brazo con o sin afectación del antebrazo y de la mano, aunque lo habitual es la afectación de la totalidad del miembro. Su origen se encuentra en la distensión o rotura de los nervios del plexo braquial en partos difíciles.

La posición característica del brazo es aducción (acerca un miembro o un órgano al plano medio del cuerpo) con pronación (rotación interna) de brazo y antebrazo; se conserva el poder de extensión del antebrazo, pero no del brazo.

El tratamiento consiste en rehabilitación física, la cual tiene como finalidad estimular la regeneración y reparación de los nervios dañados. El nivel de recuperación que se alcanza depende de la gravedad de la lesión, sin embargo la mayoría de los casos quedan con secuelas.


</doc>
<doc id="29748" url="https://es.wikipedia.org/wiki?curid=29748" title="Zeta">
Zeta

El término Zeta puede tener cualquiera de las siguientes acepciones:










</doc>
<doc id="29749" url="https://es.wikipedia.org/wiki?curid=29749" title="Ϡ">
Ϡ

Sampi (mayúscula Ϡ, minúscula ϡ; forma arcaica epigráfica (jónica): ) es una letra obsoleta del alfabeto griego. Se utilizó esta letra como letra alfabética en algunos dialectos jónicos del griego antiguo. Su valor fonemático era [ts] o [ss]. Se mantuvo en uso como numeral en la numeración griega. Tiene un valor de 900 (ϡʹ) en el sistema de numeración griego. El nombre "sampi" parece proceder de "[o]sàn pî": "como pi", ya que su forma bizantina (ϡ) era semejante al de una pi cursiva ("π")




</doc>
<doc id="29751" url="https://es.wikipedia.org/wiki?curid=29751" title="Ι">
Ι

Iota (Ι ι) es la novena letra del alfabeto griego y tiene un valor de 10 (ιʹ) en el sistema de numeración griega.

La palabra iota es usada en inglés y en francés para expresar pequeñas cantidades. Según algunos relatos su uso proviene de una disputa teológica del siglo III y IV de la doctrina arriana de que Jesús (Hijo) y el Dios (Padre) eran naturalezas distintas pero similares, en griego "homoi-ousios", contra la doctrina ortodoxa para la cual eran una misma naturaleza, en griego "homo-ousios". Las dos palabras difieren únicamente en una iota.


</doc>
<doc id="29752" url="https://es.wikipedia.org/wiki?curid=29752" title="Σ">
Σ

Sigma (Σ σ ς) es la decimoctava letra del alfabeto griego. La sigma minúscula tiene dos formas: al final de una palabra, se usa la forma ς; al inicio y en medio de palabra se usa la forma σ.

En el sistema de numeración griega tiene un valor de 200 (σʹ).

La mayúscula Σ se usa como símbolo para:

La minúscula σ se usa como símbolo para:




</doc>
<doc id="29754" url="https://es.wikipedia.org/wiki?curid=29754" title="Commonwealth (desambiguación)">
Commonwealth (desambiguación)

Commonwealth, término inglés para mancomunidad (literalmente "riqueza común", y que puede traducirse también por"república" -"res publica"-) puede referirse a:










</doc>
<doc id="29756" url="https://es.wikipedia.org/wiki?curid=29756" title="Medicamento huérfano">
Medicamento huérfano

Los medicamentos huérfanos son medicamentos no desarrollados ampliamente por la industria farmacéutica por razones financieras, ya que van destinados a un reducido grupo de pacientes, y que, sin embargo responden a necesidades de salud pública.

Fue Estados Unidos el primer país que desarrolla una ley, la Orphan Drug Act en 1983, haciendo que el estatus de "huérfano" permita subvenciones para el desarrollo de estos productos, así como otras medidas de apoyo, hasta su aprobación comercial. La Unión Europea inició una política común en este ámbito en 1999.

Según la Unión Europea, un medicamento huérfano es aquel que cumple los requisitos:





</doc>
<doc id="29757" url="https://es.wikipedia.org/wiki?curid=29757" title="Κ">
Κ

Kappa o Cappa (Κ κ) es la décima letra del alfabeto griego. Tiene un valor de 20 veinte (κʹ) en el sistema de numeración griega.


</doc>
<doc id="29758" url="https://es.wikipedia.org/wiki?curid=29758" title="Λ">
Λ

Lambda (Λ λ) es la undécima letra del alfabeto griego.

La letra lambda se usa como símbolo para representar:
El signo de lambda, representa sacos amnióticos con corion entre las membranas y es patognomónica de bicorionicidad en los embarazos gemelares.


</doc>
<doc id="29760" url="https://es.wikipedia.org/wiki?curid=29760" title="Ω">
Ω

Omega (Ω: mayúscula; ω: minúscula; en griego ὦμέγα) es la vigésima cuarta y última letra del alfabeto griego. Su forma recuerda a una Ο abierta por abajo. Fonéticamente se pronunciaba como una O larga de apertura media. Su nombre literal, "o grande", se contrapone al de la letra ómicron (ὂμικρόν, "o pequeña"), que en griego antiguo se pronunciaba como una o breve cerrada. En griego moderno, ambas letras se pronuncian como o abierta. Esta denominación es de origen bizantino: en la Grecia antigua, se llamaba "ō" (ὦ) a la omega y "ou" (οὖ) a la ómicron. En el sistema de numeración griega tiene un valor de 800 ochocientos (ωʹ).

Como es la última letra del alfabeto, puede ser usada para denotar el fin de algo, como antónimo de alfa, que simboliza el comienzo. Por ejemplo, «Yo soy el alfa y el omega, el primero y el último, el principio y el fin» ("Apocalipsis" 22, 13). 

El origen del símbolo de omega mayúscula lleva tiempo siendo tema de debate entre los expertos, pues es anterior a la creación del alfabeto griego y se da en diferentes culturas sin relación entre sí. Dio origen a letras de otros alfabetos, como la omega cirílica (Ѡ), actualmente obsoleta, o la runa odal (ᛟ) del futhark antiguo.

La letra minúscula ω se usa como símbolo:

La omega se ha empleado para simbolizar:




</doc>
<doc id="29762" url="https://es.wikipedia.org/wiki?curid=29762" title="Μ">
Μ

Mi (según la Real Academia Española desde 1992), my (según la RAE hasta 1992) o mu (denominada así, de manera extraordinaria, en el ámbito de la ciencia) (Μ, μ) es la duodécima letra del alfabeto griego. Su fonema corresponde al de la eme española. En griego los grafemas Μ y μ son llamados "mŷ". En el sistema de numeración griega el signo μ proseguido con un signo de numeral (μʹ) tiene el valor de 40.

La letra μ minúscula se emplea como símbolo en muchas disciplinas diferentes. Su forma mayúscula en cambio es raramente usada, pues es indistinguible de la letra M latina. Dentro de sus usos se puede destacar:





Raras veces se usa para distinguir entre un objeto representado por la letra "m" de otro de la misma naturaleza.


</doc>
<doc id="29763" url="https://es.wikipedia.org/wiki?curid=29763" title="Mu (continente perdido)">
Mu (continente perdido)

Mu, el continente de Mu o el continente perdido de Mu, es el nombre de un supuesto continente perdido cuyo concepto y nombre fueron propuestos por el escritor y viajero del siglo XIX Augusto Le Plongeon, quien afirmó que varias civilizaciones antiguas, como las de Egipto y Mesoamérica, fueron creadas por refugiados de Mu, el cual estaba localizado en el Océano Pacífico. Para formular tales afirmaciones se apoyaba en la traducción al español (posteriormente desechada por fantasiosa) del códice maya conocido como "Códice Tro-Cortesiano" realizada por Brasseur de Bourbourg. Él lo situaba en el Océano Pacífico y sostenía que esa civilización perdida hace miles de años extendió su avanzada tecnología por todo el mundo; la que habría permitido la construcción de las grandes pirámides que se encuentran esparcidas en distintos continentes. Además sostenía que, tal como la Atlántida, este continente habría sido destruido hace 12.000 años por los dioses como castigo por ser una civilización decadente.

Más tarde este concepto fue popularizado y difundido por , pero a diferencia de los anteriores autores sostenía que Mu estuvo localizado en el Océano Pacífico.

La existencia de Mu ya era cuestionada en la época de Le Plongeon. Actualmente los científicos descartan el concepto de Mu (y de otros presuntos continentes perdidos, tales como Lemuria) como físicamente imposible, ya que el avance de la ciencia ha demostrado la imposibilidad de este hecho por las pruebas ofrecidas por la tectónica de placas, que descarta la existencia de un continente perdido, argumentando que un continente no puede ni hundirse ni ser destruido en un periodo tan breve como el que pretenden sus partidarios. Actualmente se considera que las pretensiones sobre la existencia de Mu carecen de fundamento.

Además los arqueólogos rechazan el parentesco entre las culturas nativas de América y las de la cuenca mediterránea, de Oriente Medio o de la India, tal como lo sugieren los defensores de Mu.

En la actualidad la creencia en la existencia de este supuesto continente solo es sostenida por grupos ocultistas o de la "New Age", quienes apoyan sus afirmaciones en algunos descubrimientos, tales como el de Yonaguni.

Según los partidarios de la existencia de Mu, las referencias supuestamente encontradas por Churchward sobre una tierra más allá del gran mar oriental, el Océano Pacífico, patria de una gran civilización solar, cuna de la antigua cultura del valle del Indo, se convirtió en un importante descubrimiento para Mu sumado al de Le Plongeon; y la hipótesis de Churchward estaría confirmada, ya que los jeroglíficos de la cultura del Indo eran sorprendentemente parecidos a los de la isla de Pascua, cercana a la ubicación más conocida de Mu..

Igualmente se dice que se habrían hallado gran cantidad de textos budistas que mencionan una tierra que, según parece, se encontraba en el océano Pacífico..

Se alude a su vez sobre la existencia de una prueba bastante contundente. Parece ser la serie de indicios de una supuesta emigración masiva ocurrida hace miles de años desde la zona este del Pacífico, cerca de la costa sudamericana.Teoría del poblamiento temprano.

Estos indicios (que constituían hechos aislados) sugerirían que varias civilizaciones antiguas tendrían un origen común en Mu. Estas son el antiguo Egipto, Asiria, la civilización del valle del Indo, el Perú preincaico, la isla de Pascua y, en menor medida, las culturas pre-mayas. Compartirían similitudes artísticas, arquitectónicas y lingüísticas (todas ellas utilizaban un sistema de escritura jeroglífica), creencias comunes (de tipo solar), y Egipto, Asiria y los nativos pascuenses conservarían una leyenda bastante similar: en Egipto y Asiria se trataba del mito originario de la Atlántida, y en Pascua existe la memoria de una antigua patria llamada Hiva, que se hundió por un cataclismo, y que produjo una emigración a la isla de Pascua..

El testimonio de los aymaras de Perú y Bolivia constituye otra evidencia de esta memoria común, ya que también hacen referencia a esta tierra perdida y en la misma ubicación, aunque en este caso la isla-continente se llama "Atl-Antis" (tierra antigua), de cuyo nombre no puede negarse el enorme parecido con Atlántida. Lo cual no tiene sentido, ya que Mu y la Atlántida son dos continentes distintos.

Los defensores de esta teoría afirman que más al sur del continente americano, las leyendas sobre la desaparecida tierra ancestral se realzan en la mitología del pueblo indígena mapuche, del sur de Chile (y en la actualidad también en la Patagonia Argentina), quienes incluso mencionarían ser descendientes de una raza venida de una tierra tragada por las aguas, la cual justamente según su historia llevaría el nombre de Mu. Además esta tradición se podría apreciar en la leyenda de Ten Ten y Cai Cai, quienes luchan representando el bien y el mal por la existencia de la "gente de la tierra" (mapuche). Según los mapuches hace mucho tiempo existieron dos serpientes, llamadas Ten ten Vilu y Cai cai Vilu, Ca Cai era la que dominaba las aguas y Ten Ten la que dominaba el fuego. Un día Cai Cai se enojó y con su cola (que era como la de un pez) comenzó a golpear las aguas, las cuales inundaron toda la región. Como la población y los animales estaban desesperados, pues ya casi no quedaba terreno sin agua y las aguas seguían subiendo, invocaron desesperados la ayuda de Ten Ten, quien vino en su ayuda y subió a todos (humanos y animales) sobre su lomo y llevándolos hacia los montes y lugares altos, salvándoles de morir ahogados. Bajaron las aguas y todos siguieron su vida tranquila, pero sin embargo, las aguas no volvieron del todo a su lugar original, y se le da esta razón a la existencia de muchas islas pequeñas (que hacen de Chiloé un gran archipiélago); y por la misma subida de agua, hubo mucha gente que no alcanzó a ser salvada por Ten Ten Vilú, por lo que fueron arrastrados por las aguas, pero según el mito, en vez de morir ahogados, se convirtieron en toninas y lobos marinos (especies de la zona). Hasta que fue esta vez Ten Ten quien se encolerizó y comenzaron a hacer erupción todos los volcanes y la población tuvo que mudarse a otros lugares más seguros. Sin embargo, dentro de las leyendas y mitos que conforman la Mitología mapuche no existe mención alguna a una tierra ancestral que haya sido tragada por las aguas ni mucho menos que su nombre haya sido Mu, ni que su pueblo haya provenido de tal tierra inexistente.

El hundimiento de estas tierras, sin embargo, parece haberse producido por la inminente colisión de esta isla con la placa sudamericana, que está deslizándose continuamente sobre el fondo del océano Pacífico, dando lugar al hundimiento de Mu bajo la corteza y la formación del altiplano boliviano, de formación bastante reciente.. No obstante, desde la aparición del Homo sapiens hace 200.000 años en África hasta la actualidad, la subplaca de Nazca se ha deslizado solo 500 metros bajo el continente americano (a razón de 2.5 cm anuales), lo que invalida esta teoría.

Michel Desmarquet en su libro Thiaoouba Prophecy (1993) informa que Mu (Lamar) fue colonizada hace 250,000 años por extraterrestres, quienes serían ancestros de la actual raza polinesia, trajeron el perro, el armadillo y el puerco. Eran muy avanzados espiritual y tecnológicamente, fundaron 19 ciudades, 7 de ellas sagradas y en la capital Savanasa construyeron una pirámide 3 veces más grande que La Gran Pirámide de Giza. Hace 200,000 años se expandieron por Nueva Guinea y el sudeste de Asia (el oeste de Mu), también en Centroamérica y Sudamérica no muy lejos del lago Titicaca). En Tiahuanaco se construyó un enorme puerto, en ese entonces había un mar interior (ahora Brasil) que conectaba con el océano Atlántico. Llegaron a la Atlántida hace 30,000 años y a Europa hace 17,000. Influyeron en los griegos (el alfabeto griego es el mismo de Mu), en el norte de África dieron conocimientos materiales y espirituales a los árabes (los números arábigos provienen de los mayas-atlantes de Mu), construyeron la Gran Pirámide en Egipto (cada vez que ellos consideraban que una nueva colonia tenía el potencial de ser grandiosa, tanto espiritual como materialmente hablando, construían una pirámide). Luego hace unos 14,500 años atrás un cataclismo volcánico, en el que se crean las montañas en toda América, destruye el continente de Mu. Nunca más se volvió a ver el esplendor de esta civilización.

Todas las similitudes de culturas y leyendas argumentadas para apoyar esta hipótesis son compatibles con la teoría de la Llegada del hombre a América a través del Puente de Beringia durante una glaciación, cuya fusión da lugar a las leyendas del hundimiento continental en las culturas a ambos lados del estrecho de Bering.

El primero en proponer la existencia del continente Mu como una posibilidad fue el coronel James Churchward, oficial del ejército británico en la India. Todo empezó cuando se hizo amigo de un sumo sacerdote de un templo hindú que tenía en su poder unas tabletas de barro que habían sido guardadas y olvidadas a lo largo de los años por los sacerdotes hindúes. 
Con el paso del tiempo, Churchward y el sacerdote hindú fueron descifrando la existencia de una civilización madre que había crecido, florecido y repentinamente decaído. Churchward siguió recopilando datos de este enorme rompecabezas cuyo resultado fue una extensa imagen de Mu narrada en el libro "Mu el continente perdido".

Sin embargo, otra versión de la historia, mucho más escéptica, dice lo siguiente: Que todo habría sido originado por un error de traducción.

En 1864, el abate Brasseur estaba intentando traducir un códice maya usando un “alfabeto” compilado por el conquistador Diego de Landa.

Ahora bien, la escritura maya era algo similar a la japonesa o la egipcia, ya que usaba ideogramas que también tenían valor fonético: por lo tanto carecía de alfabeto. Lo que el español había encontrado era un conjunto de símbolos que, leídos en voz alta, sonaban como las letras del alfabeto español.

Brasseur entendió que el códice narraba una catástrofe volcánica que había destruido un continente entero. Su nombre se expresaba en dos símbolos que correspondían a las letras “M” y “U”. Nacía Mu. 

Apenas cuatro años después apareció en escena el coronel James Churchward. Churchward decía haber descubierto en las bóvedas de un templo hindú toda una biblioteca de tablillas escritas en una lengua desconocida. En ellas había logrado descifrar toda la historia, la ciencia y la filosofía de Mu.

Posteriormente Churchward escribiría una decena de libros sobre Mu. En ellos convertiría a Mu en la Atlántida del Pacífico, atribuyéndole una antigüedad que oscilaba entre los 25.000 y los 20.000 años. Mu desplazaba a la Atlántida como origen de todas las civilizaciones conocidas, desde la egipcia hasta la maya, incluyendo también a los atlantes. En la sabiduría de Mu se habían originado tanto la Biblia como los principios de la masonería. Sus habitantes habían ido tan lejos como para hacer revelaciones acerca de Jesucristo, que recién iba a nacer muchos milenios después.

En la actualidad los libros de Churchward se siguen reeditando y ofreciendo en Internet. En algunas páginas de “turismo energético”, Lemuria y Mu aparecen encarnando “el espíritu de Hawái”.


En La Canción de La Agrupación Griega Therion "Lemuria " , se realiza mención de la "Tierra De MU " comparándola a su vez Con la Ciudad del Dorado .




</doc>
<doc id="29764" url="https://es.wikipedia.org/wiki?curid=29764" title="Sibila Herófila">
Sibila Herófila

Herófila, también llamada la sibila eritrea, era una sibila oriunda de Marpeso en la llamada Tróade (región donde se encontraba Troya). Era hija de una ninfa y un padre mortal llamado Teodoro, que era pastor del Ida. Herófila fue quien predijo la guerra de Troya, anunciando que la responsable sería una mujer llamada Helena. Pasó la mayor parte de su vida en Samos. Se dice que compuso un himno en honor de Apolo (el dios que inspiraba las profecías).

Llevaba siempre con ella una piedra sobre la que se subía para presagiar. Esta piedra se conservó en Delfos después de su muerte y en tiempos del historiador Pausanias se mostraba a los interesados. También se enseñaba su tumba que estaba en el bosque de Apolo Esminteo. Los habitantes de Eritras decían que había nacido en su territorio, en una cueva del monte Córico.

Murió en Tróade.




</doc>
<doc id="29769" url="https://es.wikipedia.org/wiki?curid=29769" title="Υ">
Υ

Ípsilon (o ipsilón, según la pronunciación griega ὒψιλόν) (Υ υ) es la vigésima letra del alfabeto griego. Antiguamente se usó la ortografía ýpsilon, que tiene el inconveniente de usar un acento sobre la letra "y". En griego antiguo clásico se pronunció [y], como la u en francés o la ü en alemán. En griego moderno se pronuncia [i], salvo en algunos diptongos en que se pronuncia como [f] (ej. ελευθερία, "elefthería") o [v] (ej. ευαγγέλιον "evaggélion"). Es el origen de la letra "y" (la "i griega"), al igual que de las letras "u, v, w" del alfabeto latino.



</doc>
<doc id="29773" url="https://es.wikipedia.org/wiki?curid=29773" title="Π">
Π

Pi (Π π ϖ) es la decimosexta letra del alfabeto griego. Tiene un valor de 180 (πʹ) en el sistema de numeración griega.

La letra mayúscula Π se usa como símbolo para:

La letra minúscula π se usa como símbolo para:
----


</doc>
<doc id="29776" url="https://es.wikipedia.org/wiki?curid=29776" title="Ο">
Ο

Ómicron, ὂμικρόν (Ο ο), es la decimoquinta letra del alfabeto griego, tiene un valor de 70 (οʹ) en el sistema de numeración griega y significa literalmente "o pequeña", en contraposición al de la letra omega (Ω: mayúscula; ω: minúscula; en griego, ὦμέγα, "o grande"), pronunciada en griego antiguo como una o larga de apertura media.

La letra minúscula Ο se usa como símbolo para:


</doc>
<doc id="29777" url="https://es.wikipedia.org/wiki?curid=29777" title="Turbo Pascal">
Turbo Pascal

Turbo Pascal es un sistema de desarrollo de software que incluye un compilador y un entorno de desarrollo integrado (IDE) para el lenguaje de programación Pascal, desarrollado por Borland y liderado por Philippe Kahn. 

Fue lanzado en 1983 para MS-DOS, CP/M, CP/M-86 y, posteriormente, para Microsoft Windows, fue durante mucho tiempo la versión más extendida del lenguaje Pascal. También hubo una versión de corta vida para Apple Macintosh.

El nombre de "Borland Pascal" fue generalmente reservado para los paquetes de gama alta para Microsoft Windows (con más bibliotecas y la biblioteca estándar de código fuente), mientras que la original, más barato y ampliamente conocida versión fue vendido como "Turbo Pascal".

Borland lanzó siete versiones de Turbo Pascal: 1.0 a 5.5 (Orientado a Objetos), 6 y 7 para MS-DOS. Fue sustituido por Borland Delphi.

Turbo Pascal 1.0, 1983. Compila directamente en código máquina. Requiere 32 kilobytes de RAM. Tiene un compilador integrado / editor, de alta velocidad de compilación. 

Turbo Pascal 2.0, 1984. Se aumentó el tamaño del programa generado – permite hasta 64 kilobytes de código, pila y datos. Versión para DOS soporta coprocesador matemático y decimales aritmética binaria (con código binario decimal. 

Turbo Pascal 3.0, 1985. Apoyo a la estructura de recubrimiento. Apoyo a los modos gráficos. Especial sub-imagen para el PC compatible ordenadores de IBM, incluyendo la “concha de tortuga” horario. Kit de herramientas. 

Turbo Pascal 4.0, 1987. Separa la compilación de los módulos. El tamaño está limitado solo por la RAM. Menú impulsado por el entorno de desarrollo integrado, módulos inteligentes de diseño. Stand-alone de línea de comandos del compilador. Ayuda sensible al contexto del sistema. 

Turbo Pascal 5.0, 1988. Restaura el apoyo a las estructuras de recubrimiento. Construido el depurador. Separa depurador (Turbo Debugger). Emulación del coprocesador matemático. Soporte para controladores gráficos BGI (Borland Graphic Interface). 

Turbo Pascal 5.5, 1989. Programación orientada a objetos. Posibilidad de copiar los ejemplos de programa desde el sistema de ayuda. Posee el Turbo Profiler que permite optimizar el código. 

Turbo Pascal 6.0, 1990. Incluye la Biblioteca Turbo Vision. El IDE nuevo, volver a escribir utilizando Turbo Vision, empleo del ratón y soporta múltiples archivos al mismo tiempo de edición en diferentes ventanas. La capacidad de especificar el depurador integrado y condicionales en el número de puntos de interrupción. Incorpora ensamblador, permite MLTB en el 
Turbo Pascal para Windows. Creación de programas de 16 bits en Windows. Servicios de biblioteca de objetos de Windows Library (OWL), Similar en la ideología a la de Turbo Visión. IDE gráfico. 

Borland Pascal 7.0, 1992. Fecha de lanzamiento de Borland Pascal 7.0, incluye un Turbo Pascal 7.0 más barato y menos potente que también suministra por separado. BP 7.0, permite crear programas en modo real.

El lenguaje de programación Pascal es un lenguaje de alto nivel y propósito general (aplicable a una gran cantidad de aplicaciones diversas) desarrollado por el profesor suizo Niklaus Wirth (Instituto Tecnológico de Zúrich, Suiza). El propósito de Wirth era crear un lenguaje para la enseñanza de técnicas de programación a estudiantes universitarios. Pero a medida que pasaban los años, Pascal se iba convirtiendo en un estándar en el mundo de la programación.

Una versión preliminar del lenguaje apareció en 1968 y el primer compilador totalmente completo apareció a finales de 1970. Desde entonces, muchos compiladores han sido construidos y están disponibles para diferentes máquinas. Durante muchos años, el libro Pascal User Manual and Report, publicado por Wirth y Kathleen Jensen en 1974, ha servido de facto como estándar de todas las versiones.

Las diferentes versiones ofrecían interpretaciones ligeramente diferentes que impedían la compatibilidad entre ellas. Por estas razones, diferentes proyectos se iniciaron para producir una definición estándar del lenguaje y culminaron en dos estándar: uno de la Internacional Standard Organization (ISO) en 1982 y otro por un comité conjunto del American National Standards Institute (ANSI) y del Institute of Electrical and Electronics Engineers (IEEE).

Estas dos versiones o definiciones se conocen como ISO Pascal y ANSI/IEEE Pascal, y difieren en algunos aspectos no especialmente significativos. Sin embargo, una versión no estándar se ha popularizado considerablemente: Turbo Pascal (marca registrada por Borland International, Inc.). Esta versión ha contribuido en gran medida a la popularización del lenguaje Pascal.

El nombre de Borland Pascal fue generalmente reservado para los paquetes de gama alta para Microsoft Windows (con más bibliotecas y la biblioteca estándar de código fuente), mientras que la original, versión más barata y ampliamente conocida fue vendido como Turbo Pascal.

Turbo Pascal es un entorno de desarrollo para el lenguaje de programación Pascal. Se utiliza en Turbo Pascal basado en el anterior UCSD Pascal, ganó aceptación, en especial en los equipos que de la serie de Apple II. El compilador de Turbo Pascal se basó en Blue Label Pascal, creado originalmente en 1981 por Anders Hejlsberg para el sistema operativo NasSys, microcomputadora Nascom. reescrito más tarde como Pascal para el sistema operativo CP / M, y luego como un Turbo Pascal para DOS y CP / M. Una de las versiones de Turbo Pascal estaba disponible para Apple Macintosh alrededor de 1986, pero su desarrollo se detuvo alrededor de 1992.

El lenguaje de programación Pascal apareció por primera vez en 1971, de la mano de Niklaus Wirth.

Wirh inventó el lenguaje como una forma de mejorar el por aquel entonces arcaico Algol. No es el primer lenguaje que Wirth diseña, posteriormente inventó el Modula-2 y el Oberon. Sin embargo, paradójicamente, esos dos lenguajes han sido ampliamente superados por el Extended Pascal y el Object Pascal. De todos esos lenguajes, Pascal es el que más éxito ha tenido.

Aunque nace a principios de los 70, Pascal cobra auténtica vida a partir de principios/mediados de los 80, popularizado por el fabuloso Turbo Pascal de MS-DOS para PC, y sobre todo el Apple Pascal. Pascal ha sido tan popular hasta mediados de los 90, que una gran parte (la mayor parte) de las aplicaciones desarrolladas para Mac estaban realizadas en Pascal, así como una enorme parte también de los programas de MS-DOS.

Todavía hoy, Pascal se enseña en las universidades como primer lenguaje, pues se trata de un lenguaje muy legible.

Fue Borland la que, hasta el Turbo Pascal 7, se encargó de la mayor parte de la evolución de este lenguaje. A mediados de los 90, con el boom de Windows y el renacimiento de los sistemas Unix (entre ellos Linux) como ordenadores servidores primero y luego como estaciones de trabajo, motivó que Pascal pasara a un segundo plano en beneficio de C.

En ese momento, prácticamente solo Delphi (Object Pascal para Windows) consiguió mantenerse en la brecha, pero conformándose con una pequeña parte del mercado solamente.

Casi todo el mundo intentó pasarse a C++. Fíjate que decimos intentó. C++ no consiguió calar del todo por su extrema complejidad. Su sistema de objetos, los macros, los crípticos nombres de funciones, los namespaces tan difíciles de manejar, los templates, la STL y otra serie de cosas, hicieron que rápidamente mucha gente pasara de querer usar esa herramienta que todo el mundo proclamaba que era tan potente, a buscar alternativas que fueran más simples y prácticas. Sun, la compañía del Java, se gastó una millonada en promocionar su lenguaje, y tuvo un gran éxito, sin duda debido no solo al dinero en publicidad, sino a la gente que escapaba escaldada del C++. En realidad, Java no es un lenguaje fácil de aprender (obliga a pensar en objetos desde el principio, algo que los novatos no llevan bien), pero comparado con C++, se podía considerar como algo sencillo.

Borland, una compañía con no tanto dinero como Sun, no pudo competir con Pascal, y se tuvo que conformar sacando su propia máquina virtual de Java. Pero Delphi no ha dejado nunca de existir. Aun hoy sigue sacando versiones Delphi, con gran éxito, aunque no consigue subir significativamente su porcentaje de participación en el mercado.

El paso de Turbo Pascal a Delphi también trajo como consecuencias que apareciera Kylix, que no es más que un Delphi multiplataforma. No caló lo suficiente, debido a la licencia cerrada, y a que imponía que el programa compilado fuera GPL.

Posteriormente Microsoft compró la mayor parte de Borland. Con ello se abandonó Kylix, Linux, y se hizo a Borland 100% Windows. Esta estrategia prácticamente ha arruinado el avance de Pascal frente a otros lenguajes, en el entorno corporativo.

En el momento en que Borland pasó de Turbo Pascal a Delphi, apareció Free Pascal. Free Pascal es un intento de la comunidad de código libre de hacer un compilador compatible con Turbo Pascal para todas las plataformas más famosas (incluido Linux, OS X y Windows). Desde la versión 2.0, Free Pascal es además compatible con Delphi.

Free Pascal es, hoy en día, el sistema preferido para programar en Pascal multiplataforma. Su IDE principal, el Lazarus, un clon del Delphi, corre tanto en Windows como en OS X como en Linux.

Lazarus y Free Pascal son la gran esperanza del lenguaje de programación Pascal, y de los entornos de desarrollo multiplataforma en general. Su desarrollo es muy activo, y con toda seguridad tiene un futuro prometedor.

Fue Borland la que, hasta el Turbo Pascal 7, se encargó de la mayor parte de la evolución de este lenguaje. A mediados de los 90, con el boom de Windows y el renacimiento de los sistemas Unix (entre ellos Linux) como ordenadores servidores primero y luego como estaciones de trabajo, motivó que Pascal pasara a un segundo plano en sustitución de C. 

En ese momento, prácticamente solo Delphi (Object Pascal para Windows) consiguió mantenerse en la brecha, pero conformándose con una pequeña parte del mercado solamente.

Casi todo el mundo intentó pasarse a C++ pero este no consiguió calar del todo por su extrema complejidad. Su sistema de objetos, los macros, los crípticos nombres de funciones, los namespaces tan difíciles de manejar, los templates, la STL y otra serie de cosas, hicieron que rápidamente mucha gente pasara de querer usar esa herramienta que todo el mundo proclamaba que era tan potente, a buscar alternativas que fueran más simples y prácticas. Sun, la compañía del Java, se gastó una millonada en promocionar su lenguaje, y tuvo un gran éxito, sin duda debido no solo al dinero en publicidad, sino a la gente que escapaba escaldada del C++. En realidad, Java no es un lenguaje fácil de aprender (obliga a pensar en objetos desde el principio, algo que los novatos no llevan bien), pero comparado con C++, se podía considerar como algo sencillo.

Borland, una compañía con no tanto dinero como Sun, no pudo competir con Pascal, y se tuvo que conformar sacando su propia máquina virtual de Java. Pero Delphi no ha dejado nunca de existir. Aun hoy sigue sacando versiones Delphi, con gran éxito, aunque no consigue subir significativamente su porcentaje de participación en el mercado.

El paso de Turbo Pascal a Delphi también trajo como consecuencias que apareciera Kylix, que no es más que un Delphi multiplataforma. No caló lo suficiente, debido a la licencia cerrada, y a que imponía que el programa compilado fuera GPL.

Posteriormente Microsoft compró la mayor parte de Borland. Con ello se abandonó Kylix, Linux, y se hizo a Borland 100% Windows. Esta estrategia prácticamente ha arruinado el avance de Pascal frente a otros lenguajes, en el entorno corporativo.

En el momento en que Borland pasó de Turbo Pascal a Delphi, apareció Free Pascal. Free Pascal es un intento de la comunidad de código libre de hacer un compilador compatible con Turbo Pascal para todas las plataformas más famosas (incluido Linux, OS X y Windows). Desde la versión 2.0, Free Pascal es además compatible con Delphi.

Free Pascal es, hoy en día, el sistema preferido para programar en Pascal multiplataforma. Su IDE principal, el Lazarus, un clon del Delphi, corre tanto en Windows como en OS X como en Linux.

Lazarus y Free Pascal son la gran esperanza del lenguaje de programación Pascal, y de los entornos de desarrollo multiplataforma en general. Su desarrollo es muy activo, y con toda seguridad tiene un futuro prometedor.

Turbo Pascal es un sistema de desarrollo de software que incluye un compilador y un entorno de desarrollo integrado (IDE) para el lenguaje de programación Pascal, desarrollado por Borland y liderado por Philippe Kahn. Saliö a la venta en 1983 para MS-DOS, CP/M, CP/M-86 y, posteriormente, para Microsoft Windows. También hubo una versión de corta vida para Apple Macintosh.

El compilador de Pascal de Borland, famoso en todo el mundo, fue presentado en 1985. El compilador Turbo Pascal ha sido una de las series de compiladores que mejor se han vendido de todos los tiempos, e hizo de Pascal un lenguaje especialmente importante en la plataforma PC, gracias a su equilibrio entre simplicidad y potencia. Turbo Pascal introdujo un entorno integrado de programación (IDE) en que se podía editar el código (en un editor compatible con WordStar), ejecutar el compilador, ver los errores, y volver directamente a las líneas que contenían los errores. Ahora suena trivial, pero antes de eso había que salir del editor, volver a MS-DOS, ejecutar el compilador de línea de comandos, anotar las líneas erróneas, abrir de nuevo el editor y buscarlas.

Además, Borland puso a la venta Turbo Pascal por 49 dólares (USA), mientras que el compilador de Pascal de Microsoft estaba a unos cuantos cientos de dólares. Los muchos años de éxito de Turbo Pascal contribuyeron a que Microsoft finalmente retirase su compilador del mercado. 



</doc>
<doc id="29780" url="https://es.wikipedia.org/wiki?curid=29780" title="Hatshepsut">
Hatshepsut

Hatshepsut, reina-faraón de la dinastía XVIII de Egipto. Quinta gobernante de dicha dinastía, reinó de ca. 1490-1468 a. C. Gobernó con el nombre de Maatkara Hatshepsut, y llegó a ser la mujer que más tiempo estuvo en el trono de las "Dos Tierras".

El nombre de Hatshepsut con el que se la reconoce hoy en día en principio era un título con el significado de ""La primera de las nobles damas"" o ""la principal dama de la nobleza', que también se presentaba en su forma completa de Hatshepsut Jenemetamón"', esto es, ""La primera de las nobles damas, unida a Amón"".

Se ignora el momento exacto del nacimiento de Hatshepsut, aunque es de suponer que sucediese en la por entonces capital del estado, Tebas, a finales del reinado de Amenhotep I. Ante la falta de descendencia del faraón, el sucesor designado era el padre de Hatshepsut, el futuro Tutmosis I (Tutmose I), quien para poder legitimar su inminente acceso al trono se había tenido que casar con la princesa Ahmose.

Este matrimonio trajo al mundo, aparte de a Hatshepsut, al menos a otros tres niños, de nombres Amenmose, Uadymose y Neferubity. Desgraciadamente, y debido a la alta tasa de mortalidad infantil, solo Hatshepsut y su hermana mayor, Neferubity (y esta solo por un corto espacio de tiempo) llegarían a edad adulta.

Además de sus hermanos, Hatshepsut tuvo, aparentemente, medio hermanos de parte de su padre con esposas secundarias y concubinas. Del único del que nos ha llegado constancia es de quien fuera su esposo, Tutmosis II, hijo de Tutmosis I y de una esposa secundaria, de nombre Mutnefert.

El padre de Hatshepsut, Tutmosis I, había logrado expandir el Imperio egipcio de manera que nunca antes se había visto en tan solo trece años de reinado. Este prodigioso monarca pasaría a la historia por llevar a sus tropas al curso de un río enorme que, al contrario que el Nilo, no discurría de sur a norte, sino a la inversa: el Éufrates.

A la muerte, algo temprana, de Tutmosis I, Hatshepsut era la mejor situada para sucederle en el trono, pues sus hermanos varones ya habían muerto. Es posible que incluso el propio Tutmosis I tratase en vida de asociar a su hija al trono, como así lo demuestra que la nombrase "Heredera". Sin embargo, sus deseos fueron incumplidos, pues al parecer una conjura palaciega encabezada por el chaty y arquitecto real, el poderoso Ineni consiguió sentar en el trono a Tutmosis II, nacido de una esposa secundaria. Hatshepsut tuvo que soportar convertirse en la Gran Esposa Real de su medio hermano, y se cree que este fue un duro golpe a su orgullo.

La joven reina era descendiente directa de los grandes faraones libertadores de los hicsos y además ostentaba el importantísimo título de Esposa del dios, lo que la hacía portadora de la sangre sagrada de la reina Ahmose-Nefertari. Es lógico que su orgullo fuera inmenso, y que no soportase muy bien la idea de supeditarse a su marido. Así, no es de extrañar que mientras su débil y blando esposo ceñía la doble corona, Hatshepsut comenzara a rodearse de un círculo de adeptos que no dejaron de crecer en poder e influencias: entre ellos destacamos sobre todo a Hapuseneb y a Senenmut. La gran esposa real se había convertido, para temor del visir Ineni, en un peligroso oponente. Fue muy agresivo con su familia.

Tutmosis II tuvo un reinado muy breve, y murió en plena juventud cuando sus dos únicos hijos conocidos aún estaban en la primera infancia. Como había pasado en la generación anterior, la gran esposa real Hatshepsut no había traído al mundo un varón, sino una niña, por lo que volvió a abrirse una crisis sucesoria. Una vez más, Ineni consiguió que la nobleza aceptara como único candidato factible a un hijo de Tutmosis II y de una simple concubina, que sería nombrado rey como Tutmosis III. No obstante, la reina viuda Hatshepsut no quería que la historia se repitiera por segunda vez, y lo cierto es que la modificó considerablemente.

Dado que Tutmosis III era demasiado pequeño para gobernar, la gran esposa real de Tutmosis II asumió la regencia y pospuso indefinidamente el matrimonio entre el nuevo rey y su hija, la princesa real Neferura, única persona que podría legitimar su ascenso al poder absoluto. La situación no era rara, hubo muchos casos de regencia a lo largo de la historia egipcia, aunque nunca de una mujer que no fuera madre del rey.

Durante los primeros años de reinado de Tutmosis III, Hatshepsut estuvo preparando minuciosamente un "golpe de Estado" que revolucionaría a la tradicional sociedad egipcia. Alejó para siempre de la escena política a Ineni, y elevó a sus fieles Hapuseneb y Senenmut a los más altos cargos. Parece ser que la figura política más importante de la época fue Hapuseneb, quien unió para sí los cargos de chaty y de sumo sacerdote de Amón. Con unos aliados tan poderosos, Hatshepsut tenía ahora los medios y el apoyo suficientes para sorprender al mundo.
Cuando se vio lo suficientemente fuerte, la hasta entonces gran esposa real y esposa del dios, Hatshepsut, en presencia del faraón Tutmosis III, se autoproclamó también faraón de las Dos Tierras y primogénita de Amón, con el beneplácito de los sacerdotes, encabezados por Hapuseneb. El golpe de efecto fue magistral, y el inexperto Tutmosis III no pudo hacer otra cosa más que admitir la superioridad de su tía y madrastra. Hatshepsut se había convertido en la tercera reina-faraón conocida en la historia egipcia.

Hatshepsut asumió todos los atributos masculinos de su cargo excepto el título de "Todo poderoso" haciéndose representar a partir de entonces como un hombre y tocándose de barba postiza. Estableció una insólita corregencia con su sobrino, aunque hubo un clarísimo predominio de la primera sobre el segundo, hasta tal extremo de colocarlo en un segundo plano impropio del papel futuro que tendría Tutmosis III en la historia. Tal era el carisma y la personalidad de esta mujer.

Sin embargo, es necesario contextualizar por qué aludimos a Hatshepsut como faraón y no como reina-faraón o faraona. En el antiguo Egipto el título de reina no existía, puesto que las soberanas ostentan, entre otros, el título de gran esposa real y esposa del dios, es decir, la esposa del rey. En ningún caso podemos aludir a ellas como reinas, pues carecende poder de decisión y solo tienen funciones rituales.De hecho, el título más parecido al de reina en la Antigüedad era de Regente del Sur y del Norte, que alude a un poder temporal no confirmado.

Aun así, no se puede ver de ninguna forma a Hatshepsut como una usurpadora, visión que han trasladado a nuestra época algunos autores. Al menos no se vio así en su tiempo, pues de haber sido el caso, Hatshepsut habría eliminado con total facilidad a sus adversarios o se habría producido una guerra civil. Tutmosis III no estuvo encerrado en palacio, como se ha llegado a pensar, ni tampoco Hatshepsut evitó hacer mención alguna a su existencia. La sociedad de entonces asumió sin problemas la nueva situación, y Hatshepsut gozó de uno de los reinados más prósperos de toda la historia egipcia, gracias también al apoyo recibido por Hapuseneb y Senenmut.

Hatshepsut no hubiera podido ni soñar siquiera acceder al trono de no contar con los apoyos que consiguió entre el clero del dios Amón en Tebas mientras era la esposa de Tutmosis II. Las cuantiosas donaciones y los privilegios que concedió a los sacerdotes, encabezados por la eminencia gris del régimen, el visir Hapuseneb, fueron una forma de pago por los servicios prestados, pues de no ser por el inmenso regalo que recibió Hatshepsut de ellos, su legitimidad habría sido menor. Y este valioso obsequio de la casta sacerdotal a la reina-faraón fue la célebre "Teogamia".

En la Teogamia, Hatshepsut declara al pueblo egipcio que su verdadero padre no es Tutmosis I, sino el propio dios Amón, que con su sabia previsión visitó una noche a la gran esposa real Ahmose y la permitió concebir a la mujer que estaba sentada ahora en el trono de las Dos Tierras con el beneplácito del panteón entero. Hatshepsut se declaraba por ende primogénita de Amón, y su sustituta y fiel delegada en la tierra, con lo que su figura se trocaba en completamente sagrada.
Es necesario destacar que muy pocos faraones recurrieron a la Teogamia para validar su derecho al trono, y su estatus pasaba a ser poco menos que el de un dios vivo. El ardid de Hatshepsut y el alto precio que tuvo que pagar a los sacerdotes por él, le asegurarían un reinado tranquilo y sin disidencias, aunque acabaría pasándole factura a la dinastía por la, desde entonces, imparable creciente influencia de los sacerdotes de Amón.

Como todo rey que accedía al trono, Hatshepsut tenía derecho a usar hasta cinco nombres diferentes: el de "Horus", el de "Nebty", el de "Horus de Oro", y los dos principales, conocidos vulgarmente como nombre de nacimiento y nombre de coronación. Este último resultó ser el de Maat-Ka-Ra, es decir, "El espíritu de Ra es justo" y lo utilizó siempre conjuntamente con su nombre de nacimiento.

Sin embargo, este último apelativo sufrió una serie de cambios a lo largo del reinado de Hatshepsut. Si bien la forma original del nombre de nacimiento era Hatshepsut, en numerosos monumentos aparece de formas bien distintas: añadiendo la segunda parte de nombre y quedando como Hatshepsut-Jenemetamón, masculinizándolo en parte como Hatshepsu o completamente como Hashepsut. Solo así se puede comprender la sorpresa de los egiptólogos que descubrieron la existencia de esta mujer que jugaba en sus apariciones, siendo representada varón, con sus nombres unas veces escritos tal que había nacido hombre o mujer. Un curioso juego de intercambio de sexos que sin duda realzó su carácter divino y concentró en sí misma la dualidad que tanto veneraba el pueblo egipcio.

Hatshepsut en todos los edificios y obras tanto escultóricas como relieves en los que aparece es representada de forma masculina ya que la figura del faraón solo podía ser desempeñada por un hombre, las mujeres tenían otro tipo de funciones y papeles dentro del gobierno como el de Esposa Real y Gran esposa del dios. ante tal situación tanto para legitimarse como para ser aceptada a pesar de ser mujer decide ser representada de forma masculina, con el tocado nemes, el ureus y la perilla característica de los faraones. Por otro lado decide no tener descendencia y nombrar a su hija Neferura Esposa real y Gran esposa del dios. Hatshepsut será la primera mujer-faraón que se hizo esculpir como esfinge, un ejemplo es la que se encuentra en el MMNY.

"El" faraón Hatshepsut dedicó la mayor parte de su reinado a embellecer el país y a restaurar los templos, con el beneplácito de sus aliados los sacerdotes. Egipto, también había sufrido hacía dos generaciones la última de sus guerras, cuando el abuelo de la reina, el rey Ahmose, expulsó a los hicsos, un pueblo semita que había conseguido dominar el país durante cien años. Como habían hecho sus antecesores, Hatshepsut invirtió mucho en borrar todos los daños ocasionados por la guerra de liberación que había elevado a su dinastía a lo más alto.

Dejó su impronta en el templo de Satet, en la isla Elefantina, en el Speos Artemidos en honor de la diosa Pajet.

Sin embargo, el centro de acción principal de la reina fue su ciudad, la pujante Tebas. Se implicó en el recinto de las barcas sagradas de Luxor, edificó la llamada Capilla Roja del enorme templo de Amón en Karnak y, de las canteras de Asuán, mandó hacer los obeliscos más grandes que se habían erigido en Egipto hasta entonces, y los llevó a Karnak decorados con "electrum", aleación de oro y plata. Se cree que el obelisco inacabado que aún hoy se puede ver en Asuán data del reinado de Hatshepsut, y de haberse acabado habría sido el mayor de toda la historia del país.

Aunque no fue en Karnak donde Hatshepsut desplegó toda su imaginería, sino en la orilla oeste de Tebas, la necrópolis de entonces. En aquella época, los faraones hacían construir, además de su tumba, un templo funerario algo alejado de ésta, que sirviera a un mismo tiempo para proteger y recordar al difunto. Hatshepsut escogió el paraje de Deir el-Bahari para edificar su templo de millones de años, y encargó la tarea a su arquitecto favorito, Senenmut. 

El resultado final fue envidiable, construido al lado del templo de Mentuhotep II, el de Hatshepsut es una de las joyas del Antiguo Egipto y uno de los destinos más visitados por los turistas. Conocido por aquel entonces como el Dyeser-Dyeseru (el "sublime de los sublimes"), su estructura en forma de largas terrazas y de rampas con suave inclinación, de estilo similar al de Mentuhotep II, le hacen fundirse a la perfección con la roca y el entorno.
Uno de los misterios en dicho templo radica en un sector sellado como una caja en la pared en que se puede observar por un lado a Hatshepsut en actitud amatoria y a Senenmut en la otra cara, como receptor de la pose amatoria de la reina, lo que deduce un íntimo vínculo (prohibido por su linaje) entre el arquitecto y la reina-faraón.

Hatshepsut ha pasado a la historia como una gobernante pacífica y que prefirió gastar parte de su tesoro en construir templos en vez de conquistar territorios, pero lo cierto es que hubo al menos seis campañas durante sus 22 años de reinado. Hay que destacar que la mayoría de estas no pasaron de ser meras escaramuzas o actividades disuasorias cuya única finalidad era disuadir a los siempre belicosos pueblos fronterizos de atacar a las Dos Tierras.

Otro hecho relevante del reinado de Hatshepsut fue la doble misión a Punt, el país legendario de donde procedían los mejores árboles de incienso y mirra, que probablemente estaba en una región de la actual Somalia, aproximadamente en el año 15º de su reinado. Comandada por Nehesi, portador del sello real, la expedición fue tanto por tierra como por mar, y durante ella no solo se dedicó la delegación egipcia a comerciar, sino que también hizo un minucioso estudio de la fauna y la flora de Punt, así como de la organización política y social del lugar.

Tuvo que ser tan importante esta acción para la posición de Hatshepsut, que no dudó en decorar gran parte de las paredes del Dyeser-Dyeseru con escenas de aquel mágico periplo por el que sería recordada durante mucho tiempo por la población llana. No solo fue un éxito al conseguir importar la preciosa mirra a Egipto, sino que trajo extrañas especies animales antes nunca vistas y generosos cargamentos de oro, marfil, ébano y otras maderas preciosas que enriquecieron considerablemente las arcas reales y las de los templos.

Aun así, es extraño que Hatshepsut pusiera tanto empeño en promocionar el viaje a Punt, un país que se conocía ya desde la época de las pirámides, y solo puede explicarse como una parte más de la intensa propaganda que distribuyó por el Dyeser-Dyeseru y por otros lugares del país con el único fin de legitimar su posición. Sin lugar a dudas, en aquel momento de su reinado, con la inauguración de su hermoso templo y el regreso de los viajeros del Punt, Hatshepsut había llegado al cenit de su gobierno.
Lo único que se sabe a ciencia cierta es que Hatshepsut fue madre de una hija, a la que puso de nombre Neferura y cuyo cuidado encargó a su arquitecto favorito, Senenmut. Se ignora el verdadero papel de este hombre en la trama; no son pocas las voces que dicen que fue él el padre de Neferura y no Tutmosis II, y que hubo una tórrida historia de amor entre el arquitecto y canciller real y la reina, una historia que pese a ser muy interesante desde el punto de vista novelesco, sigue sin estar demostrada. A favor de todo esto hay algunas "pruebas", como que aparezcan en actitud ciertamente cariñosa Senenmut y Neferura, o un "ostracón" hallado en las cercanías del templo de Deir el-Bahari donde se ve a un faraón femenino teniendo relaciones sexuales con un hombre. Aun así, aunque cada vez más voces se alzan a favor de un romance de Hatshepsut con Senenmut, se sigue pensando que Neferura era hija de Tutmosis II.

También se ha divagado mucho acerca de la posible maternidad de Meritra Hatshepsut, quien sería más tarde gran esposa real de Tutmosis III. Debido a su nombre, siempre se pensó que era la segunda hija de Hatshepsut, pero era realmente extraño que nunca se la mencionase en vida de su presunta madre, mientras que Neferura apareciese tan a menudo. Actualmente parece haber quedado claro que, pese a llevar su nombre, Meritra Hatshepsut era en realidad hija de la dama Huy, una mujer muy influyente en la corte de entonces, y quizás aquel apelativo tuviese como destino halagar a la reina-faraón. Así podría entenderse por qué cuando Tutmosis III comenzó a perseguir la memoria de su madrastra, su gran esposa real optase por llamarse simplemente Meritra.

Según los estudios del Museo de El Cairo, patrocinados por Discovery Channel y liderados por el egiptólogo Zahi Hawass, la descendencia de Tutmosis padecía de una variedad de viruela hereditaria, a la que no escapó ningún descendiente.

Sin embargo, fue a raíz de la finalización del templo de Deir el-Bahari, sobre el año 15-16 de reinado, cuando la estrella de Hatshepsut comenzó a menguar a favor de la de Tutmosis III. El rey era un joven que cada vez ansiaba más el poder, y a cualquier precio. Así, no es de extrañar que en apenas un año murieran los dos principales sustentos de la reina y sus más grandes apoyos, Hapuseneb y Senenmut. Y por si no fuera poco, poco después murió la gran esperanza, el arma secreta de la reina, la princesa Neferura.

Los golpes que sufrió Hatshepsut en torno al año 16 de su reinado fueron tan grandes que a partir de entonces la reina se retiró parcialmente del cargo y el otro rey, Tutmosis III, comenzó a tomar las riendas del gobierno. Al parecer, la ambición de Hatshepsut era aún más grande y no estaba satisfecha con ser ella sola "faraón", sino que se proponía inaugurar una auténtica dinastía femenina de reyes, y por esa razón declaró "Heredera" a su amada hija Neferura. La muerte de la princesa fue tan repentina y favorable a Tutmosis III que hay quien piensa que fue intencionada, y que consiguió su objetivo, derrumbar a la reina-faraón.

Hatshepsut murió en su palacio de Tebas tras un largo reinado de 22 años, abandonada por todos. Se ignora la edad de su muerte, pero se estima que debería oscilar entre los cuarenta y los cincuenta años. Años atrás no se sabía cómo murió exactamente, si fue muerte natural o durante un golpe de estado liderado por su hijastro, Tutmosis III, que en esa época era virtualmente el único rey, pues Hatshepsut se había retirado de la lucha. 

Según el National Geographic y el arqueólogo Zahi Hawass, la momia fue escaneada y se encontró que la reina había padecido en vida, de una avanzada osteoporosis y un cáncer maligno en la zona del abdomen que le pasó al hueso de la cadera; además había contraído un absceso séptico en su cavidad bucal que bien pudo provocar un shock septicémico como causa más probable de su muerte que un atentado a su vida. Según estas últimas investigaciones, su muerte estuvo precedida de largos meses de intensos dolores y fiebres.

Su tumba se encuentra en el Valle de los Reyes y está catalogada como KV20. Existen indicios de que mandó ampliar la tumba de su padre para ser utilizada también para ella. El amor y la lealtad que la hija profesó al padre tuvo que ser tan grande que quiso permanecer junto a él eternamente.

A su muerte, Tutmosis III se convertiría en un gran faraón que, emulando a su abuelo Tutmosis I, realizó numerosas campañas y ascendió a Egipto al rango de gran potencia mundial. Pero jamás lo habría logrado sin la preparación a la que lo sometió su tía-madrastra. 

El nombre de Hatshepsut y el de su fiel colaborador Senenmut fueron borrados sistemáticamente de los anales y edificios egipcios siendo víctimas de una damnatio memoriae. Durante mucho tiempo se creyó que el postergado Tutmosis III había sido quien ordenó el virtual "olvido" de esta enérgica reina, pero cada vez más egiptólogos apoyan la teoría que su nombre fue borrado por cuestiones más bien de conveniencia que de venganza. Existía en Egipto un conjunto de familias identificadas con Hatshepsut, "su familia" antes de desposarse con Tutmosis II. Al morir la reina, Tutmosis III pudo haber borrado el nombre de su madrastra a fin de legitimar su ascendencia al trono, como heredero real de Tutmosis II, y así frenar las pretensiones de la poderosa familia de Hatshepsut. Esta postura está adquiriendo cada vez más fuerza, por las evidencias arqueológicas encontradas en Deir el-Bahari. Si hubiera habido una venganza, su legado artístico y arqueológico hubiera sido borrado de Egipto. Sin embargo, la reina fue hallada en un excelente estado de conservación con parte de su ajuares funerarios.

En el 2005 Zahi Hawass, director del Egyptian Mummy Project y secretario general del Consejo Supremo de Antigüedades y su equipo, se enfocaron en una momia llamada KV60a, descubierta más de un siglo antes. En ningún momento se creyó que esta momia era tan importante como para retirarla del suelo de una tumba menor en el Valle de los Reyes ya que se encontró sin un ataúd y sin los tesoros que distinguían a los faraones, descubriéndose muchos años más tarde que era la momia de la reina faraón.

La momia de Hatshepsut fue presentada al público en junio de 2007, después de un largo periodo de incertidumbre acerca de su correcta identificación. Zahi Hawass, Secretario General del Consejo Supremo de Antigüedades en Egipto, aseguró que se trataba del descubrimiento arqueológico más importante desde el hallazgo de la tumba de Tutankamón, en 1922. 

Ambas momias fueron descubiertas en la tumba KV60 del Valle de los Reyes. Este sepulcro fue mandado construir por la propia Hatshepsut destinado a su nodriza, a la que profesaba un gran cariño, la dama Sitra. En él se hallaron los cuerpos de una mujer de unos cuarenta o cincuenta años y de una anciana de más de sesenta años, que presentaba la peculiaridad de tener el brazo izquierdo doblado en la posición típica de las reinas difuntas. El descubrimiento de la momia fue motivo de varios interrogantes: ¿cómo llegó allí, quién era la dama del sarcófago, y por qué la momia "real" estaba en el suelo?. 

Se sabe que la momia fue encontrada en medio de gran cantidad de lienzos de lino, desnuda, calva, obesa, y con signos de haber sido trasladada de su ubicación original. En un principio se consideró que la momia obesa era alguien poco importante que no merecía una gran sepultura. Cuando fue hallada, sus descubridores no prestaron atención a la postura del brazo, limitándose a escribir de ella que "tenía pechos enormes que caían como péndulos". Después, los descubridores de la KV60 se limitaron a ordenar la muy desordenada tumba (según ellos, había muchísimos objetos diseminados por todo el suelo) y a depositar la momia en un ataúd nuevo de madera fabricado en El Cairo.

Mucho tiempo después, se comenzó a estudiar la identidad de cinco momias femeninas no identificadas. Se presumía que una de esas momias sería la de Hatshepsut. Zahi Hawass ordenó traer un escáner TAC (donado por Siemens) hasta el Valle de los Reyes, donde se tomaron imágenes de alta resolución de las momias guardadas en KV60. 

Antes de verificar la identidad de la momia ya se había descubierto el hígado momificado que, con toda certeza, pertenecía a Hatshepsut. Junto al hígado estaban los intestinos y un molar con una sola raíz; esta pieza fue la clave para su correcta identificación. La caja de vasos canopos fue hallada en el escondrijo de momias reales DB320, lo cual hizo pensar inicialmente que el cuerpo de Hatshepsut sería alguno de los no identificados en DB320. 

El escáner de la mandíbula de la momia obesa mostró la ausencia de una pieza molar, de la que solo quedaba una raíz. Inmediatamente se llamó a un odontólogo forense quien determinó que la raíz y la pieza dental hallada en los vasos canopos de Hatshepsut eran partes de la misma muela; ambas piezas coincidían perfectamente. Basándose en este hallazgo, se determinó que la momia obesa era el cuerpo de Hatshepsut.

El interés que ha despertado Hatshepsut en la sociedad moderna es innegable, y las posiciones respecto a ella que tienen arqueólogos, historiadores o simples lectores no pueden ser más variadas. Hatshepsut se halla en la actualidad convertida en una maquiavélica usurpadora, en un animal político que no retrocede nada con tal de satisfacer su ambición, en una mujer que tuvo que elegir entre el amor y su reino, en una amante de la paz o en un modelo feminista, o todo esto a un mismo tiempo, dependiendo de la persona que opine acerca de ella.

Por ello, no es de extrañar que exista un amplio abanico de libros dedicados a Egipto, al Imperio Nuevo o incluso a ella por completo en los que existan todos los puntos de vista posibles, y más. Tampoco falta su presencia en las novelas, que suelen pintarla como una bella y ambiciosa mujer que vivió una vida digna de ser recordada, junto a Tutmosis I, Tutmosis III o Senenmut, para algunos un faraón sin corona. Sea como fuese, es indiscutible el encanto que emana de Hatshepsut que solo puede compararse al que rodea a otras grandes figuras de la civilización egipcia como Akenatón, Nefertiti, Tutankamón, Ramsés II o Cleopatra VII.








</doc>
<doc id="29781" url="https://es.wikipedia.org/wiki?curid=29781" title="Ν">
Ν

Ni (según la Real Academia Española actualmente) o ny (según la RAE hasta 2002) (Ν ν) es la decimotercera letra del alfabeto griego y tiene un valor de 50 (νʹ) en el sistema de numeración griega.

La letra minúscula ν se usa como símbolo para:


</doc>
<doc id="29782" url="https://es.wikipedia.org/wiki?curid=29782" title="Ξ">
Ξ

Xi (nombre pronunciado [ksi]), Ξ ξ, es la decimocuarta letra del alfabeto griego y tiene un valor de 60 (ξʹ) en la numeración griega.

Un ejemplo de uso conocido de esta numeración es el "Número de la Bestia", en el "Apocalipsis" de San Juan, donde aparece como χξςʹ ("ji", "xi", "stigma"), que representa el valor 666.

En matemáticas puede denotar las raíces de un sistema de ecuaciones, especialmente las raíces unitarias (las raíces complejas del polinomio "X" - 1, donde "n" es un número natural). En el caso general, suele llevar subíndices, mientras que, en el caso de las raíces unitarias, todas ellas se pueden expresar como potencias de una raíz unitaria primitiva ξ.

También se usa en termodinámica para denotar el avance de una reacción, y en física de partículas para denotar la partícula cascada.

En Física de ondas representa el desplazamiento cinemático de una partícula cualquiera dentro de una onda viajera:

formula_1

Recientemente, en el campo de la geotecnia investigadores de la Universidad de Cantabria, adoptaron esta letra para dar nombre a la constante PECM, que relaciona la capacidad por punta y capacidad por fuste en un pilote para un determinado material y perfil de suelo.


</doc>
<doc id="29784" url="https://es.wikipedia.org/wiki?curid=29784" title="Ϻ">
Ϻ

San (Ϻ ϻ) es una letra obsoleta del alfabeto griego que tuvo probablemente en valor fonético de /s/ o /ṣ/. Localizada entre pi y qoppa, terminó cayendo en desuso y fue reemplazada por sigma. Se utilizó esta letra hasta el siglo VI a. C. El grafema original de esta letra, aunque recuerda al de la My griega y al de la M latina, proviene de la letra fenicia ṣade (𐤑‏).



</doc>
<doc id="29785" url="https://es.wikipedia.org/wiki?curid=29785" title="Ϙ">
Ϙ

Qoppa (Ϙ ϙ) es una letra obsoleta del alfabeto griego que tenía un valor numérico de 90 y cuya transcripción latina era Q. Posteriormente fue sustituida por kappa.

Hoy en día se utiliza el glifo alternativo, la qoppa numérica (ϟ), para representar la cifra equivalente a 90 (ϟʹ).

Cuando los pueblos griegos tomaron en el siglo VIII antes de la era cristiana las letras fenicias para crear sus alfabetos (existieron en efecto numerosas modificaciones del alfabeto griego antes de que el modelo jonio de Mileto se impusiera en Atenas hacia el 403 a. C.), se sirvieron de la letra "qōf" (nombre reconstruido, no atestiguado, como los de las restantes letras fenicias) para transcribir el alófono /ḵ/ del fonema /k/, sonido que, a oídos griegos, era el más próximo al valor fenicio de la letra: /q/.

La qoppa fue usada como símbolo para la ciudad de Corinto, que tenía la temprana grafía de "Ϙόρινθος". La qoppa es también la fuente del número arcaico cirílico "koppa" ("Ҁ").




</doc>
<doc id="29786" url="https://es.wikipedia.org/wiki?curid=29786" title="Ρ">
Ρ

Ro (según la RAE) o rho (Ρ ρ ϱ) es la decimoséptima letra del alfabeto griego. En griego antiguo, la ro al principio de palabra se escribía con espíritu áspero (un símbolo que normalmente se reservaba para vocales y que representaba una aspiración) que en caso de la ro representaba, probablemente, una pronunciación de vibrante múltiple (como en castellano). Lo mismo atestigua que, al igual que en español, en griego se escribe con ro doble aquellas palabras compuestas cuyo segundo elemento empieza con esta letra. Por ejemplo: πολύρριζος [po'lyrridzos] "polirrizo" ("de varias raíces").

En el sistema de numeración griega tiene un valor de 100 (ρʹ).

La letra minúscula ρ es usada para simbolizar:


</doc>
<doc id="29787" url="https://es.wikipedia.org/wiki?curid=29787" title="Politeísmo">
Politeísmo

El politeísmo es una concepción religiosa o filosófica basado en la existencia de varios seres divinos o dioses. En la mayoría de las religiones que aceptan el politeísmo, los diferentes dioses son representaciones de fuerzas de la naturaleza o principios ancestrales, y pueden verse como autónomas o como aspectos o emanaciones de una deidad creadora o principio absoluto trascendental ( teologías monistas), que se manifiesta de manera inmanente en la naturaleza ( teologías panteístas y panenteístas). Muchas deidades politeístas, a excepción de las o hindúes, son concebidas en un plano corpóreo más que etéreo. 

El politeísmo es un tipo de teísmo. Dentro del teísmo, contrasta con el monoteísmo, la creencia en un Dios singular, en la mayoría de los casos trascendente. Los politeístas no siempre adoran a todos los dioses por igual, ya que pueden ser henoteístas, que se especializan en la adoración de una deidad en particular. Otros politeístas pueden ser katenoteístas, adorando a diferentes deidades en diferentes momentos. 

A decir de David Hume, el politeísmo «"fue la primera religión de los seres humanos"». En verdad fue una forma típica de religión durante la Edad del Bronce y la Edad del Hierro hasta la Era Axial y el desarrollo de las religiones abrahámicas, la última de las cuales propugnaba el monoteísmo. Está bien documentado en las religiones históricas de la antigüedad clásica, especialmente la antigua religión griega y la antigua religión romana, y después de la decadencia del politeísmo grecorromano en religiones tribales como el paganismo germánico, eslavo y báltico. 

Otros ejemplos históricos son las antiguas religiones egipcia, griega, romana, celta o nórdica, en la zona europea y norteafricana, como tampoco hay que olvidar las religiones amerindias como inca, maya o Religión mexica, por mencionar algunas precolombinas. 

Pero no debe considerarse tanto una creencia del pasado sino de total presente y actualidad: las religiones politeístas más importantes existentes hoy en día son la religión tradicional china, el hinduismo, el sintoísmo japonés, la santería y varias religiones neopaganas.

La palabra politeísmo, del griego [poli] ""muchos "" y [theos] ""Dios"", es decir «muchos dioses ». Fue el escritor judaíco Filón de Alejandría quién utilizó el término por primera vez argumentando con los griegos. Tras la expansión del cristianismo entorno al Mediterráneo y por parte de Europa, se pasó a utilizar más la palabra pagano, gentil o la más peyorativa de idólatra para referirse a los no cristianos. Sería Juan Bodino en el siglo XVI quien recuperaría el término. 
El politeísmo es un tipo de teísmo. Dentro del teísmo, contrasta con el monoteísmo, cuya creencia en un Dios singular, en la mayoría de los casos trascendente. Los politeístas no siempre adoran a todos los dioses de igual modo, ya que pueden ser henoteístas, los cuales se especializan en la adoración de una deidad particular. Otros politeístas pueden ser katenoteistas, los cuales adoran a diferentes deidades en diferentes momentos. 

El sistema engloba la personificación de elementos naturales, sentimientos y actividades humanas, normalmente organizados en una jerarquía o panteón. No se trata de diferente nomenclaturas para una sola deidad, sino de diversos dioses con características individuales claramente identificables. En el politeísmo cada deidad puede ser honrada e invocada de manera individual, dependiendo de los aspectos que le definen. 

En las sociedades politeístas no suele existir una teología propiamente dicha, aunque suele coexistir con sistemas filosóficos y éticos bastante complejos. Cada fuerza sobrenatural o acontecimiento trascendental (como el rayo, la muerte o el embarazo) atiende a unos mecanismos establecidos, que conforman un complejo orden cósmico muy jerarquizado, descrito mediante mitos, leyendas y obras sagradas. En el politeísmo, debido a un entramado muy consolidado de transmisión, oral o escrita, el conocimiento es acumulativo, es decir, es ampliado por la especulación de los individuos dedicados a ello ( chamanes, brujos, sacerdotes, poetas), o bien por contacto intercultural.

Se suele señalar que el politeísmo corresponde, a menudo, a sociedades igualmente jerarquizadas, con una fuerte estratificación en clases sociales. Ejemplos habituales se darían en el Antiguo Egipto, Mesopotamia, en la cultura clásica griega y romana o en el hinduismo. Algunas creencias politeístas sitúan además la preeminencia de un dios sobre el resto del panteón (culto conocido como henoteísmo), lo que hizo creer a los antropólogos que este sería el paso natural hacia el monoteísmo.

El politeísmo está considerado por algunos antropólogos como el paso siguiente al animismo, una forma más avanzada de religiosidad (propia de un cierto nivel de civilización), en la que las fuerzas de la naturaleza son discriminadas, separadas y seleccionadas, y, finalmente, representadas por una serie de dioses antropomórficos.

Dentro de la cultura occidental es también posible encontrar desde finales del siglo XX formas actuales de politeísmo. El neopaganismo en sus diferentes variantes, como la Wicca, el Ásatrú, el neodruidismo, la Streghería, etc. reivindica el antiguo culto pagano y busca revivir el politeísmo occidental precristiano.

En las Islas Canarias (España), los aborígenes canarios profesaban una mitología politeísta (véase Mitología guanche).



</doc>
<doc id="29791" url="https://es.wikipedia.org/wiki?curid=29791" title="Φ">
Φ

Fi (Φ, φ ϕ) es la vigésima primera letra del alfabeto griego. Los romanos, al transliterar esta letra a caracteres latinos, lo hicieron con el dígrafo "ph", con lo que representaron el sonido de "p" aspirada () que tenía en griego antiguo: por ejemplo, en "Phidias", "philosophia" o "Pharao" (en castellano: Fidias, filosofía, faraón). En griego moderno se pronuncia simplemente como φέτα (feta queso). En el sistema de numeración griega tiene un valor de 500 (φʹ).

La letra minúscula φ se usa para simbolizar:

La letra mayúscula Φ se usa para simbolizar:


</doc>
<doc id="29792" url="https://es.wikipedia.org/wiki?curid=29792" title="Χ">
Χ

Χ (en minúscula χ) es la vigésima segunda letra del alfabeto griego. Se romaniza en latín como chi y en español como ji.

Los romanos la trasliteraron con el dígrafo "ch" en palabras tales como: "chorus", "chaos", "Christus", o "Charon" (coro, caos, Cristo y Caronte respectivamente) para representar el sonido [kʰ], oclusiva velar sorda aspirada, que tenía en griego antiguo. En griego moderno se pronuncia como [ç] (fricativa palatal sorda) antes de [e̞] o [i]; o como [x] (fricativa velar sorda) en cualquier otra posición.

La ji minúscula (χ) es usada para simbolizar:



</doc>
<doc id="29793" url="https://es.wikipedia.org/wiki?curid=29793" title="Ψ">
Ψ

Psi (Ψ ψ) es la vigésima tercera letra del alfabeto griego. Fue trasliterada con el dígrafo "ps" por los romanos en palabras tales como "psalmus", "psalterium" y "psyche" (salmo, salterio y psique).

El significado surge a partir de la asociación de la letra Psi con la palabra griega “Psiqué”, que originalmente tenía el significado de “mariposa” (de hecho podemos asimilar fácilmente la forma de la letra con el dibujo estilizado de una mariposa con las alas desplegadas). Posteriormente pasó a utilizarse el término con los sentidos de “soplo de brisa”, “aliento”, “ánimo” y, finalmente “alma” o "mente".

Tal es el origen del nombre “Psicología” (ciencia del comportamiento) y del empleo de la letra Psi para representarla. Los griegos creían que cuando moría una persona y exhalaba su último aliento, el alma abandonaba el cuerpo volando en forma de mariposa. La mitología griega representa a la diosa “Psiquis” o “Psique” como una adolescente con alas de mariposa, siendo la menor de las tres hijas del rey de Anatolia, la más hermosa de ellas y la representación del alma humana.





</doc>
<doc id="29796" url="https://es.wikipedia.org/wiki?curid=29796" title="Idioma kweyol">
Idioma kweyol

El kwéyòl o criollo francés de Santa Lucía ("kwéyòl") es una varinte del criollo antillano hablado en el oriente del Caribe, particularmente en las islas de Dominica y Santa Lucía y en las Antillas Menores.

El nombre es una variación de la palabra criollo en francés ("créole"). En términos generales la lengua mezcla el léxico de la lengua francesa con la sintaxis de los idiomas africanos y sólo hasta hace poco comenzó a escribirse. En los últimos 10 años algunos talleres de ortografía se han impartido y en la actualidad un grupo multidisciplinario compuesto por expertos de la Universidad de las Indias Occidentales, la Universidad de las Antillas en Guyana, el Grupo de Estudio e Investigación del Espacio Criolloparlante (en francés, "Groupe d'étude et de Recherche en Espace Créolophone, GEREC") y algunos otros movimientos locales se han dado la tarea de desarrollar todo un sistema de escritura "ad hoc".

Según algunos autores (Parkvall, 1997) en 1995 unas 123 mil personas lo hablaban en la isla de Santa Lucía, y a nivel internacional el número ascendía a unos 985 mil. Entre el kweyol hablado en Santa Lucía y el hablado en Dominica existe de un 97 a 99% de similitud, mientras que sólo el 10% de la población de Santa Lucía entiende el francés continental. En Santa Lucía los discursos políticos suelen ser en kweyol, y también existen periódicos, programas de radio y estaciones de televisión que utilizan el idioma. La clasificación del idioma de acuerdo a la SIL International es dom y acf y el código ISO 639-2 para el mismo son cpf.

"Ethnologue" registra hablantes en Panamá por la inmigración.

Ejemplos de la similitud entre el kweyol y el francés:




</doc>
<doc id="29801" url="https://es.wikipedia.org/wiki?curid=29801" title="The Residents">
The Residents

The Residents es un concepto concebido por un colectivo de arte pop estadounidense de "avant-garde" activo desde principios de los años 70. Este es reconocido por su música pop/rock experimental y sus obras multimedia, compuestas por videos musicales, cortometrajes, tres CD-ROMs, diez DVD y mini-series para Internet. El grupo se identifica por el anonimato de sus integrantes, consolidado a partir de la teoría de la oscuridad (la cual establece que el arte puro solo se produce cuando es anónimo).

El origen de The Residents se remonta (aproximadamente) a 1966 en Shreveport, Luisiana, donde los cuatro o cinco miembros se conocieron en la escuela.

Para 1969 se habían mudado a San Mateo, California. Mientras intentaban ganarse la vida, experimentaban con cintas, fotografía y cualquier forma de arte que llegue a ellos. En esta época se les unen el guitarrista inglés Phil Lithman y el misterioso compositor de Baviera, N. Senada. Ambos serían grandes influencias para el grupo.

Lithman (apodado "Snakefinger") siguió colaborando con ellos hasta su muerte en 1987, mientras N. Senada (de quien se duda de su existencia) les enseñó sus dos teorías: la de la oscuridad y la de organización fonética (según la cual la música puede "construirse" a partir de los sonidos), ésta se nota en el estilo minimalista que tendrían para componer. Se rumorea también que N Senada podría ser Captain Beefheart o Harry Patch.

Los primeros demos datan de 1971: "The Warner Bros Album" y "Baby Sex". Ambos fueron eliminados de la discografía oficial, porque la banda aún no tenía nombre, así como por su carácter amateur.

En 1972 fundan su propia discográfica Ralph, con la que editan su primer sencillo "Santa Dog". Mientras grababan su primer LP, empiezan a filmar la película "Vileness Fats". El proyecto finalmente fue abandonado en 1976 por problemas técnicos y económicos.

Entre 1973 y 1980 graban sus primeros siete discos, considerados por muchos fanes la mejor época de The Residents. Las presentaciones en vivo eran todavía muy pocas, siendo un proyecto puramente de estudio.

En 1976 se funda The Cryptic Corporation, el equipo de "management" del grupo, por John Kennedy, Jay Clem (1947-), Homer Flynn (1945-) y Hardy W. Fox (1945-2018), todos negaron ser integrantes. Flynn y Fox dieron entrevistas para los medios a nombre de The Residents.

En 1981 sale "Mark of the Mole" como primera parte de una trilogía conceptual. Entre ese año y 1982 salen de gira, la primera de la banda. El tour fue un desastre comercial que casi disuelve al grupo e hizo abandonar a Kennedy y Clem del proyecto. La trilogía quedó inconclusa, con sólo la parte uno, dos ("The Tunes of Two Cities", 1982) y la cuatro ("The Big Bubble", 1985).

Así empieza una época de crisis, menos productiva. En 1983 lanzan "Title in Limbo", colaboración con Renaldo and the Loaf grabada dos años antes. La falta de ideas nuevas se refleja en "George & James" (1984), "Stars & Hank Forever" (1986) y "The King & Eye" (1989). Estos LP consisten de material ajeno (de George Gershwin, James Brown, John Philip Sousa, Hank Williams y Elvis Presley) como parte de la saga "American Composers Series", que terminó siendo abandonada.

Otros álbumes de la época son las bandas sonoras "Whatever Happened to Vileness Fats?" (1984) y "Census Taker" (1985), así como el conceptual "God in Three Persons" (1988).

En 1990 publican "Freak Show" en CD y CD-ROM, con el que los Residents comienzan su interés en la emergente tecnología de computación. En 1998 volvieron a llamar la atención de la crítica con "Wormwood", obra conceptual con historias alternativas de la Biblia.

Sus discos más destacados en los últimos años son "Demons Dance Alone" (2002, influenciado por los hechos del 9-11) y "Animal Lover" (2005, historias desde el punto de vista de animales). Aprovechando la tecnología actual, crearon miniseries en su sitio oficial y mini-álbumes para descarga mp3. También en los últimos años el cantante del grupo se dio a conocer como Randy Rose, quien en realidad es Homer Flynn (su voz es reconocible desde "Not Available" de 1974).

En 2012, celebrando el aniversario 40 del primer sencillo, publicaron el "Ultimate Box Set". Se trata de un refrigerador con las obras completas de The Residents, por 100 mil dólares.




</doc>
<doc id="29803" url="https://es.wikipedia.org/wiki?curid=29803" title="Compost">
Compost

El compost o la composta es un producto obtenido a partir de diferentes materiales de origen orgánico , los cuales son sometidos a un proceso biológico controlado de oxidación denominado compostaje. Posee un aspecto terroso, libre de olores y de patógenos, es empleado como abono de fondo y como sustituto parcial o total de fertilizantes químicos. El término deriva del latín "compositus" el cual significa "poner junto".

La composta se usa en agricultura y jardinería como enmienda para el suelo (Abono orgánico), aunque también se usa en paisajismo, control de la erosión, recubrimientos y recuperación de suelos. Fue estudiada por el químico alemán Justus von Liebig.Es una forma para cuidar el medido.

La construcción de pilas o silos para el compostaje tiene el objetivo la generación de un entorno apropiado para el ecosistema de descomposición. El entorno no solo mantiene a los agentes de la descomposición, sino también a otros que se alimentan de ellos. Los residuos de todos ellos pasan a formar parte del compost.

Los agentes más efectivos de la descomposición son las bacterias y otros microorganismos. Los microorganismos eficientes son un conjunto de bacterias (caldo microbiano) que unidas producen a temperaturas favorables un aprovechamiento de los componentes de la materia a compostar para optimizar el proceso de compostaje.
También desempeñan un importante papel los hongos, protozoos y actinobacterias (o actinomycetes, aquellas que se observan en forma de filamentos blancos en la materia en descomposición).

A nivel macroscópico, se encuentran las lombrices de tierra, hormigas, caracoles, babosas, milpiés, cochinillas de humedad, etc., que consumen y degradan la materia orgánica.

En cielo abierto, resulta ser un foco de infecciones, gusanos y malos olores. Una buena política encaminada a reciclar los materiales orgánicos reduce la contaminación y fomenta la producción, reconstruyendo la estructura de la tierra y devolviendo a la naturaleza los nutrientes que la actividad del hombre ha tomado prestados previamente.

Cualquier material biodegradable podría transformarse en composta una vez transcurrido el tiempo suficiente, debido a los diferentes tiempos de descomposición de los mismos. Otros materiales deben evitarse, debido a la producción de malos olores y plagas.






Existen variadas técnicas de compostaje, las que se ajustan a diferentes necesidades; la elección de una técnica u otra depende, entre otras cosas, de la cantidad y tipo de material a procesar, inversión disponible y disponibilidad de terreno, complejidad operacional y del producto final que se quiere obtener.
Los distintos sistemas están determinados por los mecanismos de aireación que se utilizan en el proceso, generalmente los podemos agrupar en: aireación pasiva, aireación forzada, y aireación por volteos del material.

La mayoría de plantas industriales y comerciales de compostaje utilizan procesos activos, porque garantizan productos de mejor calidad en un plazo menor. El mayor grado de control y, por tanto, la mayor calidad, suele conseguirse compostando en un recipiente cerrado con un control y ajuste continuo de temperatura, flujo de aire y humedad, entre otros parámetros.

El compostaje casero es más variado, fluctuando entre técnicas extremadamente pasivas hasta técnicas activas propias de una industria. Para ello se escoge un lugar al aire libre ya sea patio o jardín de preferencia lejos de la casa o la cocina, le debe dar el sol y la sombra durante el día. Cada vez que se integren nuevos desechos orgánicos a la composta o una vez a la semana se revuelve todo con una varilla, este paso es muy importante para ventilar los materiales. En tres o cuatro semanas se observará que es difícil distinguir lo que se fue depositando a excepción de los desperdicios más recientes. Se pueden utilizar productos desodorantes, aunque una pila bien mantenida raramente produce malos olores.

Después de cuatro meses se convertirá en humus (material orgánico complejo y estable) y esto resulta en un abono estupendo con vida, con una gran densidad y variedad de microorganismos que sintetizan enzimas, vitaminas, hormonas, etc. y que repercuten favorablemente en el equilibrio biótico del suelo.

Los microorganismos necesitan agua como medio para transportar nutrientes y otros elementos, además es determinante en el intercambio gaseoso. En el proceso de compostaje es importante que la humedad alcance niveles cercanos al 40-60%. Si el contenido en humedad es mayor, el agua ocupará todos los poros y por lo tanto el proceso se volvería anaeróbico (sin oxígeno), es decir, se produciría una putrefacción de la materia orgánica. Si la humedad es excesivamente baja se disminuye la actividad de los microorganismos y el proceso es más lento. El contenido de humedad dependerá de las materias primas empleadas.

Es un parámetro útil que permite dar seguimiento al proceso de descomposición de la materia orgánica, cuando el material se está compostando pasa por un ciclo de temperaturas que es ocasionado por la actividad metabólica microbiológica. El aumento de la actividad metabólica genera calor y como consecuencia aumenta la temperatura, de manera inversa la disminución de la actividad se evidencia con el descenso de la temperatura. La existencia de estos ciclos de temperaturas divide el proceso de compostaje en cuatro etapas (léase: Etapas del proceso de compostaje): mesófila (menor de 40 ºC), termófila (de 40 a 60ºC), fase de enfriamiento (menor de 40 ºC) y fase de maduración (temperatura ambiente).

El manejo y control de las temperaturas permiten ejercer naturalmente un tratamiento de sanitización especialmente con respecto a microorganismos patógenos, así como también logran destruir semillas de malezas, esporas de hongos y algunas fitotoxinas que posteriormente significarían un problema al adicionar el compost sobre cultivos agrícolas. Sin embargo, dicho manejo y control debe ser adecuado ya que de lo contrario no solo destruiría organismos patógenos sino también flora benéfica antes de que el proceso lo haga naturalmente en el momento justo. Las lecturas periódicas mediante el uso del termómetro ayudan a determinar el momento en que la pila debe ser volteada, si esta alcanza sobre los 70°C. O si una pila de compostaje no logra subir su temperatura por sobre los 48°C pasados algunos días, es indicativo de que probablemente no hay suficiente nitrógeno en la pila para seguir el proceso.

La temperatura está condicionada por la humedad y la aireación, debido a la relación existente entre estos dos parámetros y la actividad metabólica de los microorganismos.

En términos generales, los microorganismos absorben treinta partes de carbono por cada parte de nitrógeno. El carbono se utiliza como fuente de energía siendo 10 partes incorporadas al protoplasma celular y 20 partes eliminadas como dióxido de carbono. En forma práctica, la relación carbono/nitrógeno permite conocer la velocidad de descomposición y determinar el tiempo de compostaje, siempre y cuando las condiciones de humedad, aireación y temperatura sean las óptimas. Para obtener un compost de buena calidad es importante que exista una relación equilibrada entre ambos elementos. Teóricamente una relación carbono/nitrógeno de 25-35 es la adecuada, pero esta variará en función de las materias primas que conforman el compost. Si la relación es mayor a 35 no existe suficiente nitrógeno para el crecimiento microbiano por lo cual disminuirá la actividad biológica y por ende se retrasará el proceso. En cambio si es menor a 30 el nitrógeno se encontrará en exceso por lo que puede perderse como amoniaco (NH), lo que traerá como consecuencia olor desagradable.

Este es un parámetro importante para evaluar el ambiente microbiano y la estabilización de los residuos. El valor del pH, depende de los materiales de origen y varía en cada fase del proceso compostaje. El pH inicial esta normalmente entre 5 y 7. En los primeros días de compostaje, el pH cae a 5 o menos, debido a la presencia de ácidos orgánicos simples, y la temperatura sube debido a la producción de organismos mesófilos. Después de aproximadamente 3 días, la temperatura llega a la etapa termófila y el pH comienza a subir hasta aproximadamente 8 a 8,5 debido a la conversión del amonio en amoniaco, alcalinizando el medio durante el resto del proceso aeróbico. En la fase de maduración, el valor del pH llega a un valor de entre 7 a 8 en el compost.

El proceso de compostaje es un proceso aerobio, es decir, necesita la presencia de oxígeno para el desarrollo adecuado de los microorganismos, por lo tanto la aireación es un factor importante en el proceso de compostaje ya que el oxígeno es esencial para el metabolismo y la respiración de los microorganismos que participan en él. La aireación tiene un doble objetivo, primero aportar el oxígeno suficiente a los microorganismos y, segundo, permitir al máximo la evacuación de CO producido. Así mismo, la aireación evita que el material se compacte o se encharque. Las necesidades de oxígeno varían durante el proceso, alcanzando la mayor tasa de consumo durante la fase termofílica. La saturación de oxígeno en el medio no debe bajar del 5%, siendo el nivel óptimo el 10%. Un exceso de aireación provocaría el descenso de temperatura y una mayor pérdida de la humedad por evaporación, haciendo que el proceso de descomposición se detenga por falta de agua. Igualmente, una baja aireación, impide la suficiente evaporación de agua, generando exceso de humedad y un ambiente de anaerobiosis.

La actividad de los microorganismos ocurre generalmente en la superficie de las partículas, por lo tanto el tamaño de éstas debe ser pequeño, a fin de aumentar la superficie y favorecer la actividad de los microorganismos y la tasa de descomposición. El tamaño ideal de partículas es de 2 a 5 cm. Además mientras menor sea el tamaño de las partículas, la pila se tiende a compactar los que trae como consecuencia una menor aireación y por ende una menor actividad microbiana, retardando el proceso. La densidad del material, y por lo tanto la aireación de la pila o la retención de humedad, están estrechamente relacionados con el tamaño de la partícula, siendo la densidad aproximadamente 150 -250 kg/m³, conforme avanza el proceso de compostaje, el tamaño disminuye y por tanto, la densidad aumenta, 600-700 kg/m³.

A veces se añaden otros ingredientes con el fin de enriquecer la mezcla final, controlar las condiciones del proceso o activar los microorganismos responsables del mismo. Espolvorear cal en pequeñas cantidades puede controlar la aparición de un excesivo grado de acidez que reduzca la velocidad de fermentación. Las
algas proporcionan importantes micronutrientes. Algunas rocas pulverizadas proporcionan minerales, al contrario que la arcilla.

La fracción de estiércol puede provenir de heces humanas. No obstante, el riesgo de que no se alcancen temperaturas suficientemente altas para eliminar los patógenos hace que no suelan utilizarse en cultivos alimentarios. Tampoco se recomienda en el compostaje casero la utilización en general de heces de animales carnívoros pues contienen patógenos difícilmente eliminables. Aun así pueden ser útiles para el abonado de árboles, jardines, etc.

En clima mediterráneo la madurez del compost se obtiene tras 3 o 6 meses en primavera/verano y de 6 a 9 meses en invierno.

Domésticos: Esta categoría considera materiales residuales de la preparación de comidas (partes de frutas, verduras, y cáscara de huevo, entre otros) y desechos de origen animal (carne, piel, sangre, huesos y otros).

De jardín: Incluye los restos de cultivos de las huertas, flores muertas, tallos, pastos y hojarascas.

Subproductos agrícolas: Los más utilizados son los residuos de cosecha de prácticamente todo cultivo (por ejemplo arroz, trigo, cebada, maíz, caña de azúcar, frijol, girasol, etc.) así como cascarillas y salvado obtenidos de la trilla o molienda.

Desechos del ganado: Los estiércoles, orina y deyecciones de todo tipo de animales, son excelentes para el compostaje ya que contienen un alto porcentaje de nutrientes. 

Forestales: Los restos de los árboles, hojas y ramas caídas son fuente importante de material para la elaboración de compostas. Estos desechos contienen grandes cantidades de celulosa y lignina que se descomponen parcialmente en la pila de compostaje y continúan mineralizándose en el suelo después de aplicados. 

Desechos urbanos y agroindustriales: Se constituyen de la fracción biodegradable de la basura, como cartón, papel, residuos finos de comida y fibras naturales y los residuos que proceden de la industrialización de productos tales como hortalizas, cacao, café, arroz, maíz, trigo, sorgo, maderas y semillas, entre otros. Debe evitarse el uso de materiales no biodegradables, como vidrios, metales, alambre, plásticos, caucho, cenizas frescas, fibras sintéticas o frutos con espinas, ya que pueden causar problemas a las personas encargadas de su manejo.

El compostaje de café se ejecuta con la recolección de los residuos orgánicos de café, el cual tiene por objetivo servir de abono para las plantas y áreas verdes, ya que proporciona nutrientes para generar un desarrollo sostenible.

El procedimiento para hacer compost de café es:


Una vez pasado el tiempo de transformación del café se obtiene el abono, el cual puede distribuirse en las áreas verdes.

Se puede obtener vermicompost como producto de excreción de la lombriz roja u otros miembros de la familia Lumbricidae.Estos organismos se alimentan de residuos orgánicos y los transforman en un producto rico en nutrientes y microbios del suelo utilizado para fertilizar o enriquecer la tierra como medio de cultivo. Existe una actividad llamada lumbricultura, que trata las condiciones de cría, reproducción y supervivencia de estas lombrices. Incluso existe un mercado mundial para comercializarlas.

Es un sistema de aprovechamiento sistémico detritófago (permacultural) en el que se introducen gallinas. Se aporta de modo sucesivo y diario al compostero restos de materia orgánica de origen doméstico y residuos verdes de la huerta y jardín que sirven de alimento a las gallinas y a otra microfauna. Al cabo de unos dos meses, se completa el primer compostero con la gallinaza que aporta nitrógeno, se sella y se riega para permitir y acelerar la fase térmica. Al concluir la fase térmica se vuelve a permitir el acceso a las gallinas que aprovechan como complemento proteico la alta densidad de microfauna y lombrices, removiendo semanalmente el mismo hasta que el compost madure. Este proceso de compostaje introduce complejidad ecológica y permite aumentar el rendimiento y aprovechamiento. El compostero actúa pues como comedero (que se puede realizar con palés, conformando un m de base) dejando entradas para las gallinas en dos laterales, y/o por la parte superior con escala. Generalmente, dependiendo del tipo y cantidad de residuos, es necesario complementar la alimentación de las gallinas con algo de grano-pienso y calcio (conchas) y proteínas.

En Torremocha de Jarama se inició en 2010 una experiencia de dos avicomposteros comunitarios cada uno de ellos con 13 gallinas y usado por 7 familias que aportan sus restos orgánicos, cada familia tenía un día fijo a la semana para recoger los huevos. Se obtiene de este modo unos tres metros cúbicos de compost de calidad y unos 3.000 huevos al año.. En 2012 se inició un proceso de mayor envergadura en el municipio de Noain en Navarra en un parque municipal y con una implicación del ayuntamiento (parque de los sentidos) en que participan 30 familias que se reparten los días del mes para aportar los biorresiduos del contenedor marrón donde los vecinos depositan sus biorresiduos. El caso de Noain sirvió para el diseño de un modelo prefabricado de gallinero compostero -modelo avicompo- por parte de la empresa vermican. 

A partir del 2012 la red de municipios TERRAE impulsó el modelo en varios municipios de Toledo (Campillo de la Jara), Madrid (Redueña) y Cáceres (Carcaboso) aportando los ayuntamientos las instalaciones y/o los materiales para la autoconstrucción de las mismas.

El compostaje es un proceso durante el cual la temperatura de los residuos varia dependiendo de la actividad metabólica de los microorganismos. De acuerdo con este parámetro, el proceso de compostaje se puede dividir en cuatro etapas: mesófila, termófila, enfriamiento y maduración.

En esta fase tenemos un material fresco, sin humidificar y a temperatura ambiente. Los microorganismos mesófilos presentes en los materiales empiezan a desarrollarse utilizando fuentes sencillas de carbono y nitrógeno, de esta forma crecen y se multiplican descomponiendo los materiales. Esta actividad microbiana provoca el aumento de la temperatura a 40-45 ºC en pocos días (entre dos y ocho). Además, la descomposición de fuentes sencillas de carbono, como azúcares, produce ácidos orgánicos y, por tanto, el pH puede bajar (hasta cerca de 4.0 o 4.5).

Si el material inicial tiene poca humedad la degradación será lenta o inapreciable. Si en cambio tiene un exceso de humedad tenderá a la putrefacción en vez de a la descomposición aeróbica que caracteriza el compostaje. Esta degradación con exceso de humedad facilita la proliferación de bacterias anaeróbicas y hongos que, además de desprender malos olores, convierten la materia orgánica en un producto no adecuado para el suelo. La relación Carbono/Nitrógeno debe ser la adecuada (entre 25/1 y 30/1) para garantizar las fuentes de energía y proteínas para los microorganismos mesófilos.

Esta fase también se conoce como fase de higienización, el material alcanza temperaturas mayores que los 45 °C, los microorganismos mesófilos son reemplazados por aquellos que crecen a mayores temperaturas (entre 45 ºC y 70 ºC), en su mayoría bacterias (bacterias termófilas), que actúan facilitando la degradación de fuentes más complejas de carbono, como la celulosa y la lignina. 

En un comienzo bacterias y hongos termófilos empiezan a degradar la celulosa y parcialmente la lignina, con lo cual la temperatura aumenta, además, actúan transformando el nitrógeno en amoníaco por lo que el pH del medio sube. A partir de los 60 °C, los hongos termófilos cesan su actividad y aparecen las bacterias que producen esporas y actinobacterias, que son las encargadas de descomponer las ceras, hemicelulosas y otros compuestos de carbono complejos. Durante varios días (o algunos meses, según material de partida, condiciones climáticas y otros factores) se mantiene la temperatura alta y disminuye la actividad biológica, se produce la pasteurización del medio, permitiendo la destrucción de bacterias y contaminantes de origen fecal como "Escherichia coli y Salmonella spp." Igualmente, esta fase es importante pues las temperaturas por encima de los 55°C eliminan los quistes y huevos de helminto, esporas de hongos fitopatógenos y semillas de malezas que pueden encontrarse en el material de partida, dando lugar a un producto higienizado. En esta etapa se deben realizar frecuentes volteos con el objeto de aportar oxígeno, el que es rápidamente consumido por los microorganismos.

Cuando prácticamente las fuentes de carbono y, en especial de nitrógeno han sido agotadas en el material orgánico, la temperatura desciende nuevamente hasta los 40-45°C ya que el calor que se genera en el interior de la pila es menor que el que se pierde. Como consecuencia de este descenso de temperatura, bacterias y hongos (algunos visibles a simple vista) mesófilos reinvaden el compost, reinician su actividad y degradan la celulosa y la lignina restantes descendiendo levemente el pH. Esta fase se reconocerá cuando después de voltear la pila no exista un aumento de temperatura posterior.

Es un período que demora meses a temperatura ambiente, durante los cuales se producen reacciones secundarias de condensación y polimerización de compuestos carbonados para la formación de ácidos húmicos y fúlvicos en el ambiente.

= Resumen =

El compost o la composta es un producto obtenido a partir de diferentes materiales de origen orgánico, los cuales son sometidos a un proceso biológico controlado de oxidación denominado compostaje. Posee un aspecto terroso, libre de olores y de patógenos, es empleado como abono de fondo y como sustituto parcial o total de fertilizantes químicos. 






En esta fase tenemos un material fresco, sin humidificar y a temperatura ambiente. Los microorganismos mesófilos presentes en los materiales empiezan a desarrollarse utilizando fuentes sencillas de carbono y nitrógeno, de esta forma crecen y se multiplican descomponiendo los materiales

En un comienzo bacterias y hongos termófilos empiezan a degradar la celulosa y parcialmente la lignina, con lo cual la temperatura aumenta, además, actúan transformando el nitrógeno en amoníaco por lo que el pH del medio sube.

Cuando prácticamente las fuentes de carbono y, en especial de nitrógeno han sido agotadas en el material orgánico.

Es un período que demora meses a temperatura ambiente, durante los cuales se producen reacciones secundarias de condensación y polimerización de compuestos carbonados para la formación de ácidos húmicos y fúlvicos.



</doc>
<doc id="29804" url="https://es.wikipedia.org/wiki?curid=29804" title="Nu">
Nu

Nu puede referirse a:


</doc>
<doc id="29807" url="https://es.wikipedia.org/wiki?curid=29807" title="Ceropegia woodii">
Ceropegia woodii

Ceropegia woodii , es una especie de planta de la familia Apocynaceae. Llamada comúnmente "collar de corazones" es una planta colgante con hojas en forma de corazón de color verde oscuro con manchas de apariencia marmórea.

Es endémica de Sudáfrica, Suazilandia, y Zimbabue.

Es una planta perenne suculenta con forma de parra que crece a 2-5 cm de altura y se extiende para llegar a un máximo de 2-4 m de longitud. Sus hojas son en forma de corazones, de alrededor de 1-2 cm de ancho y largo. Cuando se exponen a la luz suficiente tienen un color verde profundo, con poca luz las hojas son de color verde pálido. Con la edad se desarrolla una rama caudal en su base. Las raíces, y de vez en cuando los tallos, a menudo desarrollan tubérculos. En los tallos se forman nodos, y es probable la razón por el nombre común de rosario vid.

La flor es en general similar a los de otras especies de Ceropegia. La corola crece hasta los 3 cm de longitud y es una mezcla de colores de blanco pálido y magenta. Los cinco pétalos son un púrpura profundo.

Ceropegia woodii es una planta muy popular de interior, a menudo creciendo en cestas colgantes por lo que del largo tallo rastrero pueden colgar las ramas con sus hojas espaciadas como una hilera de grandes piedras. Varios cultivares han sido seleccionados, algunos con variadas hojas.

Se requiere excelente drenaje del agua, debe ser regado sólo cuando está seco, y nunca debe estar en el agua. El exceso de agua debe ser eliminado del platillo de la planta después del riego. Pueden ser cultivadas al aire libre sólo en zonas subtropicales y tropicales, con una temperatura mínima de 15 ° C. El sombreado parcial es útil cuando la planta se cultiva al aire libre.
"Ceropegia woodii" fue descrito por Rudolf Schlechter y publicado en "Botanische Jahrbücher für Systematik, Pflanzengeschichte und Pflanzengeographie" 18(Beibl. 45): 34. 1894. 




</doc>
<doc id="29808" url="https://es.wikipedia.org/wiki?curid=29808" title="Caza">
Caza

La caza (también denominada "actividad cinegética") es la actividad o acción en la que se captura o abate un animal en estado salvaje, tras su pisteo y persecución. Según el filósofo español José Ortega y Gasset, «la caza es todo lo que se hace antes y después de la muerte del animal. La muerte es imprescindible para que exista la cacería».

La especie humana ha practicado la caza desde la prehistoria, era la primera y principal ocupación de los hombres. Se considera que los primeros grupos humanos utilizaron un sistema de caza, pesca y recolección el cual fue muy eficiente para garantizar el poblamiento del planeta. Se estima que más del 80 % de los grupos humanos en la actualidad son herederos de este modo de producción basado en el desarrollo de incipientes tecnologías y técnicas primitivas de recolección, cacería y pesca.

El humano comenzó a cazar para subsistir, y así sigue siendo actualmente en muchas partes del mundo. La caza de subsistencia es aquella actividad que se realiza con la finalidad de obtener proteína animal o subproductos de caza para satisfacer las necesidades propias de los grupos humanos ligados a zonas rurales donde la disponibilidad de especies cinegéticas es alta.

El ejercicio de la caza se refleja en textos religiosos y mitológicos. Por ejemplo, la Biblia dice que Nemrod nieto de Noé era cazador. Ismael, hijo de Abrahan y de Agar, se distinguió en este ejercicio. Esaú vendió su herencia a Jacob por un plato de lentejas al llegar hambriento de la caza. David fue cazador, etc.

La mitología griega representa a Artemisa como la divinidad de los cazadores. Quirón, que cuidó de la instrucción de la mayor parte de los héroes de la antigüedad, fue instruido por Artemisa en el arte de la montería. La misma atribuye a Pólux la gloria de haber enseñado o adiestrado los perros en la caza; y Cástor introdujo los caballos en la caza de los ciervos.

En Babilonia y Media tenían también una afición particular a la caza y los últimos habían construido grandes parques, en los que tenían encerrados leones, jabalíes, leopardos y ciervos. En la Grecia de los tiempos heroicos eran apasionados también por la caza. Platón llamaba a la caza «ejercicio divino» y la escuela de las virtudes militares. Leemos en Homero que Ulises fue herido en el muslo por un jabalí cuya señal le duró toda la vida. Tenían una cierta vanagloria en poseer perros bien enseñados a los que les daban nombres diferentes, distinguiéndolos según el país de donde provenían. Tampoco les era desconocida la caza de pájaros con el halcón y gavilán.

En Roma solo los esclavos y la gente de baja extracción eran los que iban a la caza, a pesar de que consideraron esta ocupación como un ejercicio honesto. Paulo Emilio regaló a Escipión un equipaje de caza semejante a los de los reyes de Macedonia; y el joven héroe después de la derrota de Perseo cazó en el reino de este príncipe durante todo el tiempo que sus tropas permanecieron en el. Pompeyo, vencedor en las regiones africanas, se entregó entre estos pueblos a los placeres de la caza.

En Roma se iban a cazar en los bosques, en los campos, etc. y en los últimos tiempos de la república, en los sotos o parques en donde tenían encerrados animales de toda especie. La caza con perros les pareció siempre la más noble; a pesar de que esto no impedía, como dice Plinio, que cazasen también con el halcón o el gavilán.

Los francos después de la conquista de las Galias encargaron a los locales el cultivo de las tierras y se reservaron para sí la caza, que pasó a ser entre ellos un ejercicio noble.

La caza era antiguamente permitida a todo el mundo. Los romanos no habían formado todavía de ella un punto de jurisprudencia. La ley Sálica contenía ya algunos reglamentos relativos a la caza, pero no coartaba en nada el derecho natural de esta. Poco a poco se fueron introduciendo leyes y formando reglamentos para el ejercicio de ella, no permitiendo en ciertos países el dedicarse a cazar sino a la clase distinguida de la sociedad.

En los primeros siglos del cristianismo el celo de los fieles no les permitía ir a cazar durante la cuaresma y días de ayuno, aunque se guardase este, destinando el tiempo para ejercicios de penitencia. Muchas mujeres de Inglaterra y algunas de Francia tienen afición por la caza.

Las especies cinegéticas aplicables al término de «caza mayor» difieren de la legislación de un país a otro. Por ejemplo, en España se considera caza mayor a las especies que en estado adulto son más grandes que un zorro (sin incluirlo): jabalí, ciervo, corzo, cabra montés, rebeco, lobo (al norte del río Duero), gamo, muflón y arruí.

Aunque en caza menor también es necesario tener en cuenta las características meteorológicas (viento, lluvia, fases lunares, temperatura, etc.) es en la mayor donde hay que tenerla más en cuenta, ya que estas características condicionan los movimientos de los animales, o delatan la presencia del cazador.
De esta modalidad, con más de 300 años de historia, existen dos variantes practicadas en España.

En la variante más conocida, practicada en casi toda la península, los cazadores (denominados monteros) se colocan en puestos rodeando una mancha (zona de monte más o menos espeso donde se refugian los animales) dispuestos en líneas (denominadas armadas) que rodean la mancha. Una vez instalados todos los cazadores una serie de rehalas se comienzan a mover ordenadamente por dicha mancha para lograr que los animales huyan, de forma que los cazadores puedan disparar sobre ellos.

En esta variante las distintas armadas en las que se colocan los puestos reciben las siguientes denominaciones en función de su posición:


Una vez situado el montero en su puesto (o postura), deberá permanecer en él hasta la finalización de la montería sin moverse de su posición. Es, por tanto, un método de caza estático donde el cazador espera a que los perros agrupados en rehalas conduzcan a los animales a la posición en la que se encuentra este para intentar abatirlos con su rifle o escopeta.

En la otra variante, más típica del norte de España, antes de colocar los puestos se buscan los encames con perros atraillados (principalmente sabuesos). Sabiendo la localización de los animales a cazar se colocan los puestos en función de sus escapatorias y querencias.

Durante la montería, además de a las normas de seguridad el cazador debe prestar atención a disparar únicamente sobre los animales sobre los que está permitido hacerlo y abatiendo únicamente el número autorizado, pudiendo existir distintas limitaciones en función de la especie, sexo, edad, etc.

Finalizada la montería, se realiza la denominada «junta de carnes», donde los porteadores situarán los animales abatidos durante la cacería para que los monteros puedan observar sus trofeos y los de los demás asistentes.

Modalidad similar a la anterior, pero con limitaciones de puestos, perros y total de participantes. Cabe destacar que la organización de las posturas es diferente a la de una montería, ya que en una batida se colocan los puestos en una única línea para cortar la huida de las piezas a cazar.
Consiste en la aproximación a un animal previamente seleccionado. Es la modalidad que más esfuerzo precisa del cazador, pues debe de conseguir llegar a una distancia óptima de disparo, sin que el animal perciba su presencia. La aproximación se realiza en el medio natural del animal, siendo comúnmente en cumbres, laderas escarpadas o montes cerrados. Esta modalidad se caracteriza por ser la más selectiva de todas, ya que desde el primer momento, el cazador sabe con certeza el animal al que va a dar caza, así como su sexo y su edad aproximada.

El cazador se sitúa en una posición próxima a un lugar asiduo de los animales (generalmente jabalíes, en bañas o zonas comida). En el caso que apareciesen, el cazador después de la observación de los animales efectuará el disparo sobre el animal escogido.
Una de las variantes más practicada es la espera nocturna, en la que el cazador se ayuda o no, de una fuente luminosa artificial.

Un solo cazador, con o sin perros, busca el animal a cazar, bien en su encame o cerca de este, para terminar el lance con un disparo o después del agarre por parte de los perros con cuchillo o lanza.

Similar a la anterior, pero más de un cazador. Aunque las definiciones de «al salto» y «en mano» difieren poco, en realidad la ejecución es muy diferente. En esta cacería se abaten más animales.

Las especies cinegéticas de caza menor difieren de un país a otro. En España se considera a los menores que el zorro y los principales en función de sus capturas son: conejo, perdiz, codorniz, liebre y paloma.

Similar a la de caza mayor. Si se practica con perros también se llama «a rabo».

Similar a la de caza mayor.

Similar a la montería de mayor, pero con especies menores. Principalmente perdiz, aunque también se suele dar con liebre.

Caza con la ayuda de una perdiz viva enjaulada puesta en el centro de un claro para atraer a sus congéneres, los cuales son atrapados por redes o generalmente disparados por el cazador escondido en un puesto.

Caza similar al «aguardo» o «espera» en mayor, salvo que aquí los cazadores principalmente esperan a los animales en el trayecto de sus dormideros a las zonas de comida, o sus pasos migratorios. Son cazados principalmente palomas y zorzales.

Caza usando un ave de presa.

Se trata de una modalidad en la que no se utilizan armas y en la que solo se utilizan perros de raza galgo. Por otra parte es similar a la caza «al salto» o «en mano» sobre terrenos llanos y limpios de vegetación arbórea o arbustiva.

Caza realizada sobre mamíferos de madriguera (zorros y conejos) utilizando perros terrier o hurones.

Es una modalidad en la cual los perros de caza de rastro persiguen a la especie cinegética mediante el olfato. En un principio se rastrean los rastros que el animal ha dejado antes de acostarse. A continuación se levanta al animal de la cama o refugio utilizado. Finalmente se le persigue. Esta modalidad es propia del norte de España utilizada para la caza del zorro y liebre en la caza menor, y jabalí y corzo en la caza mayor.

Debido a la conciencia hacia el cuidado y protección de los animales que organizaciones de defensa de los animales han estado promoviendo, y el rechazo de muchos sectores de la sociedad contra la caza, ha llevado a en algunos países esta práctica haya sido prohibida, como es el caso de Kenia, Costa Rica, ya que desde el 10 de diciembre de 2012 la caza deportiva fue declarada ilegal por la Asamblea Legislativa de ese país convirtiéndose en el primer país de Latinoamérica en prohibir tal actividad. En 2019 Colombia también prohíbe la caza deportiva, siendo así el segundo país en Latinoamérica en declararla ilegal. Mientras que en algunos países africanos mantuvieron la prohibición de caza mayor durante algún tiempo a raíz de la indignación internacional y la mala imagen de algunos países que la caza generaba al matar animales como leones, jirafas o elefantes en los safaris, como ocurrió con la muerte del león Cecil a manos del cazador norteamericano Walter Palmer, provocando la indignación de las autoridades de Zimbabue y del mundo al cazarlo en un área donde la caza estaba prohibida, trayendo como consecuencia que se prohibiera la caza de leones en aquel país después de su episodio durante un tiempo, o el escándalo que provocó en 2012 el rey Juan Carlos I de España cuando en plena crisis económica fue a un safari a cazar elefantes salvajes, generando una ola de críticas e indignación internacional, a raíz de aquel escándalo, Botsuana prohibió la caza de elefantes durante algunos años.

Para el ejercicio de la caza, el humano se ha servido, y lo sigue haciendo, de otros animales: perros, hurón y aves rapaces (cetrería). El uso de perros es necesario prácticamente en todas las modalidades de caza, tanto menor como mayor, siendo habitual en la caza de animales como el conejo, o como ocurre en la caza de liebre con galgo, siendo este último el único medio que usa el hombre para capturar al animal.

Cada vez más la práctica de la caza es reconocida como un foco importante de transmisión de enfermedades de animales a humanos (zoonosis). El contagio se produce por la exposición de los cazadores a los agentes presentes en el suelo de los bosques y áreas de caza en general, y también por el contacto directo o consumo de animales infectados.





</doc>
<doc id="29809" url="https://es.wikipedia.org/wiki?curid=29809" title="Avión de caza">
Avión de caza

Un avión de caza (también llamado avión de combate), o simplemente caza, es una aeronave militar diseñada fundamentalmente para la guerra aérea con otras aeronaves, en oposición a los bombarderos, que están diseñados principalmente para atacar objetivos terrestres mediante el lanzamiento de bombas. Los cazas son pequeños, veloces y de gran maniobrabilidad. Muchos cazas poseen capacidades secundarias de ataque a tierra, y algunos son de doble propósito para actuar como cazabombarderos, término también usado para nombrar a los aviones de ataque a tierra con capacidades de caza.

Los cazas son el principal medio con el que las fuerzas armadas consiguen la superioridad aérea sobre sus oponentes en batalla. Por lo menos desde la Segunda Guerra Mundial, lograr y mantener la superioridad aérea ha sido un componente clave a la hora de conseguir la victoria en la guerra, particularmente en una guerra convencional entre ejércitos regulares (no así en una guerra de guerrillas). De este modo, la adquisición, el entrenamiento y el mantenimiento de una flota de cazas representa una parte muy sustancial de los presupuestos de defensa para las fuerzas armadas actuales.

Entre las principales misiones cumplidas por los cazas destacan la patrulla aérea de combate (CAP) y la interceptación de aeronaves enemigas. Cuando el enfrentamiento entre cazas se produce en persecución a corta distancia, recibe el nombre de "dogfight" o combate aéreo cercano.

El término ‘caza’ en español, igual que en portugués ("caça") e italiano ("caccia"), hace referencia a «cazar», y en francés ("chasseur") significa literalmente «cazador». En cambio, en inglés ("fighter") hace referencia a la «lucha» ("fight"). En inglés ese término no se hizo oficial hasta después de la Primera Guerra Mundial, ya que los británicos llamaron a este tipo de aviones «exploradores» ("scouts") hasta principios de los años 1920 y los estadounidenses denominaron a sus cazas como aviones de «persecución» ("pursuit") hasta finales de los años 1940 (designados P- desde 1916). En la mayoría de lenguajes los cazas reciben nombres de significado similar, a excepción de en ruso, donde son llamados "истребитель" (pronunciado "istrebitel") y que significa «exterminador».

Los cazas fueron desarrollados en respuesta al incipiente uso de aviones y dirigibles en la Primera Guerra Mundial para tareas de reconocimiento aéreo y ataque a tierra. Los primeros cazas eran aviones muy pequeños y dotados de armas ligeras; la mayoría eran biplanos. Como la guerra aérea fue adquiriendo cada vez más importancia, también lo hizo el control del espacio aéreo. Para la Segunda Guerra Mundial, los cazas ya eran en su mayoría monoplanos completamente metálicos armados con cañones y ametralladoras. Hacia el final de la guerra, los motores turborreactor ya estaban comenzando a reemplazar a los motores de pistones como medio de propulsión, y ya estaban apareciendo cada vez más mejoras sofisticadas para el armamento.

Los cazas de reacción modernos son propulsados mayoritariamente por uno o dos motores turbofán y están equipados con un radar como principal método de localización de objetivos. El armamento principal consiste en misiles aire-aire (desde tan solo dos en algunos cazas ligeros a tanto como ocho o doce en cazas de superioridad aérea como el Sujoi Su-27 y el F-15 Eagle) y un cañón automático como armamento de reserva (normalmente de calibre entre 20 y 30 mm); no obstante, si disponen de capacidad polivalente también pueden emplear misiles aire-superficie, así como bombas guiadas o no guiadas para ataque a tierra.

La palabra "caza" fue usada por primera vez para describir un avión biplaza con la fuerza de sustentación suficiente para llevar, además del piloto, a un observador y su ametralladora. El primero de estos cazas pertenecía a la serie "Gunbus", una serie de aviones experimentales provistos de armas de la compañía británica Vickers que culminó en el modelo Vickers F.B.5 de 1914. El principal inconveniente de este tipo de aviones fue su falta de velocidad. En seguida se vio que un avión con intención de destruir a otro de su tipo en el aire por lo menos necesitaba ser lo suficientemente rápido como para alcanzar a su presa.

Por fortuna ya existía otro tipo de avión militar, que debía servir de base para un caza efectivo en el sentido moderno de la palabra. Estaba basado en el pequeño y rápido avión desarrollado antes de la guerra para carreras aéreas tales como la Copa Gordon Bennett y la Copa Schneider. Este era el avión "explorador" (en inglés: "scout") o de reconocimiento militar, que no estaba preparado para poder llevar armamento importante, sino que más bien se confiaba en su velocidad para poder llegar a la posición que se quería explorar o reconocer y luego regresar rápidamente para informar. Al mismo tiempo, por su velocidad era un blanco difícil para la artillería antiaérea o aviones armados enemigos. Los aviones británicos "exploradores" en este sentido incluían el Sopwith Tabloid y el Bristol Scout; entre los equivalentes franceses destacaba el ligero y veloz Morane-Saulnier N.

En la práctica, poco después del inicio efectivo de la guerra, los pilotos de los pequeños aviones exploradores comenzaron a armarse con pistolas, carabinas, granadas y un surtido de armas improvisadas con las que atacar aviones enemigos. Era inevitable que tarde o temprano se encontrara la manera de armar a los "exploradores". Un método fue construir el avión explorador en configuración propulsora como el Airco DH.2, con la hélice montada detrás del piloto. El principal inconveniente era que la alta resistencia aerodinámica de la estructura de cola de un avión de este tipo significaba ser más lento que otro avión similar de configuración tractora. Otra opción fue montar la ametralladora de manera que permitiera al piloto dispararla fuera del arco de la hélice.

Inicialmente en los aviones tractores solo eran posibles a efectos prácticos dos configuraciones del armamento. Una implicaba tener un segundo tripulante añadido (artillero) detrás del piloto para apuntar y disparar la ametralladora montada en un afuste giratorio. Sin embargo, esto limitaba el área de cobertura principalmente al hemisferio trasero, y la incapacidad para coordinar de manera efectiva las maniobras del piloto con el apuntamiento del artillero, lo que reducía la precisión y eficacia del armamento además de añadir el peso del segundo hombre. Esta opción fue empleada principalmente como medida defensiva en aviones de reconocimiento a partir de 1915. La configuración alternativa era montar la ametralladora sobre el ala superior para disparar por encima de la hélice. Si bien es más eficaz para el combate ofensivo, dado que el piloto podía mover y apuntar el arma en unidad con el avión, este emplazamiento hacía más difícil determinar el punto de mira adecuado. Además, esta ubicación hacía casi imposible para un piloto maniobrar su avión y acceder al mismo tiempo a la recámara del arma —una consideración muy importante dada la tendencia a encasquillarse que tenían las primeras ametralladoras— por lo tanto esa fue una solución provisional. No obstante, una ametralladora disparando por encima de la hélice tenía algunas ventajas, y continuó en servicio desde 1915 (Nieuport 11) hasta 1918 (Royal Aircraft Factory S.E.5). El afuste Foster británico estaba específicamente diseñado para este tipo de aplicaciones, permitiendo bajar el arma al frente del piloto para recargar munición o desatascarla.

La necesidad de armar un explorador tractor con un arma de disparo frontal, cuyos proyectiles pasaran entre las palas de la hélice, era evidente incluso antes del estallido de la guerra, y su abordaje motivó a los inventores tanto de Francia como de Alemania a llevar a la práctica la idea de que un mecanismo de sincronización evitara que el arma disparase cuando tenía la hélice en frente. Franz Schneider, un ingeniero suizo, había patentado tal dispositivo en Alemania en 1913, pero su trabajo original no fue seguido. El diseñador de aviones francés Raymond Saulnier patentó un dispositivo práctico en abril de 1914, pero las ensayos no resultaron exitosos debido a la poca fiabilidad de la munición de la ametralladora empleada.

En diciembre de 1914, el aviador francés Roland Garros pidió a Saulnier que instalara su mecanismo de sincronización en el avión monoplano Morane-Saulnier L de Garros. Desafortunadamente la ametralladora accionada por gas Hotchkiss tenía un ciclo de disparo que causaba que la bala saliera del arma demasiado tarde para sincronizar con eficacia y coherencia los disparos con una hélice en movimiento. Debido a esto, blindaron las palas de la hélice, y el mecánico de Garros, Jules Hue, añadió cuñas metálicas a las palas para proteger al piloto de rebotes de balas. El monoplano modificado de Garros voló por primera vez en marzo de 1915 y ya comenzó las operaciones de combate poco después. Disparando balas de latón macizo calibre 8 mm, Garros consiguió tres victorias en tres semanas antes de que él fuera derribado el 18 de abril; su avión —junto con el sistema de sincronización— fue capturado por los alemanes.

Sin embargo, el mecanismo de sincronización (llamado "Zentralsteuerung" en alemán) ideado por los ingenieros de la firma de Anthony Fokker fue el primero en atraer el patrocinio oficial, e hizo que el pionero monoplano Fokker "Eindecker" fuera un avión temido sobre el Frente Occidental, a pesar de que era una adaptación de un obsoleto avión de carreras Morane-Saulnier de preguerra, con un rendimiento mediocre y pobres características de vuelo. La primera victoria conseguida por el "Eindecker" fue el 1 de julio de 1915, cuando el "Leutnant" Kurt Wintgens, pilotando con la unidad "Feldflieger Abteilung 6" en el Frente Occidental, le ganó a un monoplano biplaza Morane-Saulnier L al este de Lunéville. El avión de Wintgens, uno de los cinco ejemplares Fokker M.5K/MG producidos como prototipos del "Eindecker", estaba armado con una versión de aviación de la ametralladora Parabellum MG 14 refrigerada por aire y sincronizada, que no necesitaba hélices blindadas. Desde varios puntos de vista, ésta fue la primera victoria de un «verdadero» caza en la historia de la aviación militar.

El éxito del "Eindecker" inició un disputado ciclo de mejoras entre los combatientes, que estimuló la construcción de cazas monoplazas cada vez más capaces. El Albatros D.I de finales de 1916, diseñado por Robert Thelen, estableció el patrón clásico seguido por casi todos los aviones por cerca de veinte años. Al igual que el D.I, casi todos fueron biplanos (solo en contadas ocasiones eran monoplanos o triplanos). La fuerte estructura de caja del ala biplano ofrecía un ala rígida que permitía un control lateral muy preciso, algo esencial para el tipo de maniobras de los cazas. Tenían un único tripulante, que pilotaba el avión y también manejaba su armamento. Estaban armados con dos ametralladoras tipo Maxim —que había resultado mucho más fácil de sincronizar que otros tipos– que disparaban entre las palas de la hélice. Las recámaras de las armas normalmente estaban a la derecha en frente de la cara del piloto. Esto tenía consecuencias evidentes en caso de accidente, pero hacía que si un arma se encasquillaba (algo muy probable en las ametralladoras tipo Maxim) podía ser desatascada en vuelo y también hacía que apuntar fuera una tarea más fácil.

El uso del metal en los aviones de caza fue aplicado por primera vez en la Primera Guerra Mundial por Alemania, cuando Anthony Fokker usó tubos de acero al cromo-molibdeno (similar al acero inoxidable) para la estructura del fuselaje de todos sus diseños de cazas, y el innovador ingeniero alemán Hugo Junkers desarrolló dos diseños de caza monoplano completamente metálicos con alas en voladizo: el proyecto privado estrictamente experimental Junkers J 2, hecho de acero, y en torno a cuarenta ejemplares del Junkers D.I, hecho de duraluminio corrugado, ambos basados en la célula metálica de su pionero avión de demostración de tecnología Junkers J 1 de finales de 1915.

Cuando la experiencia de combate colectivo creció, los pilotos más exitosos como Oswald Boelcke, Max Immelmann y Edward Mannock desarrollaron innovadoras maniobras y formaciones tácticas para mejorar la eficacia en combate de sus respectivas unidades aéreas y acelerar el aprendizaje —e incrementar la esperanza de vida prevista— de los nuevos pilotos que llegaban a la línea del frente. En septiembre de 1916 Oswald Boelcke publicó la doctrina del combate aéreo "Dicta Boelcke", su manual contenía ocho reglas.

Los pilotos aliados y —hasta 1918— los alemanes de la Primera Guerra Mundial no estaban equipados con paracaídas, así que la mayoría de los casos en los que una aeronave se incendiaba o se rompía su estructura las consecuencias era fatales. Los paracaídas fueron desarrollados en 1918, y fueron adoptados por los aviadores alemanes en el transcurso de ese año (el famoso «Barón Rojo» llevaba uno cuando murió en combate), pero el mando Aliado siguió oponiéndose a su uso, por diversos motivos.

El desarrollo de los cazas se desaceleró entre las guerras, y los cambios más significativos comenzaron a llegar hacia el final de ese periodo, cuando los aviones clásicos del estilo de la Primera Guerra Mundial dieron paso a monoplanos con monocasco o semimonocasco metálico y estructura de ala en voladizo (también llamado "cantilever"). Con limitados presupuestos de defensa en esa época, las fuerzas aéreas tendían a ser conservadoras en sus compras de aviones, y los biplanos seguían siendo populares entre los pilotos debido a su agilidad. Hasta mediados de la década de 1930, la gran mayoría de los aviones de caza continuaban siendo biplanos. Diseños como el británico Gloster Gladiator, el italiano Fiat CR.42 y el soviético Polikarpov I-15 eran comunes incluso hasta finales de los años 1930, y muchos aún llegaron a estar en servicio hasta 1942.

El armamento de los cazas comenzó a ser montado en el interior de las alas, fuera del área de giro de la hélice, aunque la mayoría de los diseños conservaban dos ametralladoras sincronizadas sobre el motor (ofrecían mayor precisión). Las ametralladoras con calibre de fusil eran la norma, ya que las ametralladoras de 12,7 mm (.50) o más y los cañones automáticos de 20 mm aún se consideraban una "exageración". Teniendo en cuenta que muchos aviones se construían de manera similar a los diseños de la Primera Guerra Mundial (aunque con armazones de aluminio), no se consideró poco razonable usar armamento del estilo de la Gran Guerra para contrarrestarlos. No hubo suficientes combates aéreos durante la mayor parte del período para refutar esta idea. También comenzaron a aparecer los primeros trenes de aterrizaje retráctiles.

El motor rotativo, popular durante la Primera Guerra Mundial, desapareció rápidamente, sustituido principalmente por el motor radial estacionario. Los motores aeronáuticos multiplicaron su potencia en varias unidades durante este período, pasando de los típicos 180 HP (130 kW) del Fokker D.VII de 1918 a los 900 HP (670 kW) del Curtiss P-36 de 1935. Se inició el debate entre los elegantes motores de cilindros en línea frente a los más confiables modelos radiales. Mientras las fuerzas aéreas navales preferían los motores radiales, las fuerzas con base en tierra solían escoger los modelos en línea. Los diseños radiales no necesitaban un sistema de refrigeración independiente —más vulnerable—, pero ofrecían mayor resistencia aerodinámica. Los motores en línea solían tener una mejor relación potencia a peso, pero había motores radiales que seguían funcionando incluso después de haber sufrido un daño importante en combate.

Algunas fuerzas aéreas experimentaron con cazas pesados, llamados "destructores" ("Zerstörer") por los alemanes. Estos aviones eran grandes y normalmente bimotores, a veces adaptaciones de bombarderos ligeros o medios. Tales diseños por lo general tenían una mayor capacidad interna para combustible, y en consecuencia un mayor alcance, y disponían de armamento más pesado que sus homólogos de un solo motor. En combate, se mostraron lentos y vulnerables ante los más ágiles cazas monomotor.

El principal impulsor de la innovación en los cazas, hasta el período de rearme vivido a finales de los años treinta, no fueron los presupuestos militares, sino las carreras de aviones civiles. Los aviones diseñados para esas carreras fueron pioneros en innovaciones, como los diseños aerodinámicos y los motores más potentes, y fueron la base de los cazas de la Segunda Guerra Mundial.

Al final del período de entreguerras vino la Guerra Civil Española. Esto supuso la oportunidad que la "Luftwaffe" alemana, la "Regia Aeronautica" italiana y la Fuerza Aérea Roja de la Unión Soviética necesitaban para probar sus diseños de aviones más recientes. Cada parte envió varios tipos de aviones para respaldar a su bando en el conflicto. En los combates aéreos sobre España, le fue bien al entonces reciente caza Bf 109 del diseñador alemán Messerschmitt, al igual que al Polikarpov I-16 soviético. El diseño alemán, sin embargo, tuvo un margen considerable para su desarrollo, y las lecciones aprendidas en España dieron lugar a modelos extremadamente mejorados en la Segunda Guerra Mundial. Los rusos, cuyo bando perdió en el conflicto español, sin embargo determinaron que sus aviones eran suficientes para sus necesidades inmediatas. Posteriormente, los I-16 serían aplastados en los combates de la Segunda Guerra Mundial por esos modelos mejorados alemanes, aunque continuó siendo el caza soviético más común en el frente hasta bien entrado 1942. Por su parte, los italianos estaban satisfechos con el rendimiento de sus biplanos Fiat CR.42 y, con pocos fondos, continuaron con ese diseño a pesar de que era obsoleto.

La Guerra Civil Española también constituyó una oportunidad para la actualización de las tácticas de combate. Una de las innovaciones resultado de la experiencia en combate durante este conflicto fue el desarrollo de la formación en V asimétrica o "finger-four" por parte del piloto alemán Werner Mölders. Cada escuadrón de cazas (en alemán: "Staffel") era dividido en varias escuadrillas ("Schwärme") de cuatro aviones. Cada "Schwarm" era dividida en dos "Rotten" o parejas de aviones. Cada "Rotte" estaba compuesta de un líder y un escolta. Esta flexible formación permitía a los pilotos mantener una gran consciencia de la situación, y las dos "Rotten" podían separarse en todo momento y atacar por su cuenta. La "finger-four" sería ampliamente adoptada como una formación táctica fundamental en el transcurso de la Segunda Guerra Mundial.

El combate aéreo formó una parte importante de la doctrina militar de la Segunda Guerra Mundial. La capacidad de los aviones para localizar, hostigar y atacar fuerzas terrestres jugó un papel decisivo en la doctrina alemana de armas combinadas, y su incapacidad para lograr la superioridad aérea sobre Gran Bretaña hizo inviable la invasión alemana de la isla. El Mariscal de Campo alemán Erwin Rommel apuntó sobre el efecto del poder aéreo: «Cualquier persona que tenga que luchar, incluso con las armas más modernas, contra un enemigo que tiene el dominio completo en el aire, lucha igual que un salvaje contra tropas europeas modernas, en virtud de las mismas desventajas y con las mismas oportunidades de éxito.»

Durante los años 1930, comenzaron a surgir dos corrientes de pensamiento distintas acerca del combate aire-aire, que dieron como resultado dos enfoques diferentes para el desarrollo de cazas monoplanos. En Japón e Italia especialmente, seguía habiendo un fuerte pensamiento de que los cazas monoplazas altamente maniobrables y ligeramente armados seguirían desempeñando un papel primordial en el combate aire-aire. Aviones como los Nakajima Ki-27, Nakajima Ki-43 "Hayabusa" y Mitsubishi A6M "Zero" en Japón, y los Fiat G.50 "Freccia" y Macchi M.C.200 "Saetta" en Italia resumen una generación de monoplanos diseñados para este concepto.

La otra corriente de pensamiento, que surgió principalmente en el Reino Unido, Alemania, la Unión Soviética y Estados Unidos, fue la convicción de que las altas velocidades de los aviones de combate modernos y las fuerzas G impuestas por el combate aéreo significaban que los combates aéreos cerrados o "dogfights" en el sentido clásico de la Primera Guerra Mundial serían imposibles. Cazas como el Messerschmitt Bf 109 alemán, el Supermarine Spitfire británico, el Yakovlev Yak-1 soviético y el Curtiss P-40 "Warhawk" estadounidense fueron todos diseñados para velocidades de alto nivel y un buen régimen de ascenso. Que tuvieran una buena maniobrabilidad era conveniente, pero no era el objetivo principal.

La batalla de Jaljin Gol de 1939 entre soviéticos y japoneses (11 de mayo-31 de agosto de 1939), y la subsiguiente invasión alemana de Polonia el día siguiente, fueron demasiado breves, no proporcionaron mucha información a los participantes para una mayor evolución de sus respectivas doctrinas de caza. Durante la Guerra de Invierno, la Fuerza Aérea Finlandesa con superioridad numérica, que había adoptado la formación alemana de cuatro aviones en V asimétrica o "finger-four", machacó a la Fuerza Aérea Soviética, que se basó en la táctica menos eficaz de formación en delta de tres aviones.

Durante la guerra se incrementó bastante la potencia de los motores de pistones. Por ejemplo, el Curtiss P-36 Hawk introducido en la preguerra tenía un motor radial de 900 HP (670 kW) pero pronto fue rediseñado como P-40 Warhawk con un motor en línea de 1.100 HP (820 kW). En 1943, el más reciente P-40N tenía un motor Allison de 1.300 HP (970 kW). Hacia el final de la guerra, el interceptor alemán Focke-Wulf Ta 152 podía lograr 2.050 HP (1.530 kW) con un solo motor y un sobrealimentador de MW-50 (inyección de metanol-agua); el North American P-51H Mustang estadounidense equipado con el motor Packard V-1650-9 podía llegar a los 2.218 HP (1650 kW) en potencia de emergencia. El Spitfire Mk I de 1939 estaba motorizado con un Rolls-Royce Merlin II de 1.030 HP (770 kW); su sucesor de 1945, el Spitfire F.Mk 21, estaba equipado con el Rolls-Royce Griffon 61 de 2.035 HP (1.520 kW). De igual forma, durante el mismo período de tiempo los motores radiales preferidos para muchos cazas también pasaron de tener como mucho 1.100 HP (820 kW) a los 2.090 HP (1.560 kW) que, por ejemplo, tenía el motor Pratt & Whitney R-2800.

Los primeros diseños de cazas propulsados por turborreactores entraron en estado operacional en 1944, y superaron claramente a sus homólogos con motores de pistones. Los nuevos diseños, como el Messerschmitt Me 262 y el Gloster Meteor, demostraron la efectividad del nuevo sistema de propulsión. (Los interceptores propulsados por cohete —principalmente el Messerschmitt Me 163— aparecieron al mismo tiempo, pero resultaron ser menos efectivos.) Muchos de esos cazas podían pasar de superar los 660 km/h en vuelo horizontal, y eran lo suficientemente rápidos en picado para acercarse al vuelo transónico y comenzaron a encontrarse cerca de la velocidad del sonido; las turbulencias provocadas en ocasiones causaban la rotura de los reactores en vuelo debido a la pesada carga que sufrían los aviones cerca de la llamada "barrera del sonido". Se le añadieron frenos de picado a los cazas de reacción de finales de la Segunda Guerra Mundial para minimizar esos problemas y restablecer el control a los pilotos de combate.
La incorporación de armamento más potente se convirtió en una prioridad al principio de la guerra, una vez que se hizo evidente que los nuevos cazas monoplanos con recubrimientos reforzados no podían ser derribados fácilmente con ametralladoras con calibre de fusil. Las experiencias de los alemanes en la Guerra Civil Española les llevó a colocar cañones de 20 mm en sus cazas. Los británicos pronto siguieron la adaptación, añadiendo cañones en las alas de sus cazas Hurricane y Spitfire. Los estadounidenses, al carecer de un diseño de cañón propio, en su lugar optaron por colocar múltiples ametralladoras de 12,7 mm (.50) en sus cazas. La cantidad y potencia del armamento continuaron aumentando en el transcurso de la guerra, por ejemplo el Me 262 de reacción alemán tenía cuatro cañones de 30 mm en el morro. Los cañones disparaban proyectiles explosivos y podían abrir boquetes en el avión enemigo directamente en lugar de confiar en que la energía cinética de una bala sólida dañara un subsistema crítico (conductos de combustible, sistemas hidráulicos, cables de control, etc.) o elimine al piloto. Hubo un debate entre la alta cadencia de tiro de las ametralladoras contra los más lentos, pero más devastadores, cañones automáticos.

Con la creciente necesidad de apoyo aéreo cercano en el campo de batalla, los cazas eran equipados con soportes para bombas y cada vez más usados como cazabombarderos. Algunos diseños, como el Focke-Wulf Fw 190 alemán —a pesar de que el diseñador Kurt Tank lo había creado como un interceptor puro— o el Republic P-47 Thunderbolt demostraron ser extremadamente capaces en esa función. Mientras portaban armamento aire-superficie, como bombas y cohetes bajo sus alas, la maniobrabilidad de los cazas se reducía debido a la menor sustentación y mayor resistencia aerodinámica, pero una vez la carga bélica era liberada el avión ya volvía a ser de nuevo un caza totalmente capaz. Por su capacidad polivalente, los cazabombarderos ofrecían al personal de mando la libertad de asignar un grupo aéreo a la superioridad aérea o a misiones de ataque a tierra según fuera requerido.

Los rápidos avances en la tecnología del radar, que había sido inventado poco antes de que comenzara la Segunda Guerra Mundial, hicieron posible su instalación en algunos cazas, como el Messerschmitt Bf 110 alemán, los Bristol Beaufighter y de Havilland DH.98 Mosquito británicos y el Northrop P-61 Black Widow estadounidense, para permitirles localizar objetivos por la noche. Los británicos, que habían creado los primeros cazas nocturnos equipados con radar entre 1940 y 1941, perdieron su ventaja técnica con la "Luftwaffe". Los alemanes desarrollaron varios tipos de cazas nocturnos (Heinkel He 219 "Uhu", Focke-Wulf Ta 154 "Moskito"), ya que estaban bajo el constante bombardeo nocturno del Mando de Bombardeo de la RAF. Como los radares de la época eran bastante primitivos y difíciles de utilizar, en vez de en caza monoplazas, normalmente se empleaban en aviones más grandes de dos o tres plazas con tripulantes especializados en el manejo del radar.

Varios de los programas de cazas comenzados a principios de 1945 fueron continuados después del fin de la guerra y dieron lugar a avanzados cazas con motores de pistones que entraron en producción y servicio en 1946. Un ejemplo típico es el Lavochkin La-9 'Fritz' soviético, que fue una evolución del exitoso caza de guerra Lavochkin La-7 'Fin'. Trabajando con una serie de prototipos (los La-120, La-126 y La-130), la oficina de diseño Lavochkin buscaba reemplazar la estructura de madera del La-7 por una de metal, también incorporar alas de flujo laminar para mejorar el rendimiento en maniobrabilidad, e incrementar el armamento. El La-9 entró en servicio en agosto de 1946 y fue producido hasta 1948; también sirvió como base para el desarrollo de un caza de escolta de largo alcance, el La-11 'Fang', del que fueron fabricados cerca de 1.200 ejemplares entre 1947 y 1951.

En el transcurso de la Guerra de Corea, sin embargo, se hizo evidente que la era de los cazas con motores de pistones estaba llegando a su fin y que el futuro se encontraba en los cazas de reacción.

Este período también fue testigo de la experimentación con aviones de pistones asistidos por motores de reacción. Entre los derivados del La-9 hubo ejemplares equipados bajo las alas con dos motores pulsorreactores auxiliares (el La-9RD) y de manera similar con un par de motores estatorreactores (el La-138); sin embargo, ninguno de ellos entró en servicio. Uno que entró en servicio —con la Armada de los Estados Unidos en marzo de 1945– fue el Ryan FR-1 Fireball, cuya producción fue detenida al llegar el final de la guerra con la victoria sobre Japón. Para entonces solo habían sido entregados 66 aparatos, y el modelo fue retirado de servicio ya en 1947. La USAAF había encargado los primeros 13 prototipos de preproducción del caza de propulsión mixta turbohélice-turborreactor Consolidated Vultee XP-81 "Silver Bullet", pero este programa también fue cancelado con el fin de la guerra, cuando se había completado un 80% del trabajo de ingeniería.

El primer avión cohete fue el alemán Lippisch Ente, que realizó con éxito un primer vuelo en marzo de 1928. El único avión cohete puro que llegó a ser producido en masa fue el Messerschmitt Me 163 en 1944, uno de los muchos proyectos alemanes destinados a desarrollar aviones propulsados por cohete durante la Segunda Guerra Mundial. Algunas variantes del Me 262 (los C-1a y C-2b) también fueron equipados con propulsores cohete, pero no fueron fabricados en masa con esas modificaciones.

La Unión Soviética experimentó con un interceptor propulsado por cohete en los años inmediatamente posteriores a la Segunda Guerra Mundial, el Mikoyan-Gurevich I-270, pero solo se llegaron a producir dos unidades.

En los años 1950, los británicos crearon diseños de interceptores de propulsión mixta empleando tanto motores de cohete como de reacción para cubrir la interrupción de rendimiento que existía en los diseños turborreactores de la época. El cohete era el motor principal para lograr la velocidad y altitud requerida para la intercepción a alta velocidad de bombarderos de gran altitud, y el turborreactor proporcionaba una mejor economía de combustible en otras etapas del vuelo, principalmente para asegurar que la aeronave pudiera realizar un buen aterrizaje convencional en lugar de un arriesgado e impredecible regreso planeando como hacía el Me 163. El Saunders-Roe SR.53 fue un diseño exitoso y estaba planeado para entrar en producción cuando la economía forzó el recorte de la mayoría de los programas aeronáuticos británicos a finales de los años 1950. Además, las rápidos avances en la tecnología de motores de reacción habían dejado obsoletos los diseños de aviones de propulsión mixta como los SR.53 y SR.177 de Saunders-Roe. El Republic XF-91 Thunderceptor estadounidense —que fue el primer caza de Estados Unidos en superar la velocidad de Mach 1 en vuelo horizontal— tuvo un destino similar por la misma razón. A partir de entonces ya no se volvieron a desarrollar diseños de cazas de propulsión híbrida motor de reacción-cohete. La única implementación operacional de propulsión mixta fue el despegue asistido por cohetes (RATO), un sistema utilizado en aviones pesados pero raramente usado en los cazas.

El final de la Segunda Guerra Mundial trajo una revolución en los aviones y fue el turborreactor el que eliminó la hélice de los aviones de caza, abriendo una nueva época de estudios en cuanto a superficies de control, sistemas de puntería y armas de ataque.

En la comunidad de la aviación se ha hecho común clasificar los aviones de combate por "generaciones" con fines históricos. No hay definiciones oficiales de estas generaciones; más bien, representan la noción en la que hay etapas en el desarrollo de enfoques de diseño, capacidades de rendimiento y evolución tecnológica de los cazas.

Los períodos de tiempo asociados a cada generación son inexactos y solo son indicativos del período durante el cual el empleo de su tecnología y sus filosofías de diseño han disfrutado de una influencia preponderante en el diseño y desarrollo de cazas. Estos períodos también abarcan la etapa de máxima actividad en servicio de cada generación.

La Primera generación de cazas de reacción comprende los diseños iniciales de aparatos subsónicos introducidos a finales de la Segunda Guerra Mundial y a principios del período de posguerra. Difieren ligeramente en apariencia de sus homólogos con motor de explosión, y muchos emplearon alas rectas. Los cañones continuaron siendo su armamento principal. El ímpetu por el desarrollo de aviones propulsados por turborreactores buscaba obtener una ventaja decisiva en cuanto a velocidad máxima. Las velocidades máximas de los cazas aumentaron de manera constante a lo largo de la Segunda Guerra Mundial a medida que se desarrollaban motores de pistones más potentes, y habían comenzado a aproximarse al régimen de vuelo transónico donde la eficiencia de las hélices movidas por motores de pistones cae considerablemente.

Los primeros reactores fueron desarrollados durante la Segunda Guerra Mundial y entraron en combate en los dos últimos años de la guerra. El fabricante alemán Messerschmitt desarrolló el primer caza de reacción operacional, el Me 262. Este era considerablemente más veloz que los aviones contemporáneos propulsados por motores de pistones, y en manos de un piloto competente, era bastante difícil de derrotar por los pilotos aliados. El diseño nunca fue desplegado en un número suficiente como para parar la campaña aérea aliada, y la combinación de diversos factores como la escasez de combustible, las pérdidas de pilotos, y las dificultades técnicas con los motores mantuvieron un bajo número de salidas. Sin embargo, el Me 262 acusó la obsolescencia de los aviones de pistones. Estimulado por los informes sobre los nuevos cazas alemanes, el Gloster Meteor británico entró en producción poco después y los dos entraron en servicio prácticamente al mismo tiempo en 1944. Los Meteor normalmente fueron usados para interceptar las bombas voladoras V-1, ya que eran más rápidos que los cazas con motor de explosión disponibles. Hacia el final de la guerra ya casi se había parado de trabajar sobre cazas de pistones. Durante un breve tiempo hubo unos pocos diseños que combinaron motores de pistones y reactores, como el Ryan FR-1 Fireball, pero a finales de los años 1940 virtualmente todos los nuevos aviones de combate eran de reacción.

A pesar de sus ventajas, los primeros cazas de reacción estaban lejos de ser perfectos, particularmente en los primeros años de la generación. Sus periodos de vida podía ser medido fundamentalmente en horas; los motores en sí mismos eran frágiles y voluminosos, y la potencia solo podía ser ajustada despacio. Debido a esto, se mantuvieron muchos escuadrones de cazas propulsados por motores de pistones hasta principios y mediados de los años 1950, incluso en las fuerzas aéreas de las principales potencias (aunque los modelos mantenidos eran los mejores diseños de la Segunda Guerra Mundial). En este periodo fueron introducidas varias innovaciones, entre las que se incluyen los asientos eyectables y los estabilizadores horizontales de cola completamente móviles.

Los estadounidenses fueron unos de los primeros en comenzar a usar los cazas de reacción en la posguerra. El Lockheed P-80 Shooting Star (posteriormente llamado F-80) de alas rectas era menos elegante que el Me 262 de alas en flecha, pero tenía una velocidad de crucero (660 km/h) tan alta como la máxima en combate de muchos cazas de pistones. Los británicos diseñaron varios cazas nuevos, incluyendo el representativo de Havilland Vampire que fue vendido a las fuerzas aéreas de muchas naciones.

Irónicamente, los británicos transfirieron la tecnología del motor de reacción Rolls-Royce Nene a los soviéticos, quienes pronto la emplearon en sus avanzados cazas Mikoyan-Gurevich MiG-15, que fueron los primeros en introducir las alas en flecha en combate, una innovación que ya había sido propuesta antes por los investigadores alemanes y que permitía volar mucho más cerca de la velocidad del sonido que los diseños con alas rectas como el F-80. La velocidad máxima del MiG-15 de 1.075 km/h dejó impresionados a los pilotos de caza estadounidenses que se los encontraron en la Guerra de Corea, junto con su armamento de dos cañones de 23 mm y otro de 37 mm frente a las ametralladoras de los cazas F-80. Sin embargo, en el primer combate aéreo cerrado entre cazas de reacción de la historia, que ocurrió el 8 de noviembre de 1950 durante la Guerra de Corea, un F-80 (como había sido renombrado el P-80) interceptó dos MiG-15 norcoreanos cerca de Yalu River y los derribó.

Los estadounidenses respondieron apresurándose a desplegar sus escuadrones de cazas de ala en flecha F-86 Sabre para combatir contra los MiG, que tenían un rendimiento transónico similar. Los dos aviones tenían diferentes puntos fuertes, pero eran tan similares que solo la superioridad tecnológica en el uso del radar para los sistemas de puntería y las habilidades de los veteranos pilotos de la Fuerza Aérea Estadounidense les permitieron prevalecer. Las marinas con portaaviones también realizaron la transición a los cazas de reacción durante este periodo, a pesar de que estos nuevos aviones necesitaban lanzamiento con catapulta para despegar desde los portaaviones. En primer caza de reacción de la Marina Real Británica fue el de Havilland Sea Vampire. La Armada de los Estados Unidos adoptó el Grumman F9F Panther como su principal caza de reacción en el periodo de la Guerra de Corea, este fue uno de los primeros cazas de reacción en emplear un postquemador. El radar era usado en cazas nocturnos especializados como el Douglas F3D Skyknight, que también derribaron cazas MiG sobre Corea, y posteriormente fue equipado en el McDonnell F2H Banshee y los Chance Vought F7U Cutlass y McDonnell F3H Demon de ala en flecha como cazas nocturnos / todo tiempo. Las primeras versiones de los misiles aire-aire guiados por infrarrojos como el AIM-9 Sidewinder y los misiles guiados por radar como el AIM-7 Sparrow que serían desarrollados posteriormente fueron introducidos por primera vez en los cazas navales Demon y Cutlass.

El desarrollo de la Segunda generación de cazas se llevó a cabo por los adelantos tecnológicos, las lecciones aprendidas en las batallas aéreas de la Guerra de Corea, y un enfoque en conducir las operaciones militares hacia un entorno de guerra nuclear. Los avances tecnológicos en aerodinámica, propulsores y materiales de construcción aeroespacial (principalmente las aleaciones de aluminio) permitieron a los diseñadores experimentar con innovaciones aeronáuticas, como las alas en forma de flecha, las alas en delta, y fuselajes de acuerdo con la regla del área. El uso generalizado de motores turborreactores con postcombustión hizo posible que esos primeros aviones producidos de nueva generación rompieran la barrera del sonido, y que la capacidad de mantener velocidades supersónicas en vuelo horizontal pasara a ser una habilidad común entre los cazas de esta generación.

Los diseños de cazas de esta época también aprovecharon los avances en tecnología electrónica adoptando radares eficaces de tamaño suficientemente reducido como para ser llevados a bordo de pequeños aviones. Los radares de a bordo permitieron la detección de aeronaves enemigas más allá del alcance visual, mejorando de este modo el manejo de objetivos de los radares terrestres de alerta y rastreo con mayor alcance. De manera similar, con los avances en el desarrollo de misiles llegaron los misiles aire-aire para empezar a complementar al cañón como principal arma ofensiva por primera vez en la historia de los cazas. Durante este período, los misiles guiados por infrarrojos de rastreo pasivo se hicieron habituales, pero los primeros sensores infrarrojos tenían poca sensibilidad y un campo de visión muy reducido (normalmente inferior a 30°), lo cual limitaba su uso efectivo solo a una posición cercana de persecución (detrás del avión enemigo). Asimismo, se introdujeron misiles guiados por radar, pero los primeros ejemplares demostraron ser poco fiables. Estos misiles de búsqueda semiactiva (SARH) podían rastrear e interceptar un avión enemigo "pintado" por el radar a bordo del avión que lanzaba el misil. Los misiles aire-aire guiados por radar de medio y largo alcance prometían abrir una nueva dimensión de combates más allá del alcance visual (BVR), por lo que se puso un gran esfuerzo en el desarrollo de esta tecnología.
La perspectiva de una posible tercera guerra mundial caracterizada por grandes ejércitos mecanizados y ataques con armas nucleares dio lugar a un grado de especialización en los aviones de combate hacia dos enfoques de diseño: interceptores (como el English Electric Lightning y el Mikoyan-Gurevich MiG-21F) y cazabombarderos (como el Republic F-105 Thunderchief y el Sujoi Su-7). Al combate aéreo cerrado o "dogfight", "pero se", se le restó importancia en ambos casos. El interceptor de esta época tuvo su origen en la idea de que los misiles reemplazarían por completo a los cañones y el combate tendría lugar a distancias mayores del alcance visual. A consecuencia de esto, los interceptores fueron diseñados para portar una gran carga de misiles y un potente radar, sacrificando agilidad en favor de unas buenas prestaciones de velocidad, techo de vuelo y régimen de ascenso. Con una función principal de defensa aérea, se dio énfasis a la habilidad de interceptar bombarderos estratégicos que vuelan a grandes altitudes. Los interceptores especializados en defensa puntual solían tener un alance limitado y pocas, o nulas, capacidades de ataque a tierra. Los cazabombarderos podían alternar entre las misiones de superioridad aérea y ataque a tierra, y solían ser diseñados para hacer ataques a alta velocidad y baja altitud, y lanzar su carga bélica. Para mejorar la eficacia de las bombas de caída libre tradicionales se introdujeron los misiles aire-tierra guiados por televisión e infrarrojos, y algunos cazabombarderos también estaban preparados para lanzar bombas nucleares.

La tercera generación presenció como continuaron madurando las innovaciones de la segunda generación, pero más marcadas por el énfasis renovado en la maniobrabilidad y en las capacidades de ataque a tierra tradicionales. Durante los años 1960, la creciente experiencia en combate con misiles aire-aire demostró que el combate aéreo solía terminar en combate aéreo cerrado o "dogfight". Se comenzó a introducir aviónica analógica, reemplazando los antiguos instrumentos de vuelo. Entre las mejoras para mejorar el rendimiento aerodinámico de los cazas de tercera generación se incluían superficies de control como los planos delanteros o "canards", aletas de borde de ataque ("slats") móviles, y aletas ("flaps") sopladas. Durante estos años se probaron multitud de tecnologías para realizar despegue y aterrizaje verticales/cortos (V/STOL), pero el método más exitoso fue el empuje vectorial aplicado al Harrier.

El aumento en la capacidad de combate aéreo se enfocó en la introducción de mejores misiles aire-aire, sistemas de radar y otra aviónica. Mientras los cañones continuaron siendo equipamiento estándar —excepto en los primeros modelos del F-4 Phantom II—, los misiles aire-aire se convirtieron en las principales armas de los cazas de superioridad aérea, estos aviones empleaban radares más sofisticados y misiles aire-aire guiados por radar de alcance medio para lograr una mayor distancia de acción, sin embargo, las probabilidades de derribo de los misiles guiados por radar resultaron ser inesperadamente bajas debido a su escasa fiabilidad y las mejoras en contramedidas electrónicas (ECM) para burlar los localizadores radar enemigos. Los misiles aire-aire guiados por infrarrojos vieron ampliado su campo de visión hasta los 45°, mejorando su facilidad de uso táctico. Sin embargo, los malos resultados en el combate aéreo cercano experimentados por los cazas estadounidenses en los cielos de Vietnam llevó a la Armada de los Estados Unidos a establecer su famosa escuela de combate aéreo TOPGUN para entrenar a los pilotos de cazas en maniobras de combate aéreo avanzadas, y en tácticas y técnicas de entrenamiento en combate aéreo disimilar (DACT).

En esta era también se registró una expansión en las capacidades de ataque a tierra, principalmente en misiles, y se presenció la introducción de los primeros equipos de aviónica realmente efectivos para el ataque a tierra de precisión, incluyendo el sistemas de seguimiento del terreno. Los misiles aire-superficie equipados con buscadores de contraste electro-óptico (E-O) —como el modelo inicial del ampliamente utilizado AGM-65 Maverick— pasaron a ser armas estándar, y aparecieron las bombas guiadas por láser como un esfuerzo para mejorar las capacidades de ataque de precisión. El guiado de ese armamento guiado o bombas inteligentes era proporcionado por pods de búsqueda de objetivos montados externamente, que fueron introducidos a mediados de los años 1960.

También se llegó al desarrollo de nuevos cañones automáticos, principalmente los «cañones de cadena» o "chain gun", que usan un motor eléctrico para mover el mecanismo disparo y recarga. Esto permitió la introducción de armas individuales con varios cañones (como el M61 Vulcan) con mayor cadencia de fuego y precisión. La fiabilidad y eficiencia de los motores se incrementó y se redujo el humo emitido por los reactores para hacerlos menos visibles a largas distancias.
Los aviones de ataque puros (como el Grumman A-6 Intruder, el SEPECAT Jaguar y el LTV A-7 Corsair II) ofrecían un mayor alcance, sistemas para ataque nocturno más sofisticados o un menor coste que los cazas supersónicos. Con ala de geometría variable, el supersónico General Dynamics F-111 introdujo el motor Pratt & Whitney TF30, el primer turbofán equipado con postquemador. El ambicioso proyecto buscó crear un versátil caza común para muchas funciones y servicios. Podía servir como un bombardero todo tiempo, pero carecía del rendimiento necesario para derrotar a otros cazas. El McDonnell F-4 Phantom II fue diseñado en base al radar y los misiles como interceptor todo tiempo, pero surgió como un versátil cazabombardero suficientemente ágil como para prevalecer en el combate aéreo. A pesar de las numerosas deficiencias que no serían abordadas hasta la aparición de nuevos cazas, al Phantom se le atribuyen 280 derribos, más que ningún otro caza estadounidense sobre Vietnam. Con un alcance y capacidad de carga similares a los bombarderos de la Segunda Guerra Mundial como el B-24 Liberator, el Phantom se convertiría en un avión polivalente de gran éxito.

Los cazas de cuarta generación continuaron la tendencia hacia configuraciones polivalentes, y fueron equipados con sistemas de armas y aviónica cada vez más sofisticados. El diseño de los cazas de esta generación fue significativamente influenciado por la teoría Energía-Maniobrabilidad (E-M) desarrollada por el coronel John Boyd y el matemático Thomas Christie, basada en la experiencia de combate de Boyd en la Guerra de Corea y como instructor en tácticas de combate durante los años 1960. La teoría E-M hizo hincapié en el valor de mantener la energía específica de la aeronave como una ventaja en el combate entre cazas. Boyd percibió la maniobrabilidad como el medio principal de conseguir adelantarse al ciclo de la toma de decisiones de un adversario, un proceso al que Boyd llamó el "bucle OODA" ("Observación-Orientación-Decisión-Acción"). Este enfoque destacó los diseños de aviones que fueran capaces de realizar "rápidas transiciones" – cambios rápidos en velocidad, altitud, y dirección – en lugar de basarse solamente en la alta velocidad como virtud principal.

Las características E-M fueron aplicadas por primera vez al McDonnell Douglas F-15 Eagle, pero Boyd y sus partidarios creían que esos parámetros de rendimiento requerían un avión pequeño y ligero con alas más grandes y con mayor sustentación. El pequeño tamaño reduciría el arrastre e incrementaría la relación empuje a peso, mientras que las grandes alas reducirían la carga alar; aunque la carga alar reducida tiende a disminuir la velocidad máxima y reducir el alcance, incremente la capacidad de carga útil y la reducción de alcance puede ser compensada por el incremento de capacidad para combustible en las alas de mayor tamaño. Los esfuerzos de la "Fighter Mafia" de Boyd darían lugar al General Dynamics F-16 Fighting Falcon.

La maniobrabilidad del F-16 fue mejorada al ser diseñado para ser aerodinámicamente un poco inestable. Esta técnica, llamada "estabilidad estática relajada" (RSS), fue posible gracias a la introducción del sistema de control de vuelo (FLCS) ""fly-by-wire"" (FBW), que a su vez vino dada por los avances en ordenadores y técnicas de integración de sistemas. La aviónica analógica, necesaria para las operaciones FBW, se convirtió en un requisito fundamental y comenzó a ser reemplazada por sistemas de control de vuelo digitales en la segunda mitad de los años 1980. De igual forma, se introdujo con el turbofán Pratt & Whitney F100 el control digital de autoridad total del motor (FADEC) para gestionar electrónicamente el rendimiento del motor. La dependencia exclusiva de la electrónica y los cables eléctricos para transmitir órdenes de vuelo del F-16, en lugar de los controles conectados mecánicamente y cables habituales, le valió el sobrenombre de «el reactor eléctrico». Los FLCS electrónicos y el FADEC rápidamente se convirtieron en componentes esenciales en todos los posteriores diseños de cazas.
Otros tecnologías innovadoras introducidas en los cazas de cuarta generación incluyen el radar de control de tiro de impulsos Doppler (con capacidad "look-down/shoot-down"), la pantalla frontal de presentación de datos (HUD), controles HOTAS (en el mando de gases y en la palanca de control), y pantallas multifunción (MFD), todos las cuales se han convertido en equipamiento esencial. Los materiales compuestos en forma de elementos estructurales de aluminio con forma de panal de abeja y recubrimientos laminados de polímero reforzado con fibra de carbono comenzaron a ser incorporados en las superficies de control de vuelo y en los recubrimientos de la estructura para reducir el peso de la aeronave. Se generalizó el uso de sensores de búsqueda y seguimiento por infrarrojos (IRST) para el lanzamiento de armas aire-tierra, y también aparecieron para el combate aire-aire. Los misiles aire-aire guiados por infrarrojos pasaron a ser armas estándar de superioridad aérea, estas armas permitieron alcanzar un avión enemigo desde cualquier ángulo (aunque el campo de visión seguía siendo relativamente limitado). El primer misil aire-aire de largo alcance guiado por radar activo entró en servicio con el AIM-54 Phoenix, modelo que solo fue equipado por el Grumman F-14 Tomcat, uno de los pocos diseños de caza con ala de geometría variable que entraron en producción. Incluso con los tremendos avances en los misiles aire-aire de esta época, los cañones internos continuaron siendo armamento estándar.

Otra revolución llegó en forma de una mayor pretensión en la facilidad de mantenimiento, que llevó a la estandarización de partes, reducción de paneles de acceso y puntos de lubricación, y en general en una reducción de piezas en el equipamiento más complicado como son los motores. Algunos de los primeros cazas de reacción requerían 50 hombre-horas de trabajo del personal de tierra por cada hora que el avión permanecía en el aire; modelos posteriores redujeron esto sustancialmente para permitir tiempos de respuesta más rápidos y más salidas en un día. En cambio, algunos aviones militares modernos solo requieren 10 hombre-horas de trabajo por hora de vuelo, y los hay que son incluso más eficientes.

Las innovaciones aerodinámicas incluyeron alas de curvatura variable y el aprovechamiento del efecto de sustentación del vórtice para conseguir mayores ángulos de ataque mediante la adición de dispositivos de extensión del borde de ataque (LEX) (a veces llamados "strakes").
A diferencia de los interceptores de épocas anteriores, la mayoría de los cazas de superioridad aérea de cuarta generación se diseñaron para ser ágiles en el combate aéreo cerrado o "dogfight", aunque hubo excepciones como los interceptores Mikoyan MiG-31 y Panavia Tornado ADV. Además, el continuo aumento del coste de los cazas manifestó la importancia de los cazas polivalentes. La necesidad de ambos tipos de cazas llevó al concepto "high/low mix" que supuso un núcleo de alta capacidad y alto coste de cazas de superioridad aérea puros —como el F-15 y el Su-27— complementados por un mayor contingente de cazas polivalentes de menor coste —como el F-16 y el MiG-29.

La mayoría de los cazabombarderos de cuarta generación, como el F/A-18 Hornet y el Dassault Mirage 2000, ya eran verdaderos aviones polivalentes, diseñados como tales desde el principio. Esto fue posible gracias a la aviónica multimodo que podía cambiar perfectamente entre modos ‘aire’ y ‘tierra’. De este modo las anteriores formas de añadir capacidades de ataque o diseñar modelos separados especializados en distintas misiones por lo general quedó pasado de moda, siendo el Panavia Tornado una excepción ene este sentido. Las tareas de ataque a tierra puro generalmente eran asignadas o a aviones de interdicción aérea como el Sujoi Su-24 y el F-15E Strike Eagle o bien a especialistas en el apoyo aéreo cercano como el Fairchild-Republic A-10 Thunderbolt II y el Sujoi Su-25.

Tal vez la tecnología más novedosa que se introdujeron en los aviones de combate sea la «tecnología furtiva», que implica el uso de técnicas de diseño y materiales especiales de "baja observabilidad" (L-O por sus siglas en inglés) para reducir la susceptibilidad de una aeronave a ser detectada por los sistemas de sensores enemigos, particularmente por los radares. El primer avión furtivo en ser introducido fue el avión de ataque Lockheed F-117 Nighthawk (en 1983) y después el bombardero estratégico Northrop Grumman B-2 Spirit (que voló por primera vez en 1989). Aunque no aparecieron cazas furtivos en sí en la cuarta generación, algunos revestimientos absorbentes de radar y tratamientos L-O desarrollados para esos programas fueron aplicados posteriormente a los cazas de cuarta generación.

El final de la Guerra Fría en 1989 llevó a muchos gobiernos a disminuir significativamente los gastos militares. Los inventarios de las fuerzas aéreas fueron recortados, y los programas de investigación y desarrollo para producir los que se esperaba que serían cazas de «quinta generación» sufrieron las consecuencias; muchos programas fueron cancelados durante la primera mitad de los años 1990, y los que sobrevivieron fueron aplazados. Si bien la desaceleración del ritmo de desarrollo reduce los gastos anuales de inversión, tiene como consecuencia a largo plazo un aumento en los costes del programa general y en los costos unitarios. Este momento, sin embargo, también permite a los diseñadores hacer uso de los enormes logros alcanzados en los campos de las computadoras, aviónica y otra electrónica de vuelo, que habían sido posibles en gran parte debido a los avances realizados en las tecnologías de semiconductores y circuitos integrados en los años 1980 y 1990. Esta oportunidad permitió a los fabricantes desarrollar los diseños de la cuarta generación – o rediseños – con capacidades significativamente mejoradas. Estos diseños mejorados pasaron a ser conocidos como cazas de "generación 4.5" o de "generación 4++", reconociendo su carácter intermedio entre las generaciones 4ª y 5ª y su contribución al desarrollo de distintas tecnologías propias de la quinta generación.

Las principales características de esta subgeneración son: la aplicación de materiales aeroespaciales avanzados y de moderna aviónica digital, reducción parcial de la firma (principalmente en radiofrecuencia), y la alta integración de sistemas y armas. Estos cazas han sido diseñados para operar en un entorno de batalla centrado en redes de comunicaciones y son principalmente aviones polivalentes. Las tecnologías de armas clave introducidas en estos cazas incluyen los misiles aire-aire con autonomía «más allá del alcance visual» (BVR); armas guiadas por sistema de posicionamiento global (GPS), radares de antenas en fase de estado sólido; miras montadas en casco; y enlaces de datos resistentes a interferencias y con seguridad mejorada. Los cazas de la generación 4,5 también adoptaron el empuje vectorial para mejorar aún más las capacidades de maniobrabilidad, y los motores de alta potencia permitieron que algunos diseños puedan lograr un grado de capacidad supercrucero. Las características furtivas está enfocadas principalmente en técnicas de reducción de la firma frontal de la sección radar equivalente (RCS) que incluyen el uso de materiales absorbentes de radar, revestimientos y formas de baja observabilidad.

Los diseños de cuarta y media generación pueden o estar basados en células de cazas de la 4ª generación o bien en nuevas estructuras que siguen la misma teoría de diseño; sin embargo, las modificaciones introdujeron el uso estructural de materiales compuestos para reducir el peso, mayor autonomía para aumentar el alcance, y el tratamientos de reducción de la firma para lograr una menor RCS en comparación con sus predecesores. Ejemplos principales de tales aviones, que están basados en nuevos diseños estructurales haciendo uso extensivo de compuestos de fibra de carbono, son el Eurofighter Typhoon, el Dassault Rafale y el Saab 39 Gripen NG. Aparte de esos cazas de reacción, la mayoría de los aviones de la 4,5° generación son variaciones de estructuras existentes. Estos cazas suelen ser versiones más pesadas y con mayor alcance; y como ejemplos se pueden citar el Boeing F/A-18E/F Super Hornet que es una evolución del diseño del F/A-18 Hornet, el F-15E Strike Eagle que es una variante de ataque a tierra del McDonnell Douglas F-15 Eagle, el Sujói Su-30MKI que es un desarrollo del Su-30 y el Mikoyan MiG-35, una versión actualizada del MiG-29. El Su-30MKI y el MiG-35 usan empuje vectorial de dos y tres dimensiones respectivamente para aumentar la maniobrabilidad. La mayoría de los aviones de la 4,5ª generación están siendo equipados en forma de actualización con radares activos de barrido electrónico (radares AESA) y últimos avances en sistemas de aviónica.

Los primeros cazas de 4,5 generación entraron en servicio a principios de los años 1990, y la mayoría de ellos siguen en fabricación y desarrollo. Es muy posible que continúen en producción junto a los cazas de quinta generación debido al elevado coste de desarrollo del avanzado nivel de tecnología furtiva necesario para lograr diseños de aviones de muy baja observabilidad (VLO), que es una de las características que definen a los cazas de quinta generación. De todos estos diseños, solo entraron en combate el Super Hornet, el Strike Eagle y en menor medida el Rafale.

La Cámara de Representantes de los Estados Unidos define como avión de caza de 4,5 generación aquel que «(1) dispone de capacidades avanzadas, incluyendo— (A) radar AESA; (B) enlace de datos de alta capacidad; y (C) aviónica mejorada; y (2) tienen la habilidad para desplegar armamento avanzado actual y el previsto para un futuro cercano razonable.»

Si bien la aparición del concepto de generaciones de cazas de combate asumía la existencia de cinco generaciones, son muchos los autores como Näsström, Keijsper, Pearson o Taylor que opinan que hasta el siglo XXI solo han existido cuatro generaciones de cazas; así, según ellos, modelos como el JAS 39 Gripen y el F22 Raptor pertenecen a la misma generación, pese a la distancia tecnológica que los separa.

Por su parte, otros expertos, opinan que a principios del siglo XXI ya habían hecho su aparición cazas de quinta generación, cuyo único integrante sería el F-22 Raptor, a menos que se consideren los prototipos del F-35 Lightning II. Esa es también la postura oficial de la Fuerza Aérea de los Estados Unidos. Finalmente, algunos más, como Cate, adelantan la aparición de esta nueva generación a los modelos europeos, como el Eurofighter Typhoon o el Dassault Rafale, la cual parece ser la postura oficial de las fuerzas aéreas usuarias de los mismos

La definición y catalogación aún se complica más cuando varias empresas han creado nuevas versiones de sus modelos con características que se consideran dentro de una generación posterior. Pese a que aviones como el F-14 Tomcat no fueron modernizados, sino que fueron retirados sin seguir diseñando nuevas versiones, los fabricantes de otros modelos como el F-16, el F/A-18 y el F-15 (este ya con armamento interno) han mantenido las líneas de producción abiertas y han sacado nuevas versiones/bloques/tranchas con la información digitalizada y centralizada, además de con capacidad de comunicarse con otros aviones, barcos, centros de mando y control, radares de barrido electrónico, etc. Por ese motivo, se habla de una generación intermedia, 4,5 o 4+ para referirse a esos modelos. El gobierno de EE. UU., por ejemplo, define oficialmente como "generación 4,5ª" a aquellos que tienen radar AESA, enlace de datos de alta capacidad, aviónica avanzada y capacidad para desplegar armas futuras.

La quinta generación comenzó con la introducción del caza estadounidense Lockheed Martin F-22 Raptor a finales de 2005. Estando actualmente en la vanguardia de diseño de aviones militares, los cazas de quinta generación se caracterizan por estar diseñados desde el principio para operar en un entorno de batalla centrado en redes, y porque ofrecen firmas multiespectro extremadamente bajas en todos los aspectos empleando avanzadas técnicas de forma y materiales. Disponen de radares AESA multifunción con capacidad de transmisión de datos con un gran ancho de banda y baja probabilidad de intercepción. Los sensores de búsqueda y seguimiento por infrarrojos (IRST) incorporados en la 4,5ª generación para combate aire-aire así como para el lanzamiento de armas aire-tierra ahora están fusionados con otros sensores dando lugar a los IRST de conciencia situacional o SAIRST (siglas en inglés), que rastrean constantemente todos los objetivos de interés en torno al avión de modo que el piloto los tiene siempre visibles. Esos sensores, junto con la avanzada aviónica, la cabina de "cristal", las miras montadas en el casco (no disponibles actualmente en el F-22), y enlaces de datos con baja probabilidad de ser interceptados, resistentes a interferidores y más seguros, están altamente integrados para suministrar una minería de datos multisensor y multiplataforma que mejora ampliamente la conciencia situacional al mismo tiempo que facilita la carga de trabajo del piloto. Los equipos de aviónica se basan en el uso extensivo de tecnología de circuitos integrados de muy alta velocidad (VHSIC), módulos comunes, y buses de datos de alta velocidad. En general, la integración de todos esos elementos pretende proporcionar a los cazas de quinta generación la capacidad «primero en ver, primero en disparar y primero en derribar».

El radar AESA ofrece capacidades únicas para los cazas (está siendo rápidamente adoptado por los diseños de 4,5ª generación, así como también está siendo incorporado como actualización en algunos aviones de cuarta generación). Además de su alta resistencia a contramedidas electrónicas y baja probabilidad de interceptación, permite que el caza pueda funcionar como un «mini-AWACS», proporcionando medidas de apoyo a la guerra electrónica (ESM) y funciones interferidoras de guerra electrónica (EW).

Otras tecnologías comunes para esta última generación de cazas incluyen sistemas de guerra electrónica integrados (INEWS), aviónica de identificación, navegación y comunicaciones integrada, sistemas de monitorización del estado del avión centralizados para facilitar el mantenimiento, transmisiones de datos por fibra óptica y tecnología furtiva.

La maniobrabilidad continúa siendo importante y es mejorada mediante toberas que permiten orientación de empuje, esto también ayuda a reducir las distancias de despegue y aterrizaje. La capacidad supercrucero, que puede ser ofrecida o no, permite volar a velocidades supersónicas sin necesidad de usar postquemador, dispositivo que incrementa considerablemente la señal infrarroja del avión durante su uso.

Un atributo clave de los cazas de quinta generación es su capacidad furtiva de muy baja observabilidad. Se ha tomado gran cuidado en el diseño de su estructura interna y disposición para minimizar su sección radar equivalente (RCS) en un amplio ancho de banda de frecuencias radar de detección y rastreo. Para mantener su firma de muy baja observabilidad durante las operaciones de combate, las armas principales son transportadas en bodegas internas que solo se abren el tiempo necesario para permitir el lanzamiento del armamento. Por otra parte, la tecnología furtiva ha avanzado hasta el punto de que puede ser utilizada sin menoscabar el rendimiento aerodinámico, en contraste con tecnologías anteriores (F-117 por ejemplo). El costo de desarrollo de aviones tan sofisticados es tan elevado como sus capacidades. La Fuerza Aérea de los Estados Unidos tenía planeado en un principio adquirir 650 F-22, pero solo se construirán 187 ejemplares, como resultado de su elevado coste de despegue unitario que ronda los 150 millones de dólares. Para extender los costes de desarrollo – y base de producción – de manera más amplia, el programa Joint Strike Fighter (JSF) enrola a otros ocho países como socios de riesgos y costes compartidos. En conjunto, los nueve países socios anticiparon que serán adquiridos en torno a 3.000 cazas Lockheed Martin F-35 Lightning II con un coste de despegue medio de 80–85 millones de US$. El F-35, sin embargo, fue diseñado para ser una familia de tres aviones, un caza de despegue y aterrizaje convencionales (CTOL), un caza de despegue corto y aterrizaje vertical (STOVL), y un caza de despegue asistido por catapulta pero recobro mediante detención (CATOBAR), cada uno de cuales tiene un precio unitario distinto y sus especificaciones varían ligeramente en cuanto a capacidad de combustible (y alcance por consiguiente), tamaño y carga.

Otros países iniciaron proyectos de desarrollo de cazas de quinta generación, Rusia fue uno los primeros países en seguir la investigación y desarrollo de la nueva generación con el Sukhoi Su-57 y el Mikoyan LMFS. En octubre de 2007, Rusia y la India firmaron un acuerdo para la participación conjunta en un programa de Avión de Caza de Quinta Generación (FGFA), que dará a la India la responsabilidad de desarrollar un modelo biplaza del PAK FA. China por su parte cuenta con el caza furtivo Chengdu J-20. El J-20 realizó su primer vuelo en enero de 2011 y se estima que puede ser introducido entre 2017 y 2019. India también está desarrollando su propio avión de quinta generación llamado Medium Combat Aircraft. Japón está evaluando la viabilidad técnica para producir sus cazas de quinta generación con el prototipo Mitsubishi ATD-X.

La mayoría de los aviones caza utilizados en la actualidad tienden a cumplir distintos roles o son multipropósitos. Los aviones pueden ser cargados con diferente variedad de arsenal y responder a distintas misiones, sin embargo, a la hora de ser fabricados, existen ciertos perfiles que responden a tareas y misiones que son específicas, por ejemplo, en cuanto a su rol en el alcance visual o la cantidad de armamento que deben ser capaces de portar.

Avión caza diseñado para la defensa del espacio aéreo y el combate aire-aire Dogfight. Son naves ligeras que dan prioridad la velocidad, el despegue y el ascenso, porque están pensados para el enfrentamiento aéreo, por tanto, no cuentan con mucho espacio para abastecimiento y armamento, limitándose este último a misiles aire-aire. Por lo general no son aptos para recorrer un radio de acción extenso y su mayor diferencia con todos los cazas, es que no están acondicionados para hacer ataques a superficie, sin embargo están adaptados especialmente para interceptar bombarderos y otras naves enemigas en combates dentro, como fuera del alcance visual. Claros ejemplos son el MiG-21, Northrop F-5 y el F-20 Tigershark, estos últimos muy maniobrables. Versiones aún más veloces, aunque más pesadas y menos maniobrables han sido el MiG-25 y el potente MiG-31. En la actualidad entre los aviones que podrían ser clasificados con este carácter figuran el Dassault Rafale, el Chengdu/PAC JF-17 Thunder y el Saab 39 Gripen, pero todos ellos con un perfil bastante más polivalente.

Propios de la época de la Segunda Guerra Mundial y la Guerra Fría . Eran aviones más pesadas que los interceptores, porque cargan con una mayor capacidad de armamento y abastecimiento, para un rango de acción más autónomo y extenso. Así mismo pueden no ser tan veloces, pero deben asegurar la maniobrabilidad para el enfrentamiento, pues su principal característica es estar preparados para el combate WVR (Within Visual Range), "dentro del alcance visual". Un claro ejemplo fue el P-51 Mustang. En la actualidad quedan algunos ejemplos de cazas de escolta, si bien la aparición de bombarderos con tecnología furtiva como el B-2 Spirit ha limitado su papel. Es el caso del Mig-29 en sus primeras versiones o el Panavia Tornado ADV.

La interdicción aérea es una misión donde se realiza un bombardeo específico de los recursos enemigos con la intención de inhabilitar al oponente. Este tipo de naves están pensados para el bombardeo táctico a larga distancia y están armados con misiles aire-tierra (En un pasado con torpedos y cohetes de alcance lejano). Una de las diferencias entre estos bombarderos interceptores con los cazabombarderos, y los bombarderos estratégicos (que también atacan a superficie), es que tienden a ser naves mucho más ligeras, porque no tienen que recorrer grandes distancias dejándole espacio al combustible, y porque portan un armamento no pesado y específico, pues estos aviones no están ideadas para intervenir en los combates, sino que priorizarían las acciones "más allá del alcance visual" BVR (Beyond Visual Range). Por otro lado, suelen estar bien equipados con potentes radares para el rastreo, combate nocturno y hasta capacidad de guerra electrónica. Un ejemplo concreto es el General Dynamics F-111 Aardvark, el Super Étendard y el Panavia Tornado IDS (sin confundirlo con el Panavia Tornado ADV). También se pueden clasificar como cazas interdictores al McDonnell Douglas F/A-18 Hornet que tiene un claro perfil de ataque a superficie desde larga distancia, pero que cuenta con un mayor espectro de rol con carácter de polivalente, y otro es el Mikoyan MiG-29 específicamente a partir de sus últimas versiones.

Al igual que el avión bombardero son aeronaves fuertemente armados y pesados que pueden volar a gran altitud. Priorizan la velocidad renunciando la maniobrabilidad. por lo general suelen ser aviones de caza reasignados a misiones de bombardeo Surgieron durante la Segunda Guerra Mundial, período del que destacan el Republic P-47 Thunderbolt, el Vought F4U Corsair y el Hawker Typhoon. Su gran capacidad para llevar armamento está pensado principalmente para el bombardeo estratégico a superficie, por tanto tienen cierta debilidad contra caza de intercepción o cazas de superioridad aérea, ya que no privilegian el armamento aire-aire. Ejemplos de cazabombardero son el Xian JH-7, el Dassault Mirage 2000N/2000D o el F-15 Eagle.

Son aviones caza con una amplia diversidad de perfil, pero que en común, tienen capacidades para el ataque a superficie marina y submarina (torpedos y misiles), que normalmente cuentan con armamento de interdicción BVR "más allá del alcance visual" y tienen un diseño aprueba del ambiente marino. Deben adaptarse al despegue en espacios reducidos como dentro de portaaviones, CATOBAR. Los aviones Harrier y algunos Yakovlev incluso tienen la particularidad del despegue en ascenso vertical STOVL. La diversidad de perfiles va desde la defensa del espacio aéreo, el patrullaje costero, la intercepción aire-aire en incluso la superioridad aérea, posteriormente proliferaron los llamados cazas polivalentes. Ejemplos evidentes son el Yakovlev Yak-38, el Mikoyan MiG-29K, y el Sea Harrier. El cásico A-4 Skyhawk y el AV-8B Harrier han sido cazas navales con una disposición al apoyo aéreo cercano y ataque a superficie. El F-18 Hornet y Super Hornet tienen un perfil de caza naval multipropósito. El F-14 Tomcat tenía un carácter más bien de superioridad aérea. El avanzado Sukhoi Su-34 además de superioridad aérea, tiene perfil de patrullaje costero. Las versiones B y C del Lockheed Martin F-35 considerado caza de V generación furtivo, tienen una clara disposición naval con despegue CATOBAR y STOVL respectivamente. 

Estas generaciones de aviones además de tener tecnología más avanzada, aseguran la capacidad de maniobrabilidad para el combate aire-aire "dentro del alcance visual" (Combate WVR Within Visual Range). Es por ende que, cumpliendo con toda gama de roles de ataque, a saber enfrentamiento aéreo, bombardeo e interdicción, estas aeronaves centran sus distinciones en cuestiones de peso, capacidad y variedad en el uso de armamentos. Por otra parte siguen surgiendo nuevas tecnologías y en esa línea aparecen los cazas furtivos. La nueva clasificación a partir de la cuarta generación entre, aviones de superioridad aérea y polivalentes, se basa en el paradigma soviético del Sukhoi Su-27 y el Mikoyan MiG-29; el primero de superioridad aérea y el segundo más pequeño y polivalente.

Son aeronaves que aparecen a partir de la IV generación y vienen a suprimir la necesidad de grandes cazas bombarderos. Por norma general son más ligeros y menos potentes que los caza de superioridad aérea, sin embargo al igual que ellos y a diferencia de los cazas bombarderos, tienen una gran maniobrables para el combate aéreo (Dogfight). Se clasifican como polivalentes o multirol porque no renuncian a ninguna tarea, ya que pueden ser utilizados de igual forma como cazas de combate aire-aire, así como aviones de ataque a distancia, o incluso y a pesar de su tamaño, para cargar armas de ataque a superficie. Existen sin embargo dos subtipos en cuanto al enfoque que pueden privilegiar. Por un lado están las aviones polivalentes que privilegian las tareas de interdicción "más allá del alcance visual" BVR (Beyond Visual Range), como por ejemplo el F-18 Super Hornet y el Sukhoi Su-34, y aquellos aviones polivalentes que siguieron perfeccionándose en el combate aire-aire como el F-16 Fighting Falcon, el Dassault Rafale, Saab 39 Gripen, Chengdu J-10B, Mikoyan MiG-35 o el Eurofighter Typhoon.

Los cazas de superioridad aérea son aviones confeccionados a partir de la generación 4. que cumplen tareas de control del espacio aéreo del enemigo. Son cazas sólidos en el combate y potentes en el porte misiles y bombas. Una diferencia con los caza polivalentes es que casi si bien tiene una enorme capacidad para el bombardeo estratégico, no cumplen con tareas de apoyo aéreo cercano. Por otro lado, este tipo de aviones privilegian la confrontación propiamente tal antes que la interdicción. Son aviones mucho más pesados que los polivalentes, pueden llevar mayor armamento, sin embargo esto no quiere decir que pierden capacidad de maniobrabilidad, incluso por el contrario, son aviones muy aerodinámicas y maniobrables. Ejemplos claros son el F-15 Eagle, el Shenyang J-11 y el Sukhoi Su-35.

Son aviones de 5° generación que agregan a los caza la opción de ser usados en misiones furtivas, a saber que cuentan con una confección y diseño hecho para no ser detectados por radares enemigos. Claros ejemplos de este tipo de aeronaves son el F-22 Raptor, F-35 Lightning, el Sukhoi PAK FA, el Chengdu J-20 y el Shenyang J-31.





</doc>
<doc id="29810" url="https://es.wikipedia.org/wiki?curid=29810" title="Geografía de Luxemburgo">
Geografía de Luxemburgo

Luxemburgo (luxemburgués: "Groussherzogtum Lëtzebuerg", francés: "Grand-Duché de Luxembourg", alemán: "Großherzogtum Luxemburg") es un estado de Europa occidental, situado entre Francia que queda al sur y Alemania que queda al este; al norte y al oeste limita con Bélgica.

Con sus 2.586 kilómetros cuadrados, Luxemburgo es el séptimo país más pequeño de Europa, encontrándose en el puesto 167 a nivel mundial.

El paisaje de Luxemburgo está dominado por colinas suavemente redondeadas y anchas y valles poco profundos. Es ligeramente montañosa en el norte y va descendiendo hacia las llanuras drenadas por el río Mosela al sureste. Comprende dos regiones de distinta orografía: el Oesling al norte y Gutland al sur. 

Al norte de la ciudad de Luxemburgo se extiende el Oesling (también "Éislek, Œsling, Oesling, Ösling" o "Eisling"). Abarca aproximadamente el tercio norte del territorio, posee un área de 828 km² con una altura media de 450-500 msnm. Este conjunto de tierras altas se corresponde con las estribaciones meridionales de la meseta de las Ardenas. Su relieve es accidentado, más alto que el resto del país y está cubierto por bosques de coníferas (en especial piceas). Por ella discurren los afluentes del Mosela. 

Gutland, en el sur y centro del país, se extiende por 1.758 km². Continúa las onduladas planicies de la Lorena que están profundamente excavadas por los ríos que forman estrechos y serpenteantes valles que bajan de suroeste a noreste. Es la tierra de terrenos más fértiles, clima más benigno y subsuelos ricos en minerales como el hierro y el carbón. Por ello se la conoce como "Bon Pays" o "Gutland" ("Buen País"). Posee una altitud media de 215 msnm, y se interrumpe al Este en los valles vitivinícolas del río Mosela. 

El punto más bajo del país se encuentra en el río Mosela en Wasserbillig (punto de confluencia con el Sauer) 133 msnm, en la comuna de Mertert. El punto más alto es Op Kneiff en Wilwerdange 595 msnm (comuna de Ëlwen/Troisvierges). Frecuentemente se considera como sitio más elevado la Buergplatz (plaza de la ciudad) de Huldang/Huldange aunque en tal punto la altitud es de 558,35 msnm.

Luxemburgo tiene una serie de ríos menores como el Eisch, el Alzette y el Pétrusse, pero el principal río es el Mosela, que discurre por el sector oriental, trazando la frontera con Alemania, con sus afluentes el Sûre y el Our. Con la excepción del Chiers afluente del Mosa, el conjunto de los cursos de agua del país pertenecen a la cuencia del Mosela y por el conjunto del Rin. Los ríos más importantes son el Sûre en el centro, el Alzette en el sur y el Wiltz en el norte.

El río Mosela realmente surge en el noreste de Francia y fluye hacia el norte a través de Luxemburgo durante 31 km para unirse al poderoso Rin en Coblenza, Alemania. El Mosela tiene 515 km de largo, y es navegable, debido a la canalización, durante 64 km. Las laderas verdes, cubiertas de vides, flanquean el río, lleno de meandros.

Nacido en Bélgica, el río Sûre recorre 172 km en dirección este a través de Luxemburgo y luego desemboca en el Mosela. Su sinuoso curso esencialmente corta Luxemburgo de este a oeste. El río Our, que recorre la frontera nororiental, es un afluente del Sûre. Su valle se encuentra rodeado por un paisaje no tocado por el hombre.

Luxemburgo tiene un clima oceánico modificado con inviernos suaves y veranos temperados (Cfb). Disfruta de un clima templado sin extremos. La temperatura media anual en la capital Luxemburgo es de 9°C. El clima es más riguroso en las regiones altas del norte ("Eisléck"), y más moderado en el sur ("Guttland"). Las temperaturas bajas y la humedad hacen que se le llame "clima vigorizante", de manera un poco irónica, por los que viven en el norte del país. En el sur, aunque la lluvia no es significativamente baja, la diferencia está en las temperaturas entre el invierno y el verano, especialmente altas en el valle del Mosela. Las cosechas, especialmente las vides, crecen aquí. Con una temperatura media anual de 10 °C, los meses más soleados son mayo a agosto.

En el invierno, hay considerable influencia de los sistemas atlánticos, en los que el paso de frecuentes bajas presiones dan lugar a condiciones de tiempo inestable. Esto da como resultado cielos nublados y considerable llovizna. Sin embargo, la nieve no es muy frecuente, tampoco en las regiones mas frías. Las precipitaciones en Luxemburgo disminuen de oeste a este. La lluvia alcanza 1.200 mm al año en algunas partes del oeste y cae a 700 mm en el valle del Mosela. En el verano, el calor excesivo es raro y las temperaturas bajan significativamente por la noche.

Según WWF, el territorio de Luxemburgo corresponde a la ecorregión denominada bosque de frondosas de Europa occidental. Conforme a la normativa de la Unión Europea, el territorio de este país pertenece a la región biogeográfica continental. 313 hectáreas están protegidas como humedales de importancia internacional al amparo del Convenio de Ramsar, en total, dos sitios Ramsar: Haff Réimech y Vallée de la Haute-Sûre. 

Preocupaciones ecológicas: Contaminación atmosférica y acuífera en las zonas urbanas, particularmente lluvia ácida y contaminación del suelo de tierras de labranza.

Luxemburgo tiene una población de 590.667 habitantes (est. enero de 2017), lo cual significa que tiene una densidad de población alta, de 228,4 h/km². La población en su mayoría urbana (82% en 2008), posee un índice de crecimiento anual que no supera el 1,172 % (est. 2009). Principales grupos étnicos: luxemburgueses 63,1%, portugueses 13,3%, franceses 4,5%, italianos 4,3%, alemanes2,3%, otros nacionales de la Unión Europea 7,3%, otros 5,2% (censo de 2000). Se habla luxemburgués, idioma nacional dialecto del alemán con elementos del francés y del neerlandés; alemán (idioma administrativo) y francés (idioma administrativo). Debido al número elevado de inmigrantes lusofonos y trabajadores fronterizos francófonos, el francés está muy presente, particularmente en las ciudades. En cuanto a la religión, la católica es la más practicada con 87%, aunque existen minorías protestantes, judías y musulmanas que integran el 13% restante (año 2000).

La capital, Luxemburgo es la población más grande del país, con 114.000 habitantes en 2017. Otras ciudades importantes son Esch-sur-Alzette, centro industrial que se encuentra hacia el sudoeste de la capital, en la frontera con Francia, y Diekirch, al norte.

Hasta el año 2015, el Luxemburgo se dividía en tres distritos: Diekirch, Grevenmacher y Luxemburgo. Tradicionalmente, en Luxemburgo se distinguían doce cantones: Luxemburgo, Capellen, Esch, Mersch, Clervaux, Diekirch, Redange, Vianden, Wiltz, Echternach, Grevenmacher y Remich.

Los principales recursos naturales de Luxemburgo son: minerales ferrosos, que dieron lugar a una gran industria siderúrgica; ya no se explota) y carbón mineral y las tierras cultivables] que representan el 27,42% del uso de la tierra e incluye viñedos. A cultivos permanentes se dedica el 0,69% (2005); pastos permanentes 20%, forestal y boscosa, 21% y otra 34% (est. 1993). En 1993 se estimó que el regadío representaba 10 km², actualmente (2009) es un dato no disponible.
La composición del PIB por sector es: agricultura 0,4%, industria 13,6% y servicios 86% (est. 2007). La agricultura emplea al 2,2% de la población activa, la industria el 17,1% y los servicios el 80,6% (2007). La población activa se calcula en 208.000 trabajadores. La economía depende de los trabajadores extranjeros y transfronterizos en el 60% de su población activa: 125.400 son trabajadores extranjeros que viven al otro lado de la frontera que provienen principalmente de Francia, Bélgica y Alemania.

Tiene una economía pequeña, estable, con altos ingresos. Se beneficia de su proximidad con Francia, Bélgica y Alemania. Históricamente tanto la inflación como el desempleo se han mantenido bajos. El sector industrial, inicialmente dominado por el acero, se ha ido diversificando para incluir productos químicos, goma y otros productos. El crecimiento en el sector financiero, al que ahora se debe el 28% del PIB, ha hecho más que compensar el declive del acero. Los bancos están en su mayor parte en manos extranjeras, y la mayor parte del negocio lo tienen en las operaciones exteriores.

Como el resto de la Unión Europea, tras experimentar una fuerte expansión entre 2004 y 2007, ha padecido la crisis de 2008-2009, pero a pesar de ello sigue gozando de un alto nivel de vida, pues su PIB per cápita es el tercero del mundo, tras Liechtenstein y Catar, y es el más alto de la Unión Europea.

Los principales productos agrícolas son: vino, uvas, cebada, avena, patatas, trigo y frutas. De la ganadería se obtienen productos lácteos. En cuanto a los productos industriales, cabe citar: servicios banqueros y financieros, hierro y acero, tecnología de la información, telecomunicaciones, transporte de mercancías, industria alimenticia, sustancias químicas, productos metálicos, ingeniería, neumáticos, cristal, aluminio y turismo. 

Hay 275 km de ferrocarril, y 5.227 km de carreteras, todas ellas pavimentadas e incluyendo 147 km de autopista (2004). En cuanto a las vías fluviales, hay 37 km en el curso del río Mosela (2008). El puerto fluvial más importante es Mertert, en el distrito de Grevenmacher.



</doc>
<doc id="29819" url="https://es.wikipedia.org/wiki?curid=29819" title="Lázaro Cárdenas">
Lázaro Cárdenas

El término Lázaro Cárdenas puede referirse:









</doc>
<doc id="29821" url="https://es.wikipedia.org/wiki?curid=29821" title="Bandera de Suiza">
Bandera de Suiza

La bandera de Suiza consiste en un campo rojo cuadrado con una cruz griega blanca y equilateral en el centro de la bandera. Es una de las dos únicas banderas cuadradas de , la otra es la bandera del Vaticano.

Seguramente la bandera de la Confederación Helvética se inspiró en la insignia del Cantón de Schwyz, el que recibió una cruz de plata en conmemoración de su lucha junto a las tropas del Sacro Imperio Romano Germánico. Los antiguos confederados usaban ya la "cruz suiza" como distintivo en los campos de batalla. El dato más antiguo se remonta a la batalla de Laupen en 1339. Al principio, los brazos de la cruz eran estrechos y largos, llegando hasta el borde del paño, como en los países escandinavos.

La bandera presentada a continuación fue utilizada durante el período de 1798 a 1803, época durante la cual el país estuvo invadido por las fuerzas napoleónicas y convertido en república por estas. El cambio de bandera a la actual se debió al cambio de régimen. Tras la derrota de las fuerzas de Napoleón, el Congreso de Viena reconoció la neutralidad universal de Suiza, además de definir las fronteras de su territorio. Esto permitió que Suiza se organizara de nuevo, cambiara su configuración, sus símbolos y que pasara de estado unitario a confederación.

La bandera de la Cruz Roja se inspiró igualmente en la de Suiza, solo que invirtiendo los colores, en honor de la patria de Henri Dunant, el fundador de la misma.

La versión rectangular de la bandera suiza es usada por el equipo olímpico del país desde los Juegos Olímpicos de Atenas 2004, siguiendo la regla del Comité Olímpico Internacional (COI) de que todos los países utilicen el mismo formato para sus emblemas nacionales.


</doc>
<doc id="29823" url="https://es.wikipedia.org/wiki?curid=29823" title="Bandera de la Unión Soviética">
Bandera de la Unión Soviética

La bandera de la Unión Soviética corresponde al emblema utilizado por dicho Estado desde su establecimiento en 1922 hasta su disolución en 1991.

A lo largo de su historia, el emblema tuvo diversas modificaciones, pero en general mantuvo la misma estructura desde su adopción, el 12 de noviembre de 1923. La bandera, en proporción 1:2, era completamente roja (color tradicional del socialismo y el comunismo) y en su cantón tenía en dorado el símbolo de la hoz y el martillo y sobre este una estrella roja con borde dorado.

La bandera tuvo gran importancia para los diversos movimientos políticos de carácter marxista alrededor del mundo y sirvió de inspiración para diversos emblemas, especialmente de países socialistas durante la época de la Guerra Fría. A su vez, las diversas banderas de las repúblicas que conformaban la URSS eran modificaciones de la bandera nacional.

La bandera de la Unión Soviética fue adoptada por primera vez en diciembre de 1922 durante el "I Congreso de los Soviets de la URSS". Se llegó a la conclusión de que ""el estandarte rojo del Partido se ha transformado en el símbolo del Estado, y alrededor de esa bandera se han agrupado los pueblos de las repúblicas soviéticas para unirse en un Estado: la Unión de Repúblicas Socialistas Soviéticas"". El 30 de diciembre de 1922 el Congreso adoptó una Declaración de Acuerdo sobre el establecimiento de la URSS. El artículo 22 del Acuerdo afirma que ""la URSS tiene una bandera, un escudo de armas y un sello estatal"".

La Constitución soviética de 1924 describía la primera bandera, adoptada el 6 de julio de 1923 en la segunda sesión del "Comité Ejecutivo Central de la URSS" (CIK). El artículo 71 dice: ""La bandera estatal de la Unión de Repúblicas Socialistas Soviéticas consiste de un campo rojo o escarlata con el escudo de armas del Estado"". Se la diseñó con una proporción de 1:4, inusual en vexilología, y consistía en una bandera roja con el escudo de armas de la Unión Soviética en el centro. No obstante, nunca se fabricó esa bandera y fue oficial sólo durante 24 meses. El 12 de noviembre de 1923, la tercera sesión del CIK designó bandera oficial al más conocido diseño con la hoz y el martillo (☭).

En esa misma sesión del CIK se cambió la descripción constitucional de la bandera, y el artículo 71 pasó a decir: ""La bandera estatal de la Unión de Repúblicas Socialistas Soviéticas consiste de un campo rojo o escarlata, y en el canto una hoz y martillo dorados, y sobre ellos una estrella de cinco puntas bordada en oro. La relación entre ancho y alto es 1:2"". El anverso de la nueva bandera consistía en un fondo rojo con un martillo cruzado con una hoz y una estrella roja en la parte superior. El martillo simbolizaba a los trabajadores industriales de la nación, mientras que la hoz simbolizaba a los trabajadores agrícolas. La estrella roja simbolizaba a su vez el gobierno del Partido Comunista.

El 15 de agosto de 1980 se adoptó una leve modificación de la bandera, haciendo el reverso rojo sin hoz, martillo ni ninguna inscripción o decorado. Adicionalmente se cambió el color de la tela por un tono más brillante. Con la desarticulación de la URSS el 3 de diciembre de 1991, la bandera dejó de ser una bandera nacional.

El 15 de abril de 1996 el entonces presidente ruso Borís Yeltsin firmó un decreto presidencial elevando a la bandera soviética a un nivel similar a la nueva bandera de Rusia, con la diferencia de que esta versión no tendría la hoz y el martillo, sino sólo la estrella roja. Se dio en llamarlo "Estandarte de la Victoria", conmemorando la famosa ocasión en que soldados del Ejército Rojo la hicieron ondear sobre el Reichstag el 1 de mayo de 1945. En algunas fechas conmemorativas se hace flamear al Estandarte de la Victoria junto con la bandera rusa. Durante el gobierno de Vladímir Putin se incluyó a la estrella roja en la bandera oficial del Ejército ruso.

En 2007 una ley federal de la Federación Rusa, firmada por Vladímir Putin, devolvió la hoz y el martillo a las copias del Estandarte de la Victoria, ya que estaban presentes en la bandera izada en el Reichstag. El artículo que describía el "Símbolo de la Victoria" sin la hoz y el martillo fue excluido de la Ley. Ahora los estandartes idénticos al Estandarte de Victoria se denominan "copias" en vez de "símbolos". Llevan además una inscripción: "150 стр. ордена Кутузова II ст. идрицк. див. 79 С.К. 3У.А.1Б.Ф".

La bandera de la Unión Soviética ha tenido una evidente influencia en banderas de otros Estados socialistas surgidos a posterioridad, o que recibieron influencia soviética. La siguiente tabla muestra algunas de ellas:

También ha tenido influencia en las banderas de diversas organizaciones políticas de tendencia comunista o socialista en varias partes del mundo:

La bandera sigue izada en algunas localidades y ciudades de los países que conformaban la Unión Soviética. Una de las más importantes es Pripiat, en Ucrania, ciudad que debido al desastre nuclear de Chernobil, algunas banderas quedaron izadas en los edificios de la administración soviética, por lo que a pesar de la disolución soviética y la independencia ucraniana de 1991 los símbolos patrios del Estado socialista seguirán izadas hasta que bajen los niveles de radioactividad y la zona vuelva a ser habitable o apta para la presencia prolongada de seres humanos, algo que actualmente es imposible.



</doc>
<doc id="29824" url="https://es.wikipedia.org/wiki?curid=29824" title="Anticomunismo">
Anticomunismo

El anticomunismo es una corriente ideológica históricamente opuesta de manera activa al comunismo. A través del tiempo, la palabra "comunismo" ha sido usada para referirse a varios tipos de organización social y sus partidarios, pero, desde mediados del siglo XIX, la corriente dominante del comunismo en el mundo ha sido el marxismo. El comunismo marxista consiguió muchos más seguidores y oponentes que todas las demás formas de comunismo juntas. Asimismo, el término "anticomunismo" se emplea principalmente para referirse a la oposición activa a tal movimiento político.

El marxismo, y la forma de comunismo asociado a él, alcanzó su apogeo en el siglo XX. El anticomunismo organizado se desarrolló como reacción a la creciente popularidad del movimiento comunista, y adoptó muchas formas a lo largo del siglo XX. Los monárquicos conservadores europeos se opusieron a las primeras oleadas de revoluciones comunistas desde 1917 a 1922. El fascismo y el nazismo se basaron en una forma violenta de anticomunismo; incitaban el miedo a la revolución comunista para obtener poder político, e intentaron destruir el comunismo en la Segunda Guerra Mundial. Los nacionalistas lucharon contra los comunistas en numerosas guerras civiles por todo el mundo. Tanto el conservadurismo como el liberalismo clásico conformaron gran parte de las políticas exteriores anticomunistas de las potencias occidentales, y dominaron el pensamiento intelectual anticomunista en la segunda mitad del siglo XX. Actualmente, buena parte de la crítica al comunismo viene de los sectores libertarios y anarquistas de libre mercado. 

Tras la Revolución de Octubre en Rusia, el comunismo marxista quedó principalmente asociado a la Unión Soviética en la imaginación pública (aunque había muchos marxistas y comunistas que no apoyaban a la misma). Como resultado, el anticomunismo y la oposición a la Unión Soviética se hicieron prácticamente indistinguibles, especialmente en política exterior. El anticomunismo fue un elemento importante en la política exterior de las Potencias del Eje durante los años treinta (Pacto Antikomintern) y de los Estados Unidos, el Reino Unido, Japón, Corea del Sur, Australia, Canadá, Israel y otros países capitalistas durante la Guerra Fría.

Casos extremos de anticomunismo fueron las matanzas anticomunistas, es decir, asesinatos en masa de personas comunistas o sospechosas de serlo.

Ha habido numerosos conflictos entre los comunistas y los conservadores. La mayoría de las revoluciones comunistas han tenido lugar en países relativamente conservadores, y la mayoría de los gobiernos derrocados por los comunistas han sido gobiernos conservadores. El nacionalismo anticomunista ha aparecido generalmente por tres razones: defensa de los valores tradicionales, de la identidad nacional y de las estructuras sociales como parte del programa de los nacionalistas para preservar el poder y el prestigio nacional.

Dado que los comunistas dicen aspirar a una igualdad social extrema, son teóricamente opuestos a la monarquía, la aristocracia, y otras formas de privilegio hereditario. Al final del siglo XIX y al principio del XX, el primitivo movimiento comunista se enfrentó a las monarquías tradicionales que gobernaban en la mayoría de los países de Europa. Por entonces, los monárquicos eran los anticomunistas más prominentes, y muchas monarquías europeas ilegalizaron la expresión pública de ideas comunistas. La defensa del comunismo era ilegal en el Imperio ruso, el Imperio Alemán y el Imperio austrohúngaro, las tres monarquías más poderosas de Europa continental antes de la Primera Guerra Mundial. Hasta el final del siglo XIX los monárquicos (excepto los constitucionalistas) creían que la desigualdad en la riqueza y en los derechos políticos formaban parte del orden divino.

Durante la Primera Guerra Mundial, en la mayor parte de las monarquías Europeas, estas ideas habían sido sustituidas por los movimientos liberales y nacionalistas que creían que los monarcas deberían ser cabezas simbólicas de la nación mientras que el poder real quedaría en manos de gobiernos elegidos. La monarquía más conservadora de Europa, el Imperio Ruso, fue reemplazada por la comunista Unión Soviética. La Revolución Bolchevique inspiró otras revoluciones comunistas por toda Europa durante los años 1917 a 1922. Muchas de ellas, como el Levantamiento Espartaquista en Alemania, fueron sofocadas por unidades militares monárquicas.

Los años 20 y 30 vieron el declive del conservadurismo tradicional. La primera línea del anticomunismo fue tomada por los entonces ascendentes movimientos fascistas por un lado, y por los conservadores liberales inspirados por Estados Unidos por otro. El comunismo siguió siendo un fenómeno fundamentalmente europeo, por lo que el anticomunismo estuvo también concentrado en Europa. Cuando grupos y partidos políticos comunistas empezaron a aparecer por todo el mundo, como en el República de China a finales de los años 20, sus oponentes fueron generalmente las autoridades coloniales o los movimientos nacionalistas locales.

Después de la Segunda Guerra Mundial, el comunismo se convirtió en un fenómeno global, y el anticomunismo en parte integral de las políticas exterior e interior de los Estados Unidos y sus aliados de la OTAN. Los conservadores de posguerra abandonaron sus raíces monárquicas y aristocráticas, centrándose en la defensa del mercado libre, la propiedad privada, la cooperación entre las diferentes clases y la defensa de costumbres, valores y normas sociales tradicionales. Para esos conservadores el comunismo es peligroso por su intención de abolir la propiedad privada y su deseo de destruir las normas culturales tradicionales.

Los Estados Unidos nunca tuvieron un conservadurismo tradicional en el siglo XX. Por tanto, la ideología llamada conservadurismo americano no comparte el legado monárquico de sus equivalentes europeos. Por el contrario, está basado en el individualismo y una visión de la competición económica como beneficiosa para la sociedad, todo acompañado por fuertes sentimientos religiosos y de defensa de la familia tradicional. Los conservadores estadounidenses siempre se opusieron al comunismo, pero esta oposición solo se convirtió en una piedra angular del conservadurismo en los años 40 y 50. Los Estados Unidos hicieron del anticomunismo la principal prioridad de su política exterior, y muchos conservadores estadounidenses combatieron en su país todo aquello que les parecía influencia comunista. Esto llevó a la adopción de un conjunto de medidas en política interior conocidas colectivamente como «macarthismo».

Durante la Guerra Fría, los gobiernos conservadores en Asia, África e Hispanoamérica buscaron al apoyo político y económico de los Estados Unidos. Algunos de éstos eran regímenes autoritarios que, según sus críticos, usaron el miedo al comunismo como medio de legitimar la represión, la supresión de los derechos individuales y la abolición de la democracia. Como ejemplos se podría citar Corea del Sur durante el mandato de Syngman Rhee, la República de China durante el de Chiang Kai-shek, Vietnam del Sur durante el de Ngo Dinh Diem, Indonesia durante el del general Suharto, Zaire durante el de Mobutu Sese Seko, Paraguay durante el de Alfredo Stroessner y Chile durante el de Augusto Pinochet

En los años 80, los gobiernos conservadores de Ronald Reagan en los Estados Unidos, Margaret Thatcher en Gran Bretaña y Brian Mulroney en Canadá siguieron una política exterior claramente antisoviética que es considerada por muchos como la causa principal de la caída de la Unión Soviética y de la llegada del capitalismo a Europa Oriental y a otros países revolucionarios.

El fascismo y el comunismo son sistemas políticos que alcanzan su cima tras la Primera Guerra Mundial. Historiadores del período entre la Primera Guerra Mundial y la Segunda Guerra Mundial como E. H. Carr o Eric Hobsbawm señalan que las democracias liberales estaban seriamente acosadas en este período y parecían ser una filosofía en extinción. El movimiento socialista se dividió cuando los líderes de los partidos socialdemócratas apoyaron la guerra, mientras que los partidarios de la Revolución rusa de 1917 formaron Partidos Comunistas en la mayoría de los países industrializados (y en muchos no industrializados).

Tras la Primera Guerra Mundial y la Revolución Rusa, hubo sublevaciones socialistas y marxistas o amenazas de sublevaciones socialistas por toda Europa; la más notable fue la de Alemania, donde el Levantamiento Espartaquista en enero de 1919 fracasó. En Baviera, los comunistas derrocaron el gobierno y establecieron la República Soviética de Baviera, que duró unas pocas semanas en 1919. Una vida de brevedad similar tuvieron las repúblicas soviéticas que surgieron en otros estados alemanes y el gobierno soviético establecido en Hungría por Béla Kun en 1919.

Muchos historiadores consideran el fascismo como una reacción contra estos movimientos. El fascismo italiano, fundado y dirigido por el antiguo socialista Benito Mussolini, tomó el poder con la aquiescencia del rey Víctor Manuel III tras años de revueltas izquierdistas, y contó con el apoyo de muchos conservadores que temían que la revolución comunista fuese inevitable. Por toda Europa, numerosos aristocrátas e intelectuales conservadores así como capitalistas y empresarios dieron su apoyo a movimientos fascistas que en sus respectivos países surgieron tomando como modelo el fascismo italiano. Mientras en Alemania, aparecieron grupos nacionalistas de extrema derecha, particularmente entre los Freikorps postbélicos, que fueron usados para aplastar tanto el Levantamiento Espartaquista como la República Soviética de Baviera.

Sin embargo, algunos autores anticomunistas disienten de la idea de que el fascismo fue una reacción contra los movimientos socialistas revolucionarios y en cambio se centran en lo que, para ellos, son similitudes esenciales entre el Estado Comunista y el Estado Fascista, tanto en la teoría como en la práctica, siendo conocida como la teoría del totalitarismo. El reconocido economista de la Escuela Austriaca Friedrich Hayek, autor de "Camino de servidumbre", argumentaba que los diversos movimientos totalitarios, incluyendo el fascismo y el comunismo, tienen unas raíces filosóficas comunes que nacen de su oposición al liberalismo del siglo XIX. Los defensores de estas teorías ven algo más que una coincidencia en el hecho de que el propio Benito Mussolini era Marxista y miembro del Partido Socialista Italiano antes de la Primera Guerra Mundial, mientras que algunos ideólogos del fascismo, como Sergio Panunzio y Giovanni Gentile, tuvieron un pasado marxista o sindicalista que repudiaron en sus escritos posteriores. Aun así, estos autores reconocen que la teoría de ambas ideologías difieren en cual debe ser la base de la sociedad ideal (los comunistas enfatizan en la lucha de clases para conseguir una sociedad sin clases, mientras que los fascistas sugieren una solidaridad de clases nacional dirigida por un estado corporativo). Hayek afirma que aún en 1938, Hitler decía que el marxismo y el nacionalsocialismo eran prácticamente la misma cosa.

Con la llegaba de la Gran Depresión de los años 30, parecía que el liberalismo y el capitalismo liberal estaban condenados a desaparecer mientras los movimientos comunista y fascista crecían. Estos movimientos se oponían ferozmente el uno al otro y se enfrentaron frecuentemente. El ejemplo más notable de estos enfrentamientos fue la Guerra Civil Española, que se convirtió parcialmente en una guerra subsidiaria entre los países fascistas y sus partidarios internacionales que apoyaban a Franco y el movimiento comunista (sostenido principalmente por la Unión Soviética) que, aliado con los anarquistas y los trotskistas, apoyaba al gobierno republicano.

Inicialmente, la Unión Soviética apoyó al idea de una coalición con las potencias occidentales contra la Alemania Nazi, a la vez que fomentaba la formación de frentes populares en varios países contra los fascistas locales. Estas políticas tuvieron poco éxito debido a la desconfianza de las potencias occidentales (especialmente Gran Bretaña) hacia la Unión Soviética. El Pacto de Múnich entre Alemania, Francia y Gran Bretaña aumentó los temores soviéticos de que las potencias occidentales estaban tratando de obligarlos a llevar el peso de la lucha contra el nazismo. Los soviéticos cambiaron su política y negociaron un pacto de no-agresión con la Alemania de Hitler, conocido como el Pacto Mólotov-Ribbentrop en 1939. Más tarde los soviéticos alegaron que el pacto era necesario para ganar tiempo para prepararse para la guerra prevista contra Alemania. Sin embargo, algunos críticos dudan de esta afirmación, señalando que junto a la cláusula de no agresión, el pacto también establecía una amplia colaboración económica entre la Unión Soviética y Alemania, en la forma del Acuerdo Comercial Germano-Soviético, que suministraba a la Alemania nazi algunos de los materiales necesarios para construir su maquinaria de guerra. Este acuerdo de colaboración es recordado por los citados críticos para deducir que Stalin esperaba que la guerra fuese solo entre Alemania y los aliados occidentales, es decir, que la Unión Soviética mantuviera su neutralidad mientras sus dos grandes enemigos se destrozaban mutuamente.

En cualquier caso, está claro que Stalin no esperaba que los alemanes atacaran hasta 1942, por lo que fue sorprendido cuando la Alemania Nazi invadió la Unión Soviética en junio de 1941, con la Operación Barbarroja. El fascismo y el comunismo volvieron a ser enemigos mortales.

La Iglesia católica tiene una larga historia de anticomunismo. El Catecismo de la Iglesia católica de 1992 afirma: "La Iglesia ha rechazado las ideologías totalitarias y ateas asociadas en los tiempos modernos al ‘comunismo’ o ‘socialismo’. Por otra parte, ha rechazado en la práctica el abuso del ‘capitalismo’, el individualismo y la primacía absoluta de la ley de mercado sobre el trabajo humano. La regulación de la economía por la sola planificación centralizada pervierte en su base los vínculos sociales; su regulación únicamente por la ley de mercado quebranta la justicia social, porque ‘existen numerosas necesidades humanas que no pueden ser satisfechas por el mercado’. Es preciso promover una regulación razonable del mercado y de las iniciativas económicas, según una justa jerarquía de valores y con vistas al bien común".

El papa Juan Pablo II fue un duro crítico del comunismo, y otros papas compartieron este punto de vista, por ejemplo el papa Pío IX publicó la encíclica papal "Quanta cura" en la que llamaba al "comunismo y el socialismo" el error más fatal.

Durante la Guerra Civil Española, la Iglesia católica se opuso a las fuerzas frentepopulistas, y apoyó decisivamente al bando sublevado, calificando la guerra de «cruzada».

Aunque muchos anarquistas (especialmente los anarcocomunistas) se describen a sí mismos como comunistas, todos los anarquistas critican el comunismo autoritario. Los comunistas libertarios coinciden con los demás comunistas en que el capitalismo es un herramienta de opresión, es injusto y debe ser destruido de una forma u otra. Los anarquistas, sin embargo, van más allá al decir que "todo" poder centralizado o coercitivo (no solo la riqueza) es dañino para el individuo. Por lo tanto, los conceptos de dictadura del proletariado, propiedad estatal de los medios de producción y otros similares del pensamiento marxista son un anatema para los anarquistas, independientemente de si el Estado en cuestión es democrático o no. Sin embargo, muchos otros anarquistas tienen críticas de corte individualista hacia el comunismo, a menudo desde puntos de vista individualistas, anarcocapitalista o desde corrientes como el Libertarismo, que es una sub-rama del Anarcocapitalismo que consiste en una tendencia contracultural de este, cuya única diferencia es que a pesar de apoyar el Mercado como una contraparte al Estado y sus servicios, igualmente su lucha no se centra en ello a pesar de que esta si incluya la sustitución del Estado por el Mercado al igual que en el Anarcocapitalismo. 

Hay también fuertes tendencias antianarquistas entre los marxistas, que han calificado el anarquismo de anticientífico, romántico o burgués independientemente de si son anarquistas de mercado o anarquistas colectivistas-comunistas.

Son famosos los debates en la Primera Internacional entre Mijaíl Bakunin y Karl Marx. Mientras la filosofía de Bakunin debía mucho a la crítica del capitalismo de Marx, sus ideas divergían en cómo debía organizarse la sociedad post-capitalista. Bakunin veía el Estado Marxista simplemente como otra forma de opresión: "La cuestión es que si el proletariado gobierna, ¿sobre quién gobierna? Esto significa que quedará otro proletariado que estará subordinado a esta nueva dominación, a este nuevo estado." Detestaba la idea de un partido-vanguardia que dirigiera a las masas desde arriba, y argumentaba "cuando el pueblo está siendo golpeado con un palo, no es más feliz si el palo es el 'Palo del Pueblo'".

Los anarquistas al principio consideraron la Revolución Bolchevique como un ejemplo de cómo los trabajadores pueden tomar el poder por sí solos, y, de hecho, tomaron parte en la revolución (véase Anarquismo en Rusia). Pero pronto fue evidente que los Bolcheviques y los anarquistas tenían ideas muy diferentes sobre el tipo de sociedad que querían construir. La anarquista Emma Goldman, por ejemplo, deportada desde Estados Unidos a Rusia en 1919, al principio fue una entusiasta de la Revolución, pero pronto perdió el entusiasmo y empezó a escribir su libro "Mi desilusión con Rusia". El que quizá era el más famoso anarquista ruso de la época, Piotr Kropotkin, criticó incisivamente la emergente burocracia bolchevique en unas cartas que escribió a Lenin (quien alguna vez le había visitado en su casa). En 1920 escribió: "[una dictadura de partido] es decididamente perjudicial para la construcción de un nuevo sistema socialista. Lo que se necesita es una construcción local por fuerzas locales" y "Rusia se ha convertido en una República Asamblearia sólo de nombre" (refiriéndose al dominio de los comités de Partido Bolchevique sobre los Soviets -consejos- de campesinos y trabajadores).

Los anarquistas a menudo citan el aplastamiento de la Rebelión de Kronstadt, en la que el Ejército Rojo acabó a sangre y fuego con un embrión de comuna anarquista y derrotó un levantamiento de marineros soviéticos descontentos con el autoritarismo del gobierno bolchevique, como un ejemplo específico de la tiranía que ellos veían en el gobierno bolchevique. La epidemia de tifus y el subsiguiente aplastamiento del debilitado "Ejército Negro" anarquista de Néstor Majnó en Ucrania es también una acción de los primeros bolcheviques de triste recuerdo para los anarquistas.

Durante la Guerra Civil Española, el Partido Comunista de España obtuvo una influencia considerable por la manipulación política de la ayuda recibida desde la Unión Soviética. Los comunistas, junto a los otros partidos del Frente Popular lucharon contra el ejército sublevado, la Falange Española y otros grupos menores, pero también lucharon contra la Revolución social española de 1936 que estaban realizando sus aliados, los trabajadores anarcocomunistas, supuestamente para reforzar el frente antifascista (la respuesta de los anarquistas, antiestalinistas y trotskistas fue: "La revolución y la guerra son inseparables"). La acción más dramática contra los anarquistas ocurrió en mayo de 1937, cuando fuerzas de la policía dirigidas por comunistas trataron de tomar el edificio de la Telefónica de Barcelona que estaba ocupado por la Confederación Nacional del Trabajo. Los trabajadores de la Telefónica repelieron el ataque, levantaron barricadas y rodearon el "Cuartel Lenin" de los comunistas. Esto continuó durante cinco días de luchas en las llamadas Jornadas de mayo de 1937.

En marzo de 1954 la Organización de los Estados Americanos (OEA) lleva a cabo en Caracas (Venezuela), la X Conferencia Interamericana en la que se emite la "Declaración de Caracas" donde se declara como enemigo al "movimiento comunista internacional" y se le considera como una amenaza a la soberanía y un peligro para la paz: “"La dominación o el control de las instituciones políticas de cualquier Estado del continente americano por el movimiento comunista internacional, que extienda a este hemisferio el sistema político de un poder extracontinental, constituiría una amenaza a la soberanía e independencia política de los Estados americanos, poniendo en peligro la paz del continente, y exigiría la realización de una reunión de consulta para considerar la adopción de una acción apropiada de acuerdo con los tratados existentes"”. La tesis anticomunista de la declaración fue la que redactó y propuso el secretario de Estado de Estados Unidos Jhon Dulles (hermano mayor de Allen Dulles, primer director civil de la CIA de 1953 -1959); Argentina y México se abstuvieron de votar esta declaración y Guatemala fue el único voto en contra, considerando que la declaración en realidad abría la posibilidad de intervención de Estados Unidos en los asuntos internos de otros países; el observador diplomático de Italia consideró que los representantes de Estados Unidos iban mal preparados para la Conferencia, existían fuertes diferencias y contradicciones internas, además la oposición de estos tres países había logrado mermar la influencia de Estados Unidos, aun cuando al final todos excepto los tres mencionados votaran a favor de la propuesta de Estados Unidos, por cuanto al parecer del diplomático italiano no existió un liderazgo fuerte en contra de Estados Unidos.

Del 22 al 31 de enero de 1962, en Punta del Este (Uruguay), a petición de Colombia del 9 de noviembre de 1961 en la cual solicitaba ""la convocación de una Reunión de Consulta de Ministros de Relaciones Exteriores, de acuerdo con el Artículo 6° del Tratado Interamericano de Asistencia Recíproca, para considerar las amenazas a la paz y a la independencia política de los Estados Americanos que puedan surgir de la intervención de potencias extracontinentales encaminadas a quebrantar la solidaridad americana""; se realiza la "VIII Reunión de Consulta de Ministros de Relaciones Exteriores" de miembros de la OEA, órgano asesor del TIAR, donde se expulsa a Cuba de la OEA, bajo la aplicación de una doctrina anticomunista, en la cual se considera que “el comunismo es un peligro para la unidad interamericana” y que “los principios del comunismo son incompatibles con los del Sistema Interamericano”, que los Estados Americanos se hallan unidos en pro del objetivo común de “contrarrestar la acción subversiva del comunismo internacional” y se les dice a los Estados miembros que “adopten las medidas que estimen convenientes a los efectos de su legítima defensa individual o colectiva, y cooperen según sea necesario o conveniente, con el fin de fortalecer su capacidad de contrarrestar las amenazas o los actos de agresión, subversión u otros peligros para la paz y la seguridad que resulten de la intervención continuada en este Continente de las potencias chino-soviéticas”; en esta acta final resuelven:

El 1 de julio, un grupo de cubanos liderados por Ovidio Escalona, exilado en Suecia, provocaron una polémica en las redes sociales, al apoyar la creación y difusión de una bandera que denominan “Cubana Anti-Castrista y Anti-Comunista”.

La bandera “de las 4 estrellas” está compuesta por dos franjas horizontales del mismo tamaño, de color blanco en la parte superior y de azul marino en la inferior. En el borde más cercano al mástil figura un triángulo equilátero de color rojo que contiene en su centro una estrella blanca de 5 puntas y tres estrellas color amarillo con cinco puntas cada una, situadas cerca de los vértices del triángulo.

La bandera anticomunista fue presentada mediante un manifiesto el 25 de julio de 2020 en la página de Ovidio Pavel Escalona Damas

Las primeras grandes muestra de anticomunismo en los Estados Unidos tuvieron lugar entre 1919 y 1920, durante el mandato de Alexander Mitchell Palmer como Fiscal General de los Estados Unidos, quien fue uno de los primeros en usar la expresión Peligro Rojo.

Tras la Segunda Guerra Mundial y ante el poder que adquirió la Unión Soviética, muchas de las objeciones al comunismo ganaron fuerza debido a la declaración de los comunistas de que su ideología era universal. Los temores de muchos anticomunistas de los Estados Unidos de que el Comunismo podría triunfar por todo el mundo e incluso llegar a ser una amenaza directa al gobierno de los Estados Unidos. Este punto de vista condujo a la elaboración de la Teoría del dominó, según la cual la toma del poder por los comunistas en cualquier nación no podía ser tolerada porque produciría una reacción en cadena que llevaría a la toma del poder por los comunistas en el mundo entero. Hubo temores de que naciones poderosas como la Unión Soviética y la República Popular China estaban usando su poder para instaurar por la fuerza regímenes comunistas en otros países. La expansión de la Unión Soviética por Europa Central tras la Segunda Guerra Mundial se vio como una evidencia de esto. Estas acciones llevaron a muchos políticos a adoptar una especie de anticomunismo pragmático, oponiéndose a esta ideología como forma de limitar la expansión del área de influencia de la Unión Soviética, denominada imperio soviético por sus detractores. La política estadounidense de parar cualquier nueva expansión comunista fue conocida como contención.

El gobierno de los Estados Unidos solía justificar su anticomunismo aludiendo a la falta de respeto por los derechos humanos en los estados comunistas, como en la Unión Soviética, la China maoísta, la Camboya de los Jemeres Rojos dirigidos por Pol Pot, y Corea del Norte, porque estos estados mataron millones de sus propios ciudadanos y suprimieron las libertades públicas para la población superviviente.

El anticomunismo cambió significativamente tras la caída de la Unión Soviética y los regímenes comunistas del Bloque del Este entre 1989 y 1991, ya que el temor a una toma del poder mundial por los comunistas prácticamente desapareció. Queda algo de anticomunismo en la política exterior estadounidense hacia Cuba, China y Corea del Norte. En el caso de Cuba, los Estados Unidos siguen manteniendo sanciones económicas contra el régimen de la isla en una política que tiene más detractores que partidarios en el extranjero, pero que tiene un sustancial apoyo en los Estados Unidos, particularmente de los votantes de origen cubano, entre los que hay muchos exiliados que viven en Florida que se oponen a cualquier normalización con el Gobierno cubano.

En 1962, el presidente de Estados Unidos, John F. Kennedy dijo en un discurso en la Academia Militar de West Point: “La subversión es otro tipo de guerra, nuevo en su intensidad aunque de antiguo origen (...) Cuando debemos contrarrestar este tipo de guerra, estamos obligados a emplear una nueva estrategia, una fuerza militar diferente, lo que requiere una preparación y adiestramiento militar nuevos y distintos”, con lo que Estados Unidos se compromete a difundir en sus tropas y en las fuerzas militares aliadas su "nueva estrategia", enfocada en el elemento "contrainsurgente", de donde nace la idea del "enemigo interno" (el anticomunismo, antisovietismo) y una nueva Doctrina denominada de Seguridad Nacional; otros elementos de la estrategia contrainsurgente son las operaciones sicológicas (o guerra sicológicas), cuyo fundamento está en la utilización de la amenaza y el terror "para lograr la cooperación civil o al menos para desarticular y revertir el apoyo dado a la insurgencia"

Reagan derrota en las elecciones de 1980 al presidente Jimmy Carter (1977 - 1981), que intentaba reelegirse, ambos tenían diferencias respecto a la política exterior con la URSS y sobre la defensa de los derechos humanos, es por eso que las relaciones con Latinoamérica cambian profundamente: “Se pasa de una política de convivencia con el comunismo a un enfrentamiento directo”. (...) durante la presidencia de Carter tuvo una política exterior basada en la contención de la URSS, Carter firmó el acuerdo del SALT II y continua con la política de acercamiento con China iniciada por Nixon, disminuye el presupuesto militar y la presencia de tropas y armas de Corea del Sur. Carter retiró el apoyo al régimen de Somoza en Nicaragua, que era respaldado por Estados Unidos y hizo críticas al gobierno de facto de Alfredo Stroessner en Paraguay y al de Augusto Pinochet en Chile. En definitiva Reagan fortalece nuevamente el discurso anticomunista de antaño, señala directamente a la URSS como su enemigo al que denomina el Imperio del Mal y enemigo de toda América, critica la anterior administración por considerar que debilitó la posición de Estados Unidos al aplicar los DDHH como absolutos, y condenar y apartarse de aliados estratégicos en América que violaban esos derechos, por tal razón Reagan considera los DDHH como algo relativo, que deben respetar sus enemigos pero no sus aliados. Reagan es el primer presidente en basarse en los Documentos de Sante Fé, usándolos como un programa a seguir durante su gobierno, tomando medidas de bajo riesgo o baja intensidad para combatir el comunismo, operaciones encubiertas que evitaran la relación directa con Estados Unidos, la promoción de un "culto" hacia el modo de vida estadounidense para contrarrestar la influencia soviética y el entrenamiento en territorio estadounidense de los militares de los ejércitos aliados del continente americano, donde aprenderían no solo a admirar a Estados Unidos sino a defenderlo con la tortura, la crueldad y la barbarie en sus respectivos países. Reagan invade la isla de Granada al norte de Venezuela para evitar que construyan un aeropuerto que podría usar la URSS y Cuba para repostar sus aviones, invade Panamá para tomar control del canal y los dos mares legitimándose en la OEA, promueve una estrategia de mercenarios, paramilitares, o grupos armados contrainsurgentes, para liquidar bajo operaciones todo vestigio de comunismo en América, reflejado en Nicaragua con la creación de los grupos de "contras" para impedir primero el ascenso del FSLN y luego para intentar derrocarlo. Reagan nombró como su embajador en Colombia Lewis Tambs, uno de los redactores del documento SantaFe I, y además miembro de la Liga Mundial Anticomunista

Los gobiernos del socialismo real del siglo XX, de orientación comunista, fueron fuertemente criticados por liberales europeos, debido a que se les acusaba de ser totalitarios. Movimientos democráticos en contra de gobiernos autoproclamados comunistas se dieron en diversas partes del mundo en la década de los años 80 del siglo XX aprovechado los cambios producidos en la URSS bajo el gobierno de Mijaíl Gorbachov, algunos de ellos en:

Numerosos think tanks conservadores, así como medios de comunicación conservadores, han seguido sosteniendo algunos de los argumentos clásicos del anticomunismo, basándose en fracasos económicos y violaciones de los derechos humanos ocurridos en regímenes cuya ideología oficial era el comunismo. Sin embargo, algunos puntos tradicionales promovidos inicialmente por movimientos comunistas europeos como la amplia educación pública y la protección del estado de las personas de renta baja han sido ampliamente adoptados en los países capitalistas de renta alta. Por esa razón, el anticomunismo contemporáneo está más centrado en otros aspectos como la conveniencia de un sector industrial público o hasta qué punto conviene la existencia de redistribución de la renta.

Una de las ideas centrales del marxismo es el llamado materialismo histórico, una metodología para el estudio de la historia que sostiene que en las sociedades humanas los factores materiales y la tecnología han moldeado el desarrollo de las mismas. Así la explicación marxista tradicional divide la historia humana en una serie de períodos o fases, en términos del modo de producción predominante en cada período. La transición de una fase a la siguiente, incluiría la descomposición del orden socioeconómico existente asociado al viejo modo de producción. Esta idea fue introducida por Georg Wilhelm Friedrich Hegel, y Karl Marx la reelaboró para formular sus reflexiones. Amparándose en un razonamiento materialista del mismo tipo que se usó para explicar la transición del feudalismo al capitalismo moderno, el marxismo ortodoxo encontró motivos objetivos para predecir el agotamiento del sistema económico capitalista, y sugerir que este sistema podría ser sucedido por un sistema de tipo socialista. Por último, ciertas corrientes marxistas asumen que el socialismo podría ser seguido por el comunismo, del cual Karl Marx afirmaba que no podría ser mejorado porque no tendría contradicciones internas.

La mayoría de los anticomunistas rechaza el concepto de materialismo histórico, o al menos no cree que la aparición del socialismo y el comunismo sean inexorables tras una eventual evolución del capitalismo industrial. Algunos anticomunistas ponen en tela de juicio la idea de Karl Marx de que el estado solo desaparecerá en una auténtica sociedad comunista.

Otros críticos anticomunistas no creen que, tal como sugieren ciertas reflexiones dentro de la teoría económica marxista, que en las sociedades capitalistas, la burguesía acumulará una cantidad siempre creciente de capital y bienes, mientras que las clases más bajas se harán más y más dependientes de los clases dominantes quedando al amparo de estas al no tener más remedio que vender su fuerza de trabajo por salarios miserables. Los anticomunistas, arguyendo que esta hipótesis es equivalente a la frase "los ricos se harán cada vez más ricos y los pobres más pobres", señalan el incremento general del nivel de vida en los países industrializados de Occidente como prueba de que, en contra de lo predicho por Karl Marx, tanto los ricos como los pobres se enriquecen de forma constante. Hay, sin embargo, un ataque comunista a esta objeción. Se basa un argumento anticipado en el libro de Lenin "El imperialismo, fase superior del capitalismo". En dicho libro Lenin predecía, a la vista del ascenso del imperialismo de principios del siglo XX, que la lucha de clases adquiriría una dimensión internacional, produciendo una división internacional del trabajo en que los países más ricos y los países más pobres orientarían su producción hacia sectores diferentes. Muchos miembros de la Izquierda moderna afirman que esta tendencia se ha visto confirmada en los años recientes, y mientras las economías occidentales se desarrollan, las de los países del tercer mundo son comparativamente más pobres existiendo una brecha cada vez mayor.

Por otro lado, la crítica anticomunista no toma en cuenta las medidas sociales que se introdujeron después de la Segunda Guerra Mundial, con la introducción de la socialdemocracia y el modelo Keynesiano, que paliaron la desigualdad creciente que produce el sistema capitalista. De hecho, tras el abandono del keynesianismo y el resurgimiento del modelo liberal, o "neoliberal", la desigualdad económica entre la población de un mismo país se ha vuelto a disparar, lo cual se puede ver reflejado en prácticamente todos los países que abrazaron dicho modelo.

Los comunistas también alegan que el occidente industrializado se aprovecha inmensamente de la explotación del Tercer Mundo mediante la globalización, que la brecha entre los países capitalistas ricos y pobres (a veces llamada "brecha Norte-Sur") ha aumentado mucho en los últimos cien años, y que los países capitalistas pobres son muchos más que los ricos.

La respuesta de los anticomunistas a este argumento es señalar algunos ejemplos de países de renta baja que han logrado salir de la pobreza en las últimas décadas con sistemas capitalistas, especialmente los llamados dragones asiáticos, India e incluso la teóricamente comunista China (aunque es discutible que esos sistemas hayan sido ejemplos de capitalismo liberal, habiendo tenido el capitalismo de estado un papel importante). Los anticomunistas también citan numerosos ejemplos de regímenes comunistas del Tercer Mundo que no lograron ni desarrollo ni crecimiento económico, como el régimen de Mengistu Haile Mariam en Etiopía o el del gobierno de Corea del Norte. Algunos comunistas, como los trotkistas, aunque están de acuerdo en que el imperialismo perjudicó esos países, también dicen que Etiopía y Corea del Norte nunca fueron comunistas, que solo fueron estalinistas, es decir, que han sido gobernados por un puñado de burócratas que decían actuar por el interés del pueblo pero que realmente lo traicionaron, llegando a ser más opresivo para sus clases trabajadoras.

Ocasionalmente aprovechando corrientes de opinión públicas favorables al anticomunismo, como las que existieron durante la guerra fría en Estados Unidos, la excusa de una supuesta agresión comunista o un peligro comunista ha sido usada como pretexto para intervenciones bélicas, particularmente por parte de diversos gobiernos de Estados Unidos. A este respecto la "amenaza comunista" se usó infundadamente para justificar:






</doc>
<doc id="29825" url="https://es.wikipedia.org/wiki?curid=29825" title="Antifascismo">
Antifascismo

El antifascismo es la oposición y resistencia a ideologías, organizaciones, gobiernos y personas de carácter totalitario, autoritario y antidemocrático, que se asocien con el fascismo en cualquiera de sus vertientes: nazismo, franquismo o fascismo propiamente dicho.

En el ámbito de los activistas políticos, el antifascismo ha servido en determinados momentos históricos de lugar de encuentro y colaboración entre organizaciones de izquierda, marxistas-leninistas, el movimiento obrero, anarquistas, y militantes de partidos liberales, democráticos y de centroderecha. Lo que unió a estos grupos fue su oposición al ejercicio de autoritarismo o represión del gobierno o formas de discriminación como la homofobia, el sexismo, el racismo y supresión de libertades.

En Italia, la OVRA, la organización para la vigilancia y la represión del antifascismo, fundada en 1927, era la policía secreta del Reino de Italia.

Uno de los movimientos antifascistas más importantes se produjo durante 1936-1939 en la Guerra Civil Española cuando personas de decenas de nacionalidades lucharon en las Brigadas Internacionales, creadas bajo órdenes de la Internacional Comunista, contra la facción del ejército español que se había sublevado bajo el liderazgo de Franco contra la Segunda República del Frente Popular.

Tras la victoria del franquismo en España se produjo un movimiento de resistencia antifascista denominado maquis.

Durante la Segunda Guerra Mundial muchos de los grandes movimientos de resistencia ante la ocupación nazi eran de carácter antifascista. Muchos de los que fueron capturados por los nazis fueron encerrados y asesinados en campos de concentración como Buchenwald o Dachau.

Durante la Guerra Fría, las autoridades de la República Democrática Alemana (socialista) llamaron al Muro de Berlín la «Muralla Antifascista» .

En Europa, también se han declarado como antifascistas muchos de los grupos terroristas como ETA, Resistência Galega, EGGC, Terra Lliure o los GRAPO en España, los alemanes de la Fracción del Ejército Rojo o las Brigadas Rojas italianas.

Grupos de extrema izquierda en ocasiones se han organizado con el nombre de Acción Antifascista o Antifa. La primera organización con este nombre fue un grupo dentro de la organización paramilitar Frente Rojo de Combate —a su vez brazo armado del Partido Comunista de Alemania— que estuvo activo entre 1923 y 1933.

En la década de 1980 en la República Federal Alemana surgió un movimiento político extraparlamentario y de combate callejero contra lo que su discurso político determine como «fascismo», con el mismo nombre e insignias del antiguo grupo paramilitar de los años 20. Desde entonces una red de grupos laxamente vinculados ha adoptado el mismo nombre, insignias y tácticas en Europa Central, Escandinavia y Estados Unidos. Usualmente están formados por militantes comunistas o marxistas, progresistas y anarcocomunistas. En mayo de 2020 el gobierno de Estados Unidos anunció que declarará organización terrorista a la red Antifa.



</doc>
<doc id="29826" url="https://es.wikipedia.org/wiki?curid=29826" title="Potasio">
Potasio

El potasio es un elemento químico de la tabla periódica cuyo símbolo químico es K (del latín "Kalium" y del árabe. القلية, DMG al-qalya, "ceniza de plantas"), cuyo número atómico es 19. Es un metal alcalino de color blanco-plateado, que abunda en la naturaleza en los elementos relacionados con el agua salada y otros minerales. Se oxida rápidamente en el aire, es muy reactivo, especialmente en agua, y se parece químicamente al sodio. 

Es el quinto metal más ligero y liviano; es un sólido blando que se corta con facilidad con un cuchillo, tiene un punto de fusión muy bajo, arde con llama violeta y presenta un color plateado en las superficies expuestas al aire, en cuyo contacto se oxida con rapidez, lo que obliga a almacenarlo recubierto de aceite.

Al igual que otros metales alcalinos reacciona violentamente con el agua desprendiendo hidrógeno, incluso puede inflamarse espontáneamente en presencia de agua


Otras sales de potasio importantes son el bromuro, cianuro, hidróxido, yoduro, y el sulfato.

El ion K está presente en los extremos de los cromosomas (en los telómeros) estabilizando la estructura. Asimismo, el ion hexahidratado (al igual que el correspondiente ion de magnesio) estabiliza la estructura del ADN y del ARN compensando la carga negativa de los grupos fosfato.

La bomba de sodio es un mecanismo por el cual se consiguen las concentraciones requeridas de iones K y Na dentro y fuera de la célula —concentraciones de iones K más altas dentro de la célula que en el exterior— para posibilitar la transmisión del impulso nervioso.

Las hortalizas (brócoli, remolacha, berenjena y coliflor) judías y las frutas (los bananos y las de hueso, como aguacate, albaricoque, melocotón, cereza, ciruela), son alimentos ricos en potasio.

El descenso del nivel de potasio en la sangre provoca hipopotasemia.

Es uno de los elementos esenciales para el crecimiento de las plantas —es uno de los tres que se consumen en mayor cantidad— ya que el ion potasio, que se encuentra en la mayoría de los tipos de suelo, interviene en la respiración.

El potasio (del latín científico "potassium", y este del alemán "pottasche", ceniza de pote) nombre con que lo bautizó Humphry Davy al descubrirlo en 1807, fue el primer elemento metálico aislado por electrólisis, en su caso del hidróxido de potasio (KOH), compuesto de cuyo nombre latino, "Kalĭum", proviene el símbolo químico del potasio.

El propio Davy hacía el siguiente relato de su descubrimiento ante la "Royal Society of London" el 19 de noviembre de 1807: «Coloqué un pequeño fragmento de potasa sobre un disco aislado de platino que comunicaba con el lado negativo de una batería eléctrica de 250 placas de cobre y zinc en plena actividad. Un hilo de platino que comunicaba con el lado positivo fue puesto en contacto con la cara superior de la potasa. Todo el aparato funcionaba al aire libre. En estas circunstancias se manifestó una actividad muy viva; la potasa empezó a fundirse en sus dos puntos de electrización. Hubo en la cara superior (positiva) una viva efervescencia, determinada por el desprendimiento de un fluido elástico; en la cara inferior (negativa) no se desprendía ningún fluido elástico, pero "pequeños glóbulos de vivo brillo metálico completamente semejantes a los glóbulos de mercurio". Algunos de estos glóbulos, a medida que se formaban, ardían con explosión y llama brillante; otros perdían poco a poco su brillo y se cubrían finalmente de una costra blanca. Estos glóbulos formaban la sustancia que yo buscaba; era un principio combustible particular, era "la base de la potasa: el potasio".»

La importancia del descubrimiento radica en que confirmó la hipótesis de Antoine Lavoisier de que si la sosa y la potasa reaccionaban con los ácidos de igual modo que los óxidos de plomo y plata era porque estaban formados de la combinación de un metal con el oxígeno, extremo que se confirmó al aislar el potasio y tan solo una semana después el sodio por electrólisis de la sosa. Además, la obtención del potasio permitió el descubrimiento de otros elementos, ya que dada su gran reactividad es capaz de descomponer óxidos para combinarse y quedarse con el oxígeno; de este modo pudieron aislarse el silicio, el boro y el aluminio.

El potasio constituye del orden del 2,4 % en peso de la corteza terrestre siendo el séptimo más abundante. Debido a su solubilidad es muy difícil obtener el metal puro a partir de sus minerales. Aun así, en antiguos lechos marinos y de lagos existen grandes depósitos de minerales de potasio (carnalita, langbeinita, polihalita y silvina) en los que la extracción del metal y sus sales es económicamente viable.

El potasio debe ser protegido del aire para prevenir la corrosión del metal por el óxido e hidróxido. A menudo, las muestras son mantenidas bajo un medio reductor como el queroseno. Como otros metales alcalinos, el potasio reacciona violentamente con agua, produciendo hidrógeno. La reacción es notablemente más violenta que la del litio o sodio con agua, y es suficientemente exotérmica para que el gas hidrógeno desarrollado se encienda. Como el potasio reacciona rápidamente con aún los rastros del agua, y sus productos de reacción son permanentes, a veces es usado solo, o como NaK (una aleación con el sodio que es líquida a temperatura ambiente) para secar solventes antes de la destilación. En este papel, el potasio sirve como un potente disecante. El hidróxido de potasio reacciona fuertemente con el dióxido de carbono, debido a la alta energía del ion K. El ion K es incoloro en el agua. Los métodos de separación del potasio incluyen precipitación, algunas veces por análisis gravimétrico.

Se conocen diecisiete isótopos de potasio, tres de ellos naturales K (93,3 %), K (0,01 %) y K (6,7 %). El isótopo K, con un periodo de semidesintegración de 1,278x10 años, decae a Ar (11,2 %) estable mediante captura electrónica y emisión de un positrón, y el 88,8 % restante a Ca mediante desintegración β.

La desintegración del K en Ar se emplea como método para la datación de rocas. El método K-Ar convencional se basa en la hipótesis de que las rocas no contenían argón cuando se formaron y que el formado no escapó de ellas si no que fue retenido de modo que el presente proviene completa y exclusivamente de la desintegración del potasio original. La medición de la cantidad de potasio y Ar y aplicación de este procedimiento de datación es adecuado para determinar la edad de minerales como el feldespato volcánico, moscovita, biotita y hornblenda y en general las muestras de rocas volcánicas e intrusivas que no han sufrido alteración.

Más allá de la verificación, los isótopos de potasio se han utilizado mucho en estudios del clima, así como en estudios sobre el ciclo de los nutrientes por ser un macro-nutriente requerido para la vida.

El isótopo K está presente en el potasio natural en cantidad suficiente como para que los sacos de compuestos de potasio comercial puedan emplearse en las demostraciones escolares como fuente radiactiva.

El potasio es el catión mayor del líquido intracelular del organismo humano. Está involucrado en el mantenimiento del equilibrio normal del agua, el equilibrio osmótico entre las células y el fluido intersticial y el equilibrio ácido-base, determinado por el pH del organismo. El potasio también está involucrado en la contracción muscular y la regulación de la actividad neuromuscular, al participar en la transmisión del impulso nervioso a través de los potenciales de acción del organismo humano. Debido a la naturaleza de sus propiedades electrostáticas y químicas, los iones de potasio son más pequeños que los iones de sodio, por lo que los canales iónicos y las bombas de las membranas celulares pueden distinguir entre los dos tipos de iones; bombear activamente o pasivamente permitiendo que uno de estos iones pase, mientras que bloquea al otro. El potasio promueve el desarrollo celular y en parte es almacenado a nivel muscular, por lo tanto, si el músculo está siendo formado (periodos de crecimiento y desarrollo) un adecuado abastecimiento de potasio es esencial. Una disminución importante en los niveles de potasio sérico (inferior 3,5 meq/L) puede causar condiciones potencialmente fatales conocida como hipokalemia, con resultado a menudo de situaciones como diarrea, diuresis incrementada, vómitos y deshidratación. Los síntomas de deficiencia incluyen: debilidad muscular, fatiga, astenia, calambres, a nivel gastrointestinal: íleo, estreñimiento, anormalidades en el electrocardiograma, arritmias cardiacas, y en causas severas parálisis respiratorias y alcalosis.

La hiperkalemia, o aumento de los niveles de potasio por encima de 5,5 meq/L, es uno de los trastornos electrolíticos más graves y puede ser causado por aumento del aporte (oral o parenteral: vía sanguínea), redistribución (del líquido intracelular al extracelular) o disminución de la excreción renal. Por lo general, las manifestaciones clínicas aparecen con niveles mayores a 6,5 meq/L, siendo las principales: cardiovasculares: con cambios en el electrocardiograma, arritmias ventriculares y asístole (paro cardíaco), a nivel neuromuscular: parestesias, debilidad, falla respiratoria y a nivel gastrointestinal náuseas y vómitos.

El potasio es absorbido de forma rápida desde el intestino delgado. Entre 80 y 90 % del potasio ingerido es excretado en la orina, el resto es perdido en las heces. Los riñones mantienen los niveles normales de potasio en suero a través de su habilidad de filtrar, reabsorber y excretar potasio bajo la influencia de la hormona aldosterona. Conjuntamente con el sodio, ambos regulan el balance entre fluidos y electrolitos en el organismo, ya que son los principales cationes del líquido intracelular (potasio) y extracelular (sodio) de los fluidos corporales totales del organismo. La concentración del sodio en el plasma es cerca de 145 meq/L, mientras que la del potasio es de 3,5 a 4,5 meq/L (en plasma). El plasma es filtrado a través de los glomérulos de los riñones en cantidades enormes, cerca de 180 L/día. Diariamente el sodio y potasio ingerido en la dieta debe ser reabsorbido; el sodio debe ser reabsorbido tanto como sea necesario para mantener el volumen del plasma y la presión osmótica correctamente, mientras que el potasio debe ser reabsorbido para mantener las concentraciones séricas del catión en 4,8 meq/L (cerca de 190 miligramos) (6). La bomba de sodio debe mantenerse siempre operativa para conservar el sodio. El potasio debe ser conservado algunas veces, pero dado que las cantidades de potasio en plasma son tan pequeñas, y la concentración de potasio a nivel celular es cerca de tres veces más grande, la situación no es tan crítica para el potasio. Dado que el potasio se transporta pasivamente en respuesta a un flujo contrario al sodio, la orina nunca puede disminuir las concentraciones de potasio en suero, excepto algunas veces donde se observe una excreción activa de agua. El potasio es secretado doblemente y reabsorbido tres veces antes de que la orina alcance los túbulos colectores del riñón. A este punto usualmente se alcanza la misma concentración en plasma. Si el potasio fuese eliminado de la dieta, obligaría al riñón a una excreción mínima de potasio alrededor de 200 mg/día cuando el potasio en suero decline a 3,0 meq/L en una semana aproximadamente. La bomba de sodio/potasio es un mecanismo por el cual se consiguen las concentraciones requeridas de iones K y Na dentro y fuera de la célula —concentraciones de iones K más altas dentro de la célula que en el exterior— para posibilitar la transmisión del impulso nervioso.

La ingesta adecuada de potasio puede ser generalmente garantizada al consumir una variedad de alimentos que contengan potasio, y la deficiencia es muy rara en individuos que consuman una dieta equilibrada. Los alimentos que son fuente alta de potasio incluyen: las hortalizas (papa o patata, brócoli, remolacha, berenjena y coliflor) y las frutas (las bananas o plátanos) y las de hueso (como las uvas, albaricoque, melocotón, cereza, ciruela, etc.), son alimentos ricos en potasio.
El potasio es el tercer mineral más abundante en nuestro cuerpo. Está implicado en la reacción de los nervios, en el movimiento muscular y en su mantenimiento saludable.

Los alimentos que poseen más potasio son las judías, que aportan 1300 mg de potasio c/ 100 g; el germen de trigo, que aporta unos 842 mg de potasio c/ 100 g; el aguacate o palta, que aporta 600 mg c/ 100 g; la soja, aporta 515 mg c/ 100 g; las nueces, que aportan 441 mg de potasio c/ 100 g; la papa o patata, que aporta 421 mg de potasio c/ 100 g, y la banana o plátano, que aporta 396 mg c/ 100 g.

Las dietas altas en potasio pueden reducir el riesgo de hipertensión, y la deficiencia de potasio (hipokalemia) combinada con una inadecuada ingesta de tiamina ha producido muertes en ratones experimentales.

Las sales de potasio, al poseer sabor salado, pueden sustituir fácilmente a las de sodio en aquellas dietas donde deba restringirse este último elemento.

Los suplementos de potasio en medicina son usados en la mayoría en conjunto con diuréticos de asa y tiazidas, una clase de diuréticos que disminuye los niveles de sodio y agua corporal cuando esto es necesario, pero a su vez causan también perdida de potasio en la orina. Individuos nefrópatas o que sufran de una enfermedad renal pueden sufrir efectos adversos sobre la salud al consumir grandes cantidades de potasio. En la insuficiencia renal crónica, los pacientes que se encuentran bajo tratamiento recibiendo diálisis renal deben seguir una dieta estricta en el contenido de potasio aportado, dado que los riñones controlan la excreción de potasio y la acumulación de potasio por falla renal puede causar problemas graves como una arritmia cardiaca fatal. La hipercalemia aguda (exceso de potasio) puede ser reducida a través de tratamiento con soda vía oral, glucosa, hiperventilación y perspiración.

El potasio sólido reacciona violentamente con el agua, más incluso que el sodio, por lo que se ha de conservar inmerso en un líquido apropiado como aceite o queroseno.





</doc>
<doc id="29830" url="https://es.wikipedia.org/wiki?curid=29830" title="Esclerosis tuberosa">
Esclerosis tuberosa

La esclerosis tuberosa es una enfermedad hereditaria autosómica dominante con penetrancia completa, poco frecuente, que produce la formación de masas anormales (tumores no cancerosos) en algunos órganos del cuerpo, como pueden ser: la retina, la piel, los pulmones, los riñones y el corazón. Generalmente también suele afectar al Sistema Nervioso Central (la médula espinal y el cerebro).
Esta enfermedad está dentro de un grupo de enfermedades llamadas síndromes neurocutáneos. 
Las lesiones cerebrales de la enfermedad fueron descritas por primera vez en el año 1862, por Heinrich von Recklinghausen. Bourneville, años más tarde, hizo públicas las manifestaciones anatomo-clínicas. 
El nombre de esclerosis tuberosa se debe a los crecimientos producidos en el cerebro, en forma de raíz, que se van calcificando con la edad y se vuelven duros.

Tiene una apariencia genética similar a la linfangioleiomiomatosis que afecta casi exclusivamente a mujeres fértiles. Es por ello que esta enfermedad femenina se haya divido en dos tipos: uno con enfermedad esporádica y otro con enfermedad asociada a esclerosis tuberosa.

La Esclerosis tuberosa se da en personas de diferentes grupos étnicos y de ambos sexos. 
A nivel mundial, se afirma que hay cerca de 1 a 2 millones de individuos, y se cree que incide en 1 de cada 6000 recién nacidos. En EE. UU., existirían entre 25 000 y 40 000 casos. La incidencia se ha calculado en menos de 1 caso por 100.000 persona/año.

La esclerosis tuberosa es causada por mutaciones en dos genes (TSC1 y TSC2). Si se afecta uno de los genes puede ocurrir la enfermedad. El gen TSC1 se encuentra en el cromosoma 9 y da lugar a una proteína llamada hamartina. A diferencia del gen TSC2, que se encuentra situado en el cromosoma 16 y causa la proteína llamada tuberina. El gen TSC1 fue hallado en el 1997 y el gen TSC2 se descubrió en el año 1993. 

Se considera que estas proteínas hamartina y tuberina intervienen como supresores del crecimiento del tumor, regulando la diferenciación y la proliferación celular en las que las células nerviosas se dividen para dar lugar a las nuevas generaciones de células con características individuales.

En los últimos años, se ha detectado que el gen de la esclerosis tuberosa está enlazado al lugar de grupo sanguíneo ABO y al oncogen c-abl (situados, los dos, en el brazo largo del cromosoma 9 (9 q 34)). Se necesitan más estudios para confirmar más locus genéticos, algunos ya evidenciados: 9q34, 16q13.3.

Los síntomas de la esclerosis tuberosa pueden presentarse en el momento de nacer. Aunque en algunas personas el avance de los síntomas pueden presentarse más tarde. 
Existe variabilidad en el grado de la enfermedad, es decir, algunos pacientes presentan una forma leve de la enfermedad, otros pueden presentar discapacidades severas. En casos excepcionales, las masas anormales pueden poner en peligro la vida.
No se necesita a los dos padres para transmitirse la mutación, con un solo miembro es suficiente para que el niño sufra la enfermedad. Aun así, en la mayoría de los pacientes la Esclerosis Tuberosa se produce por nuevas mutaciones, por lo que generalmente no existe un antecedente familiar de la enfermedad, por lo que la enfermedad se obtiene a través de un proceso llamado mosaicismo gonadal (la mutación afecta a una parte de los gametos: óvulos o espermatozoides).

Se trata de unas manchas que aparecen en la piel, en forma de hoja lanceolada o en hoja de fresno.


Son tumores redondeados de color rojizo que nacen de la dermis protruyendo sobre la epidermis, con un tamaño que oscila entre una cabeza de alfiler a un guisante, suelen aparecer en torno al mentón, mejillas y nariz en forma de alas de mariposa (surcos nasogenianos). Otra localización característica de los angiofibromas son subungueales o periungueales en las manos y los pies.


Con una textura irregular como la naranja, generalmente en la región dorsal o lumbar.

Alrededor del segundo año de vida en un 80-90% de los casos. Si aparecen tempranamente, suelen cursar en forma de síndrome de West o de Lennox-Gastaut.

Trastornos mentales, trastornos del comportamiento o alteraciones psicóticas, pueden asociarse a la esclerosis tuberosa.

Los tumores ubicados en Monro tienden a crecer y se necesitan controles anuales para evaluar cirugía, el ritmo de crecimiento varía aun en un mismo paciente. Las posibilidades de recidiva están presentes, pero va disminuyendo hacia la adolescencia hasta calcificarse.
Los síntomas pueden ser tempranos o tardíos, dependiendo de la herencia del paciente, entre estos puede presentarse con convulsiones, mareos, trastornos del sueño, presión intracraneal por obstrucción, presión ocular.

Que no crecen, pero juegan un papel fundamental en las convulsiones al funcionar como pilas, (que al descargarse causan crisis o ser un factor que empeore una epilepsia preexistente) y en el comportamiento del paciente dependiendo del área del cerebro dónde se localice la mayoría.

Son lesiones que se encuentran en el cerebro con forma de tubérculo (de allí el nombre) se compone de tejido diferente, que, según su ubicación, puede causar retardo mental, autismo, percepción diferencial del entorno y lo cotidiano, estereotipias, retraso en el habla, carencia de reconocimiento corporal propio, hiperactividad (mal diagnosticada como déficit atencional) entre otros.






Las lesiones óseas suelen ser geodas de 1 a 3 mm, seudoquísticas, metacarpianas y metartasianas o falángicas, o bien zonas de hiperostosis.

El diagnóstico de esta enfermedad se basa en las manifestaciones clínicas. En aquellos individuos que cumplan con el diagnóstico clínico, es posible identificar la mutación existente en alrededor del 85% de los casos. De estos pacientes en los que la mutación se puede identificar, en un 31% de los casos la mutación se presenta en el gen TSC1, mientras que en el 69% restante dicha mutación está presente en el gen TSC2. 
La variante TSC1 presenta mutaciones en el cromosoma 9 (9q34) en el gen que codifica para la proteína hamartina.
La variante TSC2 presenta mutaciones en el cromosoma 16 (16p13) en el gen que codifica para la proteína tuberina

Aproximadamente dos tercios de los individuos afectados lo son como consecuencia de una mutación "de novo" (surge una nueva mutación que no estaba presente en los progenitores del paciente). 
También se dan casos de mosaicismo genético, en los cuales no se ha diagnosticado la enfermedad en una familia hasta que aparece un individuo afecto. En alguno de los padres del individuo afectado ,y por tanto su ascendencia (abuelos, tatarabuelos, etc), coexistirán células sanas y células mutadas que posean el alelo que causa la esclerosis tuberosa, pero su fenotipo será sano. Cuando en la descendencia cambian las proporciones de estas células, junto con otros muchos factores aún desconocidos, provocan que en sus hijo/as sí se exprese la enfermedad.

Debido a la gran variedad de síntomas, y del amplio espectro que estos pueden presentar, no existe un tratamiento específico para esta enfermedad. Así que el tratamiento se basa en tratar cada síntoma que presente la persona afectada.

En EE. UU., la Administración de Alimentos y Medicamentos (FDA) ha aprobado varios medicamentos para tratar algunas de las manifestaciones mayores de la enfermedad. El medicamento antiepiléptico vigabatrin fue aprobado en 2009 para tratar los espasmos infantiles y fue recomendado como terapia de primera línea para espasmos infantiles en niños con esclerosis tuberosa por la International TSC Consensus Conference de 2012. La hormona adrenocorticotropa fue aprobado en 2010 para tratar los espasmos infantiles, también. Everolimus fue aprobado para tratamiento de tumores ligados con la esclerosis tuberosa en el cerebro (astrocitoma subependimario de células gigantes) en 2010 y en los riñones (angiomiolipoma renal) en 2012. Everolimus también ha mostrado evidencia de eficacia en tratar la epilepsia en algunas personas con esclerosis tuberosa. En 2017, la Comisión Europea aprobó everolimus para tratar las convulsiones parciales refractarias. Según un estudio realizado en México, el uso de cannabidiol —un cannabinoide encontrado en la planta de cannabis— se ha mostrado efectivo para el tratamiento de las convulsiones. 

Intervención neurocirúgica puede reducir la severidad y frecuencia de convulsiones en pacientes con esclerosis tuberosa. La embolización y otras intervenciones cirúgicas se pueden usar para tratar angiomiolipoma renal con hemorragia aguda. Tratamientos cirúgicos para síntomas de linfangioleiomiomatosis en pacientes adultos incluye pleurodesis para prevenir neumotórax y trasplante de pulmón en casos de insuficiencia pulmonar irreversible.

Otros tratamientos incluye la dieta cetogénica para la epilepsia intractable y rehabilitación pulmonar para linfangioleiomiomatosis.

En España el Sistema Nacional de Salud dispone de una red de Centros, Servicios y Unidades de Referencia del Sistema Nacional de Salud (CSUR) los cuales nacen como respuesta a la necesidad de asegurar la cohesión territorial entre comunidades autónomas y reunir conocimientos y experiencias que ayuden a minimizar la desigualdad de acceso a los servicios de salud que tratan patologías poco comunes. El centro calificado para atender a los pacientes infantiles que presentan síndromes neurocutáneos relacionados con patologías de Esclerosis Tuberosa, designados por Resolución del Ministro de Sanidad, Servicios Sociales e igualdad, corresponde al Hospital Sant Joan de Déu en Barcelona, equipo coordinado por el Dr. Héctor Salvador, y el investigador Dr. Federico J. Ramos. El CSUR de referencia para adultos recae en Hospital U. Germans Trias I Pujol también en Barcelona.

La evolución de la esclerosis tuberosa tiene una tendencia de progreso conforme el afectado va avanzando en edad, al igual que van aumentando las alteraciones existentes. Según el órgano afectado del individuo, tendrá una evolución u otra y la edad de fallecimiento va en relación con el tamaño de los tumores. El pulmón, el riñón y el sistema nervioso central son los que determinan el pronóstico.
Desde la aparición de la enfermedad, se tienen que tomar una serie de tratamientos de estimulación precoz (psicomotricidad, fisioterapia, logopedia…) resaltar que a lo largo de su vida necesitará apoyo en lecto-escritura y practicar natación para fortalecer los músculos por la hipotonía que producen las crisis epilépticas.
En las diversas patologías, a parte de las crisis epilépticas, la más grave es el problema de conducta, por ello también es necesario apoyo psicológico para la orientación de sus familiares.

A lo largo de la evolución se realizarán una serie de intervenciones, electroencefalograma si hay crisis epilépticas, su periodicidad dependerá del grado de las crisis.
TAC craneal cada cinco años, para un correcto control de los nódulos subependimarios y de su localización en relación al agujero de Monro.
RM cerebral, en caso de que se plantee la exéresis quirúrgica de algún túber cerebral cortical, pues esta exploración define mejor las estructuras cerebrales que el TAC. 
Y por último, se hará psicometría y cuantificación del cociente intelectual, especialmente en niños con problemas escolares o en el momento de comenzar la escuela, para situarles en el nivel educativo adecuado.

Es preciso efectuar una vigilancia con práctica de controles periódicos, para detectar precozmente la aparición de complicaciones tumorales. 
Se ha señalado que la edad media del fallecimiento de estos pacientes se situaba en torno a los 24 años (Webb y Cols, 1996) pero otros estudios (Jancar, 1996) indican que su longevidad se ha incrementado en los últimos tiempos.

El tratamiento más efectivo de esta entidad es su prevención.

En caso de antecedentes familiares se recomienda el consejo genético. Si uno de los padres se encuentra afectado, la posibilidad de transmitir la enfermedad se estima en el 50%. se debería advertir a la pareja del riesgo que conlleva la procreación mas no desaconsejarla, debido a que esto solo es decisión de la pareja. 

Existe también la disponibilidad de diagnóstico prenatal de mutaciones conocidas en el ADN, pero ya que la enfermedad aparece frecuentemente como mutaciones nuevas, raramente se puede prevenir.

Esta enfermedad sigue una herencia autosómica dominante. La descencencia de un individuo portador de la mutación tendrán, por lo tanto, un 50% de probabilidad de heredar la mutación. En aquellos embarazos en los que exista una alto riesgo de que el futuro bebé padezca la enfermedad, es posible recurrir al diagnóstico prenatal. Sin embargo, esto es sólo posible en aquellos casos en los que la mutación causante del gen haya sido previamente identificada en algún miembro. 
El mosaicismo también provoca complicaciones al hacer un diagnóstico prenatal en la amniocentesis, por los posibles diferencias que haya entre las células que haya muestra tomada, y la cantidad de células mutadas del feto. Además, el fenotipo sano o enfermo que pueda expresar el nuevo individuo no se puede determinar con seguridad aun conociendo la proporción de células mutadas.




</doc>
<doc id="29833" url="https://es.wikipedia.org/wiki?curid=29833" title="Azufre">
Azufre

El azufre es un elemento químico de número atómico 16 y símbolo S (del latín "sulphur"). Es un no metal abundante con un color amarillo característico. Dicho elemento es generado en estrellas masivas en las que predominan temperaturas que provocan la fusión entre un núcleo de silicio y otro de helio en un proceso denominado nucleosíntesis de supernovas.

El azufre se encuentra en forma nativa en regiones volcánicas y en sus formas reducidas formando sulfuros y sulfosales o bien en sus formas oxidadas como sulfatos. Es un elemento químico esencial constituyente de los aminoácidos cisteina y metionina y, por consiguiente, necesario para la síntesis de proteínas presentes en todos los organismos vivos. Se usa principalmente como fertilizante pero también en la fabricación de pólvora, laxantes, fósforos e insecticidas.

Este no metal tiene un color amarillento fuerte, amarronado o anaranjado y arde con llama de color azul, desprendiendo dióxido de azufre. Es insoluble en agua pero se disuelve en disulfuro de carbono y benceno. Es multivalente, y son comunes los estados de oxidación -2, +2, +4, +6.

En todos los estados (sólido, líquido y gaseoso): según los químicos presenta formas alotrópicas cuyas relaciones no son completamente conocidas. Las estructuras cristalinas más comunes son el octaedro ortorrómbico (azufre α) y el prisma monoclínico (azufre β), siendo la temperatura de transición de una a otra de 96 °C; en ambos casos el azufre se encuentra formando moléculas de S con forma de anillo, y es la diferente disposición de estas moléculas la que provoca las distintas estructuras cristalinas. A temperatura ambiente, la transformación del azufre monoclínico en ortorrómbico, es más estable y muy lenta.

Al fundir el azufre, se obtiene un líquido que fluye con facilidad formado por moléculas de S. Sin embargo, si se calienta, el color se torna marrón algo rojizo, y se incrementa la viscosidad. Este comportamiento se debe a la ruptura de los anillos y la formación de largas cadenas de átomos de azufre, que pueden alcanzar varios miles de átomos de longitud, que se enredan entre sí disminuyendo la fluidez del líquido; el máximo de la viscosidad se alcanza en torno a los 200 °C. Enfriando rápidamente este líquido viscoso se obtiene una masa elástica, de consistencia similar a la de la goma, denominada «azufre plástico» (azufre γ) formada por cadenas que no han tenido tiempo de reordenarse para formar moléculas de S; transcurrido cierto tiempo la masa pierde su elasticidad cristalizando en el sistema rómbico. Estudios realizados con rayos X muestran que esta forma deforme puede estar constituida por moléculas de S con estructura de hélice espiral.

En estado vapor también forma moléculas de S, pero a 780 °C ya se alcanza el equilibrio con moléculas diatómicas y por encima de aproximadamente 1800 °C la disociación es completa y se encuentran átomos de azufre.

Además de en trozos, barras o polvo grueso, existe en el mercado una presentación en forma de polvo muy fino, llamada "Flor de azufre", que puede obtenerse por precipitación en medio líquido o por sublimación de su vapor sobre una placa metálica fría. 

Estas son sus reacciones: S+Zn=ZnS, 2Al+3S=Al²S³, S+O²=SO², 6S+HNO³=H²SO⁴+6NO²+2H²O

El azufre se usa en multitud de procesos industriales, como la producción de ácido sulfúrico para baterías, la fabricación de pólvora y el vulcanizado del caucho. 

Los sulfitos se usan para blanquear el papel y en fósforos. El tiosulfato de sodio o amonio se emplea en la industria fotográfica como «fijador» ya que disuelve el bromuro de plata; y el sulfato de magnesio (sal de Epsom) tiene usos diversos como laxante, exfoliante, o suplemento nutritivo para plantas.

También el azufre se emplea en la industria enológica como antiséptico. Uno de sus principales usos es como anhídrido sulfuroso.

El azufre tiene usos como fungicida y en la manufactura de fosfatos fertilizantes.

El azufre (del latín "sulphur", "sulfŭris", vinculado con el sánscrito "śulbāri") es conocido desde la Antigüedad, y ya los egipcios lo utilizaban para purificar los templos.

En el "Génesis" (19,24), los hebreos decían que Dios (Yahvé) hizo llover sobre Sodoma y Gomorra azufre y fuego desde el cielo.

Homero recomendaba, en el siglo IX a. C., evitar la pestilencia mediante la quema de azufre ("zeio" en griego, relacionado con "zeos-Zeus").

Según el "Diccionario sánscrito-inglés" (1899) de Monier Monier-Williams, en sánscrito al azufre se lo llamaba "śulbāri" (pronunciado "/shulbári/"), siendo "śulba" o "śulva:" ‘cobre’, y "a-rí" o "a-rís:" ‘enemigo, envidioso’ (lit. ‘no liberal’).

En se dice que el diablo será lanzado a un lago de fuego y azufre.

Durante toda la Edad Media se vinculó a Satanás con los olores de los gases sulfurosos que se desprendían de los volcanes, que se suponían eran entradas a los infiernos subterráneos).

El azufre es un elemento muy abundante en la corteza terrestre, se encuentra en grandes cantidades combinado en forma de sulfuros como (pirita y galena) y de sulfatos como (yeso). 
En forma nativa se encuentra en las cercanías de aguas termales, zonas volcánicas y en minas de cinabrio, galena, esfalerita y estibina, y en Luisiana (Estados Unidos, primer productor mundial) se extrae mediante el proceso Frasch consistente en inyectar vapor de agua sobrecalentado para fundir el azufre que posteriormente es bombeado al exterior utilizando aire comprimido.También se obtiene separándolo del gas natural, si bien su obtención anteriormente era a partir de depósitos de azufre puro impregnado en cenizas volcánicas (Italia, y más recientemente Argentina).

También está presente, en pequeñas cantidades, en combustibles fósiles (carbón y petróleo) cuya combustión produce dióxido de azufre que combinado con agua produce la lluvia ácida; para evitarlo las legislaciones de los países industrializados exigen la reducción del contenido de azufre de los combustibles, constituyendo este azufre, posteriormente refinado, un porcentaje importante del total producido en el mundo. También se extrae del gas natural que contiene sulfuro de hidrógeno que una vez separado se quema para obtener azufre:

El color distintivo de Ío, la luna volcánica de Júpiter, se debe a la presencia de diferentes formas de azufre en estado líquido, sólido y gaseoso. El azufre se encuentra, además, en varios tipos de meteoritos, y se cree que la mancha oscura que puede observarse cerca del cráter lunar Aristarco puede ser un depósito de azufre.

La variedad de compuestos azufrados son consecuencia de una gran variedad de posibles estados de oxidación del átomo de azufre. En la Tabla se recojen algunos ejemplos de familias de compuestos azufrados, en función del estado de oxidación del azufre.

El olfato humano en general tiene una sensibilidad muy alta a los compuestos con azufre, con olores que resultan ser desagradables, de forma que detecta estos compuestos incluso cuando se encuentran en cantidades muy pequeñas. Así, por ejemplo, los olores a los que da lugar la descomposición de la materia orgánica tienen su origen en que en su composición se encuentran compuestos azufrados, tales como proteínas que contienen aminoácidos con azufre (metionina, cisteina, cistina), que contienen azufre. 

El azufre disuelto en agua es ácido (pK = 7,00, pK = 12,92) y reacciona con los metales. Los sulfuros metálicos se encuentran en la naturaleza, sobre todo el de hierro (pirita) que puede presentar resistencia negativa y la galena, sulfuro de plomo que es un semiconductor natural que fue usado como rectificador.

El nitruro de azufre polímero (SN), sintetizado en 1975 por Alan G. MacDiarmid y Alan J. Heeger, presenta propiedades metálicas, a pesar de estar constituido por no metales, e inusuales propiedades eléctricas y ópticas. Este trabajo sirvió de base para el posterior desarrollo, con Hideki Shirakawa, de plásticos conductores y semiconductores que motivó la concesión del Nobel de Química, en 2000, a los tres investigadores.

Los óxidos más importantes son el dióxido de azufre, SO (formado por la combustión del azufre) que en agua forma una solución de ácido sulfuroso, y el trióxido de azufre, SO, que en solución forma el ácido sulfúrico; siendo los sulfitos y sulfatos las sales respectivas.

Se conocen 25 isótopos del azufre, de los cuales cuatro son estables: S-32 (95,02 %), S-33 (0,75 %), S-34 (4,21 %) y S-36 (0,025 %). Aparte del S-35, formado al incidir la radiación cósmica sobre el argón-40 atmosférico y que tiene un periodo de semidesintegración de 87 días, los demás isótopos radiactivos son de vida corta.

El disulfuro de carbono, el sulfuro de hidrógeno (comúnmente conocido como ácido sulfhídrico), y el dióxido de azufre deben manejarse con precaución.

El ácido sulfhídrico y algunos de sus derivados, los mercaptanos, son muy tóxicos, pudiendo llegar a provocar la muerte en el hombre a concentraciones en el ambiente muy reducidas (del mismo orden de las del ácido cianhídrico, usado en las ejecuciones con cámara de gas en Estados Unidos, o mucho más bajas que las del monóxido de carbono para provocar la muerte, y que es origen de numerosas intoxicaciones mortales con calefactores por combustión en lugares mal ventilados). Aunque provisto de "propiedad de aviso" por ser muy maloliente incluso en concentraciones muy por debajo de la que provoca la muerte, se ha de tener en cuenta que cuando su concentración se incrementa el sentido del olfato rápidamente se satura o se narcotiza, "desapareciendo" el olor, por lo que a las víctimas potenciales de la exposición les puede pasar desapercibida su presencia en el aire hasta que se manifiestan sus efectos, posiblemente mortales.

Igual que sucede con las sales del ácido cianhídrico, los cianuros, las sales del ácido sulfhídrico, los sulfuros, han de manejarse con sumo cuidado, evitando que puedan entrar en contacto con ácidos o disoluciones ácidas (incluso ligeramente ácidas), que darían lugar a la emanación del tóxico ácido sulfhídrico.

El dióxido de azufre reacciona con el agua atmosférica para producir la lluvia ácida. Irrita las mucosidades y los ojos y provoca tos al ser inhalado.

Los vapores del ácido sulfúrico pueden provocar hemorragias en los pulmones, llenándolos de sangre con la consiguiente asfixia.

En la orfebrería el uso del azufre está ampliamente extendido, en particular para la oxidación de la plata, es decir, para la creación de la pátina (de color negro).

Existen varias técnicas para este fin; una de estas es mezclar azufre en polvo con una materia grasa -vaselina, aceite-, aplicar el ungüento sobre la pieza de plata y, mediante el uso de un soplete, calentar el metal y la mezcla, hasta que obtenga un color negruzco. Posteriormente, lavar con agua y jabón neutro. El patinado es duradero.

De igual manera se puede patinar la plata con sulfato de potasio y agua.



</doc>
<doc id="29840" url="https://es.wikipedia.org/wiki?curid=29840" title="Depresión">
Depresión

La depresión (del latín "depressio", que significa ‘opresión’, ‘encogimiento’ o ‘abatimiento’) es el diagnóstico psiquiátrico y psicológico que describe un trastorno del estado de ánimo, transitorio o permanente, caracterizado por sentimientos de abatimiento, infelicidad y culpabilidad, además de provocar una incapacidad total o parcial para disfrutar de las cosas y de los acontecimientos de la vida cotidiana (anhedonia). Los trastornos depresivos pueden estar, en mayor o menor grado, acompañados de ansiedad.

El término médico hace referencia a un síndrome o conjunto de síntomas que afectan principalmente a la esfera afectiva: como es la tristeza constante, decaimiento, irritabilidad, sensación de malestar, impotencia, frustración a la vida y puede disminuir el rendimiento en el trabajo o limitar la actividad vital habitual, independientemente de que su causa sea conocida o desconocida. Aunque ese es el núcleo principal de síntomas, la depresión también puede expresarse a través de afecciones de tipo cognitivo, volitivo o incluso somático. En la mayor parte de los casos, el diagnóstico es clínico, aunque debe diferenciarse de cuadros de expresión parecida, como los trastornos de ansiedad. La persona aquejada de depresión puede no vivenciar tristeza, sino pérdida de interés e incapacidad para disfrutar las actividades lúdicas habituales, así como una vivencia poco motivadora y más lenta del transcurso del tiempo. 

El origen de la depresión es multifactorial. En su aparición influyen factores biológicos, genéticos y psicosociales. La Psico-Neuro-Inmunología plantea un puente entre los enfoques estrictamente biológicos y psicológicos. 

Diversos factores ambientales aumentan el riesgo de padecer depresión, tales como factores de estrés psicosocial, permeabilidad intestinal aumentada, intolerancias alimentarias, inactividad física, obesidad, tabaquismo, atopia, enfermedades periodontales, sueño y deficiencia de vitamina D.

Entre los factores psicosociales destacan el estrés y ciertos sentimientos negativos (derivados de una decepción sentimental, la contemplación o vivencia de un accidente, asesinato o tragedia, el trastorno por malas noticias, pena, contexto social, aspectos de la personalidad, el haber atravesado una experiencia cercana a la muerte) o una elaboración inadecuada del duelo (por la muerte de un ser querido). 

Un elevado y creciente número de evidencias indica que los episodios depresivos se asocian con cambios en la neurotransmisión del sistema nervioso central y cambios estructurales en el cerebro, producidos a través de mecanismos neuroendocrinos, inflamatorios e inmunológicos. Existe un creciente número de pruebas que demuestran que la depresión está asociada con una respuesta inflamatoria crónica de bajo grado, aumento del estrés oxidativo y aparición de respuestas autoinmunes, que contribuyen a la progresión de la depresión. Las citoquinas pro-inflamatorias causan depresión y ansiedad, y se ha demostrado que sus niveles están elevados en los pacientes con síntomas depresivos, lo que puede explicar por qué los influjos psicosociales y los traumas agudos pueden desencadenar trastornos del estado de ánimo en personas vulnerables, como aquellas con una predisposición genética o las que tienen una mayor carga inflamatoria. El vínculo entre los procesos inflamatorios y los síntomas de la depresión se confirma por la asociación de síntomas depresivos con enfermedades inflamatorias, autoinmunes o neuroinflamatorias, tales como el asma, la enfermedad pulmonar obstructiva crónica, la enfermedad cardiovascular, la diabetes, la alergia, la artritis reumatoide, la enfermedad celíaca, la esclerosis múltiple y la enfermedad de Parkinson. 

La depresión puede tener importantes consecuencias sociales, laborales y personales, desde la incapacidad laboral (ya que se puede presentar un agotamiento que se verá reflejado en la falta de interés hacia uno mismo, o incluso el desgano para la productividad, lo cual no solo afectará a quien está pasando por la depresión, sino también a quienes lo rodean) hasta el suicidio. Otros síntomas por los cuales se puede detectar este trastorno son cambio del aspecto personal, enlentecimiento psicomotriz, tono de voz bajo, constante tristeza, llanto fácil o espontáneo, disminución de la atención, ideas pesimistas, ideas hipocondríacas y alteraciones del sueño. Desde la biopsiquiatría, a través de un enfoque farmacológico, se propone el uso de antidepresivos. Sin embargo, los antidepresivos solo han demostrado ser especialmente eficaces en depresión mayor/grave (en el sentido clínico del término, no coloquial). 

El término en psicología de conducta (ver terapia de conducta o modificación de conducta) hace referencia a la descripción de una situación individual mediante síntomas. La diferencia radica en que la suma de estos síntomas no implica en este caso un síndrome, sino conductas aisladas que pudieran si acaso establecer relaciones entre sí (pero no cualidades emergentes e independientes a estas respuestas). Así, la depresión no sería causa de la tristeza ni del suicidio, sino una mera descripción de la situación del sujeto. Pudiera acaso establecerse una relación con el suicidio en un sentido estadístico, pero tan solo como una relación entre conductas (la del suicidio y las que compongan el cuadro clínico de la depresión). Es decir, en este sentido la depresión tiene una explicación basada en el ambiente o contexto, como un aprendizaje desadaptativo.

Los principales tipos de depresión son el trastorno depresivo mayor, el trastorno distímico, el trastorno ciclotímico, el trastorno afectivo estacional y la depresión bipolar. El trastorno depresivo mayor, el distímico y ciclotímico son las formas más comunes de depresión, el trastorno distímico y ciclotímico son más crónicos, con una tristeza persistente durante al menos dos años. El trastorno afectivo estacional tiene los mismos síntomas que el trastorno depresivo mayor, en lo que difiere es que se produce en una época del año, suele ser el invierno. La depresión bipolar es la fase depresiva de un trastorno llamado trastorno bipolar.

Los criterios de clasificación diagnósticos pueden recoger con bastante aproximación el espectro de presentación de los síntomas depresivos. La clasificación está detallada y explicada en el apartado Cuadro clínico.

Conocida en sus inicios con el nombre de melancolía (del griego clásico μέλας ‘negro’ y χολή ‘bilis’), la depresión aparece descrita o referenciada en numerosos escritos y tratados médicos de la Antigüedad. El origen del término se encuentra en Hipócrates, aunque hay que esperar hasta el año 1725, cuando el británico Richard Blackmore rebautiza el cuadro con el término actual de depresión. Hasta el nacimiento de la psiquiatría moderna, su origen y sus tratamientos alternan entre la magia y una terapia ambientalista de carácter empírico (dietas, paseos, música, etc.) pero, con el advenimiento de la controversia de la biopsiquiatría y el despegue de la psicofarmacología, pasa a ser descrita como acaso una enfermedad más. Su alta prevalencia y su relación con la esfera emocional la han convertido, a lo largo de la historia, en frecuente recurso artístico e incluso en bandera de movimientos culturales como el romanticismo.

Las cifras de prevalencia de la depresión varían dependiendo de los estudios, en función de la inclusión tan solo de trastornos depresivos mayores o de otros tipos de trastornos depresivos. En general, se suelen recoger cifras de prevalencia en países occidentales de aproximadamente 3 por ciento en la población general, y para el trastorno depresivo mayor, una incidencia anual del 1 al 2 por mil.

Según la OMS en el mundo hay más de 350 millones de personas con depresión.

Estudios y estadísticas parecen coincidir en que la prevalencia es casi el doble en la mujer que en el hombre, aunque no son pocos los trabajos que cuestionan esta asimétrica incidencia: existen estudios que indican que la depresión en el hombre es mucho menos admitida y su sintomatología reporta de manera diferente.

Algunos factores estresantes vitales, como el nacimiento de un hijo, las crisis de pareja, el abuso de sustancias tóxicas (principalmente alcohol) o la presencia de una enfermedad orgánica crónica se asocian con un riesgo incrementado de desarrollar un trastorno depresivo mayor. En cuanto a la asociación familiar debida a factores genéticos, la existencia de un pariente de primer grado con antecedentes de trastorno depresivo mayor aumenta el riesgo entre 1,5 y 3 veces frente a la población general.
La mayor duración del primer episodio y el mayor número de episodios en la vida de aquellos con depresión de inicio temprano se debe a la falta de detección y tratamiento oportuno en jóvenes.

El origen de la depresión es complejo, ya que en su aparición influyen factores genéticos, biológicos y psicosociales. Entre todos ellos, los factores biológicos son los que merecen especial atención, incluyendo la Psico-Neuro-Inmunología, que plantea un puente entre los enfoques estrictamente biológicos y psicológicos.

Un elevado y creciente número de evidencias indica que los episodios depresivos se asocian no solo con cambios en la neurotransmisión del sistema nervioso central, sino también con cambios estructurales en el cerebro, producidos a través de mecanismos neuroendocrinos, inflamatorios e inmunológicos. 

Algunos tipos de depresión tienden a afectar a miembros de la misma familia, lo cual sugeriría que se puede heredar una predisposición biológica. En algunas familias la depresión severa se presenta generación tras generación. Sin embargo, la depresión severa también puede afectar a personas que no tienen una historia familiar de depresión. 

Actualmente, no existe un perfil claro de biomarcadores asociados con la depresión que pueda ser usado para el diagnóstico de la enfermedad. 

Existe un creciente número de pruebas que demuestran que la depresión está asociada con una respuesta inflamatoria crónica de bajo grado, que trae como consecuencia la activación de la inmunidad celular y una respuesta anti-inflamatoria compensatoria, caracterizada por procesos inmuno-reguladores negativos. Nuevas evidencias muestran que la depresión clínica se acompaña de un aumento del estrés oxidativo y aparición de respuestas autoinmunes, que contribuyen a la progresión de la depresión. 

La teoría que mayor interés ha suscitado entre los investigadores es la participación de las citoquinas pro-inflamatorias en los cambios del comportamiento típicos de la depresión. El aumento de las mismas y sus efectos sobre el sistema nervioso central contribuyen al desarrollo de los síntomas depresivos somáticos y neuropsicológicos. De hecho, en los estudios donde a participantes sanos se les administran infusiones de endotoxinas para desencadenar la liberación de citoquinas, aparecen los síntomas depresivos clásicos que también condicionan las características conductuales y cognitivas típicas de la depresión. Por ejemplo, aproximadamente el 25% de los pacientes que reciben interferón para el tratamiento de la hepatitis C, desarrolla una depresión importante.

Muchos estudios realizados hasta el momento han demostrado la existencia de niveles elevados de citoquinas pro-inflamatorias en el suero de los pacientes con un episodio depresivo grave. Un interesante fenómeno que confirma el vínculo entre los procesos inflamatorios y los síntomas de la depresión, es la asociación de síntomas depresivos con enfermedades inflamatorias, autoinmunes o neuroinflamatorias, tales como el asma, la enfermedad pulmonar obstructiva crónica, la enfermedad cardiovascular, la diabetes, la alergia, la artritis reumatoide, la enfermedad celíaca, la esclerosis múltiple y la enfermedad de Parkinson.

Los niveles de citoquinas pro-inflamatorias se correlacionan con la gravedad de los síntomas depresivos, mientras que el tratamiento con antidepresivos y la mejoría clínica conducen a la normalización de la concentración de citoquinas pro-inflamatorias en los pacientes con depresión. Un metaanálisis de 22 estudios que evaluó la relación entre los niveles de marcadores inflamatorios y la eficacia de los medicamentos antidepresivos en el tratamiento de la depresión, demostró una disminución de los niveles de citoquinas pro-inflamatorias, tales como la IL-1β y la IL-6, asociado al uso de fármacos antidepresivos, especialmente los inhibidores selectivos de la recaptación de serotonina.

La falta de respuesta a los tratamientos antidepresivos se asocia con niveles persistentemente elevados de marcadores inflamatorios y puede explicarse por la existencia de procesos inflamatorios crónicos, daño crónico por un aumento del estrés oxidativo y por la aparición de trastornos autoinmunes.

Cualquier factor que active la inmunidad celular y los procesos inflamatorios sin una activación concomitante de la respuesta anti-inflamatoria compensatoria, puede agravar aún más los efectos perjudiciales de los procesos inmuno-inflamatorios activados. Diferentes factores ambientales potencialmente conectados con la inflamación sistémica aumentan el riesgo de desarrollar depresión; estos incluyen factores de estrés psicosocial, mala alimentación, permeabilidad intestinal aumentada, intolerancias alimentarias, inactividad física, obesidad, tabaquismo, atopia, enfermedades periodontales, sueño y deficiencia de vitamina D.

De todos los factores psicosociales posibles, el estrés y el trauma psicológico son los mejor conocidos. Tanto el trauma agudo o los factores de estrés sub-crónico, como la exposición temprana al trauma infantil, aumentan el riesgo de desarrollar depresión y de provocar alteraciones del estado de ánimo, por su impacto sobre el sistema inmunitario y el sistema nervioso central.

El estrés psicosocial puede activar la producción de citoquinas pro-inflamatorias, tales como el factor de necrosis tumoral alfa (TNF-α) y la interleucina-1 (IL-1), y disminuir los niveles de citoquinas anti-inflamatorias, tales como la interleucina-10 (IL-10). Esto se ha demostrado en relación con el estrés agudo o crónico, tanto en animales como en humanos. Las citoquinas pro-inflamatorias causan depresión y ansiedad, lo cual puede explicar por qué los influjos psicosociales y los traumas agudos pueden desencadenar trastornos del estado de ánimo en personas vulnerables, por ejemplo, las que tienen polimorfismos de genes inmunes, bajos niveles de peptidasas o una mayor carga inflamatoria.

La evidencia de los modelos animales ha sugerido durante mucho tiempo que la exposición temprana a un trauma en la infancia puede aumentar el riesgo de un mal funcionamiento futuro de los sistemas nervioso, inmunológico y endocrino. Estos hallazgos han sido corroborados posteriormente en humanos. Los estudios que exploran la influencia del estrés en otras enfermedades inflamatorias, tales como el síndrome metabólico y las enfermedades cardiovasculares, han demostrado consistentemente tendencias similares. Todos estos resultados sugieren que el estrés que se produce en edades tempranas puede ejercer efectos persistentes durante largos períodos de tiempo, provocando un aumento de la susceptibilidad a desarrollar enfermedades somáticas y psiquiátricas, y una potencial baja respuesta a los tratamientos. No obstante, este modelo no explica completamente la vulnerabilidad a padecer enfermedades inflamatorias, sino que el empleo en la edad adulta de las respuestas aprendidas de mala adaptación al estrés parece jugar un papel fundamental. Por ejemplo, hay pruebas de que la personalidad y la forma en que una persona responde a los estresores psicosociales, tales como la tensión laboral o el estrés ante un examen, pueden contribuir a desarrollar procesos inflamatorios.

Comprender y modificar los factores de riesgo relacionados con el estrés y el estilo de vida supone un paso importante en la prevención de las enfermedades inflamatorias, tales como la depresión.

En todo el mundo, desde las últimas décadas del siglo XX, se han venido produciendo importantes cambios en los hábitos alimentarios. Los patrones dietéticos saludables, abundantes en fibra, alimentos ricos en nutrientes y ácidos grasos omega-3, han sido sustituidos por dietas altas en grasas saturadas y azúcares refinados.

Diversos componentes de la dieta pueden influir negativamente sobre el funcionamiento del sistema inmunitario y aumentar los niveles de inflamación sistémica, lo cual predispone al desarrollo de la depresión. Numerosos estudios realizados desde 2009 demuestran asociaciones inversas entre la calidad de la dieta y los trastornos mentales como la ansiedad y la depresión, tanto en adultos como en niños y adolescentes de todas las culturas.

Un patrón poco saludable ("occidental"), caracterizado por una elevada carga glucémica, rico en carbohidratos refinados y azúcares añadidos, carnes rojas y procesadas, y otros alimentos muy elaborados, se asocia con un aumento de los marcadores de inflamación. Una dieta desproporcionadamente alta en ácidos grasos omega-6 (comúnmente utilizados en los alimentos procesados), aumenta la producción de citoquinas pro-inflamatorias. Los ácidos grasos “trans” inducen inflamación de manera similar.

Por el contrario, se ha comprobado que un patrón de dieta saludable (como la dieta mediterránea), caracterizado por un mayor consumo de pescado, legumbres, frutas, verduras y granos enteros, se asocia con concentraciones plasmáticas reducidas de marcadores inflamatorios. La fibra contenida en alimentos de grano entero parece tener funciones de modulación inmune y protege contra el estrés oxidativo, que es una consecuencia de la inflamación y una característica de la enfermedad depresiva. Los ácidos grasos omega-3, que son componentes importantes de muchos alimentos saludables, tales como los mariscos, verduras de hoja verde, legumbres y nueces, actúan reduciendo la inflamación. El consumo de magnesio está inversamente relacionado con los niveles séricos de proteína C reactiva (PCR), que es un importante marcador de inflamación a nivel general. 

La carencia de ciertos nutrientes también está asociada con el desarrollo de depresión, como la disminución del contenido de licopeno en los alimentos y de la disponibilidad de selenio en las aguas subterráneas.

Existe un número creciente de evidencias acerca del papel que puede desempeñar el tracto gastrointestinal en el desarrollo de la depresión.

La permeabilidad intestinal aumentada, que consiste en una disfunción de la barrera intestinal, es uno de los factores que provocan inflamación sistémica y niveles elevados de citoquinas pro-inflamatorias. Estos hallazgos han sido documentados en pacientes con depresión.

El papel principal de la barrera intestinal consiste en regular el paso de los nutrientes y bloquear el paso tanto de los microorganismos como de los antígenos. Cuando la permeabilidad intestinal está aumentada, la barrera intestinal pierde su función protectora y pasan al torrente sanguíneo moléculas que no deberían pasar, tales como ciertas bacterias intestinales, toxinas y nutrientes incompletamente digeridos. 

Se ha demostrado que el aumento de la permeabilidad intestinal puede ser provocado por la exposición a bacterias, drogas, al estrés o bien por determinados alimentos, como la gliadina (fracción proteica del gluten). La gliadina provoca un aumento de la permeabilidad intestinal, independientemente de la base genética existente, es decir, tanto en celíacos como en no celíacos.

El paso de nutrientes incompletamente digeridos de la luz intestinal a la sangre conduce a la activación del sistema inmunitario, que puede iniciar la producción de anticuerpos específicos del tipo IgG contra nutrientes. Como consecuencia, se desarrolla hipersensibilidad a ciertos alimentos (que es de tipo retardada e IgG mediada) e inflamación, que se mantiene de forma crónica por el consumo repetido de los alimentos alergénicos. Se ha confirmado el papel de los anticuerpos específicos del tipo IgG en los enfermos celíacos, en los cuales ocurre una reacción retardada contra el gluten. 

La naturaleza retrasada de la reacción IgG mediada, en la que los síntomas aparecen horas o incluso días después de la ingesta del alimento, constituye un importante obstáculo para el diagnóstico, puesto que al paciente le resulta imposible identificar la causa de la alergia. Por el contrario, los anticuerpos del tipo IgE son responsables de reacciones alérgicas agudas (alergias IgE mediadas), que aparecen inmediatamente. 

Existe actualmente un creciente interés acerca del papel de la microbiota en el mantenimiento del funcionamiento adecuado de la barrera intestinal, el eje cerebro-intestino y los trastornos psiquiátricos. Una flora intestinal equilibrada es un factor importante en la reducción de los niveles de citoquinas pro-inflamatorias y el mantenimiento de la barrera intestinal. Esta es la razón por la cual el sobrecrecimiento bacteriano intestinal puede provocar un aumento de la permeabilidad intestinal, lo que permite a los lipopolisacáridos bacterianos penetrar en la sangre. En los pacientes con depresión, se han encontrado niveles significativamente elevados de anticuerpos del tipo IgA e IgM contra los lipopolisacáridos de bacterias Gram negativas. Esta observación es muy importante, ya que los metabolitos de ciertas bacterias penetran en la sangre, además de afectar negativamente al funcionamiento del sistema nervioso central.

El comportamiento sedentario se considera un factor de riesgo importante y novedoso para una serie de trastornos de salud, por su relación con el aumento de la inflamación, si bien no se comprende plenamente la fisiología subyacente de la conducta sedentaria. La sarcopenia (pérdida general de masa muscular y fuerza, asociada al envejecimiento o el sedentarismo) está ligada además con un deterioro cognitivo en los ancianos, que parece estar mediado por la inflamación.

Practicar ejercicio de forma habitual ha demostrado ser un tratamiento eficaz para la depresión y los trastornos de ansiedad y protege contra el desarrollo de nuevas enfermedades depresivas. El ejercicio regular reduce la inflamación sistémica a través de la adaptación homeostática y disminuye la leptina, cuyos niveles elevados también están implicados en el desarrollo de la depresión. Estos hallazgos apoyan el papel de la inflamación en la mejora del estado de ánimo inducido por el ejercicio.

Por el contrario, se ha demostrado que la inactividad física durante la infancia se asocia con un mayor riesgo de desarrollar depresión en la edad adulta.

Se ha demostrado que la obesidad, la cual constituye actualmente un problema de salud creciente que ya alcanza proporciones epidémicas, puede predisponer al desarrollo de la sintomatología depresiva y la depresión clínica. Asimismo, hay evidencias de que la depresión predispone a la obesidad de una manera bidireccional. Un reciente meta-análisis encontró que la depresión eleva en un 58% las probabilidades de desarrollar obesidad y que la obesidad aumenta en un 55% el riesgo de padecer depresión a largo plazo.

La obesidad es un estado inflamatorio y se relaciona con una amplia serie de enfermedades crónicas. Las citoquinas pro-inflamatorias están involucradas en el metabolismo de la grasa. Se ha demostrado que la obesidad, independientemente de la edad y de otros factores de confusión potenciales, aumenta los niveles de citoquinas inflamatorias (o viceversa) en todos los índices de obesidad, en particular en la obesidad abdominal. Este hecho proporciona una probable explicación de los aumentos observados en enfermedades concomitantes, como la depresión.

Se ha demostrado repetidamente que las tasas de tabaquismo son significativamente más elevadas en los pacientes que padecen depresión, si bien la explicación es compleja. Las tres posibles hipótesis son que fumar provoca el desarrollo de la depresión, que la depresión aumenta los comportamientos que inducen a fumar y que los factores compartidos de vulnerabilidad aumentan el riesgo de ambos. Una importante vía es el efecto que los miles de sustancias químicas presentes en el humo del tabaco tienen sobre el aumento de la inflamación sistémica, la exposición al estrés oxidativo y la respuesta inmune.

Los resultados de diversos estudios demuestran que los trastornos atópicos, cuya prevalencia ha ido aumentando de manera constante durante las últimas décadas, se asocian con un aumento del riesgo de padecer depresión clínica y sintomatología depresiva. La atopia es el resultado de una respuesta inflamatoria a la exposición a alérgenos comunes, lo que lleva al desarrollo de síntomas alérgicos, tales como asma, eczema o rinitis.

Las enfermedades periodontales, incluyendo la gingivitis y la periodontitis, constituyen una gran preocupación de salud pública. Está documentado que los pacientes psiquiátricos tienen peor estado de salud oral. Estudios recientes sugieren que la depresión en particular puede estar asociada con la enfermedad periodontal, si bien otros estudios no han encontrado ninguna asociación.

La enfermedad periodontal es una enfermedad inflamatoria, tanto a nivel local como a nivel sistémico, y se asocia con niveles séricos elevados de proteína C reactiva. Además, tiene un significativo valor predictivo de otras enfermedades inflamatorias. No obstante, a pesar de existir ciertas evidencias de que las infecciones periodontales pueden desempeñar un papel en algunas enfermedades neurodegenerativas, actualmente sigue habiendo escasez de pruebas acerca de si la translocación de bacterias periodontales juega un papel en algunos pacientes con depresión clínica.

Como tal, la enfermedad periodontal puede considerarse un marcador de un fallo del sistema inmunitario para luchar contra la inflamación, aumentando el riesgo de depresión a través de sus efectos inflamatorios sistémicos, que pueden potenciar los síntomas de los procesos inflamatorios y oxidativos, y por lo tanto depresivos. Por otra parte, los efectos psicosociales de la falta de higiene oral, tales como vergüenza, soledad o aislamiento, pueden predisponer al desarrollo de depresión.

La regulación del sueño es un componente esencial para la comprensión de la fisiopatología y el tratamiento de la depresión. Influye en el estado de ánimo y desempeña un papel fundamental en la regulación de diversos sistemas fisiológicos y psicológicos. Las alteraciones del sueño están relacionadas con una serie de consecuencias negativas para la salud, tales como peor calidad de vida, comorbilidad y un mayor riesgo de mortalidad; a menudo persisten más allá del episodio clínico de depresión y aumentan la vulnerabilidad a la recaída. Asimismo, los cambios en el sueño predicen la respuesta al tratamiento de la depresión y muchos tratamientos antidepresivos influyen sobre el sueño. Regular los hábitos de sueño puede constituir un factor de protección frente a los problemas de salud mental.

Los pacientes depresivos padecen frecuentemente trastornos del sueño, con tasas más altas que las de la población general.Se estima que hasta un 80-90% de las personas que sufren una depresión importante también experimentan trastornos del sueño.

Varios estudios prospectivos y epidemiológicos han sugerido que las alteraciones del sueño pueden predisponer a desarrollar posteriormente trastornos del estado de ánimo, que los síntomas del insomnio a menudo aumentan el riesgo de recaída en pacientes previamente diagnosticados con trastorno depresivo mayor y que los períodos de insomnio a menudo preceden a los episodios de manía en pacientes bipolares. 

Se ha observado una serie de cambios en el sueño en los pacientes depresivos, si bien ningún marcador del sueño aislado se asocia específicamente con la depresión. Entre ellos, los más fiables incluyen alteraciones en la continuidad (por ejemplo, retraso en la conciliación del sueño y disminución de su eficiencia), inicio más temprano de la fase de movimientos oculares rápidos del sueño (REM), aumento de la actividad, la densidad y la cantidad de la fase REM, y disminución de la fase de ondas lentas. Algunos de estos marcadores del sueño se han detectado en personas sanas con alto riesgo familiar de depresión y se asocian con el desarrollo posterior de depresión.

Tanto la privación crónica como aguda del sueño producen deficiencias en el funcionamiento del sistema inmunitario, caracterizadas por el aumento de los niveles de citoquinas pro-inflamatorias, tales como la proteína C reactiva, el factor de necrosis tumoral alfa (TFN-α) y la interleucina-6 (IL-6). Un creciente número de investigaciones sugiere que las restricciones del sueño se asocian con alteraciones neuroendocrinas y neurobiológicas similares a las observadas en los trastornos del estado de ánimo. También se cree que los aumentos en las citoquinas pro-inflamatorias TFN-α e IL-6 provocados por la privación del sueño están relacionados con una reducción de la neurogénesis adulta (nacimiento de nuevas neuronas), comparable con las alteraciones que se encuentran en los pacientes depresivos. Por este motivo, se ha propuesto que la inhibición de la neurogénesis mediante el proceso de interrupción crónica del sueño también puede ser una causa de la depresión. Tanto el tratamiento farmacológico con éxito de la depresión como la mejora del sueño nocturno se asocian con una disminución de los niveles de las citoquinas IL-6.

En la población occidental, la deficiencia de vitamina D, especialmente 25-hidroxivitamina D, está muy extendida. Los niveles séricos bajos de vitamina D están vinculados con diversos trastornos de salud, como el cáncer, la osteoporosis y la depresión.

La fisiología de la vitamina D se solapa con la fisiopatología de la depresión. Existen receptores de la vitamina D en áreas clave del cerebro; y la vitamina D juega un papel en los ritmos circadianos y el sueño, afecta a los glucocorticoides e influye en el crecimiento neuronal, la proliferación celular en el cerebro en desarrollo y la embriogénesis.

Respecto a los potenciales efectos antidepresivos de la vitamina D existen resultados contradictorios, con estudios que arrojan resultados positivos y otros, por el contrario, resultados negativos. La vitamina D posee efectos moduladores sobre la inmunidad. Se ha demostrado que la suplementación con vitamina D reduce notablemente los niveles de las citoquinas pro-inflamatorias TFN-α e IL-6 (ambas claramente asociadas con la depresión) y el estrés oxidativo (que está estrechamente vinculado con la inflamación). La vitamina D derivada de una exposición segura al sol puede reducir la inflamación sistémica.

Se ha propuesto la hipótesis de que la alta exposición a los pesticidas (incluyendo envenenamiento) que experimentan los residentes en zonas rurales y los trabajadores agrícolas, constituye un riesgo elevado para el desarrollo de trastornos psiquiátricos, tales como la depresión y conductas suicidas. Sin embargo, los datos epidemiológicos que apoyan esta teoría son muy limitados y no concluyentes.

Existen evidencias de la relación entre la alteración de los ritmos circadianos y el desarrollo de síntomas depresivos. Entre ellos, destacan los cambios de humor diurnos, el patrón de actividad diaria, la concentración alterada y la organización día/noche.

Se ha encontrado durante la historia de la medicina genes vinculados a que una persona sea o no más susceptible a la depresión:
Varios estudios describen la identificación de la región cromosómica 3p25-26, situada en el brazo corto del cromosoma 3, en la que hay un total de 214 genes.

Los análisis de GWAS identifican dos SNPs en síndromes depresivos en los cromosomas 12 y 18, rs7973260 y rs62100776 respectivamente.

El primer episodio de depresión mayor puede ocurrir en cualquier momento y, en algunos casos, en los meses previos a su presencia los pacientes pueden experimentar un conjunto de síntomas, como la ansiedad, fobias, síntomas de depresión mínimos y ataques de pánico.
También conocida como depresión mayor, depresión unipolar o depresión clínica, se da en el paciente que tiene uno o más episodios depresivos mayores. Si el episodio es único, el diagnóstico es “trastorno depresivo mayor de episodio único”, mientras si ha habido más de un episodio, se diagnostica “trastorno depresivo mayor recurrente”. El término “depresión unipolar” se opone al de “depresión bipolar” o trastorno maníaco-depresivo, e indica que el estado de ánimo se mantiene en un solo polo emocional, sin existencia de períodos de manía.
Los criterios que establecen tanto el DSM-IV como el CIE-10 para el trastorno depresivo mayor son:

"Código CIE-10": F34.1
Los criterios para este tipo de trastorno depresivo son:

Por trastorno adaptativo o depresión reactiva, se acepta la aparición de síntomas cuando esta ocurre en respuesta a un acontecimiento vital estresante, y no más allá de los tres meses siguientes a su aparición. Se habla de depresión reactiva cuando el cuadro es más grave de lo esperable o tiene mayor repercusión funcional de la que cabría esperar para ese factor estresante. Debe existir, entonces, un criterio de "desproporción" para su diagnóstico.

Se denomina trastorno depresivo no especificado a aquella situación en la que aparecen algunos síntomas depresivos, pero no son suficientes para el diagnóstico de alguno de los trastornos previos. Esta situación puede darse cuando existe un solapamiento de síntomas depresivos con un trastorno por ansiedad (síndrome ansioso-depresivo), en el contexto de un trastorno disfórico premenstrual o en cuadros de trastorno depresivo post-psicótico (residual) en la esquizofrenia.

En el DSM IV se contempla la situación de duelo como un posible detonador de una reacción depresiva que puede evolucionar hacia un trastorno. El duelo patológico constituye una reacción depresiva crónica a la pérdida de un ser amado que se extiende por más de seis meses. Si bien la reacción depresiva constituye una fase natural del proceso de duelo normal (negación, ira, negociación, depresión y aceptación) en el duelo patológico esta reacción depresiva se convierte en un trastorno debido que lejos de resolverse a través de la aceptación de la pérdida, evoluciona hacia el desarrollo de sintomatología propiamente depresiva.

Desde un punto de vista diagnóstico y terapéutico, la FEPSM señala la improcedencia de las clasificaciones actuales (depresión mayor, distima) y la utilidad y vigencia de criterios clásicos, como depresión melancólica, depresión no melancólica, depresión psicótica y depresión orgánica.

La depresión se da en la mujer con una frecuencia que es casi el doble de la del varón. 

Quizás factores hormonales podrían contribuir a la tasa más alta de depresión en la mujer. Otra explicación posible se basa en el contexto social que viven las mujeres, relativas al sexismo (género).

En particular, en relación con los cambios del ciclo menstrual, el embarazo, el aborto, el periodo de posparto, la premenopausia y la menopausia. Las hormonas sexuales femeninas (estrógenos y progesterona), debido a una existencia de menores niveles de estrógenos, parecen desempeñar por tanto un cierto papel en la etiopatogenia de la depresión.

Un estudio reciente del Instituto Nacional de Salud Mental de los Estados Unidos (NIMH) demostró que las mujeres que presentaban predisposición a padecer el síndrome premenstrual (SPM) grave se alivian de sus síntomas físicos y anímicos (por ejemplo, de la depresión) cuando se les suprimen sus hormonas sexuales a través de un tratamiento farmacológico. Si ese tratamiento se interrumpe, las hormonas se reactivan y, al poco tiempo, los síntomas vuelven. Por otro lado, a las mujeres sin SPM, la supresión temporal de las hormonas no les produce ningún efecto.

La depresión posparto es un trastorno depresivo que puede afectar a las mujeres después del nacimiento de un hijo. Está ampliamente considerada como tratable. Los estudios muestran entre un 5 y un 25 por ciento de prevalencia, pero las diferencias metodológicas de esos estudios hacen que la verdadera tasa de prevalencia no esté clara.

En recientes estudios se ha demostrado una asociación entre la aparición de depresión en mujeres de edad avanzada y un aumento de la mortalidad (por diferentes causas, principalmente por accidentes vasculares cerebrales).

Existen datos con los que se afirma que la prevalencia global de la depresión es inferior entre los varones; aunque hay estudios que manifiestan que ello se debe a que estos son menos propensos a admitir su enfermedad, siguiendo las pautas estipuladas por el sistema cultural para su género, provocando en los varones una mayor cohibición para consultar y ser diagnosticado por un especialista. En cuanto al suicidio, si bien los datos afirman que los intentos son más comunes en la mujer que en el hombre, la tasa de suicidio consumado en ellos es cuatro veces más alta que en las mujeres. Utilizando los hombres una metodología más letal para asegurar su fallecimiento.

A partir de los 70 años de edad, la tasa de suicidio en el hombre aumenta, alcanzando el nivel máximo después de los 85 años.

La depresión también puede afectar la salud física del hombre, aunque en una forma diferente a la de la mujer. Algunos estudios indican que la depresión se asocia con un riesgo elevado de enfermedad coronaria en ambos sexos. Sin embargo, solo en el varón se eleva la tasa de mortalidad debida a una enfermedad coronaria que se da junto con un trastorno depresivo.

El inicio clínico de la depresión en el anciano puede cursar con una pobre alteración del estado de ánimo. Incluso puede aparecer enmascarada con otros síntomas principales, tales como la pérdida de apetito, alteraciones de la memoria, insomnio, síntomas somáticos, ansiedad o irascibilidad. Puede simular un cuadro de demencia senil, hablándose entonces de pseudodemencia depresiva.

Cuando un anciano se deprime, a veces su depresión se considera erróneamente un aspecto natural de esa etapa de la vida. La depresión en los ancianos, si no se diagnostica ni se trata, provoca un sufrimiento innecesario para el anciano y para su familia. Cuando la persona de edad avanzada acude con el médico, puede describir únicamente síntomas físicos. Esto ocurre porque el anciano puede mostrarse reacio a hablar de su desesperanza y tristeza. La persona anciana puede no querer hablar de su falta de interés en las actividades normalmente placenteras, o de su pena después de la muerte de un ser querido, incluso cuando el duelo se prolonga por mucho tiempo.

Las depresiones subyacentes en los ancianos son cada vez más identificadas y tratadas por los profesionales de la salud. Los profesionales van reconociendo que los síntomas depresivos en los ancianos se pueden pasar por alto fácilmente. También los profesionales detectan mejor los síntomas depresivos que se deben a efectos secundarios de medicamentos que el anciano está tomando, o debido a una enfermedad física concomitante. Si se elabora el diagnóstico de depresión, el tratamiento con medicamentos o psicoterapia ayuda a que la persona deprimida recupere su capacidad para tener una vida feliz y satisfactoria. La investigación científica reciente indica que la psicoterapia breve (terapia a través de charlas que ayudan a la persona en sus relaciones cotidianas, y ayudan a aprender a combatir los pensamientos distorsionados negativamente que generalmente acompañan a la depresión) es efectiva para reducir a corto plazo los síntomas de la depresión en personas mayores. La psicoterapia también es útil cuando los pacientes ancianos no pueden o no quieren tomar medicamentos. Estudios realizados acerca de la eficacia de la psicoterapia demuestran que la depresión en la vejez puede tratarse eficazmente con psicoterapia.

La depresión en la niñez se empezó a reconocer en los años 70. El diagnóstico se acoge a los mismos criterios que en el caso de los adultos, aunque la sintomatología puede ser algo más confusa. Su prevalencia en la infancia es del 1-2 por ciento y, en la adolescencia, del 4-5 por ciento. El niño deprimido puede simular estar enfermo, rehusar a ir a la escuela, juega menos o deja de hacerlo, expresa el deseo de no querer separarse de los padres o tiene miedo de que uno de los padres fallezca. En la primera infancia pueden desarrollar síntomas atípicos como somatizaciones difusas, trastornos alimenticios, enuresis, etc. El adolescente puede expresar mal humor, disminuir el rendimiento escolar, presentar conductas desafiantes o presentar brotes de irritabilidad. En ocasiones expresa el trastorno anímico con el desarrollo de conductas de riesgo (consumo de sustancias psicotrópicas, comportamientos parasuicidas, etc.). Dado que los comportamientos normales varían de una etapa de la niñez a la otra, es a veces difícil establecer si un niño está simplemente pasando por una fase de su desarrollo o si está verdaderamente padeciendo de depresión. A veces, el niño tiene un cambio de comportamiento notorio que preocupa a los padres, o el maestro menciona que el "niño no parece ser el mismo". En esos casos puede sospecharse un trastorno depresivo.

Aunque es menos conocida y poco mencionada los bebés también pueden sufrir de depresión. La depresión anaclítica se observa principalmente en bebés separados de la madre y confiados al cuidado de instituciones. Una separación radical de la madre que dure entre tres y cinco meses es suficiente para generar en el bebé la sucesión de síntomas que caracterizan esta enfermedad. En la primera etapa hay lloriqueos, exigencias y cierto retraimiento. La segunda etapa se manifiesta con gemidos, pérdida de peso, desinterés por el entorno y retrasos en el desarrollo. En la tercera etapa hay un retraimiento total, insomnio, rigidez facial, retraso motor generalizado, pérdida de peso y resfriados frecuentes. En situaciones de privación emocional prolongada, los bebés más propensos a contraer una depresión anaclítica son aquellos que durante el primer medio año de vida pudieron establecer con sus madres una buena relación. En cualquier caso, cabe destacar que, si en el curso de la enfermedad la figura materna reaparece, los síntomas y las manifestaciones patológicas van decreciendo progresivamente y los bebés pueden recuperar, en la mayoría de los casos, el nivel de desarrollo adecuado a su edad.

El primer paso e imprescindible consiste en realizar una completa evaluación del paciente, con la realización de todas las pruebas necesarias en cada caso, en busca de posibles causas orgánicas, farmacológicas o tóxicas que simulen, provoquen o empeoren un trastorno depresivo (véase Diagnóstico diferencial). En último término, es la entrevista clínica la que ofrece los datos necesarios para el diagnóstico, cuando se cumplen los criterios establecidos más arriba.

Una buena evaluación diagnóstica debe incluir una historia médica completa. ¿Cuándo comenzaron los síntomas, cuánto han durado, cuán serios son? Si el paciente los ha tenido antes, el médico debe averiguar si los síntomas fueron tratados y qué tratamiento se dio. Quien diagnostique también debe preguntar acerca del uso de alcohol y drogas, y si el paciente tiene pensamientos de muerte o suicidio. Además, la entrevista debe incluir preguntas sobre otros miembros de la familia. ¿Algún pariente ha tenido depresión y, si fue tratado, qué tratamientos recibió y qué tratamientos fueron efectivos?

Actualmente tienen competencias en este diagnóstico los psiquiatras (licenciados en medicina, especializados en psiquiatría), psicólogos clínicos (licenciado o grado en psicología, especializado en psicología clínica) y en España, según el sistema universitario previo a los grados, también tienen competencias en ello los licenciados en psicología. 

Existen también varios cuestionarios estandarizados que pueden ayudar a discriminar si existe o no un trastorno depresivo: como la Escala de Depresión de Yesavage, la Escala de Depresión de Zung, el Inventario de Depresión de Beck, el Test de Depresión de Goldberg, el Test de Depresión de Hamilton y la Escala de Depresión del Centro de Estudios Epidemiológicos (CES-D).

La Encuesta para la Pesquisa de Depresión, mediante Tres Preguntas Orales (EPD-3PO) es un test neurosicológico ultra breve (1-4 preguntas). Algún estudio ha evaluado incluso la eficacia de dos simples preguntas para un diagnóstico rápido de elevada fiabilidad. 

El diagnóstico de los trastornos depresivos continua siendo clínico debido a la inconsistencia de marcadores biológicos, las escalas o test neuropsicológicos deben ser utilizados como métodos para optimizar la pesquisa o detección y como apoyo o guía en la entrevista clínicas, así como para la evaluación y el seguimiento objetivo de la evolución clínica de los pacientes. Los test varían según su propósito, su complejidad tanto para el profesional como para el paciente. Estos tipos de instrumentos de detección podrían ser clasificados como estándares (15 ítem o más; y/o 4 min o más), cortos (5-14 ítem o más; y/o 2-4 min) y ultra cortos (1-4 ítem o más; y/o menos 2 min). Los test ultra cortos han demostrado aceptable sensibilidad y especificidad en la detección de los trastornos depresivos. Actualmente el USPSTF (US Preventive Services Task Force) recomienda la detección de la depresión en la población general de adultos, incluidas las mujeres embarazadas y en el posparto. La detección debe implementarse con sistemas adecuados para garantizar un diagnóstico preciso, un tratamiento eficaz y un seguimiento adecuado. Aunque se han propuestos test de una sola pregunta desarrollada por Harvey Max Chochinov y sus colaboradores en 1997, los test ultra breves utilizan las preguntas de Whooley   que representan los dos ítems referidos al estado de ánimo y anhedonia del PHQ-9. Para los hispanohablantes ha sido desarrollada la "Encuesta para la Pesquisa de Depresión, mediante Tres Preguntas Orales (EPD-3PO)", por el Dr González Cáceres, J.A  que incluyó una pregunta para la detección de riesgo de suicidio, y debida a sus propiedades psicométricas han sido recomendados para la identificación de la depresión en pacientes con factores de riesgo como cardiopatías y en los adultos mayores incluidos los afectados por deterioro cognitivo. Además la EPD-3PO cuenta con una aplicación y se encuentra disponible en Google Play Store. 

Desde la terapia de conducta el objetivo del diagnóstico está en realizar una evaluación individual, cuyos datos permitirán el diseño individual de tratamiento y controlar dicho proceso. Los cuestionarios no serían usados para comparar distintos sujetos o para comprobar si alcanza una determinada puntuación, sino que permiten comparar la puntuación antes y después del tratamiento, como una medida de control sobre las variables psicológicas en cuestión.

Algunos de los trastornos que cursan frecuentemente con síntomas depresivos o pueden confundirse con una depresión incluyen:


En primer lugar, es fundamental identificar y tratar una posible causa orgánica que simule, cause o potencie el cuadro depresivo, con lo que se consigue, en una buena parte de los casos, la completa recuperación del paciente o un considerable alivio de sus síntomas.

Independientemente de que se llegue a un diagnóstico fino del tipo de trastorno depresivo, si la situación anímica supone una limitación en las actividades habituales del paciente, o una disminución de su capacidad funcional en cualquiera de sus esferas (social, laboral, etc.) se considera adecuada la instauración de un tratamiento. El fin del tratamiento es el de mejorar la situación anímica, así como restaurar un adecuado funcionamiento de las capacidades socio-laborales y mejorar, en general, la calidad de vida del paciente, disminuyendo la morbilidad y mortalidad, y evitando en lo posible las recaídas.

La selección del tratamiento dependerá del resultado de la evaluación. Existe una gran variedad de medicamentos antidepresivos y psicoterapias que se pueden utilizar para tratar los trastornos depresivos. 

Los psiquiatras tienen competencias en recetar medicamentos. Los psicólogos (y la persona específicamente formada en ello mediante la especialización sanitaria en psicología clínica) tienen competencias en psicoterapia u otras formas de intervención psicoterapeuta desde la modificación de conducta y "terapia de conducta" (véase psiquiatría y psicología). En ambos casos, dependiendo del diagnóstico del paciente (según el modelo médico) y de la gravedad de los síntomas (muy especialmente la terapia de conducta). [""]

El tratamiento con antidepresivos es el único que ha demostrado una evidencia significativa de efectividad en depresiones mayores (graves) y en depresiones psicóticas (solos o en combinación con psicoterapia. Recuérdese que "grave" refiere a un diagnóstico clínico, no al uso coloquial del término). Para el resto de depresiones, la psicoterapia se ha mostrado más eficaz que el tratamiento farmacológico.

No se han evidenciado diferencias entre la eficacia de los diferentes tipos de antidepresivos, cuyas principales diferencias estriban más en el tipo de efectos secundarios que pueden provocar. En general, los pacientes presentan mejor tolerancia a los modernos inhibidores selectivos de recaptación de serotonina que los clásicos antidepresivos tricíclicos y heterocíclicos.

La decisión de emplear uno u otro se basa en criterios como la buena respuesta a un fármaco determinado en episodios previos o en familiares de primer grado, la tolerancia a los posibles efectos secundarios, las interacciones posibles con el tratamiento habitual del paciente, el precio o la existencia de alguna contraindicación relativa, por la presencia de otra enfermedad.

Hay que tener en cuenta que el efecto antidepresivo tarda unas dos semanas en aparecer, aumentando progresivamente hasta su pico de máxima eficacia en torno a los dos meses. Aún no es conocido del todo porqué tarda este periodo.

Los principales grupos de fármacos antidepresivos son los antidepresivos tricíclicos, los inhibidores selectivos de la recaptación de serotonina (ISRS) y los inhibidores de la enzima monoamino-oxidasa (IMAO), aunque se están incorporando nuevos grupos como los inhibidores selectivos de la recaptación de serotonina y noradrenalina (como la venlafaxina),los inhibidores selectivos de la recaptación de noradrenalina (reboxetina) y los inhibidores selectivos de recaptación de serotonina y agonistas parcial del receptor 5HT1A (Vilazodona). En mayor o menor grado, todos ellos pueden presentar algunos efectos secundarios, principalmente sequedad de boca, estreñimiento, mareos, náuseas, insomnio o cefalea, siendo los de las últimas generaciones los mejor tolerados.

El tratamiento con antidepresivos debe mantenerse durante seis a doce meses, para evitar el riesgo de recaídas, aunque el efecto completo puede conseguirse al mes del inicio del tratamiento. Hay que tener en cuenta que la causa más frecuente de respuesta terapéutica débil es un mal cumplimiento del tratamiento indicado (abandonos, olvidos, etc.). Alrededor del 25 por ciento de los pacientes abandonan el tratamiento en el primer mes, un 44 por ciento en el primer trimestre, y un 60 por ciento de los pacientes dentro de los seis meses iniciales.

Recientemente se han publicado resultados que hacen pensar que la fluoxetina (Prozac) no es en realidad un medicamento tan efectivo contra la depresión como se había anunciado y creído (debido a lo que parece haber sido una manipulación comercial de los datos científicos presentados inicialmente).

Muchas formas de psicoterapia, incluso algunas terapias a corto plazo (10-20 semanas), pueden ser útiles para los pacientes deprimidos. Ayudan a los pacientes a analizar sus problemas y a resolverlos, a través de un intercambio verbal con el terapeuta. Algunas veces estos diálogos se combinan con "tareas para hacer en casa" entre una sesión y otra. Los profesionales de la psicoterapia que utilizan una terapia "de comportamiento" procuran ayudar a que el paciente encuentre la forma de obtener más satisfacción a través de sus propias acciones. También guían al paciente para que abandone patrones de conducta que contribuyen a su depresión como causa y consecuencia (mantenedores).

La última revisión sistemática sobre el tema, de Hollon y Ponniah (2010), indica que existen tres psicoterapias que han demostrado ser eficaces y específicas para el tratamiento de la depresión, que son la psicoterapia interpersonal, la psicoterapia cognitiva y la psicoterapia conductual. Otras formas de psicoterapia como la psicoterapia dinámica breve y la focalizada en la emoción, se consideran posiblemente eficaces, aunque necesitan más estudio.

El tratamiento de depresión mediante la psicoterapia asistida por psilocibina está siendo examinado. En 2018 la "Food and Drug Administration (FDA)" designó como "Breakthrough Therapy" a esta terapia con psilocibina para depresiones resistentes a los tratamientos.

En los cuadros depresivos severos, para obtener mejores resultados (en especial los que son recurrentes) por lo general se requieren medicamentos, y ocasionalmente se indica terapia electroconvulsiva (TEC) en condiciones especiales, al lado de una psicoterapia, o antes de ella. No obstante, la terapia electroconvulsiva es cada vez menos practicada en el mundo.

La psicoterapia interpersonal parte de la idea de que la depresión está provocada por muchas causas pero que se da en un contexto interpersonal, y entender ese contexto es básico para poder recuperarse de la depresión y evitar recaídas futuras.

Este tratamiento se hace en 16 sesiones de 1 hora, que comprenden tres fases. En la primera fase (1ª-3ª), el terapeuta explica en qué consiste la depresión y estudia con el paciente el entorno y el momento en que apareció. En la segunda fase (4ª-12ª), se establece un tema de conversación (o área problema) que está relacionada con el inicio o el mantenimiento de la depresión. Hay 4 temas: la no superación de la muerte de un ser querido (duelo complicado), el conflicto con un ser querido (disputa de rol), el bloqueo para adaptarse a un cambio vital (transición de rol) o la carencia de relaciones con los demás (déficit interpersonal). Durante esta fase se dialoga con el terapeuta para llegar a superar la pérdida del ser querido, resolver el conflicto, adaptarse al cambio o mejorar las relaciones con los demás. En la tercera fase (13ª-16ª), se revisan los logros y se despiden terapeuta y paciente.

La terapia cognitiva parte de la idea de que la depresión se produce por una alteración en la forma de pensar, que a su vez afecta a la forma de sentir y de comportarse. El terapeuta ayuda al paciente a analizar los errores que hay en su pensamiento (lo que se llaman distorsiones cognitivas) para que el paciente piense de una forma más realista, que le ayude a sentirse mejor y a tomar decisiones para resolver sus problemas. El terapeuta suele mandar tareas para casa y experimentos para que el paciente ponga a prueba su forma de pensar.

Bajo el término de terapia conductual, se incluyen distintas formas de tratamiento que tienen en común el análisis de la conducta del paciente. Se le ayuda al paciente a programar actividades gratificantes y a organizar su tiempo. También se le ayuda a ser más asertivo y más sociable, utilizando para ello el juego de rol y ayudándole a exponerse a situaciones sociales que suele evitar por miedos diversos.

La psicoterapia psicoanalítica elabora estrategias de afloramiento del yo intrapsíquico, oculto en el inconsciente del paciente, y origen de la sintomatología. El trastorno depresivo se expresaría como resultado de la pugna entre los mecanismos de defensa del paciente y sus impulsos. Las técnicas de psicoterapia psicodinámica breve pretenden investigar y alumbrar esos conflictos para su resolución en la esfera consciente, a través de un número limitado de sesiones.

Una instancia preventiva para la internación, sobre todo para aquellas personas que viven solas o no cuentan con un círculo social y/o familiar de apoyo para su condición, son los acompañamientos terapéuticos y casas de medio camino.

A través de estos dispositivos, los pacientes logran estar incorporados a un ambiente limpio, seguro y con un apoyo profesional que puede prevenir la internación psiquiátrica. Por otro lado, sirve de soporte para aquellos que han pasado por una y que aún no están en condiciones de volver a sus hogares. Más allá de este tipo de espacios, que deben ser adecuados, es importante que el paciente reciba un tratamiento interdisciplinario y personalizado.

Diferentes estudios apuntan a que realizar ejercicio físico puede reducir hasta un 50% los síntomas, si se practica al menos tres días a la semana, descenso comparable a la proporcionada por la terapia cognitivo-conductual o los fármacos antidepresivos. Según un estudio del Instituto Nacional de Salud Mental de Estados Unidos puede elevar hasta un 15% la eficacia de los medicamentos, facilitando una respuesta terapéutica más rápida de los antidepresivos en pacientes con trastornos importantes. Tras 16 semanas de tratamiento, el ejercicio acababa siendo tan imprescindible como los propios medicamentos. No obstante la desmotivación generalizada asociada a la depresión podría dificultar en estas personas la puesta en marcha de programas de ejercicio físico.

La falta de respuesta a los tratamientos antidepresivos se asocia con niveles persistentemente elevados de marcadores inflamatorios y puede explicarse por la existencia de procesos inflamatorios crónicos, la aparición de trastornos autoinmunes o daño crónico por un aumento del estrés oxidativo.

No obstante, la mayoría de las causas conocidas de inflamación, mencionadas en el apartado Etiología (Factores ambientales), pueden ser tratadas y prevenidas. Su identificación abre la posibilidad de influir sobre la neuroprogresión de la depresión.

Evaluar y tratar las fuentes de inflamación provocadas por factores ambientales ayuda a prevenir el riesgo de desarrollar depresión. La mayoría de ellas, aunque no todas, puede desempeñar un papel en otros problemas psiquiátricos, tales como el autismo, la esquizofrenia, la conducta bipolar y el trastorno por estrés postraumático.





</doc>
<doc id="29849" url="https://es.wikipedia.org/wiki?curid=29849" title="Macroeconomía">
Macroeconomía

La macroeconomía es la parte de la teoría económica que se encarga de estudiar los indicadores globales de la economía mediante el análisis de las variables agregadas, como el monto total de bienes y servicios producidos, el total de los ingresos, el nivel de empleo, de recursos productivos, la balanza de pagos, el tipo de cambio y el comportamiento general de los precios. 
En contraposición, la microeconomía estudia el comportamiento económico de agentes individuales, como consumidores, empresas, trabajadores e inversores.

El término "macro-" proviene del griego "makros" que significa grande, e inicialmente el sentido de los términos "macro economía" y "micro economía" pretendía guardar cierto paralelismo a la distinción física entre nivel macroscópico y nivel microscópico de estudio. En el primero importaría las propiedades emergentes asociadas a miles o millones de componentes autónomos en interacción, mientras que en el nivel "micro" se trataría de describir el comportamiento de los componentes autónomos bajo las acciones a las que estaban sometidos. Sin embargo, en el uso moderno la macro economía y la micro economía, no son términos paralelos de los términos físicos "microscópico" y "macroscópico".

El enfoque microscópico se centraba en la conducta de los agentes económicos y en los resultados previsibles de sus acciones bajo ciertos estímulos, bajo cierta hipótesis de comportamiento. Sin embargo, para una economía compleja formada por miles o millones de agentes, al igual que sucedía con la física de sistemas de millones de partículas, el enfoque "micro" es inviable. Por eso se buscó un enfoque "macro" en que se hacía abstracción de un buen número de magnitudes y hechos relacionados con los agentes económicos, y se trataban de buscar equilibrios de variables agregadas. Así el enfoque macro se concentraba en niveles de renta, tipos de interés, ahorro, consumo y gasto totales debidos a todos los agentes. La conducta agregada se modernizaba por funciones hipotéticas que se supone describen el comportamiento cualitativo aproximado de ciertas relaciones entre las macrovariables.

Al comienzo de la década de 1950 los macroeconomistas desarrollaron modelos micro-basados en el comportamiento macro-económico (tal como la función del consumo). El economista holandés Jan Tinbergen desarrolló el primer modelo macroeconómico comprensivo a nivel nacional, el cual desarrolló primero para Holanda y luego aplicó en los Estados Unidos y el Reino Unido después de la Segunda Guerra Mundial.
El primer proyecto mundial de modelo económico, el "Wharton Econometric Forecasting Associates LINK" (asociados Wharton para la predicción econométrica) fue iniciado por Lawrence Klein y fue mencionado en su llamado por el Premio de ciencias económicas en memoria de Alfred Nobel del banco de Suecia en 1980.

En la década de 1970 contribuye con partes para comprender el todo. Cuando uno aprende más sobre cada escuela económica, es posible combinar aspectos de cada una para alcanzar una síntesis informada.

El origen de la macroeconomía moderna hay que situarlo en 1936, cuando el economista británico John Maynard Keynes, publicó su obra "Teoría general del empleo, el interés y el dinero", que contenía una teoría explicativa de la Gran Depresión. Los economistas que lo habían antecedido consideraron que los ciclos económicos no podían ser evitados, mientras que Keynes expuso la posibilidad de existencia de un elevado desempleo en un determinado momento y como la política fiscal y monetaria podían utilizarse como poderosas herramientas para incrementar el nivel de la producción y el empleo en una sociedad.

La macroeconomía basa su análisis en datos derivados de la observación y la estadística, la medición y estudios de los mismos muestra el éxito o fracaso de una economía. Los principales datos que se utilizan en la macroeconomía son:


El modelo de oferta y demanda agregada es el modelo que trata de explicar la realidad económica, analizando la producción de un periodo y el nivel de precios existente a través de las funciones de oferta (O) y demanda (D) agregada y proporciona el esquema necesario para comprender la evolución de las magnitudes agregadas básicas. El modelo de "O" y "D" agregadas es el instrumento fundamental para el estudio de las fluctuaciones de la producción y del nivel de precios. Sirve para comprender las consecuencias de las distintas políticas económicas. Los componentes básico de este análisis son la demanda agregada y la oferta agregada, la demanda agregada es una representación de mercado de bienes y servicios, sus componentes son el consumo privado (C), la inversión privada (I) y el gasto público (G), en una economía abierta hay que añadir las exportaciones netas (XN) (diferencia entre exportaciones (X) e importaciones (M)) de bienes y servicios.

La oferta agregada se define como la cantidad total de bienes y servicios que se ofrecen a la venta a los diferentes precios medios posibles. Este modelo resulta de utilidad para el análisis de la inflación, el desempleo, el crecimiento y, en general, el papel que desempeña la política económica.

Los temas macroeconómicos se refieren a aspectos concretos del funcionamiento general de una economía sin considerar aspectos o problemas sectoriales particulares. En ese sentido los modelos macroeconómicos y las políticas macroeconómicas tratan de representar aspectos como el crecimiento económico, el desempleo y la evolución de los salarios, la inflación, la balanza comercial, la demanda agregada, los impuestos y los tipos de interés como aspectos principales.

La economía monetaria muestra el análisis del dinero en sus diversas funciones en un sistema económico y examina los efectos de los sistemas monetarios, incluida la regulación del dinero y los asociados a las instituciones financieras. El análisis moderno de la economía monetaria proporciona una formulación microeconómica de la demanda de dinero y estudia su influencia sobre la demanda agregada y la producción.

El crecimiento económico estudia los factores que determinan el aumento de la producción, la renta o en general de los indicadores económicos de un país o región, a largo plazo. La teoría del crecimiento económico analiza por qué unas economías crecen más deprisa que otras y cuales son los límites al crecimiento.

El desempleo es un fenómeno presente en las economías actuales y constituye unos de los problemas más importantes a los que se enfrentan, poniendo de manifiesto la incapacidad de las economías de generar situaciones en las que existan puestos de trabajo para todo aquel que desee trabajar. El estudio macroeconómico del desempleo comprende el significado del mismo en la economía, su medición, las causas que lo generan y las manifestaciones del desempleo en una sociedad.

Las economías actuales de los países se caracterizan por la gran importancia que ha adquirido su relación con el resto del mundo. El área de economía internacional de la macroeconomía estudia las consecuencias de las relaciones económicas de un país con el exterior, incluyendo el comercio internacional, el proteccionismo, las relaciones financieras internacionales, la balanza de pagos y la fijación de los tipos de cambio.

El siguiente es un ejemplo de modelo (modelo IS-LM). Consideraremos la economía de un país (o cualquier otra zona) fijándonos en las variables de su Contabilidad nacional.

Consideremos la renta o ingreso nacional ('Y') como la suma de todos los bienes y servicios producidos en un período, por ejemplo, un año. Ahora bien, algunos de esos bienes y servicios han servido para el consumo de los habitantes del país, es decir ("C") será el consumo, otros habrán servido para que las empresas puedan reponer sus necesidades de capital para producir (maquinaria, herramientas, materias primas, etcétera), esto lo llamaremos inversión ("I"); por su parte, el gobierno del país también ha intervenido en la economía consumiendo bienes y servicios para hacerlos públicos o ha intervenido mediante empresas públicas en el mercado, a lo que llamaremos gasto público ("G"). También se han importado bienes del exterior, mediante las importaciones ("M") y se han exportado al exterior, mediante las exportaciones ("X"). Entonces, podemos representar la renta como esta suma: 

La razón por la que las importaciones pasan "restando", es la siguiente: el lado de la ecuación "Y + M" representa en qué hemos usado todo el dinero empleado en el periodo, el total de producción nacional de bienes y servicios, y de importaciones, y en eso ha tenido que emplearse todo lo que se ha demandado durante el periodo: "C + I + G + X" (ya que algunas de estas variables en parte han tomado de la producción nacional y en parte de las importaciones). Por tanto "Y + M = C + I + G + X", y pasando "M" al otro lado, tenemos la relación . Podemos simplificar y llamar a las dos últimas variables "Exportaciones netas", y presentarlo así:

Hay que introducir ahora factores que influyen el consumo. El consumo se supone que será una parte de la renta disponible de los consumidores. Pero, ¿qué es la renta disponible?. Podríamos pensar que es "Y", pero como el gobierno necesita parte de esa renta para financiar el gasto público ("G"), podemos suponer que la renta disponible es la renta "Y" después de que el gobierno ha retenido una parte en forma de impuestos, y los presentamos de forma simplificada por una tasa impositiva ('t') (Con 0<= "t" <=1, si bien "t" = 0 o "t" ='1 serían casos demasiado improbables en la realidad).
Así pues, la renta disponible será (1-"t")"Y". Ahora bien, el consumidor, normalmente, no se la gastará toda en consumo, sino solo una parte, podemos suponer que por término medio todos tienen la misma propensión al consumo, y la llamamos ("c") a esa propensión. Por tanto, el Consumo privado será:

Introducimos esto en nuestra ecuación y quedaría así:

Otro supuesto que se suele hacer es que la Inversión privada se ve negativamente afectada por los tipos de interés del dinero. Cuando éstos son altos, como las empresas tienden a pedir créditos bancarios para equipar sus medios de producción, tienden a invertir menos porque invertir más significa tener que pagar más de intereses y de principal. Esto lo podemos representar así: La Inversión tiene un nivel máximo posible ("I") y disminuye linealmente con los tipos de interés, o sea:

Donde "b" representa la sensibilidad de las empresas privadas al tipo de interés bancario e i ese tipo de interés. Nuestro modelo ahora es así:

La cuestión es que en este modelo vemos que la misma variable, la renta, aparece en los dos lados de la ecuación. Esto puede interpretarse como una relación dinámica, o sea, el valor de Y en la izquierda va a depender del valor que tuvo en el pasado, en la derecha de la ecuación, y del resto de los valores de las variables. E irá cambiando periodo tras periodo.

Sin embargo, si suponemos que las otras variables no cambiaran, si los parámetros fueran constantes durante suficiente tiempo, y además el gasto público "G" estuviera exógenamente generado, entonces posiblemente la renta llegaría a no cambiar tampoco con el tiempo, alcanzando lo que se llama el valor de equilibrio. Podemos hallar este valor de equilibrio:

Con esta ecuación, también llamada curva IS, se pueden hacer diversos análisis viendo como cambiaría la renta de equilibrio si variaran los parámetros o las variables implicadas. Esta curva refleja los valores de renta ("Y") y tipo de interés ("i") para los cuales el mercado de bienes y servicios está en equilibrio. Existe sin embargo una diferencia importante si se considera que el gasto no es exógeno sino endógeno y dado por el nivel de impuestos: "G" = "tY", ya que en este caso la renta de equilibrio sería:

Obsérvese que la hipótesis de exogeneidad del gasto público no es inocente, ya que la conclusión sobre el efecto del aumento de los impuestos es contraria en y ya que calculando las derivadas siguientes se tiene:
\left(\frac{\partial Y}{\partial t}\right)_{G=tY} \ge 0,</math>
Es decir en el modelo de gasto público endógeno un aumento de los impuestos conduce a una disminución de la renta, mientras que en el modelo gasto público igual a los impuestos (no-déficit) el aumento del tipo impositivo conduce a aumentos de renta.

Existe una curva que es complementaria de esta, llamada LM. Veamos en qué consiste: Los agentes demandan dinero para poder actuar en el mercado. El dinero interesa en términos reales, no nominales. ¿Qué quiere decir esto? Que importan los niveles de precios. La oferta de dinero depende del Banco Central del país, que es el único organismo que puede emitir dinero, pero este luego deja que el resto de los bancos lo distribuyan y cobren intereses por prestarlo. En cualquier caso, la Demanda Monetaria se puede representar como el cociente de dos variables, M, la cantidad total de dinero en la economía, y P, los niveles de precios. Es decir (M/P). Esa demanda se puede suponer que depende así del resto de la economía: a mayor nivel de renta, se demandará más dinero para comprar en los mercados, pero un mayor tipo de interés disuadirá generalmente de demandar dinero, ya que este debe ser reintegrado cuando se pide como préstamo. De ahí que se represente la demanda así:

Si suponemos que la oferta y demanda monetarias están igualadas en el mercado monetario, podemos coger la ecuación anterior y despejar la renta:

Que es una curva que relaciona los niveles de renta y de tipos de interés para los que el mercado monetario está en equilibrio. Esta es la curva LM.

Si tomamos las curvas IS y LM (muy simples por ser este un modelo de ejemplo), y , y las juntamos obtenemos un sistema de dos ecuaciones con dos variables, que serán la renta y el tipo de interés:
Podemos despejar, usando los métodos para sistemas de ecuaciones lineales, y obtener los valores de "Y" e "i" en función de todos los demás parámetros y variables y usar las funciones resultantes para estudiar como variarán los niveles de renta y tipo de interés en el equilibrio cuando varíen los parámetros o las variables exógenas. Es más, podemos obtener la curva de Demanda Agregada, ya que podremos expresar la renta ("Y") dependiendo de los niveles de precios ("P"). Esta curva tendría la siguiente expresión:

Se puede reducir esta expresión a una del tipo Y=A+B/P, que muestra claramente que se trata
de una curva decreciente en "P". Si hubiéramos partido de y el resultado final habría sido:

Si además desarrolláramos una curva de oferta agregada que relacionara niveles de salarios, de trabajo, de precios y de renta producida, podríamos cruzarla con la de demanda agregada y determinar por completo la renta, los niveles de precios, de empleo y otros en cada momento dado y estudiar como las políticas monetarias y fiscales del gobierno podrían influir, por ejemplo, en conseguir los niveles adecuados de precios o de empleo.

Dato relevante: Se puede aplicar el modelo de estática comparativa de IS-LM para explicar la ley de Say que dice que la oferta iguala a la demanda.

Las autoridades económicas disponen de herramientas para alcanzar los objetivos económicos, las principales son la política monetaria que consiste en la variación de oferta monetaria, gestionando el dinero, el crédito y el sistema bancario, que pueden incidir en la producción, los precios y el empleo. La otra gran herramienta de la política económica es la política fiscal, que consiste en la utilización de los ingresos públicos, básicamente los impuestos, y los gastos públicos para alcanzar los objetivos marcados. Políticas de rentas que son el instrumento de limitación de precios y salarios.

Dado que las relaciones económicas posibles son muchas y muy complejas, se hacen supuestos simplificadores para ir estudiando a grandes rasgos lo que sucede con las distintas variables económicas implicadas cuando se producen cambios en el entorno económico estudiado. Dependiendo de los supuestos que se hagan, de qué relaciones se consideren o no, de qué tipo de efectos transmitan estas relaciones, como se haga esa transmisión, y de que se suponga qué valores del mundo real representan las variables utilizadas, se obtendrán unos modelos u otros, de ahí que exista una gran variedad de modelos que predigan o expliquen cosas diferentes acerca del funcionamiento de la macroeconomía. 

Generalmente, una escuela de pensamiento económico tiene asociados unos modelos porque esta concede más importancia a ciertas variables económicas que a otras o supone que las relaciones de esas variables con el resto son de una naturaleza diferente. De ahí la diversidad de modelos. 

Por ejemplo, existe, en el modelo IS-LM, un caso en el que supone que la demanda de dinero no depende del tipo de interés, sino sólo del nivel de renta (llamado modelo clásico). Si considerara sólo este modelo (y no el caso más general, en el que la demanda de dinero depende tanto del tipo de interés como del nivel de renta), se creería que la política fiscal no podría afectar, dentro del marco sugerido por el modelo IS-LM, al nivel de renta. Conviene también destacar otro de los grandes modelos el modelo de los precios rígidos o de Keynes. 

Para superar estas limitaciones se intentan hacer modelos en los que se incluyan cada vez más variables y se supongan relaciones de tipo más genérico entre ellas, pero tales modelos resultan cada vez más difíciles de estudiar, o de usar para predecir o explicar la economía, que en el caso de las versiones más simplificadas. Pero las versiones más simples, por su misma naturaleza, tienden a fallar y a no prever sucesos económicos o a predecir correctamente los valores que tomarán las variables económicas. Un ejemplo típico es el de políticas monetarias que, en el pasado, se tomaban para reducir la inflación: se pensaba que si se reducía la oferta monetaria en un cierto nivel, el nivel de precios disminuiría aproximadamente en un nivel previsto gracias a un modelo usado. Pero la mayor parte de las veces, no era la reducción tanta como se había deseado por los responsables de la política monetaria.

La mayor parte de las veces, los modelos macroeconómicos se crean y se estudian usando técnicas matemáticas. Cuando el modelo pretende deducir la relación cualitativa entre ciertas variables económicas frecuentemente se usan ecuaciones lineales que pretenden capturar el efecto de primer orden entre la relación de variables. Este tipo de modelos frecuentemente incluye una gran cantidad de asunciones no siempre explícitas que pueden quedar ocultas tras engañosas ecuaciones simples.

Los modelos que pretenden simular sistemas reales y no simplemente tratar de formalizar relaciones entre variables frecuentemente recurren a estudios de regresión lineal múltiple. En que lo que se pretende es averiguar el efecto de pequeños cambios porcentuales en las variables de entrada. Obviamente para grandes cambios el modelo podría resultar no lineal y las predicciones de un modelo lineal ser inválidas, ya que éstas, al igual que una serie de Taylor de primer orden, sólo predicen efectos de primer orden.

Esto también puede llegar a ser un gran modelo para la vida cotidiana de empresarios y personas que se inician en los trabajos.

Un modelo macroeconómico no serviría para demostrarnos la realidad si no se pudiera comprobar la validez de este usando los valores reales de las variables que estamos considerando, así como tampoco nos serviría de nada suponer cuales son las relaciones entre las variables y cuales son los valores de los parámetros que influyen en esas relaciones, si no podemos comprobar en qué grado esas relaciones son así y cuales serían realmente los valores de esos parámetros. Por ello, se usa una técnica estadística llamada Econometría para comprobar hasta qué punto, usando valores obtenidos de la realidad (por ejemplo, de estudios realizados por los Bancos Centrales, de informes económicos diversos de instituciones gubernamentales, y otros) se puede verificar en qué grado lo afirmado por un modelo se cumple.

Por ejemplo, si, en el marco de un modelo hipotético, hemos supuesto que el consumo ("C") depende de la renta ("Y"), los tipos de interés ("I"), la riqueza acumulada ("W") y el nivel de precios ("P"), podríamos expresar esto como:

(Lo cual sería una relación lineal). Los valores de C, Y, I, W y P tendrían que averiguarse buscando informes económicos oficiales que pudieran mostrarnos estas estadísticas y los valores que estas han tomado a lo largo del tiempo (por ejemplo, los valores que han tomado cada año durante un periodo de 10 años), pero los valores de los parámetros (cy, etcétera) tendrían que ser deducidos por el investigador usando la econometría. Esta técnica también puede informar hasta qué punto este modelo lineal es válido (o sea, que acertaría a explicar el valor de C a partir de las restantes variables) o si alguna de estas variables es irrelevante, o si resultan en conjunto insuficientes para explicar el valor de C a lo largo del periodo considerado.

En algunos casos, se intenta que los modelos macroeconómicos tengan un fundamento microeconómico, o sea, que se pueda representar las variables macroeconómicas implicadas como la suma de variables microeconómicas que fluctúan en las relaciones de equilibrio de varios modelos microeconómicos que representen a los agentes económicos que operan en el área que se está estudiando. Si no se hace así, tendríamos un modelo macroeconómico basado en creencias más o menos arbitrarías sobre el funcionamiento de la economía, lo cual es un modelo "ad-hoc".





</doc>
<doc id="29853" url="https://es.wikipedia.org/wiki?curid=29853" title="Mozilla Sunbird">
Mozilla Sunbird

Mozilla Sunbird o Sunbird es una aplicación basada en el módulo (o extensión) de calendario de Mozilla y que cumple con holgura en sus funciones de agenda, lista de tareas y calendario con alarmas.

Dispone de una interfaz de diseño sencillo, con varias posibilidades de visualización, y toda una serie de funciones que permiten gestionar la agenda diaria tanto en el aspecto laboral como personal: programación de tareas, citas, aniversarios y otros eventos importantes, herramienta de alarmas, etc.

Una de las posibilidades que ofrece es gestionar un calendario compartido para un grupo de personas. Es posible de este modo la creación de calendarios colaborativos para grupos de trabajo.

La aplicación utiliza archivos estándar del tipo .ics, los cuales están alojados en un servidor. El servidor web que servirá los calendarios deberá estar configurado en modo WEB-DAV, de esta forma será posible guardar los cambios realizado por los clientes.



</doc>
<doc id="29856" url="https://es.wikipedia.org/wiki?curid=29856" title="Queen">
Queen

Queen es una banda británica de rock formada en 1970 en Londres por el cantante y pianista Freddie Mercury, el guitarrista Brian May, el baterista Roger Taylor y el bajista John Deacon. Si bien el grupo ha presentado bajas de dos de sus miembros (Mercury, fallecido en 1991, y Deacon, retirado en 1997), los integrantes restantes, May y Taylor, continúan trabajando bajo el nombre Queen, por lo que la banda aún se considera activa.

El grupo gozó de un gran éxito en el Reino Unido con álbumes como "Sheer Heart Attack" (1974) y "A Night at the Opera" (1975). Este último llamó la atención internacionalmente, en especial por el sencillo "Bohemian Rhapsody", y colocó a Queen en un primer plano de la escena musical. Tuvieron una significativa repercusión en Estados Unidos a finales de los años 1970, ya con un numeroso repertorio de éxitos. A nivel artístico, se ha destacado su diversidad musical, sus arreglos en múltiples capas y sus armonías vocales. Es considerada una banda de gran influencia en el desarrollo del hard rock y el heavy metal, incorporando elementos del glam rock, rock progresivo, folk, ópera, blues y pop. Fue una de las primeras agrupaciones musicales en hacer de sus conciertos espectáculos muy vistosos mediante el uso de bombas de humo, "flashpots" o innovadores sistemas de luces móviles, además de promover la participación del público en los mismos, contribuyendo así al auge del arena rock. La crítica ha señalado el carisma de Freddie Mercury como una parte fundamental de sus presentaciones. A este respecto, habitualmente se han reconocido actuaciones como las del Live Aid en 1985 o el concierto del estadio de Wembley en 1986 como dos de los mejores recitales de rock de la historia. Aunque el cuarteto normalmente gozó de una gran popularidad y éxito comercial, en su momento una parte de la crítica no les tomó en serio, como por ejemplo cuando en la publicación "Rolling Stone" se criticó el álbum "Jazz" llamándolo "fascista".

Tienen en su haber un total de quince álbumes de estudio, siete álbumes en vivo y numerosas recopilaciones. Desde la muerte de Mercury y el retiro de Deacon del mundo musical, May y Taylor se han presentado juntos ocasionalmente en eventos especiales y programas televisivos como músicos invitados junto a otros artistas. Desde 2004 hasta 2009, trabajaron junto a Paul Rodgers bajo el nombre de Queen + Paul Rodgers. Con unas ventas estimadas en torno a 200 millones de discos, Queen es uno de los ; su primer álbum compilatorio, "Greatest Hits" (1981), es todavía el álbum más vendido en la historia del Reino Unido con 6 millones de copias en este país. Queen fue nombrado el decimotercer mejor artista de hard rock de todos los tiempos en una lista elaborada por VH1 ("VH1's 100 Greatest Artists of Hard Rock list"); además, en un sondeo realizado por la emisora BBC Radio 2 en 2007, Queen fue elegido como «el mejor grupo británico de todos los tiempos».

En 2018, la banda recibió el premio "Lifetime Achievement Award”, un galardón de los Grammys que rinde homenaje a aquellos artistas que hicieron a lo largo de su vida contribuciones de gran importancia en el campo de la grabación de música.

En 1968 el guitarrista Brian May y el baterista Roger Taylor se unieron con el cantante Tim Staffell conformando así el conjunto Smile, un grupo de hard rock psicodélico. En esta etapa, May ya usaba su guitarra Red Special, la cual había construido junto a su padre a los 16 años. Smile firmó con Mercury Records en 1969, y tuvo su primera sesión de grabación en los Trident Studios ese año. Staffell estudiaba en Ealing Art College con Freddie Mercury, a quien presentó a la banda. Mercury, que para entonces había integrado formaciones como Ibex, Wreckage o Sour Milk Sea, era un acérrimo fanático del grupo. En abril de 1970, Staffell abandonó Smile para unirse a otra banda, Humpy Bong. May y Taylor decidieron contactar entonces con Mercury para que fuese el nuevo cantante.

Poco tiempo después de la llegada de Mercury, el grupo decidió cambiarse el nombre por el de Queen. La idea vino de Mercury y fue apoyada por Taylor, aunque May se mostró reacio al principio. Otras ideas para posibles nombres fueron Grand Dance –que viene de una trilogía de libros que Taylor y May habían leído– o The Rich Kids, que agradaba a Taylor. Pasaron una gran cantidad de tiempo ensayando, gracias a que May era muy respetado en el Imperial College y tenía el permiso especial para usar los teatros como lugar de ensayo. Generalmente tocaban canciones de otros artistas, aunque también rescataron algunas canciones de Ibex, Wreckage y Smile. Queen realizó su primer concierto en el City Hall de Truro el 27 de junio de 1970, y poco después, el 18 de julio, actuaron en el Imperial College. En esta época, Freddie cambió su apellido artístico de Bulsara a Mercury, en honor al dios Mercurio. Durante una actuación de Queen en la escuela de mujeres de St. Helen, parte del soporte del micrófono de Freddie Mercury se soltó y cayó al suelo, quedándose con el micrófono enganchado a la primera parte del soporte. Mercury siguió con la actuación. De ahí surgió el característico bastón con micrófono que usó durante todos los conciertos desde entonces.

Durante este periodo, Queen tuvo varios bajistas. El primero fue Mike Grose, quien estuvo en los tres primeros conciertos de la banda. Barry Mitchell ejerció ese rol las once actuaciones siguientes. En enero de 1971, Mitchell decidió retirarse del conjunto. En la búsqueda de un nuevo bajista, apareció Doug Ewood Bogie, quien consideraron una buena opción al tener algo de equipos. Tras dos actuaciones en febrero en el Kingston Polytechnic, el grupo le pidió amistosamente a Doug que buscara otra banda. Seguidamente, John Harris, un amigo que ayudó al conjunto con las luces y los equipos, llevó a Taylor y May a una discoteca en Maria Assumpta Teaching College. Allí les presentó a un bajista amigo suyo, John Deacon. A Deacon, que no estaba en ninguna banda, se le ofreció una audición. Durante la prueba apreciaron su potencial como músico, además de su silencio y tranquilidad. El 1 de marzo, Deacon aceptó formar parte de Queen. El primer concierto con el nuevo bajista se celebró el 2 de julio de 1971, en Surrey, y poco después volvieron a tocar en el Imperial College e iniciaron una gira por Cornualles.

Mercury diseñó en esta época el célebre logotipo de Queen, llamado "Queen crest". La insignia combina los signos del zodiaco de los cuatro componentes del conjunto: dos leones por Leo (representando a Deacon y Taylor), un cangrejo por Cáncer (representando a May), y dos hadas por Virgo (representando a Mercury). Los leones se encuentran abrazando una letra Q, el cangrejo descansa sobre la letra, y de él se elevan unas llamaradas. Las dos hadas están debajo de cada león. Dentro de la Q hay una corona. El logotipo es eclipsado por una enorme ave Fénix. Todo el conjunto tiene una similitud con el Escudo del Reino Unido, en particular por los leones. El logotipo original, que se encuentra en el reverso del primer álbum, fue un dibujo con trazos simples, pero la versión más compleja y en color, se empleó para álbumes posteriores.

Terry Yeadon, un amigo de Brian May, estaba estableciendo un nuevo complejo de estudios en Wembley perteneciente a De Lane Lea Studios. El estudio necesitaba músicos para probar los nuevos equipos, así que May fue con Queen a probarlo. Allí realizaron sus primeros registros, los cuales fueron "Liar", "Keep Yourself Alive", "The Night Comes Down" y "Jesus", en 1971. Con estos demos el grupo empezó a buscar casas discográficas que estuviesen interesadas en su trabajo, pero ninguna se mostró interesada en un primer momento.

Aún sin un sello discográfico que les respaldara, el conjunto comenzó a frustrarse por la falta de progreso. Sin embargo, a principios de 1972, el sello Chrysalis Records les ofreció un contrato. La compañía se mostró muy interesada en firmar con Queen, pero el grupo decidió rechazar la oferta considerando que los términos no los beneficiaban. En esa época mucho personal de producción pasaba por el estudio De Lane Lea verificando las conexiones, realizando pruebas de calidad del sonido, etc. Dos ingenieros del Trident Wardour Street Studio, Roy Thomas Baker y John Anthony, vieron al grupo de casualidad y Anthony quedó impresionado con el sonido, reconociendo inmediatamente a Taylor y May, ya que él había producido su sencillo "Earth" en la época de Smile. Ellos persuadieron a sus empleados, Barry y Norman Sheffield, para que investigaran al grupo. Después de que Barry Sheffield contemplase la excentricidad de Freddie Mercury y la destacada actuación en vivo en el Forest Hill Hospital Dance, decidieron que Queen tenía que firmar con Trident lo más pronto posible. Sin embargo, el grupo consideró que el contrato que le ofrecía Trident tenía términos que no les convencían. Sin embargo, tras un concierto en el Pheasanty de Kings Road, a finales del año 1972, Queen firmó. El contrato que hizo Trident se dividía en tres acuerdos distintos: derechos de publicación, contrato de grabación y contrato de representación.

Tras firmar, Trident le ofreció al grupo un nuevo sistema de grabación y nuevos instrumentos. Brian May optó por conservar su guitarra casera. Llamaron a Jack Nelson como gerente, quien ya había dado consejos a Norman Sheffield en el pasado. Nelson llegó a mostrar las cintas de los demos de Queen a EMI, pero la negociación no funcionó. A pesar de que no había aparecido ninguna empresa para distribuir el futuro disco de Queen, el conjunto entró a los Trident Studios de los hermanos Sheffield para grabar su primer álbum a lo largo de la segunda mitad de 1972, contando además como productores con Roy Thomas Baker y John Anthony. Sin embargo, solo se les concedió aquellos espacios de tiempo en los que nadie se encontraba grabando, sobre todo en horas de la madrugada o la mañana, dando como resultado grabaciones producidas de manera casual. En otro estudio del complejo de Trident, Robin Cable estaba interpretando "I Can Hear Music", una canción de The Beach Boys. Cable le ofreció a Mercury grabar esa canción, lo cual hizo. Cuando terminaron de grabar el tema, Cable llamó a Taylor y May para que agregasen pistas de batería y guitarra. Mercury también interpretó "Goin' Back", un clásico de Dusty Springfield. Ambas se publicarían en un sencillo al año siguiente con Mercury bajo el seudónimo de Larry Lurex, pero el corte tuvo muy baja difusión. En septiembre, Trident acordó pagarle a Queen, así hubieran o no editado el álbum, cosa que sucedió. Para noviembre de 1972, el disco se había terminado de grabar, pero faltaba que Trident lo ubicara en alguna discográfica.

Llegado el año 1973, un ejecutivo de la compañía EMI, Roy Fetherstone, se encontraba en el Festival de MIDEM, celebrado en el sur de Francia. Allí mismo apareció Jack Nelson, que le hizo escuchar una cinta de Queen que llamó enseguida la atención de Fetherstone. Nelson le mintió diciéndole que varias casas discográficas estaban interesadas en el grupo. Acto seguido, Fetherstone envió un telegrama vía Trident para pedir que el grupo no firmase con nadie más. Al regresar Fetherstone, todo se puso en marcha para que Queen firmase el contrato. Un primer acuerdo fue rechazado, porque Trident reclamaba más dinero. Las negociaciones seguían sin concretarse, hasta que en marzo de 1973, Queen se dirigió a las oficinas de EMI para pactar finalmente un contrato de grabación. A continuación, Jack Nelson organizó un concierto en el Marquee, para así presentarles al director general de Elektra Records, Jack Holsten. Allí el conjunto firmó con Elektra para que el sello distribuyera sus trabajos en Estados Unidos.

En mayo de 1973, la discográfica EMI editó el disco, aunque el lanzamiento no se produjo hasta el 13 de julio. Taylor quería que el álbum se llamara "Top Fax, Pix and Info", y también habían pensado "Deary Me", pero finalmente fue denominado "Queen". Fue un álbum muy influenciado por el heavy metal y rock progresivo de la época. La producción de la contratapa es un "collage" de fotos. Los miembros de Queen habían llamado a sus amigos para que en torno a Mercury, seleccionaran las mejores imágenes. Mercury y May pasaron algunas semanas pegando fotografías. Respecto a la portada del álbum, quién realizó la captura fue el fotógrafo Doug Puddifoot, que también había tomado muchas imágenes usadas en el "collage". La portada tiene a Freddie Mercury en el escenario con su característico micrófono. El efecto púrpura se logró tan solo colocando un celuloide de dicho color en la lente de la cámara. El disco también traía la cita "En este álbum no se han usado sintetizadores". Este aviso fue puesto principalmente porque mucha gente confundía algunos efectos especiales de la guitarra en capas de Brian May con un sintetizador.

Fue en general bien recibido por los críticos. Gordon Fletcher de "Rolling Stone" dijo: "Su álbum debut es excelente", el "Daily Herald" de Chicago lo llamó: "Un debut por encima del promedio" y Greg Prato de "Allmusic" lo mencionó como: "Uno de los debuts de hard rock más subvalorados de todos los tiempos". Sin embargo, no llamó mucho la atención y el sencillo principal "Keep Yourself Alive", una composición de Brian May, vendió pobremente. El corte había sido rechazado por varias emisoras y solo Radio Luxemburgo le brindó acogida. Al poco tiempo de emitirse el disco, Queen comenzó a hacer pequeñas giras. Si bien el álbum no entró en el Top 10 de Reino Unido, la banda consiguió llamar la atención de nuevos fanáticos. Esto los motivó a entrar de nuevo en el estudio de grabación en agosto de 1973. Además, recibieron la noticia de que el grupo de moda en ese momento, Mott the Hoople, quería que hicieran de teloneros de sus espectáculos por el Reino Unido. La gira se realizó entre octubre y diciembre de 1973. El álbum alcanzó el puesto n.º 83 en los Estados Unidos.

En febrero de 1974 se conformó oficialmente el club de fanáticos de Queen, reconocido por el Libro Guinness de los récords como uno de los centros de fanáticos con más asociados en el mundo. Por esa época, Ronnie Fowler, director del departamento de promoción de EMI, invirtió 20.000 libras en costos para promover a Queen. Robin Nash, productor del programa musical "Top of the Pops", le preguntó si conocía a algún grupo para que tocase en el show, y Fowler le aconsejó llevar a Queen. El 21 de febrero de 1974, Queen actuó en el "Top of the Pops" tocando "Seven Seas of Rhye", siendo la primera vez que aparecía el grupo en televisión. Al siguiente día, Jack Nelson se apresuró en promover un sencillo por las radios. EMI apresurada y oportunamente editó el sencillo el 23 de febrero, siendo este "Seven Seas of Rhye" con "See What A Fool I've Been". El lado B no fue incluido en el disco. El sencillo produjo que el conjunto ingresara en las listas británicas, permaneciendo en estas diez semanas y obteniendo el lugar más alto en el puesto n.º 10.

Cuando justo estaba todo preparado para emitir el disco, la banda descubrió un error de tipeo en la portada e insistió en que debía ser corregido. Pero además, Reino Unido había sido víctima de la crisis del petróleo, lo que significaba que existía una semana laboral de tres días, lo que retrasó aún más el lanzamiento del álbum. El 8 de marzo de 1974, finalmente se publicó "Queen II". El álbum alcanzó el número n.º 5 en las listas británicas manteniéndose en ellas treinta semanas. La segunda entrega del conjunto, fue su primer disco de oro. En los Estados Unidos, "Queen II" escaló más posiciones que su antecesor, llegando al lugar n.º 49. "Queen II", al igual que otros álbumes, fue editado con una cara de color negro y otra de color blanco. La foto de portada fue tomada por Mick Rock, la cual serviría de inspiración un año después para el videoclip de "Bohemian Rhapsody". Es considerado como un disco de transición para la banda, por su notable diferencia con el anterior en cuanto a sonido y madurez, y por otro motivo importante: habían comenzado las voces operísticas (las cuales aún estaban ausentes en el primer álbum). Esas sobregrabaciones vocales que causaban el efecto de un coro de ópera y que tanto caracterizaría a Queen a lo largo de toda su carrera, habían empezado a tomar más peso en "Queen II".

Con el éxito del sencillo y el éxito del álbum, Queen realizó su primera gira como cabeza de cartel por el Reino Unido a lo largo de marzo de 1974. En esta etapa Freddie Mercury solía vestir con las prendas que diseñaba Zandra Rhodes. El tour iba bien, hasta que en la Universidad de Stirling, el público escocés se descontroló y comenzó a lanzar botellas y latas de cerveza. En los disturbios siguientes, dos personas resultaron heridas por arma blanca, y otras dos personas del personal de Queen fueron heridas (de baja gravedad) y hospitalizadas. Entre abril y mayo de 1974 realizaron su primera gira por los Estados Unidos, siendo teloneros de Mott the Hoople. En mayo, después de brindar seis funciones en el Gershwin Theatre de Nueva York, se le diagnosticó a Brian May hepatitis, por lo que el grupo tuvo que cancelar el resto de actuaciones.

El 11 de octubre de 1974 se publicó el primer adelanto del siguiente álbum, el sencillo "Killer Queen"/"Flick of the Wrist", ambas canciones bajo la autoría de Mercury. El sencillo fue disco de plata, y alcanzó el puesto n.º 2 en las listas inglesas, manteniéndose allí doce semanas. Fue el primer sencillo de la banda en entrar a las listas estadounidenses, alcanzando el puesto n.º 12. Casi tres semanas después, el 1 de noviembre, se lanzó el álbum "Sheer Heart Attack", segundo disco de Queen en ese año. "Sheer Heart Attack" llegó al puesto n.º 2 en las listas inglesas, quedándose allí por 42 semanas. Tras la edición del disco, el 17 de enero de 1975 apareció el sencillo "Now I'm Here"/"Lily of the Valley", que se mantuvo en las listas británicas durante siete semanas, alcanzando el puesto n.º 11. En Estados Unidos, el álbum llegó al número 12, siendo su primer álbum exitoso en ambos lados del Atlántico. La tercera entrega de Queen fue considerada por la crítica como mejor que sus antecesores. De octubre a diciembre de 1974 salieron de gira por Europa, dentro del marco del Sheer Heart Attack Tour.

Ya en 1975, emprendieron su primera gira como banda cabecera en Estados Unidos, con el apoyo masivo de Elektra. La gira se complicó, ya que a Mercury se le diagnosticó nódulos en la garganta, lo que motivó a la cancelación de algunos conciertos para así disminuir la frecuencia de los mismos. Pero el cantante se recuperó, y Queen volvió rápidamente al tour. Luego de la acalorada gira estadounidense, los cuatro miembros se dirigieron a Hawái para tomarse unas vacaciones, antes de brindar en abril su primera gira por Japón. Precisamente el 18 de abril Queen llegó al aeropuerto de Tokio, el cual se encontraba sitiado por fanáticos. El cuarteto inglés recorrió todo el país con gran éxito de público. A Freddie Mercury le había agradado la cultura de aquel país, a tal punto de convertirse en un aficionado coleccionista de antigüedades y arte japonés. En el momento en que Queen retorna a casa, muchas encuestas a lectores de revistas musicales colocaban al grupo en la cima. Asimismo, Mercury ganó el Premio Ivor Novello por "Killer Queen".

Se invirtió mucho tiempo para experimentar en el próximo álbum, probando las distintas sonoridades que se podían crear. En ese momento Freddie Mercury produjo, tocó el piano y cantó los coros para un sencillo de Eddie Howell, con un sonido similar a Queen. Hubo un punto de ruptura entre Trident y Queen, lo que provocó que cancelasen una gira por Estados Unidos que estaba prevista, para precisamente resolver los problemas. Queen firmó un acuerdo independiente, pero directamente con EMI y Elektra. El grupo se encontraba momentáneamente sin gerente. En la búsqueda, surgieron varios candidatos: Peter Grant de Led Zeppelin -la idea era que su discografía, Swan Song Records produjera al grupo-, Peter Rudge -con quien no pudieron contactar- y el mánager de Elton John, John Reid. En un principio, Reid no se encontraba seguro si de podía con otro artista musical más, pero cambió rápidamente de idea cuando se enteró de que se trataba de Queen. Reid puso cien mil libras para resolver el acuerdo, además de nombrar como abogado a Peter Beach para negociar el contrato existente con Trident, el acuerdo con aquella discográfica se disolvió en 1975.

El 31 de octubre de 1975 salió a la venta el primer sencillo del álbum, "Bohemian Rhapsody". Grabada en tres semanas, la canción no posee estribillo y consiste en seis secciones: una introducción a capela, una balada, un solo de guitarra, un segmento operístico, una sección de rock y una coda que retoma el tempo y la tonalidad de la balada introductoria. Fue su primer número uno en el Reino Unido y estuvo dieciocho meses en lista, rompiendo el récord que Paul Anka había mantenido desde 1957 con "Diana". Además, fue número nueve en los Estados Unidos, significando un importante avance. Por esta canción Mercury volvió a ganar el Premio Ivor Novello. "Bohemian Rhapsody" ha sido elegida en varias ocasiones como una de las mejores canciones de todos los tiempos. La banda decidió hacer un video como apoyo para el sencillo, considerado el primer videoclip en usar efectos especiales. Aunque muchos músicos, incluyendo ellos mismos, habían hecho con anterioridad algunos videos para acompañar sus canciones, no fue hasta después del éxito de "Bohemian Rhapsody" cuando producir videoclips para promocionar los sencillos se convirtió en una práctica regular y rentable para las discográficas.

"A Night at the Opera" vio la luz el 21 de noviembre de 1975, siendo el álbum más costoso nunca antes producido. Al igual que su predecesor, experimenta con el sonido estereofónico. Se usaron múltiples capas de guitarra de su antecesor como base y el álbum experimenta con diversos géneros como el metal de "Death on Two Legs" y "Sweet Lady", el pop de "You're My Best Friend", la música campestre en "Lazing on a Sunday Afternoon" y "Seaside Rendezvous", y el rock progresivo en "'39" y "The Prophet's Song". Todos estos elementos se conjuntan para la pista pseudo ópera "Bohemian Rhapsody", que se encuentra casi al final del álbum. También se suele mencionar como característica un ligero toque de humor en el disco. El álbum tiene alguna similitud con "Led Zeppelin IV". La primera canción, "Death On Two Legs (Dedicated To...)", fue compuesta por Mercury y hace referencia al exmánager de la banda, Norman Sheffield, uno de los dueños de su anterior discográfica, Trident. La canción describe un duro retrato mostrando la animosidad de la banda con su exmánager. Aunque en la canción no era explícitamente mencionada ninguna persona, ya que en el título simplemente aparece "Dedicado a...", solo se averiguó que trataba sobre Sheffield cuando este amenazó con demandar a Queen. EMI llegó finalmente a un acuerdo económico con Norman para evitar una posible demanda que evitara o retrasara el lanzamiento del álbum. "You're My Best Friend", un corte pop, es la segunda composición para la banda del bajista Deacon. El siguiente tema del álbum, "'39", es un country/folk escrito por Brian May. En él narra un viaje espacial que hace alusión a la teoría que postula que si alguien viajara a velocidades cercanas a las de la luz, cuando este retornara a la tierra para el viajero solo habría pasado muy poco tiempo, mientras que para los que se quedaron en el planeta ya habrían pasado generaciones (se debe tener presente en este punto los estudios de astronomía por parte de su autor, Brian May). El lado B del disco se inicia con la extensa (8 minutos con 20 segundos) "The Prophet's Song" de May. Para su grabación utiliza un "toy koto", instrumento de cuerdas japonés. En la sección central de la canción se produce un corte donde surge un intrincado juego de voces sobregrabadas, un canon con frases simples en capas para crear un amplio espectro de coros. "Love of My Life" de Mercury es la balada del disco. Para ella se usó un arpa y diferentes armonías vocales. "Good Company", autoría del guitarrista, es un charleston donde Brian May recrea una banda de dixieland con todos sus instrumentos solo a partir de los sonidos de su guitarra, grabada en varios tracks.

Al igual que el primer sencillo, el álbum fue un rotundo éxito en el Reino Unido. Además de obtener el número uno y permanecer durante cincuenta semanas en las listas, fue galardonado con el disco de platino. En los Estados Unidos el álbum llegó al puesto n.º 4, y además alcanzó la certificación de triple platino. En 2003, fue ubicado en el puesto n.º 230 de la lista de los de la revista "Rolling Stone".


El 14 de noviembre comienza el A Night at the Opera Tour, cuya primera manga tendría lugar en Inglaterra hasta finales de año. 1975 concluye para Queen con un show grabado por la BBC en el Hammersmith Odeon. El año siguiente la gira continuó por Estados Unidos, Japón y Australia. El primer recital del año fue el 31 de enero en Filadelfia. Queen llevó consigo un nuevo mánager, llamado Gerry Stickells, quien organizaría otras giras de Queen.

El 18 de mayo de 1976 se editó el segundo sencillo del álbum, "You're My Best Friend", con "'39" como lado B. Alcanzó el puesto n.º 7 y 16 en las listas británicas y estadounidenses respectivamente. En septiembre el grupo hizo una breve gira de verano en Reino Unido que concluyó el 18 de septiembre (sexto aniversario de la muerte de Jimi Hendrix) con un recital gratuito en el Hyde Park londinense ante 150 mil personas. El show, que colapsó el transporte público, no fue completado debido a que la policía apagó la electricidad general del predio, al considerar que era demasiado tarde (fue el primer recital nocturno en el parque que antes había alojado a The Rolling Stones en 1969). El recital fue transmitido por Capitol Radio y también fue filmado, pero la película se deterioró, imposibilitando su publicación. Tras este concierto, la banda decidió producir un nuevo álbum, pero en esta ocasión sin la mano de Roy Thomas Baker.

El siguiente álbum de Queen, "A Day at the Races", se suele considerar una secuela de "A Night at the Opera". El grupo resalta dicha característica al contar ambos trabajos con un arte de tapa y título similar. Este último factor fue tomado de sendas películas de los hermanos Marx. Aunque ambos discos tienen casi el mismo aspecto, "A Day at the Races" era mucho más estrecho que su predecesor. Como adelanto, el 12 de noviembre de 1976 se editó el sencillo "Somebody to Love"/"White Man" que llegó al puesto n.º 2 de las listas inglesas, permaneciendo allí durante nueve semanas. Mientras el lado uno es un gospel compuesto por Mercury, el segundo es un hard rock de Brian May que relata la conquista del oeste estadounidense desde la visión de un aborigen.

"A Day at the Races" finalmente se lanzó el 10 de diciembre. Tras una introducción con guitarras sobregrabadas, el álbum se inicia con el hard rock "Tie Your Mother Down", escrito por May. "The Millionaire Waltz" obra de Mercury, incluye un solo de guitarra con fraseos de vals. El otro lado inicia con las dos canciones que se adelantaron como sencillo, "Somebody to Love" y "White Man". La última canción del disco es "Teo Torriatte (Let us Cling Together)", lanzada como sencillo en Japón. El álbum permaneció veinticuatro semanas en las listas británicas, siendo la ubicación n.º 2 el mayor puesto. Alcanzó el puesto n.º 5 en los Estados Unidos, para que al poco tiempo de su emisión llegara a disco de oro en Estados Unidos y en Reino Unido.

A principios de 1977, Queen comenzó el A Day At The Races Tour, recorriendo primero Estados Unidos con Thin Lizzy como teloneros. En dicha gira interpretaron "Bohemian Rhapsody" en su totalidad, cuando antes solo solían tocar un fragmento de la misma como un "medley". El 5 de febrero tocaron por primera vez en el Madison Square Garden de Nueva York. Aprovechando su estancia en Hollywood, la banda visitó a Groucho Marx. Mientras tanto, el 4 de marzo, deciden editar el segundo sencillo del álbum, "Tie Your Mother Down"/"You and I", que, con cuatro semanas en las listas británicas, alcanzó el puesto n.º 31 y el n.º 49 en las listas estadounidenses.

Finalizada la etapa norteamericana de la gira, iniciaron la ronda europea. Tras dos meses de recitales, éstos culminaron el 6 y 7 de junio en el Earls Court de Londres, aprovechando el año de Jubileo de la Reina. El show cerró con un "medley" de viejos rocanroles como "Lucille", "Jailhouse Rock" y "Stupid Cupid". Fue en Earls Court donde apareció por primera vez su famosa escenografía parecida a una corona iluminada. Dicha instalación costó 5.000 libras esterlinas, pero a diferencia de otras escenografías, esta era móvil, dando inicio a una nueva era de escenografías más atractivas en los conciertos de rock. Días antes, el 20 de mayo, editaron su primer y único EP titulado justamente "Queen's First EP", que incluía canciones conocidas: "Good Old-Fashioned Lover Boy", "Death On Two Legs (Dedicated To...)", "Tenement Funster" y "White Queen (As It Began)". El EP permaneció diecisiete semanas en las listas, alcanzando el puesto n.º 10.

En esa época, la relación del grupo con la prensa empezó a deteriorarse fuertemente. Un ejemplo fue una entrevista entre Mercury y el periodista Tony Stewart de "NME", la cual tuvo una comunicación algo complicada y fue titulada "Is this man a prat?" ("¿Este hombre es un imbécil?"). Taylor publicó sorpresivamente un sencillo en agosto de 1977, "I Wanna Testify", que alimentó las especulaciones por parte de la prensa sobre la posible disolución de Queen. No obstante, el fantasma de los rumores de separación estaban presentes desde 1973. Según algunos medios, el baterista del conjunto estaba probando terreno para lanzarse solo, ya que no soportaba ser un compositor en un segundo plano.

Tras dos meses de grabación, el 7 de octubre de 1977 aparece el primer resultado: el sencillo con doble lado A "We Will Rock You"/"We Are the Champions" (escritas por Brian May y Freddie Mercury respectivamente), que pasarían a convertirse en himnos utilizados en muchos acontecimientos deportivos, siendo consideradas como dos de las pioneras en el arena rock. La multitud que aparece en el videoclip brindando los aplausos del tema son miembros del club de fanáticos de Queen. Así, el 28 de octubre Queen puso a la venta el nuevo álbum de estudio, "News of the World", álbum que se pelea junto con "The Game" como el álbum de estudio más vendido de Queen en Estados Unidos, donde alcanzó cuatro veces platino. "News of the World" llegó al puesto cuatro en el Reino Unido, manteniéndose en las listas por veinte semanas, y al n.º 3 en Estados Unidos. La diversidad de géneros en el nuevo trabajo discográfico contrasta con "A Day at the Races". El título del LP se basa en un cómic de la década de 1930 y el diseño de tapa e interior fue encargado al dibujante Frank Kelly Freas. En febrero de 1978 se emitió otro sencillo de "News of the World", "Spread Your Wings"/"Sheer Heart Attack", que alcanzó el puesto n.º 34 durando sólo cuatro semanas en las listas. En abril se lanzó el tercer y último sencillo, "It's Late", compuesta por Brian May.

Cuando Queen grababa la canción "Sheer Heart Attack", Sex Pistols estaban registrando en un estudio adyacente. Mercury invitó a Sid Vicious al estudio a escuchar una canción de Queen, e intentó motivarlo para que él cantase una canción para el grupo con estilo punk rock y que Sid hiciera lo mismo con una composición de Queen. Sin embargo, Mercury recibió una rotunda negativa por parte del músico punk.

En noviembre de 1977, comienza el News of the World Tour, que les llevaría de gira por Norteamérica y, ya en 1978, por Europa. En julio de 1978 la banda se reagrupó para registrar un nuevo álbum en los Mountain Studios de Montreux (Suiza) y en los Super Bear Studios de Berre-les-Alpes (Francia). Durante la grabación, un fuerte temporal azotó a Montreux. Brian May salió y grabó la tormenta, sonido que se puede escuchar al final de la canción "Dead On Time". Para este nuevo álbum volvieron a contar con la ayuda de Roy Thomas Baker. La banda quedó sorprendida con los Mountain Studios en Montreux, al punto de comprarlos en 1979.

El 13 de octubre de 1978 salió a la venta el primer sencillo del futuro álbum, el doble lado A "Fat Bottomed Girls/Bicycle Race". El mismo alcanzó el puesto n.º 11 en Reino Unido y permaneció en las listas doce semanas. Para promocionarlo, se realizó un videoclip para "Bicycle Race" que mostraba una carrera de mujeres desnudas en bicicleta. La ganadora apareció de espaldas en la portada del sencillo, desnuda y montada en la bicicleta. La portada tuvo que ser censurada, dibujándole (con el consentimiento de la banda) un bikini. El lanzamiento fue calificado como sexista y pornográfico por parte de la prensa. "NME" publicó una foto de Freddie Mercury con el título "Fat Bottomed Queen".

El 10 de noviembre de 1978 apareció finalmente el álbum "Jazz". Al igual que el video de "Bicycle Race", el álbum se vio envuelto en polémica, ya que incluía un póster con la foto de las 65 mujeres ciclistas desnudas. El regalo fue retirado del álbum y fue reemplazado por un formulario de solicitud para reclamar el póster. Comercialmente, el álbum fue inclusive mejor que su antecesor, ubicándose en el segundo puesto por veintisiete semanas, y también tuvo repercusión en los Estados Unidos, donde llegó al n.º 6. Además, alcanzó el disco de oro tanto en el Reino Unido como en los Estados Unidos. Sin embargo, el nuevo álbum fue duramente criticado. En la revista "Rolling Stone" se lo calificó como "fascista", además de decir: "Queen no tiene creatividad para hacer jazz, Queen no tiene creatividad, por esta razón, para hacer rock and roll". El 5 de enero de 1979 se publicó el segundo sencillo, "Don't Stop Me Now", que fue n.º 9 y permaneció en las listas treinta y cinco semanas. Otros sencillos posteriores que tuvieron un menor éxito fueron "Jealousy" y "Mustapha". Sin embargo, Queen se encontraba en ese momento en el apogeo de su popularidad. En aquella época la prensa se hacía eco además de las grandes fiestas frecuentemente organizadas por la banda, como la producida en Nueva Orleans el 31 de octubre de 1978, noche de Halloween, con motivo del lanzamiento del álbum. Brian May hablaba así para "Mojo Magazine" en 1999 sobre cómo afectó a la banda el éxito y el vacío que sentía en esa etapa:

El Jazz Tour comenzó con una manga por los Estados Unidos y Canadá desde octubre de 1978 hasta finales de año, y se extendió con otras dos por Europa y Japón la primera mitad de 1979. El 22 de junio de 1979 editan su primer álbum en vivo, "Live Killers". El álbum fue producto de las grabaciones de la manga europea del Jazz Tour. El disco consiguió llegar a disco de oro en el Reino Unido, alcanzando el puesto n.º 3 y manteniéndose en las listas veintiocho semanas. A pesar de llegar al n.º 16, ganó el doble disco de platino en Estados Unidos. Hechos como pasar parte de la canción "Bohemian Rhapsody" -la sección operística- poniendo cintas pregrabadas, atrajo críticas por parte de la prensa, y la propia banda tampoco quedó satisfecha con el resultado final. Este álbum fue autoproducido por Queen y mezclado en los Mountain Studios de Montreux. En junio de 1979, Queen viajó a los Musicland Studios de Múnich (Alemania), donde se encontró y entabló relaciones laborales con el ingeniero Reinhold Mack (llamado simplemente Mack). Al conjunto le atrajeron las ideas de Mack, a tal punto de acreditarlo como coproductor del siguiente álbum.

En junio de 1979, ya establecidos en Múnich, Alemania, empezaron a trabajar en los Musicland Studios con Mack en su siguiente álbum, "The Game", y para finales de año lanzaron el primer sencillo del mismo "Crazy Little Thing Called Love", un "rockabilly" que fue n.º 2 en Reino Unido, permaneciendo en cartel por catorce semanas. El tema fue además su primer número uno en Estados Unidos. El lanzamiento de la canción fue acompañado por una pequeña gira por el Reino Unido e Irlanda bajo el nombre de Crazy Tour a finales de 1979, en el cual se vio una diferencia en la puesta en escena de la banda con respecto a las giras anteriores, que se mantendría durante los "tours" siguientes. Tocaron en lugares como Tiffany's en Purley, The Lewisham Odeon, Alexandra Palace y The Hammersmith Odeon. En diciembre de 1979 Queen tocó como banda de apertura en el Concerts for the People of Kampuchea en Londres, después de haber aceptado una solicitud por el organizador del evento, Paul McCartney. En ese evento asistieron grupos como The Clash, The Pretenders, The Who y Elvis Costello. Cuando "Crazy Little Thing Called Love" era interpretada en vivo, Freddie Mercury tocaba la guitarra rítmica (usualmente un Fender Telecaster), al igual que en la grabación de estudio. Fue en el Crazy Tour cuando el mánager de gira, Gerry Stickells tuvo un colapso y fue llevado de urgencia a un hospital.
Las grabaciones del álbum se extendieron hasta mediados de 1980. En esta época el grupo reconoció que tuvo un cierto estancamiento a nivel creativo y que sufrió varias tensiones internas importantes, provocadas principalmente por la diferente visión de cada miembro sobre la dirección artística que debía seguir la banda:
El 30 de junio de 1980, "The Game" vio finalmente la luz. Fue el primer álbum de Queen con presencia de sintetizadores, a pesar de que Mercury comentó que nunca los iba a usar. El trabajo tuvo un enorme éxito a nivel mundial, llegando a la cima en las listas del Reino Unido y Estados Unidos. Fue, de hecho, su primer disco de oro estadounidense y el único álbum en alcanzar el número uno en ese país. También fue número en gran parte de Europa y Sudamérica y en Canadá, e ingresó por siete semanas en las listas ARIA de Australia. El cuarto sencillo "Another One Bites the Dust", escrito por John Deacon, fue su segundo número uno en Estados Unidos. Aunque la banda no valoraba como suficientemente buena la canción, fue escogida como sencillo después de que Michael Jackson animara al grupo a hacerlo en el "backstage" en un concierto en Los Ángeles. Según May, parte del éxito se debe a que el sencillo fue pasado recurrentemente en emisoras de radio de música negra pensando que la canción había sido compuesta por un grupo de origen afroamericano, lo que hizo que algunas personas desconocedoras de la banda concurrieran a sus recitales, sorprendiéndose al ver que el grupo no estaba compuesto por gente negra. El tema fue además nominado en los en la categoría de . Este gran éxito produjo que en su trabajo posterior, el conjunto girase radicalmente de sus tendencias hard rock y glam rock, a tendencias más disco. Los otros sencillos destacados fueron "Save Me" y "Play the Game". En el video de esta última canción, Mercury aparece por primera vez con su célebre bigote.

La gira The Game Tour se inició en junio de 1980, pasando ese año por Estados Unidos, Canadá y Europa. A pesar del éxito del álbum y la gira, el grupo fue perdiendo algo de terreno, especialmente en los Estados Unidos, cuando en diciembre de 1980 lanzaron la banda sonora "Flash Gordon" (de la película del mismo nombre), que tuvo un frío recibimiento y ventas regulares. La idea de grabar una banda sonora provino mientras el grupo se encontraba en las sesiones de grabación de "The Game", en las que apareció el productor de cine italiano Dino De Laurentiis para ofrecerles grabar la banda sonora de su nueva película.

Ya en 1981, tras pasar por Tokio en febrero, el grupo se embarcó en su primera gira sudamericana, que fue todo un éxito, presentándose ante 479 mil personas. Brindaron cinco recitales en Argentina (Buenos Aires, Rosario y Mar del Plata), llegando a convocar a 300.000 personas en uno de ellos, siendo una de las multitudes más grandes en conciertos argentinos. Es llamativo este recibimiento por el contexto político-social que atravesaba ese país. Es en donde también Freddie estrena su cabello corto. Tras estos recitales, la venta de álbumes aumentó considerablemente en aquel país. También atrajeron a 251.000 personas en el estadio de Morumbi de São Paulo. Posteriormente, en abril, Roger Taylor lanzó su álbum en solitario "Fun in Space", siendo el primer miembro de Queen en publicar material solista. Entre los meses de julio a septiembre de 1981, Queen se dirigió a sus estudios Mountain en Suiza para grabar su siguiente álbum, "Hot Space". Casualmente, David Bowie se encontraba en la zona y aceptó la invitación del ingeniero David Richards para conocer a los miembros de Queen. El grupo comenzó entonces a dar forma a una canción con Bowie, improvisando sobre la base musical de "Feel Like", un tema inconcluso del cuarteto. De estas sesiones surgió finalmente "Under Pressure", que aparecería tanto en el "Greatest Hits" como en "Hot Space". El tema fue el segundo corte de Queen en ser número uno en el Reino Unido (el primero fue "Bohemian Rhapsody"). Esta fue la primera vez que Queen grabó con un artista invitado. En septiembre y octubre, el grupo volvió a Latinoamérica para tocar en Venezuela y México. El 3 de noviembre de 1981 Queen lanzó su primer recopilatorio, "Greatest Hits". Este disco ha sido el más vendido de la historia del Reino Unido (en segundo lugar se encuentra "Sgt. Pepper's Lonely Hearts Club Band" de The Beatles), estando 450 semanas en las listas y siendo certificado disco de platino once veces. El 24 y el 25 de noviembre, Queen ofreció sendos conciertos en Montreal (Canadá), los cuales fueron filmados en 35 mm y lanzados en VHS con el título "We Will Rock You" (1984) y en DVD bajo el nombre de "Queen Rock Montreal" (2007).

En 1982 aparece el álbum "Hot Space". Con un sonido muy diferente al de anteriores trabajos, más cercano al disco y al dance pop, fue duramente criticado y tuvo una acogida generalmente fría entre los fanáticos. El álbum no alcanzó el platino y significó el peor lanzamiento de Queen (en cuestiones comerciales) desde "Queen II". El tradicional sonido de la guitarra de Brian May había sido reemplazado por sintetizadores en varias canciones del álbum. May argumentó en alguna entrevista que Queen necesitaba probar cosas diferentes. Del álbum se editaron tres sencillos en el Reino Unido (sin contar "Under Pressure"). Las canciones fueron "Body Language", "Las Palabras de Amor" y "Back Chat". Queen volvió a aparecer en el "Top of the Pops", después de muchos años, realizando una interpretación de "Las Palabras de Amor". Esto acontecía a la vez que "Under Pressure" alcanzó la cima de las listas holandesas y argentinas, justo cuando estallaba la Guerra de las Malvinas. La dictadura gobernante de Argentina prohibió las transmisiones radiales de canciones en inglés, mientras que en Inglaterra la canción "Las Palabras de Amor" era vista como inapropiada.
En abril de 1982 comenzó la gira para promocionar el álbum, el Hot Space Tour, que en septiembre finalizó en Japón, tras pasar por Europa y Norteamérica. En esta gira se filmó el concierto realizado el 5 de junio en el National Bowl (Reino Unido), siendo posteriormente lanzado en DVD bajo el nombre de "Queen on Fire - Live at the Bowl" (2004). Esta fue la última gira que el grupo realizó en Estados Unidos.

Después de trabajar de manera constante durante más de diez años, Queen decidió en 1983 no realizar shows en vivo y tomarse el año sabático. A finales de 1983 existían conflictos entre Elektra y Queen: Mercury no quería grabar más discos para el sello. Se encontraba molesto por la forma en que la casa discográfica estaba representando a Queen en Estados Unidos. Capitol Records se encontraba muy interesada en Queen, cosa que consiguió, cuando el grupo abandono Elektra por Capitol. Jim Beach y Gerry Stickells intentaron en primavera organizar un nuevo tour por Sudamérica, pero por problemas de equipos y promotores decidieron dar marcha atrás. A principios de 1984, Freddie Mercury se encontraba realizando su álbum solista, en el cual, según sus palabras, podía expresar cosas que no podía en el contexto de Queen, como situaciones más personales.

El nuevo álbum de la banda apareció en febrero de 1984 bajo el título de "The Works", con el cual marcaron un regreso parcial a sus raíces dentro de la música rock. Alcanzó el segundo puesto en Reino Unido, con una permanencia en la lista de noventa y tres semanas, llegando al poco tiempo a disco de platino. En Estados Unidos solo fue n.º 23 en ventas. Los sencillos "Radio Ga Ga" y "I Want to Break Free" fueron muy exitosos. Para el videoclip del primero se usó material de la película "Metrópolis" de Fritz Lang. En el video de "I Want to Break Free" aparecen los miembros de Queen disfrazados de mujer queriendo ridiculizar la serie de la televisión británica "Coronation Street". Mercury recrea partes del ballet "Debussy L'Apres-midi d'un faune". La MTV de los Estados Unidos rehusó pasar el videoclip. Otros sencillos del álbum fueron la balada "It's a Hard Life" y "Hammer to Fall", que trata acerca de la guerra fría. El álbum fue generalmente bien recibido por la crítica y tuvo buenas ventas a nivel mundial, aunque también hubo quienes añoraban el sonido de la década anterior.

En septiembre de 1984 Queen inició su The Works Tour (primera gira con el tecladista Spike Edney). Con la disminución del éxito en los Estados Unidos y la menguante popularidad en el Reino Unido, Queen decidió explorar nuevos horizontes, programando citas que, además de Europa, añadía países de África, Sudamérica, Oceanía y Asia, continentes generalmente poco visitados entonces por las bandas de rock occidentales. La gira incluía una serie de nueve fechas (con entradas agotadas) en el complejo de Sun City, en Sudáfrica. La banda estuvo involucrada en una polémica a raíz de tocar en el apogeo del "apartheid", durante un boicot cultural promovido por la ONU. Queen recibió entonces una sanción del sindicato de músicos británicos y apareció temporalmente en la lista negra cultural de la ONU. El grupo permaneció en la lista hasta que firmó, junto a otros artistas, una carta en la que prometían no volver a actuar en Sudáfrica mientras continuara el estado racista. La formación respondió entonces a las críticas argumentando que estaban tocando para fanáticos de la música de ese país y que los conciertos se desarrollaron ante un público integrado, pero posteriormente tanto Roger Taylor como Brian May han calificado aquellos conciertos como un error.

El 12 y el 19 de enero de 1985, Queen tocó en la primera edición del festival Rock in Rio, en Río de Janeiro (Brasil). La segunda actuación salió a la venta en VHS bajo el nombre de "Queen Live in Rio". A principios de 1985 se celebraron los British Video Awards. Queen ganó dos premios: uno "The Works EP", una compilación de videos del álbum, y por el video de "Radio Ga Ga". Tras realizar un concierto en Nueva Zelanda, se dirigieron a Australia, haciendo una serie de ocho conciertos que se agotaron, en Melbourne y Sídney. Después de Australia, se dirigieron hacia Japón.

Después de un pequeño descanso, la banda comenzó a ensayar para el Live Aid, un macroconcierto cuyos beneficios irían destinados a luchar contra la hambruna de África y en el que participarían algunas de las mayores estrellas musicales de aquel momento. Finalmente el evento se celebró el 13 de julio. En aquel recital, que en Inglaterra tuvo lugar en el estadio de Wembley con un público de 72.000 personas, Queen fue visto por la mayor audiencia televisiva de la historia hasta el momento, un total de 1900 millones de televidentes. El grupo fue presentado por los cómicos Mel Smith y Griff Rhys Jones. El organizador del show, Bob Geldof, además de otras eminencias del ambiente musical como Elton John y varios periodistas musicales, dijeron que la actuación de Queen fue la mejor. La actuación ha sido posteriormente votada en varias ocasiones como la mejor presentación en vivo de la historia. En ese momento Mercury tenía una infección en la garganta, por lo que su médico le había recomendado no realizar el show. Tensiones existentes dentro del grupo, así como también los rumores de separación del cuarteto, se propagaron mediante los periódicos musicales y columnas de chismes. Sin embargo, con la experiencia de haber atraído a una masiva audiencia, el grupo se motivó para realizar otro trabajo de estudio.

Freddie Mercury lanzó su primer disco en solitario, "Mr. Bad Guy", en abril de 1985, aunque el mismo no tuvo masivas ventas. En diciembre se había publicado "The Complete Works", box set con todos los discos de Queen hasta el momento.

El 2 de junio de 1986 salió a la venta el álbum "A Kind of Magic", que en Reino Unido alcanzó el n.º 1, manteniéndose en las listas durante 63 semanas. Fue muy distinta la recepción en Estados Unidos, llegando solo al puesto n.º 46 y llegando al disco de oro en 2002. Mientras el cuarteto británico iba perdiendo la atención del público estadounidense, en Europa se seguía manteniendo como una banda de éxito. El álbum se encontraba constituido principalmente por canciones que fueron escritas para la película "Highlander" ("Los Inmortales"), dirigida por Russell Mulcahy e interpretada por Christopher Lambert y Sean Connery. Sin embargo, Queen grabó e incluyó en el disco otras canciones que no aparecieron en la película y sin relación con este, por lo que se considera un álbum de estudio y no una banda sonora. El primer sencillo del disco fue "One Vision", publicado en noviembre de 1985. Posteriormente se lanzaron además "A Kind of Magic" y "Princes of the Universe" (este solo en EE.UU., Canadá, Australia y Nueva Zelanda), cuyos videos fueron dirigidos por Russell Mulcahy. Mientras que "A Kind of Magic" llegó al puesto n.º 3 en el Reino Unido, el corte "Princes of the Universe" pasó desapercibido en EE.UU. Otros sencillos posteriores fueron "One Year of Love" (España y Francia), "Friends Will Be Friends", "Pain Is So Close to Pleasure" (EE.UU. y Europa) y "Who Wants to Live Forever".


En este contexto, Queen se embarcó en el Magic Tour para tocar en los estadios al aire libre de Europa. El grupo ensayó más de lo normal para este tour. En la gira se utilizó el escenario más grande y la plataforma de luces más extensa de la carrera del grupo, además de una pantalla gigante en Mannheim, Wembley y Knebworth, que era la pantalla más grande que había en Reino Unido en esa fecha. Una de las citas más señaladas del tour fueron las dos fechas, 11 y 12 de julio, que realizaron en el estadio de Wembley (Reino Unido) ante unas 72.000 personas cada noche, vendiéndose las entradas en solo seis horas. El rodaje de ambos conciertos se realizó con quince cámaras, además de un helicóptero para capturar tomas aéreas. La segunda noche se publicó en varios formatos años después. El concierto de Budapest, celebrado el 27 de julio, fue filmado por Mafilm, empresa que realizaba filmaciones del gobierno, que también grabó a modo de ensayo el concierto de Colonia una semana antes, aunque este se borró para volver a usar las mismas cintas en Budapest. Emplearon todas las cámaras de 35 mm que había disponibles en Hungría para registrar el recital. Sería publicado en VHS el 16 de febrero de 1987 bajo el nombre de "".

Su último concierto fue el 9 de agosto de 1986 en Knebworth Park ante 120.000 personas. Las entradas se vendieron en dos horas. El Magic Tour fue visto por más de 1 millón de personas, siendo 400.000 en el Reino Unido, récord de audiencia en el país entonces. Esta fue también la última gira de Queen con Freddie Mercury. En diciembre del mismo año, Queen publicó "Live Magic", su segundo disco en vivo, en el que compilan versiones extraídas de los conciertos de Wembley, Budapest y Knebworth.

Los años siguientes los miembros del grupo anunciaron que no daban conciertos porque querían un tiempo de relajación. A comienzos de 1987, Taylor y Deacon se fueron de vacaciones a Los Ángeles. En esta pausa tuvo lugar el divorcio del guitarrista Brian May (posteriormente luchó contra la depresión y el suicidio), y a Freddie Mercury le fue diagnosticado el sida tras la Pascua de 1987, según su entonces pareja Jim Hutton. Mercury aprovechó el paréntesis para trabajar en material solista. El 23 de febrero de 1987 Mercury publicó el sencillo "The Great Pretender". En el video del nuevo corte, dirigido por David Mallet, Mercury usó varios disfraces y ropa que utilizó en los recitales y videoclips de su carrera. El sencillo llegó al puesto n.º 4 en Reino Unido. Queen volvió a ganar el Premio Ivor Novello el 15 de abril de 1987. El 10 de octubre de 1988, Mercury publicó su segundo álbum solista, "Barcelona", esta vez junto a la soprano española Montserrat Caballé. Hacia comienzos de 1988, la banda se había reagrupado para preparar un nuevo trabajo.

"The Miracle" salió a la luz el 22 de mayo de 1989. El disco llegó al n.º 1, manteniéndose 32 semanas en lista y alcanzando al poco tiempo el disco de platino en Reino Unido. También llegó al puesto n.º 24 en Estados Unidos. El sencillo que gozó de mayor éxito fue "I Want It All", que llegó al número uno en varios países y al n.º 3 en el Reino Unido, manteniéndose siete semanas en listas. "Breakthru", "The Miracle", "The Invisible Man", y en menor medida "Scandal", también lograron posicionarse en los primeros lugares, reforzadas todas por videoclips que apoyaron su difusión. Algunas de las letras de los temas tienden a reflexionar sobre los logros pasados del grupo ("Khashoggi's Ship" y "Was It All Worth It"). Las canciones fueron acreditadas a 'Queen' y no a miembros concretos de la banda. La portada consiste en la unión de las caras de los cuatro integrantes, usando un Quantel Paintbox para lograr el efecto que se quería.

Queen no brindaba ninguna entrevista como grupo desde hacía un largo periodo de tiempo. Fueron contactados para una entrevista con Mike Reid, y finalmente el 29 de mayo de 1989 la misma tuvo lugar. Durante la entrevista Freddie cuenta que su negativa a salir de gira era porque quería cambiar el ciclo de disco-gira. Se tenía en mente editar un "Greatest Hits II", pero debido a las buenas ventas de "The Miracle", se abortó esta decisión. Se pensó que si publicaban un disco compilatorio, perjudicaría las ventas del nuevo álbum de estudio. El año 1989 finalizó con el lanzamiento en VHS de "The Miracle Video EP", donde aparecen todos los videos del álbum. Queen obtuvo el galardón de la BPI por su destacada contribución a la música británica y realizaron una fiesta en conmemoración de los 20 años del grupo. La fiesta se realizó en un club Soho llamado "Groucho's". Gran parte de las personas que habían trabajado alguna vez con Queen fueron citadas para la celebración. El grupo decidió terminar su contrato con Capitol en los Estados Unidos, firmando un nuevo acuerdo con Hollywood Records por una suma superior a los 10 millones de dólares. Renunciaron a EMI para la distribución mundial. Una de las primeras cosas que hizo Hollywood Records fue reeditar toda la discografía de Queen (a excepción de "Live Killers") en el nuevo y novedoso formato de CD.

A pesar del debilitado estado de salud de Mercury, él insistió en hacer un nuevo trabajo. "Innuendo" fue publicado el 4 de febrero de 1991. Mientras que solo llegó al n.º 30 en los Estados Unidos, en el Reino Unido se fue al número uno, permaneciendo en las listas durante treinta y siete semanas y ganando el disco de platino. Aunque no contó con el apoyo unánime de la crítica, el nuevo álbum también gozó de gran éxito comercial. Las letras son generalmente autobiográficas desde el punto de vista de Mercury, como la de "These Are the Days of Our Lives" y "The Show Must Go On". El primer sencillo del álbum, la homónima "Innuendo", es una de las canciones más largas de la banda y cuenta con varias secciones de distintos géneros, incluyendo un solo de guitarra flamenca a cargo de Steve Howe. El álbum también incluye una composición que habla de la imposibilidad que tiene la humanidad de vivir en armonía, "All God's People".

Según declaraciones brindadas por su entonces pareja, Jim Hutton, a Mercury se le diagnosticó sida después de la Pascua de 1987. Por aquella época, Mercury dijo en una entrevista no padecer esta enfermedad, tanto él como el resto del grupo mantuvieron este hecho en el absoluto silencio. Pero pese a las negaciones, la prensa británica alimentó rumores sobre esta posible enfermedad debido a la apariencia demacrada de Mercury y a que Queen no realizaba giras ni conciertos desde hacía varios años. Hacia el final de su vida, muchos periodistas le tomaron fotografías, mientras que "The Sun" sugería que estaba muy enfermo. Todo esto sucedía en la época de la publicación de "Innuendo".

El 22 de noviembre de 1991, Mercury llamó al mánager de Queen, Jim Beach, para discutir un asunto público. Al día siguiente, se realizó el siguiente anuncio en nombre del cantante:

Apenas dos días después de aquel comunicado, Mercury perdió su batalla contra la enfermedad, falleciendo a las 7 PM en su mansión de Londres. Tenía 45 años de edad. La última aparición pública del cuarteto fue en la entrega de los premios Brit Awards de 1990, por la contribución que tuvo Queen en la música británica. Las oficinas de Queen quedaron inundadas por flores en conmemoración del artista. El día 27 de noviembre sus restos fueron cremados en el West London Crematorium.

La canción "Bohemian Rhapsody" fue reeditada como sencillo poco después del deceso de Mercury 1991, con "These Are the Days of Our Lives" como doble lado A (del álbum "Innuendo"). El sencillo llegó al número uno, puesto en el que se mantuvo cinco semanas, y permaneció dieciséis en las listas de venta. El éxito de la reedición se produjo en parte gracias a una escena de la película "Wayne's World" en la que los personajes Wayne (Mike Myers) y Garth (Dana Carvey) cantan junto a unos compañeros "Bohemian Rhapsody", la cual suena en la radio del coche. "Bohemian Rhapsody" se convirtió en la primera canción en llegar al número uno (en su versión original) en dos oportunidades distintas, además de lograr la cantidad de catorce semanas en lo más alto (nueve en la original y otras cinco en la segunda edición). La recaudación inicial del sencillo –aproximadamente 1.000.000 de libras– fue donada a la fundación Terrence Higgins Trust.

Freddie Mercury fue premiado de forma póstuma en los Brit Awards en el año 1992 por su destacada contribución musical, mientras que Queen ganó el premio a mejor sencillo con "These are the Days of our Lives", perteneciente al disco "Innuendo".

Un mes antes de la muerte de Freddie, el 28 de octubre de 1991, EMI había publicado el segundo recopilatorio de grandes éxitos, "Greatest Hits II", que incluía 17 de los mayores hits del grupo entre 1981 y 1991. Este álbum debutó en el número uno de las listas de venta de Reino Unido. En la segunda semana bajó al n.º 2 (en lo más alto estaba Enya), en la siguiente al n.º 3 (en esa semana "We Can't Dance" de Genesis ocupaba el primer puesto), y en la cuarta semana el disco descendió al sexto lugar (en esa semana "Dangerous" de Michael Jackson hizo un ingreso triunfal al número uno). Ya en su quinta semana, con el fallecimiento de Mercury, el álbum regresó a lo más alto (esta vez con ventas más elevadas) y se mantuvo en esa posición durante otras cuatro semanas. En total, el álbum estuvo cinco semanas en el número uno, y con sólo ocho semanas en las listas, fue el tercer disco más vendido de 1991, luego de recibir el cuádruple platino por 1 200 000 de álbumes vendidos. El éxito del compilado fue incluso mayor en otros países de Europa. En Holanda, el álbum llegó al número uno en las dos últimas semanas de 1991, y se mantuvo en esa posición hasta comienzos de marzo de 1992 (un total de 11 semanas consecutivas). En mayo de ese año, el disco volvió a lo más alto, esta vez dos semanas, para completar el período de 13 en lo más alto. El álbum obtuvo la certificación de 5 discos de platino en 1997 (por 500.000 copias). En este país, tanto "The Miracle" como "Innuendo" habían llegado al número uno, permaneciendo 3 y 4 semanas en esa posición respectivamente. El disco llegó al número uno en Suiza (11 semanas), Austria (3 semanas), Finlandia, Italia (12 semanas), España (8 semanas), Francia (4 semanas) y Portugal. También llegó al número dos en Alemania y Suecia. Queen, además, fue la banda que más discos vendió entre 1991 y 1992 en Reino Unido, Holanda, Alemania, Argentina, Austria, Dinamarca, España, Francia, Finlandia, Italia y Portugal entre otros.


El concierto tributo a Freddie Mercury se realizó el 20 de abril de 1992, en el estadio de Wembley, Londres. Entre los artistas invitados estuvieron Annie Lennox, David Bowie, Def Leppard, Elizabeth Taylor, Elton John, Extreme, George Michael, Guns N' Roses, Ian Hunter, Lisa Stansfield, Liza Minnelli, Metallica, Mick Ronson, Robert Plant, Roger Daltrey, 
Seal, Spinal Tap, Tony Iommi y Zucchero, junto a los tres miembros restantes de Queen, interpretando varios de los éxitos más grandes de la banda. El concierto está registrado en el Libro Guinness de Récords como "El concierto benéfico con más estrellas de rock". Se recaudaron más de 20 000 000 de libras destinados a obras caritativas para el Mercury Phoenix Trust, una conciencia sobre el sida y el fondo de educación establecido por los miembros del grupo y su mánager, Jim Beach. Al concierto concurrieron 72 000 personas y fue visto en televisión por mil millones de espectadores de todo el mundo.

Una semana después, EMI publicó el concierto de Wembley de 1986 en formato CD y, con el impacto que provocó la trasmisión del reciente concierto, se convirtió en un gran éxito. El álbum llegó al número uno en Italia, donde se mantuvo cinco semanas y vendió 300 000 copias durante ese año, y también en Portugal. El álbum llegó al número dos en España, Francia y el Reino Unido.

En 1995, Queen logró completar el álbum "Made in Heaven". Llegó al n.º 1 en Reino Unido y al n.º 58 en Estados Unidos, tratándose del álbum de estudio más vendido en la historia de Queen. Cuenta con las voces grabadas por Mercury en el último año de su vida, además de contener el último material que grabó con el grupo. En 1995, una estatua del artista se presentó en Montreux, Suiza.

Dos años más tarde, en 1997, Brian May, Roger Taylor y John Deacon se reunieron nuevamente en un estudio para grabar el que hasta el día de hoy es el tema final de Queen, titulado "No-One But You (Only The Good Die Young)", que fue publicado como single y dentro del recopilatorio "Queen Rocks", una colección de cortes roqueros que recorren toda su carrera desde "Queen" hasta "Innuendo", omitiendo solo los álbumes "The Game" y "Made In Heaven".

En marzo de 2001 Queen ingresó en el Salón de la Fama del Rock en Cleveland, Ohio, en su segundo año de elección. Ese mismo año, Brian y Roger participaron junto a Robbie Williams en una nueva grabación de "We Are the Champions" como parte de la banda sonora de "A Knight's Tale"..

En la primavera de 2002 se estrenó el musical "We Will Rock You", escrito por Ben Elton, en el Dominion Theatre de Londres. Se dieron funciones por todo el mundo, logrando un considerable éxito.
Después de numerosas cartas que fanáticos de Queen enviaron a la Cámara de Comercio de Hollywood (se estipula que 300 por año), el grupo obtuvo la estrella en el Paseo de la Fama de Hollywood a finales del año 2002. Brian May y Roger Taylor asistieron el día que se descubrió la estrella, con el alcalde honorario de la ciudad, Johnny Grant.

Ese mismo año, Queen comenzó a lanzar de forma oficial su música en un nuevo formato, el DVD. Ya se habían lanzado anteriormente "We Will Rock You", que recoge los conciertos dados por Queen el 24 y el 25 de noviembre de 1981 en Montreal, Canadá, y "The Freddie Mercury Tribute Concert", en una edición especial para conmemorar los 10 años de aquel espectáculo. Conocidos como The DVD Collection, el encargado de abrir esta nueva serie fue el "Greatest Video Hits 1", que consiste en la colección de videos desde 1973 hasta 1980. A este se sumarían "Live At Wembley Stadium" y "Greatest Video Hits 2", ambos del 2003, y en 2004 el "Queen on Fire - Live at the Bowl" (también en CD), que corresponde al concierto dado el 5 de junio de 1982 en el Milton Keynes Bowl de Reino Unido. Posteriormente también se lanzaron en DVD "Queen Rock Montreal" (2007), "" (2012), "Live at the Rainbow '74" (2014) y "A Night at the Odeon - Hammersmith 1975" (2015).

A finales de 2004, el guitarrista Brian May y el baterista Roger Taylor anunciaron que ambos se reunirían y volverían a salir de gira en 2005 junto a Paul Rodgers (cantante fundador de las bandas Free y Bad Company). La nueva formación recibió el nombre de Queen + Paul Rodgers. El bajista John Deacon (retirado de la banda), declinó en participar, siendo reemplazado por el bajista Danny Miranda (miembro de Blue Öyster Cult). Otros miembros de la gira fueron el tecladista Spike Edney, quien tocó teclados y ocasionalmente guitarra en los conciertos en vivo de Queen desde 1984, y el guitarrista adicional Jamie Moses, quien comenzó a trabajar con Brian May en sus proyectos solistas a principios de los años 90. De esta gira, que fue bautizada como Queen + Paul Rodgers Tour, se publicaron los DVD "Return of the Champions" y "Super Live in Japan".

El 15 de agosto de 2006, Brian May confirmó a través de su sitio web y club de fanáticos que Queen + Paul Rodgers comenzarían a producir su primer álbum de estudio en octubre, para ser grabado en un "lugar secreto". El álbum, titulado "The Cosmos Rocks", fue publicado en Europa el 12 de septiembre de 2008 y en Estados Unidos el 28 de octubre. Después de esto, la banda realizó su gira Rock the Cosmos Tour, comenzando el 12 de septiembre en Járkov, Ucrania, con un concierto gratuito a beneficio de la institución local de ayuda contra el sida, ante una audiencia superior a las 350.000 personas. El concierto se publicó en CD y DVD bajo el nombre "Live in Ukraine" (2009). La gira recorrió Europa y terminó en Sudamérica, finalizando el 30 de noviembre de 2008 en Río de Janeiro (Brasil), tras pasar por Chile el 19 de noviembre, y Argentina el 21, siendo este último el segundo que más cantidad de público atrajo tras el de Ucrania.

Queen y Paul Rodgers se separaron oficial y amistosamente el 12 de mayo de 2009. Rodgers dijo: "Pienso que hicimos un gran éxito, de verdad. Hicimos dos giras mundiales y un par de grabaciones en vivo, y... hicimos un álbum de estudio histórico [para Brian May y Roger Taylor] porque ellos no habían entrado al estudio con nadie y grabado algo así por mucho tiempo. Fue un gran acontecimiento, creo". Rodgers no descarta la posibilidad de trabajar junto a los miembros de Queen nuevamente: "Es una especie de libro abierto. Si ellos se acercan a mí para hacer algo benéfico, por ejemplo, o algo así... yo estaría muy interesado en hacerlo, seguro".

Durante la ceremonia de clausura de los Juegos Olímpicos de Londres 2012, a través de varias pantallas se pudo ver a Freddie Mercury en un fragmento del concierto de Wembley en 1986 donde instaba al público a que repitiera las notas que él cantaba. A continuación una parte de "Brighton Rock" fue interpretada en el escenario por Brian May en su guitarra al que se unió posteriormente Roger Taylor para tocar "We Will Rock You" junto a la cantante Jessie J.

Desde 2011, Brian May y Roger Taylor realizan conciertos junto con el cantante estadounidense Adam Lambert bajo el nombre Queen + Adam Lambert. El 18 de septiembre de 2015 se presentaron en la 6ª edición del festival Rock in Rio ante 82.000 personas. Durante esta gira recorrieron Estados Unidos, Europa, Asia y Sudamérica.

La banda ha citado entre sus influencias a artistas estadounidenses como Elvis Presley, The Beach Boys y Jimi Hendrix al igual que grupos de rock británico de la época, como The Beatles, The Rolling Stones, The Who, David Bowie, Pink Floyd y Led Zeppelin. Queen desarrolló un estilo inspirado en varios géneros musicales. Entre los géneros con los que han sido asociados están: rock progresivo, hard rock, glam rock, heavy metal, pop rock, blues rock y rock psicodélico. Queen también ha escrito canciones que fueron inspiradas por géneros que no son típicamente asociados con el rock, como ragtime, ópera, gospel, vodevil y folk.

A partir de 2005, según el "Libro Guinness de los récords", los álbumes de Queen han pasado un total de 1322 semanas (veintiséis años) en las listas de álbumes del Reino Unido, más tiempo que cualquier otro artista. También en 2005, con el lanzamiento de su álbum en vivo con Paul Rodgers, Queen se trasladó al tercer lugar en la lista de artistas que han estado más tiempo en las listas de éxitos británicas. En 2006, su álbum "Greatest Hits" se convirtió en el más vendido de la historia del Reino Unido, con unas ventas de 5.407.587 ejemplares, más de 604.295 copias más que el segundo, "Sgt. Pepper's Lonely Hearts Club Band", y en 2014 se convirtió en el primer álbum en alcanzar los seis millones de copias en el país. Su álbum "Greatest Hits II" está en el octavo lugar, con 3.746.404 copias vendidas.

Queen es también uno de los a nivel mundial, con 103,8 millones de ventas certificadas, incluyendo 49,7 millones en los Estados Unidos, y unas estimaciones de 150 a 200 millones de álbumes en todo el mundo. El grupo ha lanzado un total de dieciocho álbumes número uno, dieciocho sencillos número uno y diez DVD números uno en todo el mundo. Ingresados en el Salón de la Fama del Rock en 2001, los cuatro miembros de Queen ingresaron en el Salón de la Fama de los Compositores en 2003. En 2002, "Bohemian Rhapsody" fue votado como el "éxito favorito del Reino Unido de todos los tiempos" en una encuesta realizada por "el libro de los récords Guiness de los sencillos británicos exitosos", y en 2004 la canción ingresó en el Salón de la Fama de los Grammy. En 2009, "We Will Rock You" y "We Are the Champions" también fueron incluidos en el Salón de la Fama de los Grammy, y la segunda fue votada canción más popular del mundo en una encuesta musical a nivel global.

Ampliamente reconocidos por su rock de estadio, en 2005 una encuesta de la industria colocó el concierto de Queen en el Live Aid en 1985 como la mejor actuación en vivo de la historia. En 2007, también fueron votados como la mejor banda británica de la historia por los oyentes de BBC Radio 2. "Rolling Stone" colocó a Queen en el número 52 de su lista de los "100 mejores artistas de todos los tiempos", mientras que Mercury se situó en el puesto 18 en la , y May en el lugar 26 en la . Queen fue nombrado decimotercer mejor artista de hard rock de todos los tiempos en una lista elaborada por VH1, y en 2010 fueron colocados en el puesto 17 en la lista de VH1 de los 100 mejores artistas de todos los tiempos.

Queen es una de las bandas con más "bootlegs", según Nick Weymouth, quien administra el sitio web oficial de la banda. Un estudio realizado en 2001 descubrió la existencia de 12.225 sitios web dedicados a "bootlegs" del grupo, el mayor número para cualquier banda. Los "bootlegs" han contribuido a la popularidad de la banda en algunos países donde se censura la música occidental, como Irán. En un proyecto llamado "Queen: Los 100 mejores bootlegs", muchos de ellos se han hecho oficialmente disponibles para descargar por un precio nominal en el sitio web de Queen, con unos beneficios destinados a la Mercury Phoenix Trust.

Queen ha sido reconocido por haber hecho contribuciones significativas a géneros como el hard rock y el heavy metal, entre otros. Por lo tanto, la banda ha sido citada como influencia por muchos otros músicos. Además, al igual que su música, los grupos y artistas que han afirmado estar influenciados por Queen son diversos y abarcan diferentes generaciones, países y géneros.
Algunos de los grupos y artistas que han citado a la banda como influencia son: Anthrax, Def Leppard, Dream Theater, Extreme, The Flaming Lips, Foo Fighters, Franz Ferdinand, George Michael, Green Day, Guns N' Roses, Iron Maiden, Journey, Kansas, Katy Perry, Keane, The Killers, Lady Gaga, Manic Street Preachers, Meat Loaf, Metallica, Mika, Muse, Mötley Crüe, My Chemical Romance, Nirvana, Panic at the Disco, Queensrÿche, Radiohead, Robbie Williams, The Smashing Pumpkins, Steve Vai, Styx, Sum 41, Trent Reznor, Trivium o Van Halen.

Queen ha sido citada como una importante influencia en el metal neoclásico, género creado por el guitarrista sueco Yngwie Malmsteen. Metallica grabó una versión de "Stone Cold Crazy", que apareció por primera vez en "" en 1990, y ganó un en 1991. En los años 70, Queen ayudó a estimular la evolución del heavy metal, descartando gran parte de su influencia blues; junto a la Nueva Ola del Heavy Metal Británico, fusionaron el género con una sensibilidad punk rock y el aumento del énfasis en la velocidad.

Thom Yorke, líder de Radiohead, recibió su primera guitarra a los 7 años de edad, animado tras ver a Brian May en un concierto televisado de Queen. Más tarde, con 10 años, Yorke construyó su propia guitarra casera, intentando imitar lo que había hecho May con su Red Special, aunque no quedó satisfecho con el resultado. Posteriormente, la banda fue una de las primeras influencias en la música de Radiohead.

Una película biográfica sobre Freddie Mercury y Queen con el título "Bohemian Rhapsody" fue estrenada en 2018. Está dirigida por Bryan Singer y la protagonizan Rami Malek como Mercury, Gwilym Lee como Brian May, Ben Hardy como Roger Taylor y Joseph Mazzello interpretando a John Deacon. La película se estrenó el 24 de octubre de 2018 en Reino Unido y el 2 de noviembre en Estados Unidos. Fue galardonada con cuatro , dos y dos , entre otros premios.

Freddie Mercury fue un personaje secundario en el drama para televisión "Best Possible Taste: The Kenny Everett Story", estrenado en la BBC en octubre de 2012. En el mismo fue interpretado por el actor James Floyd. En noviembre de 2016 se estrenó el docudrama para televisión "The Freddie Mercury Story: Who Wants to Live Forever" en Channel 5. Mercury fue interpretado por el cantante John Blunt, mientras que Patrick Warner actuó como Brian May, Martin Teall como Roger Taylor y Jack Beale como John Deacon. Aunque el film fue criticado por centrarse en la vida amorosa y sexual de Mercury, la interpretación de Blunt recibió elogios. También en noviembre de 2016 se estrenó el cortometraje luxemburgués "Freddie", dirigido por Andy Bausch y en el que Mercury es interpretado por el actor Nilton Martins.



Se incluyen los períodos con Paul Rodgers y Adam Lambert.

Álbumes de estudio:




</doc>
<doc id="29857" url="https://es.wikipedia.org/wiki?curid=29857" title="Teorías sistémicas">
Teorías sistémicas

Las teorías sistemáticas se originan en diversos aportes científicos:

Easton intenta construir una teoría general de la política, que sirviera de armazón conceptual a los estudios empíricos en su disciplina. Para ello intenta analizar el equilibrio de los sistemas en un mundo en constante cambio.
La idea más importante en la teoría de Easton es la de que los sistemas políticos persisten a través del cambio dinámico: "Re-alimentación".

En la vida política como en otros sistemas, dice David Easton ("El sistema político", 1953), el Re-alimentación puede ser mostrado como fundamental tanto para la regulación del error; esto es, para mantener el sistema apuntado en una dirección establecida -preservación del Statu quo-, como para una re-dirección intencional, esto es, para apartarse en busca de nuevas metas para conquistar.

A Easton le preocupa combinar el concepto de equilibrio y el de sistema en la ciencia política y definir conceptual-mente a esta última (tomando distancia del derecho) como la asignación autoritativa de valores, resultado de una interacción sistemáticas, marginando conceptos como Estado, institución o ley.
Busca analizar la vida como sistema de conducta, y para hacerlo termina por construir de manera analítica una abstracción: el sistema político. Para ello utiliza como unidad básica del análisis la interacción entre el sistema político y su ambiente social. El concepto de sistema político abarca el conjunto de interacciones que el investigador considera interesante estudiar y no la totalidad de las que se dan en la política. 

Easton dice que hay que ver al sistema político rodeado de otros ambientes y a la vez como un sistema abierto ya que es influido e influye a los otros ambientes. 

El ambiente total puede dividirse en 2 partes: el ambiente ínter-social y el extra-social. El primero se refiere a todos aquellos sistemas que pertenecen a la misma sociedad que el sistema político. Son segmentos funcionales de la sociedad. En cambio la parte extra social está formada por los sistemas que están fuera de la sociedad dada, es decir la sociedad internacional, el supra-sistema.

Aquello que distingue a las interacciones políticas de las demás interacciones sociales es el hecho de estas dirigidas a concretar la asignación autoritativa de valores en la sociedad (autoritativa porque los actores afectados por ella consideran obligatorias esas decisiones). Entonces la asignación autoritativa de valores y la frecuencia con la que son aceptados éstos por la sociedad constituyen las variables esenciales de un sistema político.

Para Easton los tres componentes del esquema analítico del sistema político son: 

El hecho de que algunos sistemas sobrevivan nos dice que necesitan poseer capacidad de respuesta a las perturbaciones para adaptarse a las circunstancias. Son capaces de regular su propia conducta, transformar su estructura interna y hasta llegar a re-modelar sus metas fundamentales.
Al sistema político le llegan entradas (demandas y apoyos), estas demandas que provienen del ambiente social se basan en las necesidades que se originan en la opinión pública, los intereses, etc. convertidas las necesidades en demandas expresas, estas se trasladan del ambiente social al sistema político responsable de la agregación y articulación de esas demandas. Son las funciones que cumple la ¨"caja negra" que actúa como filtro del sistema, a través de mecanismos de reducción y selección de demandas. Los responsables del filtro de las demandas son aquellos que ocupan determinados roles, sean individuos o grupos, los cuales son capaces de orientar los contenidos del proceso político. El apoyo, es indispensable para transformar las demandas en salidas, en decisiones y acciones (o para mantener decisiones ya tomadas).
Easton distingue entre apoyo difuso —que expresa confianza en la legitimidad del régimen y de la autoridad— y apoyo específico, que es el resultado de decisiones tomadas por la autoridad que han dado una respuesta satisfactoria a los entradas previos, dado que existe un permanente flujo de intercambio de entradas y salidas.

Easton toma un concepto de la cibernética "Re-alimentación" (retro-alimentación) para explicar como un proceso político tiene la posibilidad de controlar y regular los disturbios del sistema. El circuito de retro-alimentación tiene 4 partes:

Podemos emplear el concepto de perturbación para designar las influencias del ambiente total de un sistema que actúan sobre este y lo modifican. No todas las perturbaciones crean necesariamente tensión. Se produce tensión cuando una perturbación sobre una de las variables esenciales del sistema es impulsada más allá de su margen crítico.

En definitiva se trata de una re-orientación de metas tras el intercambio producido entre el ambiente social y el sistema político. Una eficaz "Circularidad" es la que da funcionalidad al cambio político. De no producirse se daría la sobrecarga del sistema político, cuya consecuencia es lo ingobernable.



</doc>
