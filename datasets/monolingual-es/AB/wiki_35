<doc id="37715" url="https://es.wikipedia.org/wiki?curid=37715" title="Fibromialgia">
Fibromialgia

La fibromialgia es una enfermedad crónica que se caracteriza por dolor musculoesquelético generalizado, con una exagerada hipersensibilidad (alodinia e hiperalgesia) en múltiples áreas corporales y puntos predefinidos ("tender points"), sin alteraciones orgánicas demostrables. Se relaciona con una gran variedad de síntomas, entre los que destacan la fatiga persistente y el sueño no reparador. Además suele coexistir con otros trastornos reumatológicos y psiquiátricos.

A mediados del siglo XX, algunos autores consideraron la fibromialgia como un trastorno de somatización, es decir, pacientes que se quejan persistentemente de varios síntomas que no tienen un origen físico identificable. Sin embargo, en los últimos años y tras diversos estudios, esta creencia se ha desestimado. La fibromialgia está considerada como enfermedad por la Organización Mundial de la Salud desde 1992 y, si bien la definición nosológica y su consideración en el ámbito médico no ha estado exenta de controversia, los resultados de las investigaciones indican con bastante consenso que su origen es neurológico, y que el dolor resultaría de desequilibrios neuroquímicos a nivel del sistema nervioso central que generan alodinia e hiperalgesia generalizadas.

No existe una cura universal para la fibromialgia. Los tratamientos empleados van enfocados a controlar los síntomas, incluyendo el ejercicio, las terapias conductuales y ciertos fármacos. En una parte de pacientes se ha demostrado una relación con la sensibilidad al gluten no celíaca y la recuperación o mejoría con el seguimiento de una dieta sin gluten.

El término fibromialgia (del latín "fibra", fibra, que se refiere al tejido conjuntivo, del griego "mio", músculo y "algia", dolor) es relativamente reciente en la historia de la descripción de esta enfermedad.

En 1750, el médico británico sir Richard Manningham en su publicación ""Síntomas, naturaleza, causas y cura de la febrícula o fiebre pequeña: comúnmente llamada fiebre nerviosa o histérica; la fiebre de los espíritus; vapores, hipo o bazo"", cita descripciones similares a las hechas por Hipócrates que podrían corresponder a fibromialgia.

En 1843, el anatomista alemán Robert R. Floriep describe "puntos sensibles", en su tratado de "patología y terapia de los reumatismos" donde propone una asociación entre reumatismo y puntos dolorosos de músculos rígidos.

En 1881 el médico estadounidense George M. Beard escribió un libro llamado «American Nervousness» donde describe a pacientes con neurastenia que presentaban pérdida de fortaleza, fatiga o cansancio de manera crónica, y en quienes se constata la presencia de múltiples puntos dolorosos e hiperalgia.

En 1904, sir William R. Gowers se refiere como "fibrositis" a una condición en que asociaba el lumbago con dolores en los brazos y que él atribuía a "inflamación del tejido fibroso del músculo". En ese mismo año, Stockman, en Escocia, estudia un grupo de pacientes con rigidez y "un movimiento muscular doloroso" en los cuales describe biopsias musculares con "nódulos fibrosíticos".

En 1915, Llewellyn y Jones, definen fibrositis como "un cambio inflamatorio del tejido fibroso intersticial del músculo estriado". En 1936, Hench niega el origen inflamatorio de la fibrositis, y da comienzo al desarrollo de las teorías acerca del origen psicológico de la enfermedad. Hallyday, en 1937, también propone un origen psicológico de la fibrositis.

En 1946, el médico australiano Michael Kelly publicó una serie de estudios de la fibrositis basado en su propia experiencia de sufrir la enfermedad. En dichos trabajos, refuerza el concepto de puntos sensibles como eje central del diagnóstico, y propone una teoría ecléctica entre el origen psicológico y orgánico. En 1947, Boland y Corr, oficiales médicos del ejército estadounidense propusieron que la fibrositis no era más que un ""reumatismo psicógeno"". Phillip Ellman, en 1950, junto con David Shaw, respaldan esta teoría.

A partir de 1963, se establece una escuela psico-reumática donde se hace referencia al dolor por autoalimentación, en donde una personalidad especial provoca una tensión mantenida que genera espasmo muscular y éste, a su vez, genera dolor, completándose de esta manera el círculo estrés-espasmo-dolor. En 1968, Kraft, Johnson y Kabam, establecen criterios diagnósticos basados en los puntos sensibles, la dermatografia y el alivio con el enfriamiento a base de cloruro de etilo.

En 1972, Hugh Smythe describe la enfermedad en los términos modernos, "dolor generalizado y puntos de sensibilidad". En 1975, Harvey Moldofsky y el mismo Smythe, realizaron el primer estudio con electroencefalogramas, donde descubrieron que los pacientes con fibrositis mostraban un patrón de inclusión de ondas alfa, propias del sueño superficial, en el registro de ondas delta del sueño profundo, dando la impresión de un sueño no reparador. En 1976, Hench propone el término de "fibromialgia", y lo define como una forma de reumatismo no articular.

En 1992, la fibromialgia fue reconocida como una enfermedad por la OMS y clasificada con el código M79.7 de la Clasificación Internacional de Enfermedades, en cuya revisión CIE-10 la clasifica dentro de los reumatismos. Y desde ese año, cada 12 de mayo se celebra el Día Mundial de la Fibromialgia y del Síndrome de Fatiga Crónica.

Durante la historia de la investigación de la fibromialgia se han propuesto varios sistemas de clasificación.


La fibromialgia afecta aproximadamente entre el 2 y el 5 % de la población general de diferentes países:
Afecta a las mujeres 10 veces más que los hombres.

Se observa mayoritariamente entre los 20 y los 50 años de edad, aunque existen casos de niños y ancianos aquejados de esta enfermedad. Entre el 10 y el 20 % de los ingresos a clínicas especializadas en reumatología reciben el diagnóstico de fibromialgia, sin embargo se estima que alrededor del 90 % de quienes deberían enmarcarse en un cuadro de fibromialgia permanecen sin diagnóstico, ya sea por desconocimiento del personal sanitario acerca de la misma o porque muchos profesionales no la reconocen como enfermedad.

Las personas con artritis reumatoide y otras enfermedades autoinmunes tienden particularmente a desarrollar fibromialgia.

La etiología y patogenia de la fibromialgia no son entendidas completamente. Se ha observado su relación con trastornos del sistema nervioso central y periférico, alteración en los neurotransmisores y hormonas, alteraciones en el sueño, factores genéticos, alteraciones en el sistema inmunológico, alteraciones psiquiátricas, estrés físico o mental, y alteración en los tejidos periféricos. En algunos casos se ha relacionado con otras enfermedades reumatológicas, infecciones, trauma físico, sustancias químicas, vacunas y con la sensibilidad al gluten no celíaca.

La principal causa de la fibromialgia es la sensibilización central, que se define como una respuesta dolorosa aumentada a la estimulación en el sistema nervioso central. Esta condición es similar a la que se presenta en otras enfermedades caracterizadas por el dolor crónico, tales como el síndrome del intestino irritable, cefalea tensional, los trastornos de la articulación temporomandibular, el síndrome miofascial, el síndrome de dolor regional complejo, el síndrome de las piernas inquietas, el síndrome uretral femenino, la cistitis intersticial y el trastorno por estrés postraumático entre otros.

Esta sensibilización central consiste en una mayor excitabilidad de la vía sensitiva y dolorosa a partir de la segunda neurona. A su vez, esta mayor excitabilidad es producto de estímulos dolorosos repetitivos, que llevan a una modulación endógena deficiente del dolor, en conjunto con un trastorno en los mecanismos de inhibición del dolor a nivel de la médula espinal. La plasticidad neural está modificada a largo plazo en los pacientes con fibromialgia, de tal forma, que un estímulo doloroso repetido o un estímulo normalmente no doloroso se percibe con mayor dolor al compararlo con personas sin este trastorno, manteniéndose esta condición durante la vida.

Hay numerosa evidencia acerca de que la génesis de la fibromialgia está fuertemente relacionada con la sensibilización central. Todos los pacientes con fibromialgia tienen una respuesta exagerada al dolor producido por compresión digital. Se ha demostrado hiperalgesia por dolorímetro o palpómetro. También se ha probado la respuesta al calor, al frío, a la electricidad, mediante pruebas sensoriales cuantitativas (QST). Los pacientes con fibromialgia son hipersensibles al calor, al frío, a la electricidad cutánea, a la electricidad intramuscular, al estímulo eléctrico del nervio sural, a la isquemia, y a la solución hipertónica salina intramuscular. Incluso más, se ha documentado alodinia frente al calor moderado, al frío y a la presión. Se ha demostrado la sumación temporal usando calor, frío, y electricidad intramuscular. También se ha demostrado hipersensibilidad al ruido en condiciones de laboratorio. Se ha documentado la sensibilidad aumentada al dolor utilizando RNMi en respuesta al estímulo de presión y al calor tanto nocivo como inocuo. Un estudio ha demostrado falta en el control inhibitorio del cerebro a estímulos somatosensoriales repetitivos no dolorosos, monitorizando potenciales evocados por evento por electroencefalograma.

Existe dentro de la comunidad médica una corriente que aboga por la utilización del concepto de Síndrome de Sensibilización del Sistema Nervioso Central ("CSS" por sus siglas en inglés) como un nuevo paradigma que sería de utilidad para enfocar la explicación y el diagnóstico de la fibromialgia y de enfermedades como la encefalomielitis miálgica y otros síndromes relacionados, que se presentan con frecuencia creciente como comorbilidades en estos pacientes. Actualmente, se definen las condiciones relacionadas con la sensibilización central como un constructo psicosocial, lo que resulta inapropiado. Términos tales como "síntomas sin explicación médica", "somatización", "trastorno de somatización" y "síndromes somáticos funcionales" deben ser abandonados en el contexto de la sensibilización central.

Los pacientes con fibromialgia no presentan anormalidades características en las pruebas rutinarias de laboratorio. Sin embargo, se han identificado anomalías en los estudios de investigación mediante neuroimagen especializada (por ejemplo, resonancia magnética) y otras técnicas, que revelan diferencias entre los pacientes con fibromialgia y los pacientes control.

En los pacientes con fibromialgia se han encontrado alteraciones en el funcionamiento del eje hipotalámico-hipofisario-adrenal que pueden significar tanto una causa o predisposición a la enfermedad o bien una consecuencia de ella. Entre los hallazgos más consistentes están la disminución del cortisol tanto plasmático como urinario, así como una reducida respuesta a la estimulación corticotrópica. Finalmente, se ha demostrado que alteraciones en este eje representa una predisposición al desarrollo de la fibromialgia. 

Una serie de alteraciones neurohormonales resultan comunes para la patología psiquiátrica y la fibromialgia:


Los estudios con polisomnografía en pacientes que padecen fibromialgia demuestran alteraciones en la continuidad, la arquitectura y la estructura del sueño (sueño fragmentado, disminución de las fases profundas del sueño No Rem, entre otras).

Estudios de electroencefalografía (EEG) han demostrado que pacientes con fibromialgia, presentan intrusiones en el sueño de onda lenta y que las circunstancias que interfieren con la etapa cuatro del sueño, tales como el dolor, la depresión, la deficiencia de serotonina, ciertos medicamentos o la ansiedad, pueden causar o empeorar el trastorno. De acuerdo con la hipótesis de la alteración del sueño, un acontecimiento como un traumatismo o una enfermedad causa trastornos del sueño de tal manera que inicie el dolor crónico que caracteriza a la fibromialgia. La hipótesis supone que la etapa cuatro del sueño es fundamental para la función del sistema nervioso mediado por la serotonina y el triptófano, ya que es durante esta fase, que algunos procesos neuroquímicos en el cuerpo se "restablecen". En particular, el dolor provoca la liberación del neuropéptido sustancia P en la médula espinal, que tiene un efecto amplificador del dolor y causando que los nervios próximos a aquellos que inician el estímulo se vuelvan más sensibles al dolor. La teoría entonces supone que la falta de sueño, por cualquier motivo, puede convertirse en una fuente de inicio de la enfermedad y sus síntomas.

Algunas hipótesis apuntan hacia una predisposición genética, pues la fibromialgia es 8 veces más frecuente entre miembros de la misma familia, destacando especialmente las investigaciones sobre el gen "COMT" que codifica la enzima catecol "O"-metiltransferasa en la vía de las catecolaminas, así como genes relacionados con la serotonina y la sustancia P. En muchos casos, los pacientes con fibromialgia presentan bajos niveles del metabolito de la serotonina, el ácido 5-hidroxindolacético (5-HIAA), así como de triptófano (neurotransmisores encargados de la regulación del impulso nervioso) y elevados niveles de la sustancia P en el líquido cefalorraquídeo. También se ha hallado un bajo nivel de flujo sanguíneo a nivel de la región del tálamo del cerebro, y anormalidades en la función de las citocinas. Sin embargo, estas no son variaciones presentes exclusivamente en fibromialgia, pues se han encontrado patrones similares en otras enfermedades, incluyendo el síndrome de fatiga crónica, el síndrome del intestino irritable y la misma depresión.

Tanto la fibromialgia como el síndrome de fatiga crónica, ambos trastornos estrechamente relacionados, han sido considerados en el pasado como somatizaciones, es decir, pacientes que se quejan crónica y persistentemente de varios síntomas físicos que no tienen un origen físico identificable. Esta polémica en el diagnóstico ha resultado altamente perjudicial para las personas afectadas, dificultando o imposibilitando el manejo de su enfermedad y el alivio del elevado sufrimiento que causa. Actualmente, después de diversos estudios y una mejor comprensión de la fisiología de estos síndromes, esta errónea creencia se ha desestimado.

Tras la demostración por pruebas de neuroimagen de la integridad funcional de los núcleos y áreas cerebrales implicados en la depresión, ha quedado descartado que la fibromialgia sea una variante depresiva.

La fibromialgia está asociada a patología psiquiátrica. Dicha asociación puede ser causal, tratarse de una comorbilidad o ser una consecuencia de la propia fibromialgia. Asimismo, influye negativamente en el curso de la enfermedad, retrasando la mejoría del paciente.

La depresión y la ansiedad son patologías en cuya aparición influyen tanto causas internas como ambientales. Se ha descrito una mayor asociación de ambos trastornos con la fibromialgia. El mecanismo etiopatogénico de dicha asociación no está bien demostrado, pero se han barajado numerosas causas. Tanto la depresión como la ansiedad son patologías muy fuertemente unidas a cualquier tipo de enfermedad crónica y aparecen en un 30% de ellas. Casi un 70% de las personas que padecen dolor crónico presentan alteraciones psicológicas o psiquiátricas. Estas alteraciones psicopsiquiátricas provocan un estado de desánimo y el paciente entra en un círculo vicioso en el que los síntomas de la fibromialgia incrementan la sensación de desesperación, lo que deriva en un empeoramiento de las alteraciones psicopatológicas, e impide sobrellevar la fibromialgia. La alta prevalencia de ansiedad y depresión en los pacientes con fibromialgia se considera más bien una consecuencia de la propia enfermedad.

Existen varios estudios en los que se sugiere que la fibromialgia es consecuencia de los trastornos afectivos o de la depresión. Otros autores afirman que se trata de una cuestión de comorbilidad y que el estado de ánimo deprimido es una consecuencia de la fibromialgia, debido a la dificultad que entraña el dolor para realizar las actividades cotidianas, así como la anticipación al mismo evitando ciertas actitudes que pudieran desencadenar dolor; esto hace que el paciente se suma en un estado de ánimo deprimido, de la misma forma que ocurre con otras enfermedades crónicas incapacitantes.

Un estudio realizado en 1992 establece tres hipótesis diferentes, describiendo la fibromialgia como una manifestación de la depresión, la depresión como una consecuencia de la fibromialgia y la existencia de anormalidades fisiopatológicas comunes a ambas patologías.

Otra hipótesis sugerida determina que el trastorno psiquiátrico se debe a la indefensión que experimenta el paciente como consecuencia del desconocimiento de las causas de sus dolores, por lo que el desconcierto ante el futuro de su enfermedad le provoca un estado de ansiedad.

Otros estudios formulan que la depresión y la ansiedad están independientemente relacionadas con la intensidad del dolor; que la existencia de comorbilidad psiquiátrica en la fibromialgia supone un empeoramiento funcional grave para los pacientes; o que la fibromialgia consiste en una activación del sistema inmunológico, mientras que la depresión es consecuencia de una supresión del mismo, lo que supondría un criterio de diagnóstico diferencial de ambas patologías.

Un estudio centrado únicamente en la ansiedad, recogió que esta está presente en todos los pacientes con dolor crónico y es mayor en los fibromiálgicos. Dicha relación se ve fuertemente beneficiada de los programas de reducción de ansiedad en pacientes con fibromialgia (círculo dolor-ansiedad-tensión-dolor).

Los trabajos de dos científicos españoles, el gastroenterólogo Luis Rodrigo Sáez y el reumatólogo Carlos Isasi Zaragozá, han servido como punto de partida para establecer la hipótesis de que la sensibilidad al gluten no celíaca puede ser la causa de al menos una parte de los casos de fibromialgia.

En agosto de 2014, Rodrigo y colaboradores publicaron, por primera vez en el mundo, una serie de casos de clara mejoría de los síntomas de la fibromialgia únicamente mediante la dieta sin gluten en personas con pruebas negativas para enfermedad celíaca. El estudio incluyó un grupo de 97 pacientes con fibromialgia y síntomas compatibles con un síndrome del intestino irritable.

A finales de ese mismo año (noviembre de 2014) se publicó un segundo trabajo en la misma línea que obtuvo similares conclusiones, corroborando los hallazgos de Luis Rodrigo Sáez, esta vez a cargo de Carlos Isasi Zaragozá. El estudio incluía 20 pacientes con fibromialgia sin enfermedad celíaca, que mejoraron cuando se les retiró el gluten de la dieta. Las investigaciones de Isasi y colaboradores se iniciaron en 2008, con un estudio prospectivo destinado a evaluar el efecto terapéutico de la dieta sin gluten en pacientes con fatiga crónica, fibromialgia y dolor músculo-esquelético crónico generalizado, en los cuales los tratamientos previos no tuvieron éxito. La dieta sin gluten logró una clara mejoría en una importante proporción de los pacientes.

Diversas teorías han propuesto que la fibromialgia puede producirse debido a ciertos factores ambientales. Existen descripciones de casos de fibromialgia en los que las manifestaciones clínicas coinciden con las de una infección viral, como el Epstein-Barr, o bacteriana (por ejemplo, la enfermedad de Lyme) que conlleve a una reacción inmune aberrante. Sin embargo, aún no se ha podido establecer una correlación bien definida entre los síntomas y estos problemas de salud.

Asimismo, se ha barajado una posible implicación de la hipersensibilidad al níquel como causa de la fibromialgia, del síndrome de fatiga crónica y de diversos síntomas, como trastornos gastrointestinales, dolor de cabeza e infecciones recurrentes en general. Sin embargo, los datos disponibles en la literatura médica no son concluyentes y los estudios carecen de evidencias claras.

John E. Sarno propuso que la fibromialgia es una forma severa de síndrome de miositis tensional. Esta teoría no está aceptada de forma oficial.

Durante mucho tiempo se ha asignado erróneamente a la fibromialgia el carácter de enfermedad psicológica o psicosomática a tal punto que se le denominaba reumatismo psicogénico. Sin embargo, estudios de resonancia magnética realizados en pacientes con este problemas de salud han permitido comprobar que los dolores se expresan como estimulaciones de baja intensidad —a diferencia de las personas sanas— de las áreas del cerebro responsables del dolor, como la corteza somatosensorial primaria y secundaria somatosensorial, la corteza prefrontal, el lóbulo parietal inferior, la corteza cingular anterior, la Ínsula, los ganglios basales, el putamen y el cerebelo. Es probable que ese bajo umbral de tolerancia al dolor cause que el paciente con fibromialgia presente "hiperestesia", un estado de dolor excesivo a la presión y otros estímulos.

De todos modos continúa el debate respecto a si la fibromialgia es una enfermedad en sí misma o si es, en realidad, un conjunto de síntomas (es decir, un síndrome) correspondiente a enfermedades reumatológicas, neurológicas o inmunológicas, de difícil y costoso diagnóstico o si, simplemente, se trata de la acentuación de un conjunto de procesos (fatiga, trastornos del sueño, dolores, etc.) que no constituyen una enfermedad en sí mismos aunque su conjunción genere importantes problemas para la vida normal de quien los padece. Como muchos de los síntomas son comunes de otros trastornos, los pacientes con fibromialgia que no reciben un diagnóstico correcto y quienes no son informados con detalle sobre su enfermedad suele provocárseles un incómodo y costoso peregrinaje por distintos sistemas y servicios de salud con la consecuente propuesta de una lista de pruebas paraclínicas y terapias no específicas, a veces agresivas y con efectos iatrogénicos, que empeora el pronóstico del proceso y vuelve incierto el futuro del paciente.
No se han detectado alteraciones físicas en los puntos que el paciente afirma dolorosos, razón por la cual las hipótesis más actuales al respecto de su causa se dirigen hacia la neurociencia, en busca de fenómenos de sensibilización a nivel del sistema nervioso central y mantenimiento del dolor por alteraciones en las respuestas de los neuromediadores. Aunque el resultado del examen físico general casi siempre es normal y las personas tienen un aspecto saludable, un examen cuidadoso de los músculos de las personas con fibromialgia revela zonas sensibles al tacto en lugares específicos, llamados "puntos hipersensibles". Estos son áreas del cuerpo que resultan dolorosas cuando se ejerce presión sobre ellas. La presencia y el patrón de estos puntos característicamente hipersensibles diferencian la fibromialgia de otras afecciones.

El principal elemento caracterizador de la fibromialgia es el dolor musculoesquelético difuso y generalizado o rigidez prominente que afecta al menos 3 localizaciones anatómicas por más de 3 meses, sin lo cual no se puede realizar el diagnóstico del trastorno. El dolor suele ser intenso y en muchas ocasiones difícil de describir, y en general, empeora con el ejercicio físico intenso, el frío y el estrés emocional.

Los sitios frecuentes en los cuales se presentan los síntomas de fibromialgia incluyen la región lumbar (espalda baja), cuello, tórax y muslos. La alteración de los músculos se refiere a un calambre doloroso y localizado que en ocasiones se asocia con otros problemas (embarazo, por ejemplo). En algunos casos se observa espasmo muscular localizado.

Otros síntomas adicionales pueden incluir incontinencia urinaria, dolor de cabeza, migrañas, movimientos periódicos anormales de las extremidades (movimientos paroxísticos), en especial de las piernas (síndrome de pierna de gatillo), dificultad de concentración y dificultad para recordar cosas (mala memoria); también es frecuente un aumento de la sensibilidad táctil, escozor generalizado, resequedad de ojos y boca, zumbidos y campanilleos en los oídos (acúfenos), alteraciones de la visión (fosfenos) y algunos síntomas neurológicos de incoordinación motora. Se ha asociado a la enfermedad de Raynaud como una manifestación clínica de rara presentación durante el curso de esta enfermedad.

Entre el 70 y el 90% de quienes padecen fibromialgia refieren también trastornos del sueño, expresados como un sueño no reparador, ligero e inestable. Se suelen asociar además un grupo heterogéneo de síntomas incluyendo debilitamiento intenso (adinamia) y hasta incapacitante (astenia), alteraciones del ritmo intestinal, rigidez en las extremidades superiores o inferiores, y muy frecuentemente episodios depresivos acompañados de crisis de ansiedad. Los trastornos del sueño son muy frecuentes en pacientes con dicha patología. Estos trastornos consisten básicamente en abundantes pesadillas, sueño no reparador, que puede ser el causante de un trastorno conocido como hipersomnio diurno, y gran cantidad de descargas dolorosas en los músculos durante el sueño.

La fatiga en grado extremo está presente en todas las actividades que realizan las personas con fibromialgia, por lo que sus tareas cotidianas se ven inevitablemente dificultadas. Dependiendo de la gravedad y de la variación del grado, este cansancio puede ser desde soportable hasta una discapacidad casi infranqueable que limita sus tareas tanto en el ámbito familiar como en el profesional.

Aunada inseparablemente a este cansancio, como causa que lo aumenta y agrava, está la mala calidad del dormir, que impide a quienes tienen este padecimiento tener un sueño reparador y, por consiguiente, impedirá el descanso lo que acentuará el cansancio y la fatiga en el futuro.

La fibromialgia se diagnostica más frecuentemente en individuos que padecen ciertas enfermedades, como la artritis reumatoide, el lupus eritematoso sistémico y la espondilitis anquilosante (artritis espinal). Asimismo, los pacientes con fibromialgia pertenecen a los denominados "grupos de riesgo" de padecer enfermedad celíaca. En un gran número de casos, existen hallazgos objetivos de otras enfermedades asociadas, en la mayoría de las ocasiones artrosis o un síndrome doloroso de partes blandas localizado.

Antes de hacer un diagnóstico de fibromialgia, los pacientes deben ser evaluados a fondo para determinar la presencia de otros trastornos.

Muchos casos de fibromialgia no se adaptan con precisión a un conjunto estandarizado de criterios diagnósticos. Sin embargo, no se cree que sea un diagnóstico por exclusión, aunque algunos autores lo hayan etiquetado como tal. Debido a que hay una ausencia de criterios diagnósticos definitivos y absolutos que se puedan aplicar de manera general a todos los pacientes, los médicos a menudo recurren al diagnóstico de fibromialgia después de que las pruebas que realizan para otros diagnósticos diferenciales resulten negativas.

En lugar de asumir un diagnóstico de fibromialgia, considerar cuidadosamente una multitud de posibles diagnósticos disminuirá la probabilidad de un diagnóstico erróneo. (Véase el apartado Diagnóstico diferencial.)

Los pacientes con fibromialgia no presentan anormalidades características en las pruebas de laboratorio. Sin embargo, los estudios de laboratorio de rutina y de imagen son importantes para ayudar a descartar enfermedades con síntomas similares y para ayudar en el diagnóstico de ciertas enfermedades inflamatorias que aparecen frecuentemente junto con la fibromialgia. No existe una prueba específica –ni analítica, ni de imagen, ni patológica– para el diagnóstico de la fibromialgia; por lo tanto, cualquier prueba que se realice se hará principalmente para excluir otras enfermedades que pueden simular a la fibromialgia o para descartar una enfermedad asociada. Si no hay motivos de sospecha clínica de otras enfermedades que requieran técnicas de diagnóstico por imágenes (por ejemplo, enfermedades articulares), no se recomienda realizar radiografías u otras pruebas de imagen.

Lo aconsejado por los expertos es solicitar unos análisis, incluyendo un hemograma -recuento celular sanguíneo completo-, velocidad de sedimentación globular (VSG), enzimas muscularespruebas de función tiroidea, proteína C reactiva (PCR), un perfil bioquímico general, estudio del hierro (hierro sérico, capacidad total de fijación del hierro, índice de saturación y ferritina), vitamina D, vitamina B12, magnesio, análisis de orina y, probablemente, algunas pruebas reumáticas, como factor reumatoide y ANAs. Algunos autores desaconsejan la realización de estas últimas pruebas de modo sistemático, pues pueden ser positivas en la población sana y por sí mismas tienen un pobre valor predictivo. Dependiendo de la historia clínica y el examen médico, pueden ser necesarias más pruebas sanguíneas, si se sospecha de otros diagnósticos diferenciales.

Es aconsejable, mediante una cuidadosa historia clínica, identificar la presencia de trastornos del sueño o del estado de ánimo, y actuar en consecuencia. Los estudios del sueño pueden ser útiles en pacientes cuyo sueño no mejora con las medidas conservadoras habituales (por ejemplo, la eliminación de la cafeína, la prescripción de hipnóticos o de antidepresivos tricíclicos nocturnos). Estos estudios se pueden realizar como parte de una evaluación formal por un neurólogo o un neumólogo con experiencia en trastornos del sueño.

Como tal, el diagnóstico de la fibromialgia se basa en los síntomas del paciente, consistentes fundamentalmente en dolor difuso y crónico. Los pacientes refieren artralgias –dolores articulares– y mialgias –dolores musculares–, pero no hay evidencias objetivas de inflamación articular o muscular en la exploración física, ni en los análisis. A la exploración se encuentran múltiples puntos dolorosos a la presión en localizaciones extraarticulares.

El diagnóstico de fibromialgia debería ser considerado en cualquier paciente que se queje de que ""me duele todo"". En general, la mayoría de los médicos diagnostican el síndrome de fibromialgia por la sintomatología clínica, la exploración física y la exclusión de otros procesos que puedan causar síntomas semejantes. Con el objetivo de un diagnóstico precoz y aplicando criterios para una buena relación de coste-efectividad, algunos autores desaconsejan una búsqueda general, sin una orientación clínica clara –“vamos a ver si encontramos algo”– para descartar cualquier potencial causa de dolor y cansancio.

En 1990 el American College of Rheumatology (ACR) propuso unos criterios de clasificación, para proporcionar cierta homogeneidad en los estudios clínicos, que han sido aceptados y usados durante estos años de modo general. Estos criterios exigen para el diagnóstico de fibromialgia la presencia de dolor generalizado, afectando ambos lados del cuerpo –izquierdo y derecho–, así como por encima y por debajo de la cintura, de más de 3 meses de duración, junto con la presencia de al menos 11 de los 18 posibles puntos dolorosos a la presión ("tender points").

Estos sencillos criterios tienen una sensibilidad y especificidad superior al 85% para diferenciar a los pacientes con fibromialgia de aquellos con otras enfermedades reumáticas. En la práctica clínica, el diagnóstico de fibromialgia puede ser hecho si la historia clínica es consistente y se han descartado otros procesos, aunque no se tengan 11 puntos dolorosos. Por otra parte, es frecuente que los pacientes con fibromialgia tengan dolor a la presión en múltiples zonas del cuerpo, además de en los puntos dolorosos.

Los criterios de la ACR para la clasificación de los pacientes se establecieron originalmente como criterios de inclusión para fines de investigación y no fueron pensados para el diagnóstico clínico, pero se han convertido "de facto" en criterios diagnósticos en el ámbito clínico. El número de puntos dolorosos, ha sido una cuestión controvertida, pues pueden variar con el tiempo y las circunstancias, y es un dato subjetivo –el médico presiona y el paciente dice que le duele-. Por otra parte, un buen número de pacientes con otros problemas y de la población general tienen un recuento elevado de puntos dolorosos, mientras que aproximadamente un 20% de los pacientes diagnosticados de fibromialgia pueden no cumplir el criterio de los puntos dolorosos.

En 2010, el ACR aprobó unos criterios diagnósticos para la fibromialgia que no requieren el examen de los puntos dolorosos y se enfocan más en la presencia de múltiples síntomas y su intensidad, pudiendo también facilitar la realización de un diagnóstico más seguro de fibromialgia a los no especialistas. Estos criterios muestran una buena correlación con los de 1990.

Para el diagnóstico de fibromialgia, estos criterios utilizan un sistema de puntuación en el que se valora el Índice de dolor generalizado (IDG) -número de zonas corporales dolorosas entre 19 posibles, en las dos semanas anteriores- y el Índice de severidad de los síntomas (ISS) -valoración del grado de cansancio, sueño no reparador, síntomas cognitivos y síntomas somáticos generales-.

En el año 2011 se realizó una modificación de estos criterios –modificación de 2011 de los criterios ACR de 2010–, para permitir su utilización en estudios clínicos y epidemiológicos sin la necesidad de un examinador, requiriendo solo el rellenado de un cuestionario por el propio paciente. Por otra parte, esta modificación evita la necesidad de remitir al paciente al reumatólogo solo para realizar el diagnóstico. Estos criterios modificados han sido validados en diversos grupos, incluyendo la población española, y en alteraciones dolorosas crónicas variadas.

La utilización de estos criterios modificados para el diagnóstico de fibromialgia ha sido sujeto de controversia en la comunidad médica.

La fibromialgia, como el dolor de cabeza, el síndrome del intestino irritable, el síndrome de fatiga crónica y otros síndromes funcionales, seguirá siendo un diagnóstico controvertido debido a la ausencia de alteraciones objetivas que expliquen las quejas. Más controvertido es aún el uso de estos criterios, que tienen una utilidad diagnóstica, para valorar el daño corporal o su uso en litigios.

Dada la diversidad de síntomas, las patologías que deben considerarse para la realización del diagnóstico
diferencial son múltiples.

Los cuadros de los que hay que diferenciar a la fibromialgia son fundamentalmente la artritis reumatoide y el lupus eritematoso sistémico (poliartritis, alteraciones sistémicas, aumento de VSG o PCR), la polimialgia reumática (aumento de VSG, rigidez, edad elevada), miositis (debilidad muscular y elevación de enzimas musculares), hipotiroidismo (alteración en las pruebas de función tiroidea), hiperparatiroidismo (hipercalcemia) y neuropatías (historia clínica y eventual estudio neurofisiológico).

Otras patologías a considerar incluyen trastornos afectivos (como la depresión), la enfermedad celíaca (poco valorada y conocida, puede causar dolor generalizado y fatiga si no se diagnostica y no se trata; es preciso asumir la importante negatividad de los anticuerpos antitransglutaminasa, que dan falsos negativos hasta en cerca de 48% de los casos), la sensibilidad al gluten no celíaca (que con frecuencia esconde una enfermedad celíaca no reconocida ni diagnosticada), la costocondritis, la hepatitis C (más del 15% de los pacientes presentan criterios de fibromialgia, al inicio de la enfermedad hepática), hipofosfatemia (debida a laxantes o antiácidos; ocasiona debilidad muscular, pero raramente dolor muscular), compresión de alguna raíz lumbar (los síntomas solo ocurren en un miembro inferior y suelen empeorar con la maniobra de Valsalva), meningoencefalitis no virales (con secuelas crónicas, con cefalea y dolor difuso), la apnea del sueño (puede originar dolor y fatiga), enfermedades paraneoplásicas (los tumores de pulmón producen de forma significativa sintomatología neurológica, que suele aparecer bruscamente, y en varones fumadores y de mayor edad), las encefalitis y meningitis postvirales, la distrofia simpático refleja (el dolor se localiza en un miembro y existen síntomas vasomotores), estenosis espinal, trastornos de la articulación temporomandibular, enfermedades metabólicas óseas (osteomalacia/osteoporosis), coexistencia de varios reumatismos de partes blandas, miopatías metabólicas, infecciones virales (parvovirus B19, Epstein-Barr) y el síndrome de fatiga crónica.

Aunque todavía no existe una cura universalmente aceptada para la fibromialgia, hay tratamientos que han demostrado durante ensayos clínicos controlados ser eficaces en la reducción de los síntomas como la educación del paciente, el ejercicio, las terapias conductuales y el consumo de ciertos fármacos.

La fibromialgia puede ser difícil de tratar y se suele tener mejores resultados si el tratamiento es manejado por médicos de varias disciplinas familiarizados con esta condición y su tratamiento, una aproximación denominada tratamiento multidisciplinario. Algunos especialistas involucrados en el tratamiento de la fibromialgia incluyen médicos de cabecera, internistas generales, reumatólogos, fisioterapeuta, entre otros. Algunas ciudades de gran tamaño cuentan con clínicas para el dolor o una clínica especializada en reumatología donde se puede obtener tratamiento específico para la fibromialgia.

Entre las terapias no farmacológicas, las intervenciones nutricionales están demostrando actualmente una creciente importancia. Los programas incluyen la educación nutricional, dietas específicas, suplementos nutricionales y estrategias para perder peso.

Se ha de tener en cuenta que mucha gente afectada por la fibromialgia ha estado parte de su vida yendo de un médico a otro sin saber qué le pasaba. La educación sanitaria, la información y la comunicación con otras personas afectadas son una forma importante de terapia. El personalizar el plan de tratamiento suele ser efectivo para que se adapte a las necesidades individuales de cada paciente. Algunos pacientes presentan síntomas leves y necesitan muy poco tratamiento una vez que comprenden el trastorno que padecen y lo que la empeora. Otras personas, sin embargo, necesitan un programa de cuidado completo, que incluirá medicamentos, ejercicio y entrenamiento acerca de las técnicas para el manejo del dolor.

La estimulación magnética transcraneana reduce el dolor en pacientes con fibromialgia, con resultados comparables con las terapias farmacológicas.

En el tratamiento de la fibromialgia se incluyen fármacos de los siguientes tipos:

Los antidepresivos tricíclicos, tales como la amitriptilina y la doxepina, se han usado extensamente para el tratamiento de la fibromialgia, y han demostrado tener resultados beneficiosos en la calidad del sueño, el bienestar general y en el nivel del dolor en un 25 a un 37% de los pacientes y en un grado de hasta cuatro veces mejor que comparado con placebo. No obstante, el dolor a la estimulación de los puntos sensibles no desaparece.

Estos fármacos actúan elevando el nivel de serotonina, noradrenalina y dopamina en el cerebro. Los niveles bajos de serotonina no están vinculados únicamente con la depresión clínica, sino también con los trastornos del sueño asociados con la fibromialgia. En dosis menores a 50 mg al día, la amitriptilina mejora los síntomas de las personas con fibromialgia.

Otras acciones de los antidepresivos tricíclicos son anticolinérgicos, antihistamínicos, y bloqueadores alfa adrenérgicos, originan efectos secundarios tales como somnolencia diurna, estreñimiento, cefaleas, sequedad de la boca y aumento del apetito lo que, a su vez, puede generar trastornos nutricionales. Estos efectos secundarios raramente son graves, pero pueden resultar molestos.

Se han utilizado fluoxetina, citalopram, sertralina y paroxetina. La fluoxetina en dosis relativamente altas mejora el dolor general, los síntomas depresivos y la fatiga. El citalopram no ha demostrado resultados positivos. La sertralina ha demostrado mejorías someras. En resumen, esta familia de medicamentos ha demostrado poca efectividad en el tratamiento de la fibromialgia.

No obstante lo anterior, la combinación de antidepresivos tricíclicos con inhibidores selectivos de la recaptación de serotonina, aumenta los beneficios de cada fármaco, logrando una mejor respuesta que cada uno por sí solo.

La venlafaxina, el milnacipram y la duloxetina son fármacos de esta familia utilizados en el tratamiento de la fibromialgia.

La moclobemida y el pridinol son fármacos de esta familia usados en el tratamiento de la fibromialgia.

Los medicamentos antiinflamatorios que se utilizan para tratar muchas afecciones reumáticas no son útiles para las personas con fibromialgia, ya que una característica de la misma es que no existen patologías en los músculos pese a sentir dolor el paciente. Sin embargo, se ha reportado que dosis moderadas de fármacos antiinflamatorios no esteroideos (AINEs) o de analgésicos pueden aliviar parte del dolor aunque no se ha evaluado la extensión del efecto placebo en estos casos, probablemente importante al tratarse de pacientes con una larga trayectoria de contactos con el sistema de salud.

Una subcategoría de los AINEs, principalmente los selectivos en la inhibición de la COX-2, también puede proporcionar alivio del dolor, causando menos efectos secundarios sobre el estómago e intestino que los AINEs tradicionales.

Recientemente se han realizado estudios para la utilización con éxito de anticonvulsivos o antiepilépticos; la pregabalina y la gabapentina se emplean con éxito en el tratamiento del dolor agudo en enfermedades neurológicas como el síndrome de Guillain-Barré, las polineuropatías periféricas y la esclerosis múltiple, así como la fibromialgia. Recientemente se ha publicado una revisión de la Colaboración Cochrane que concluye que la gabapentina reduce el dolor en un tercio de los pacientes con dolor neuropático. Estos medicamentos no provocan efectos secundarios adversos sobre el sistema digestivo (estómago, intestino e hígado). Su principal efecto secundario es el aumento de peso, que a su vez favorece una mayor fatiga y dolores en ciertos puntos como rodillas, tobillos, espalda, etc.

Recientemente se han iniciado ensayos clínicos usando la Terapia hormonal sustitutiva (estrógenos en parche) en el tratamiento de la fibromialgia (ante la hipótesis de que un descenso de los niveles plasmáticos de estrógenos puede condicionar un descenso del umbral del dolor), con pobres resultados por ahora.

Es recomendable intentar evitar determinadas terapias que no han sido sometidas a estudios relevantes, ni evaluadas mediante criterios independientes y que con frecuencia son presentadas mediante técnicas de publicidad engañosa.

Las intervenciones nutricionales están demostrando actualmente una creciente importancia. Los programas incluyen la educación nutricional, dietas específicas y estrategias para perder peso. Los suplementos alimenticios que aporten nutrientes celulares y liberen radicales libres del organismo pueden ayudar en gran manera con los dolores y combatir la enfermedad. Un suplemento alimenticio muy utilizado es el magnesio. Además, existen estudios recientes que apuntan que algunas terapias alternativas, como el yoga, pueden aliviar el dolor causado por la fibromialgia. Otras opciones consisten en masajes, ejercicios acuáticos y terapia ocupacional.

La dieta sin gluten ha demostrado ser un eficaz tratamiento que consigue la remisión total o la mejoría de los síntomas en una parte de pacientes con fibromialgia, en los cuales el cuadro clínico es motivado por la presencia de una enfermedad celíaca no reconocida ni diagnosticada, o una sensibilidad al gluten no celíaca.




</doc>
<doc id="37722" url="https://es.wikipedia.org/wiki?curid=37722" title="Toscana">
Toscana

La Toscana es una de las veinte regiones que conforman la República Italiana. Su capital y ciudad más poblada es Florencia. Está ubicada en Italia central, limitando al norte con Emilia-Romaña, al este con Marcas, al sureste con Umbría, al sur con Lacio, al oeste con los mares Tirreno y de Liguria, y al noroeste con Liguria. Con es la quinta región más extensa del país, por detrás de Sicilia, Piamonte, Cerdeña y Lombardía.

Sus 633 km de costas (397 km continentales e 236 km insulares) están bañados por el mar de Liguria en el tramo centro-septentrional entre Carrara (junto a la desembocadura del torrente Parmignola, limítrofe con la Liguria) y el golfo de Baratti; y el mar Tirreno baña a su vez el tramo costero meridional entre el promontorio de Piombino y la desembocadura del Chiarone, que marca el límite con el Lacio.

La región de Toscana es una de las mayores y más importantes regiones italianas por su patrimonio artístico, histórico, económico, cultural y geográfico. Está ubicada en la zona central de Italia y es una de los principales destinos turísticos del país.

El nombre es antiquísimo y deriva del epónimo usado por los griegos y los latinos para definir la tierra habitada por los etruscos: "Etruria", que evolucionó luego a "Tuscia" y de ahí a "Toscana". Incluso los confines de la actual Toscana se corresponden con la línea máxima de la Etruria antigua, que comprendían incluso partes de las actuales regiones del Lacio y de Umbría, hasta el Tíber.

De forma más bien triangular y situada entre la parte septentrional del mar Tirreno y los Apeninos centrales, la Toscana tiene una superficie de aproximadamente 22.993 kilómetros cuadrados. Aunque rodeada y cruzada por las principales cadenas montañosas, dejando entre sí algunas escasas llanuras caracterizadas por ser muy fértiles, en la región abunda un relieve dominado por el paisaje ondulado que conforman las colinas.

El territorio toscano está poblado de colinas y montes: el Apenino está surcado por las cuencas de la Lunigiana, la Garfagnana, el Mugello y el Casentino. Las llanuras son escasas, costeras como la Versilia y la Maremma o aluviales como la llanura del Arno. En la costa se alternan largas playas (Versilia, zona de Cecina)y promontorios (Plombino, Argentario); las islas del archipiélago toscano pueden considerarse una prolongación de los relieves costeros. 

Mientras que las montañas se extienden por el 25% del área total, y las pocas llanuras solo un 8,4% de su superficie, casi todas coincidiendo con el valle del río Arno, el conjunto suma un total de 1.930 kilómetros cuadrados, de los que las colinas suponen dos tercios (66,5%) de la superficie total de la región, que se extiende por 15.292 kilómetros cuadrados.

El monte más alto de Toscana es el Monte Prado (2.054 msnm), ubicado en Sillano Giuncugnano, Provincia de Lucca. Hay volcanes como el Monte Amiata 1.738 msnm. El principal río es el Arno (241 km); otros son el Ombrone, el Serchio y el Cecina. El lago costero más extenso es la Laguna de Orbetello con una superficie de 26,2 km². 

Los 633 km en total de costa son 397 km continentales y 230 km insulares. Limitan con la Toscana el mar de Liguria y el mar Tirreno; dos accidentes destacados el Canal de Piombino y el canal de Córcega. Las islas principales de la costa toscana son Elba, Giglio, Capraia, Montecristo y Pianosa.

El clima, que es bastante suave en las zonas costeras, es más lluvioso y severo en el interior, con considerables fluctuaciones de temperatura entre el invierno y el verano lo que da a la región un suelo que construye ciclo de helada-deshielo activo que es en parte responsable de que en el pasado fuera el almacén de cereales de la Antigua Roma.

Las reservas naturales cubren un total de 227.000 hectáreas, casi el 10 % del territorio toscano. En la región se encuentran tres , uno de ellos, el Parque Nacional del Archipiélago Toscano, se encuentra totalmente dentro del territorio toscano, mientras que los otros dos se sitúan entre Toscana y la región de Emilia-Romaña: el parque nacional delle Foreste Casentinesi, Monte Falterona y Campigna y el parque nacional de los Apeninos tosco-emilianos.

La época anterior a los etruscos en la región, a finales de las edades de Bronce y del Hierro son semejantes a los primeros griegos, los micénicos y después surgió la civilización etrusca. Los etruscos crearon la primera gran civilización en esta región, suficientemente grande para establecer una infraestructura de transporte, para implementar la agricultura y la minería y producir un arte vibrante. 

A finales del Neolítico (después del 1000 a. C. ) el territorio entre el valle del Arno y el valle del Tíber, estuvo poblado por los etruscos: pueblo misterioso, quizás de proveniencia oriental, cuya lengua pre-indoeuropea no ha sido aún completamente descifrada.

Entre los siglos VII y VI (a. C.) los etruscos extendieron su hegemonía a una parte de la Pianura Padana y de Córcega. 

En el siglo V (a. C.), derrotados por los griegos y cartagineses, se retiraron dentro de sus fronteras tradicionales y finalmente (siglo IV a. C.) fueron vencidos y sometidos por Roma y desaparecieron.

Más tarde, los romanos dominaron la Etruria anexionándosela en el año 351 a. C., que pasó a ser una de las catorce regiones en que Augusto dividió Italia. Los romanos establecieron las ciudades de Lucca, Pisa, Siena y Florencia, y dotaron a la zona con nuevas tecnologías y desarrollo, y aseguraron la paz. Entre sus logros están varias calzadas, que transformaron el aspecto de Italia en general y de la Toscana en particular: 
Estos nuevos caminos romanos evitaron cuidadosamente los grandes centros etruscos, cuya decadencia no tardó en seguir, mientras que las nuevas ciudades romanas cobraban la importancia. Es el caso, por ejemplo, de "Pistoriae" (Pistoia), que, al principio, era solo una ciudad pequeña fortificada al borde de la vía Casia.

En el siglo VI, los longobardos llegaron a la región y eligieron Lucca como la capital de su Ducado de Tuscia.

Después de la destrucción del reino lombardo por Carlomagno en el siglo VIII, se convirtió en un condado, el condado de Lucca. Luego fue un marquesado con Adalberto I en el siglo IX. En el siglo XI el marquesado pasó a la familia Attoni de Canossa, quienes también tenían Módena, Reggio Emilia y Mantua. Matilde de Canossa fue su miembro más famoso. La muerte de Matilde de gota en el año 1115 marcó el final de una era en la política italiana. Legó sus posesiones al papado, lo que determinó el comienzo de una lucha entre el Papado y el Imperio por este territorio.

Con los francos la región empezó a recuperarse; pero solamente a partir del siglo XII comenzó una auténtica mejoría económica y política, cuando Pisa estableció relaciones comerciales con el Oriente y Siena y Florencia emprendieron actividades industriales (textiles) y financieras (bancos).

La ruta de peregrinación por la Vía Francígena entre Roma y Francia llevó riqueza y desarrollo a la región en la Edad Media. El conflicto entre los güelfos y gibelinos, facciones que apoyaban al Papado y al Sacro Imperio Romano Germánico en el centro y el norte de Italia durante los siglos XII y XIII, dividieron al pueblo toscano. Estos dos factores contribuyeron al surgimiento e independencia de ricas comunidades medievales: Arezzo, Florencia, Lucca, Pisa y Siena. Una familia que se benefició de la creciente riqueza de Florencia fueron los Médicis, que pasaron a gobernarla.

La economía de la región se mantuvo próspera hasta mediados del siglo XIV, cuando una desastrosa sucesión de carestía, peste y reveses financieros hicieron zozobrar a Florencia y a Siena.

En Florencia, en 1434, la Señoría de los Médicis sucedió al gobierno democrático (que en realidad era ya oligárquico). Cosme el Viejo y Lorenzo el Magnífico la convirtieron en la ciudad cultural de Italia, estabilizaron la economía y con ellos llegó a ser una potencia nacional.

En el siglo XVI, bajo Cosme I, la Señoría se transformó en Gran Ducado de Toscana, se conquistó Siena y se fundaron Academias y Universidades; solo la república de Lucca permaneció autónoma. 

En el siglo XVI, el papa Clemente VII y Carlos V nombraron a Alejandro de Médicis como el primer gobernante formalmente hereditario. Siena no fue incorporado a Toscana hasta 1555. Con el apoyo del emperador, Cosme I de Médicis, derrotó a los sieneses en la batalla de Marciano (1554), y asedió Siena. A pesar de la desesperada resistencia de sus habitantes, el 17 de abril de 1555, después de un asedio de quince meses, la ciudad cayó, su población disminuida de 40.000 a 8000 personas. En 1559 Montalcino, el último reducto de independencia sienesa, fue anexionada a los territorios de Cosme. En 1569 el papa Pío V lo elevó a la posición de Gran Duque de Toscana.

Los sucesores de Cosme no estuvieron a la altura de sus predecesores, se empobreció la economía del Estado y disminuyó el papel de Toscana en la política italiana.

Cuando Juan Gastón, el último de los Médicis, murió sin herederos, el Gran Ducado fue heredado por la casa de Lorena. La guerra de sucesión polaca en los años 1730 significó la transferencia de Toscana de los Médicis a Francisco Esteban, duque de Lorena y marido de la emperatriz de Austria María Teresa. 

Leopoldo de Habsburgo-Lorena gobernó Toscana entre 1765 y 1790, realizando diversas reformas ilustradas; también se convirtió en emperador romano desde 1790 hasta 1792 a la muerte de su hermano José II. El Gran Ducado pasó entonces a Fernando III, quien fue depuesto por Napoleón Bonaparte en el año 1801 de manera que Toscana pudo entregarla a los duques borbones de Parma como una compensación por la pérdida de su ducado. Durante este breve período, el Gran Ducado de Toscana fue conocido como el Reino de Etruria. En 1814 volvió al poder Fernando III, pero bajo la influencia de los austriacos.

Con la revolución de 1848, el Gran Duque Leopoldo II dictó una constitución. No obstante, en 1849 se implantó una república y Leopoldo II abandonó el territorio. Lo ocupó de nuevo con fuerzas austríacas y este regreso al poder apoyado con fuerzas austriacas hizo que perdiera el apoyo popular. En las guerras de independencia italianas de los años 1850, Toscana fue transferida de Austria a la nación unificada de Italia. Leopoldo II abandonó definitivamente Toscana en al año 1859. Un plebiscito, promovido el 15 de marzo de 1860 por el Gobierno Provisional Toscano, decretó la anexión al Reino de Cerdeña regido por Víctor Manuel II y, de ahí, al naciente Reino de Italia.

Bajo Benito Mussolini, el territorio pasó bajo el dominio del líder local del Partido Nacional Fascista, Dino Perrone Compagni. Tras la caída de Mussolini y el restablecimiento del Reino de Italia, la República Social Italiana fue establecida en las regiones septentrionales de Italia, con su frontera "de facto" en la Línea Gótica, una posición defensiva justo al norte de Florencia. Después del final de la República Social, y la transición de un Reino a la moderna República Italiana, la Toscana nuevamente floreció como un centro cultural de Italia.

Toscana tiene 3.734.355 de habitantes (31-03-2010), con una densidad de población de 162,40 hab./km² en 2010, por debajo de la media nacional. Esto se debe principalmente a la baja densidad de población de las provincias de Arezzo, Siena y, sobre todo, Grosseto. La densidad más alta es la de la Provincia de Prato seguida por las provincias de Pistoia, Livorno, la Ciudad metropolitana de Florencia y la Provincia de Lucca, principalmente en las ciudades de Florencia, Livorno, Prato, Viareggio, Forte dei Marmi y Montecatini Terme. La distribución territorial de la población está cercanamente vinculada al desarrollo socio-cultura y, más recientemente, el desarrollo económico e industrial de la Toscana.

En consecuencia, las zonas menos densamente pobladas son aquellas en las que la actividad principal es la agricultura, a diferencia de las otras donde, a pesar de la presencia de un número de grandes complejos industriales, las principales actividades están relacionadas con el turismo y los servicios relacionados, junto con una plétora de pequeñas firmas en el cuero, el vidrio, el papel y los sectores del vestido.

A partir de los años 1980, la región atrajo un intenso flujo de inmigrantes, en particular de China. Hay también una significativa comunidad de residentes británicos y estadounidenses. Para el año 2008, el ISTAT calculaba que 275.149 inmigrantes nacidos en el extranjero vivien en la Toscana, lo que equivale al 7% de la población regional total.

La capital de la Toscana, Florencia, es la localidad más poblada, com 369.745 habitantes (2010). Otras ciudades que superan los 50.000 habitantes son Prato (187.526 hab.), Livorno (160.884), Arezzo (99.812), Pistoia (90.265), Pisa (87.939), Lucca (84.826), Grosseto (81.216), Massa (70.947), Carrara (65.489), Viareggio (64.311), Siena (54.441) y Scandicci (50.215).

Toscana está dividida administrativamente en una ciudad metropolitana y nueve provincias, todas ellas nombradas de la misma forma que la capital de la provincia. La más grande de ellas es la Provincia de Grosseto, mientras que la más poblada es la Ciudad metropolitana de Florencia y la de mayor densidad de población es la Provincia de Prato. 

Toscana es plaza fuerte del partido de centro-izquierda Partido Democrático, formando con Emilia-Romaña, Umbría y las Marcas el famoso "Cuadrilátero Rojo" de la política italiana. En las elecciones generales de Italia de 2008, la Toscana dio más del 50 % de sus votos a Walter Veltroni, y solo 33,6 % a Silvio Berlusconi.

Aunque su importancia decae, la agricultura aún contribuye a la economía de la región. En las zonas de tierra adentro de la región se cultivan los cereales, las patatas, las aceitunas y las uvas usadas para elaborar vino. Las zonas pantanosas, bonificadas, producen actualmente hortalizas, arroz, tabaco, remolacha y girasoles.

El sector industrial está dominado por la minería, dada la abundancia de recursos subterráneos. El subsuelo es relativamente rico en recursos minerales, con menas de oro, cobre, mercurio y minas de lignito, las famosas "soffioni" (fumarolas) en Larderello y vastas minas de mármol en Versilia. También tienen cierta importancia la industria química y farmacéutica, la industria del metal y del acero, el vidrio y la cerámica, así como el sector editorial. Áreas más pequeñas especializadas en la manufactura se encuentran tierra adentro: el cuero y el calzado en la parte suroeste de la Ciudad metropolitana de Florencia, la zona de plantas de invernadero en Pistoia, la cerámica y las industrias textiles en el área de Prato, motocicletas y motos en Pontedera, y el procesamiento de madera para la manufactura de los muebles de madera en la zona de Cascina. La industria pesada (minería, acero e ingeniería mecánica) se concentra a lo largo de la franja costera (zonas de Livorno y Pisa), donde también hay importantes industrias químicas. También debe destacarse la zona productora de mármol de Carrara y la industria del papel en Lucca.

La industria textil y de la moda son pilares de la economía florentina. En el siglo XV, los florentinos estaban ya trabajando con telas de lujo como la lana y la seda. Hoy los grandes diseñadores en Europa usan la industria textil en Toscana y especialmente Florencia.

Casi sin excepción, cada ciudad y pueblo de Toscana tiene una considerable belleza natural y arquitectónica. Hay una corriente continua de visitantes a lo largo del año. Como resultado de ello, los servicios y las actividades de distribución, que son tan importantes para la economía de la región, son particularmente diversas y altamente organizadas.

Los principales aeropuertos de la región son los de Pisa — Galileo Galilei y Florencia — Peretola.

El toscano es el dialecto italiano que menos se ha distanciado del latín y ha evolucionado de forma lineal y homogénea. El toscano es la base de la lengua italiana, adoptado por ser el dialecto en el que escribieron sus obras grandes autores italianos como Dante Alighieri, Francesco Petrarca, Giovanni Boccaccio o Nicolás Maquiavelo, que le confirieron la autoridad de ser la "lengua literaria" de la península itálica. Tras el proceso de unificación de Italia fue adoptado como lengua oficial, gracias a la teoría desarrollada por Alessandro Manzoni para la elección de la lengua para la escritura de su obra Los Novios, una lengua «aclarada en las aguas del Arno».

Los hablantes del dialecto toscano superan los tres millones, aunque cabe destacar que en la provincia de Massa-Carrara en lugar de este se habla el dialecto carrarés, clasificado entre los dialectos septentrionales del italiano.

El toscano es una mezcla de dialectos menores que presentan algunas diferencias que los distinguen entre ellos. A continuación se expone la subdivisión de los dialectos toscanos, dividida en septentrional, oriental, meridional y occidental. Dentro de los subdialectos toscanos se incluye el dialecto corso cismontano, hablado en la Córcega septentrional.

Toscana es conocida a nivel mundial por su gran riqueza de monumentos y obras de arte, así como por los conjuntos históricos de las ciudades de Florencia, Pisa, Siena y Lucca; menos conocidas para el público internacional resultan las ciudades de Arezzo, Grosseto, Pistoia y Prato y son prácticamente desconocidas para el turismo los monumentos de Livorno, Massa y Carrara.

Toscana ha sido a lo largo de la historia una región especialmente fecunda en el campo del arte. Civilizaciones como los etruscos y estilos artísticos como el Renacimiento tuvieron su origen en tierras toscanas. El actual territorio de la región cuenta con restos, yacimientos y testimonios de importancia de aquellas civilizaciones que la poblaron y de aquellos estilos artísticos que en ella se desarrollaron.

De la época etrusca destacan las necrópolis de Sovana, Vetulonia y Populonia. Entre los restos romanos que perduran en Toscana cabe destacar la colonia de Cosa, una de las excavaciones mejor conservadas de los primeros años de la República romana. De la Edad Media y sobre todo del Renacimiento proceden los restos más destacados de Toscana, declarados Patrimonio de la Humanidad por la UNESCO. 

Los lugares toscanos inscritos como Patrimonio de la Humanidad por la UNESCO son los siguientes:


Las villas de los Médici son unos complejos arquitectónicos rurales que formaron parte de las posesiones de la familia Médici entre los siglos XV y XVII y que se encuentran en los alrededores de Florencia y en otros lugares de Toscana. Las villas eran palacios reales situados en la periferia de los terrenos administrados por los Médici, además de servir como centros de la actividad económica del área en la que estaban situadas.

La simplicidad es un elemento central de la cocina toscana. Se usan legumbres, pan, queso, hortalizas, champiñones y fruta fresca. Se elabora aceite de oliva virgen extra con aceitunas de Moraiolo, Leccino y Frantoiano. La trufa blanca de San Miniato aparece en octubre y noviembre. También se recoge en esta región el hongo "funghi porcini".

Como entremés, son típicos los "crostini di fegatini", una especie de tostadas de hígado. Plato de pasta típico de Toscana son los "pappardelle", que son como unos tallarines más anchos. Otros primeros platos son: "pappa al pomodoro", la "ribollita", la "carabaccia" y la "minestra al cavolo nero".

Hay embutidos como la "finocchiona," una especie de salchichón aromatizado con hinojo, y también "prosciutto" toscano.

Carne de la mejor calidad procede del valle de Chiana, específicamente una ganadería conocida como Chianina usada para el bistec a la florentina "(bistecca alla fiorentina)", carne magra con hueso. La carne a la parrilla procede de Valdarno y del Mugello. También se produce carne de cerdo. Otras carnes destacadas son la "trippa alla fiorentina", estofado en vino y el "lampredotto", o el "filleto alla griglia".

En cuanto a los postres, son famosos los "cantucci" de Prato, bizcochos o galletas de almendras duros que se remojan en vin santo, o el "castagnaccio," elaborado con harina de castaña y piñones. La "schiacciata alla fiorentina" es un dulce típico del período de Carnaval, muy adecuado para fiestas infantiles, cuyos ingredientes principales son: naranjas y huevos.
En la tradición gastronómica toscana destacan sus vinos. El más famoso de ellos es, sin duda alguna, el Chianti, el mejor conocido internacionalmente. Actualmente, los vinos de Toscana se dividen en DOCG y 34 DOC, además de numerosas IGT, entre los que destacan algunas producciones de altísimo nivel, conocidas en el mundo enológico con el apelativo de "supertoscanos".

Mientras las zonas de la Toscana central (Ciudad metropolitana de Florencia y Provincia de Siena) son destacadas en todo el mundo desde hace décadas por la producción del Chianti Classico, del Brunello di Montalcino y del Vino Nobile di Montepulciano, en los últimos años van siempre afirmándose en la escena internacional y los vinos producidos en las zonas costeras (provincia de Livorno y Grosseto), favorecidos principalmente por el clima más suave, seco y soleado, entre los cuales destaca la excelencia del Bolgheri Sassicaia o del tignanello, la estructura y el equilibrio de los vinos tintos de la Maremma (Morellino di Scansano, Montecucco y Monteregio di Massa Marittima) y la armonía de los blancos (Ansonica). Paralelamente al gran éxito de las producciones enológicas de las zonas costeras, deben señalarse también los siempre notables vinos tintos y blancos de la provincia de Arezzo. Entre los demás vinos de la Toscana, deben mencionarse el Carmignano, uno de los más antiguos de la Toscana producido en la comuna homónima en la provincia de Prato, la célebre Vernaccia di San Gimignano, el Pitigliano, el Montescudaio y el Vin Santo.

El equipo de fútbol más exitoso de la Toscana es la Fiorentina, que ha disputado más de 70 temporadas en la Serie A de Italia. Ha conseguido dos títulos y cinco subcampeonatos de liga, seis Copas de Italia y un subcampeonato de Europa. Otros equipos de la región que han disputado la Serie A son el Livorno, Empoli, Pisa, Lucchese, Siena y Pistoiese.

El Mens Sana Basket de Siena ha obtenido la Lega Basket Serie A siete veces consecutivas entre 2006/07 y 2012/13.

El Autódromo Internacional del Mugello es sede del Gran Premio de Italia del Campeonato Mundial de Motociclismo desde el año 1994. También ha albergado carreras del Campeonato Mundial de Superbikes, Campeonato Mundial de Resistencia, Campeonato FIA GT y Deutsche Tourenwagen Masters. En albergará por primera vez a la Fórmula 1, como sede del Gran Premio de la Toscana.

La Strade Bianche es una carrera de ciclismo de un día que se corre alrededor de Siena desde el año 2007 como parte del UCI WorldTour.




</doc>
<doc id="37723" url="https://es.wikipedia.org/wiki?curid=37723" title="Síndrome del túnel carpiano">
Síndrome del túnel carpiano

El síndrome del túnel carpiano es una neuropatía periférica que ocurre cuando el nervio mediano se comprime dentro del túnel carpiano, a nivel de la muñeca. 

El nervio mediano es un nervio sensitivo motor. Conduce la sensibilidad de la cara palmar del pulgar, índice, mayor y mitad radial del anular así como la eminencia tenar en la palma. También inerva a los músculos intrínsecos de la mano fundamentalmente de la eminencia tenar.

El túnel carpiano, un pasadizo estrecho y rígido del ligamento y los huesos en la base de la mano, contiene los tendones y el nervio mediano. Está delimitado en su parte proximal por los huesos pisiforme, semilunar, piramidal y escafoides; y su parte distal por el trapecio, trapezoide, el grande y el ganchoso. El techo del túnel está formado por el ligamento denominado retináculo flexor. A través de este túnel discurren cuatro tendones del músculo flexor común superficial de los dedos de la mano, cuatro tendones del músculo flexor común profundo de los dedos de la mano y el tendón del músculo flexor largo del pulgar. Cualquier proceso que provoque ocupación del espacio (inflamación de alguno de estos tendones, presencia de líquido, etc.) provoca la disminución de espacio y el atrapamiento del nervio. Algunas veces, el engrosamiento de los tendones irritados u otras inflamaciones estrechan el túnel y hacen que se comprima el nervio mediano. El resultado puede ser dolor, debilidad o entumecimiento de la mano y la muñeca, irradiándose por todo el brazo. Aunque las sensaciones de dolor pueden indicar otras condiciones, el síndrome del túnel carpiano es de las neuropatías por compresión más comunes y ampliamente conocidas en las cuales se comprimen o se traumatizan los nervios periféricos del cuerpo. Normalmente la presión dentro del túnel del Carpio es de 7-8 mm Hg, pero en situaciones de patología alcanza hasta 30 mm Hg; a esta presión ya hay disfunción. Cuando la muñeca se flexiona o se extiende la presión puede aumentar hasta 90 mm Hg o más, lo que ocasiona isquemia en el vaso nervorum. Esto puede llevar a un ciclo vicioso, al aparecer edema vasogénico, aumentando más la presión intratúnel.

Es la neuropatía periférica de la mano por atrapamiento más frecuente, afectando hasta a un 90 % de la población general, con una mayor incidencia en mujeres entre las décadas cuarta y sexta de la vida. Constituye un lugar muy destacado en Salud Ocupacional. En Estados Unidos, la incidencia actual es del 0,1 %, y en la población trabajadora del 15% al 20% (CIB: Dr. Enrique Urrea, 2010).
Cuando el médico general o neuroanestesiólogo percibe en las radiografías el tendo ternio o comprometido (que se necesita operación), también se necesitan una gran experiencia para mantener la muñeca bien extendida.

Los síntomas generalmente comienzan gradualmente (también pueden aparecer súbitamente en algunos casos) y se manifiestan con sensaciones de calor, calambre o entumecimiento en la palma de la mano y los dedos, especialmente del pulgar y de los dedos medio e índice. Algunos pacientes que padecen el síndrome del túnel carpiano dicen que sus dedos se sienten hinchados e inútiles, a pesar de no presentar una hinchazón aparente. Los síntomas a menudo aparecen primero en una o ambas manos durante la noche, con una sensación de adormecimiento de las puntas de los dedos, originada por dormir con las muñecas dobladas. Una persona con síndrome del túnel carpiano puede despertarse sintiendo la necesidad de “sacudir” la mano o la muñeca. 

A medida que los síntomas se agravan, los pacientes comienzan a sentir el calambre durante el día. La disminución en el pulso de la mano puede dificultar cerrar el puño, agarrar objetos pequeños o realizar otras tareas manuales. En casos crónicos o sin tratamiento, los músculos de la base del pulgar pueden debilitarse o atrofiarse. Algunas personas no pueden distinguir el frío y el calor a través del tacto. A veces el dolor se manifiesta en la parte superior de la mano y muñeca. Otras veces, la muñeca y la mano se quedan dormidas. Los síntomas suelen aparecer en sujetos cuya ocupación laboral incluye la realización de movimientos repetitivos de la muñeca, lo que puede provocar inflamación ligamentosa y compresión nerviosa, aunque otras causas como fracturas o lesiones ocupantes de espacio también pueden estar en el origen de su desarrollo. La clínica aumenta con la actividad de la muñeca afectada (habitualmente la dominante) y puede remitir sacudiendo o masajeando la muñeca o elevando el miembro afectado (por mejora del retorno y descompresión).

Si progresa lo suficiente, el síndrome puede provocar atrofia de dicha musculatura (eminencia tenar) que limita de manera importante la funcionalidad de la mano afectada.

Frecuentemente, el síndrome del túnel carpiano es el resultado de una combinación de factores que aumentan la presión en el nervio y los tendones medianos en el túnel carpiano, en lugar de ser un problema del nervio propiamente dicho. El trastorno se debe muy probablemente a una predisposición congénita: el túnel carpiano es simplemente más pequeño en algunas personas que en otras. Otros factores que contribuyen al diagnóstico incluyen traumatismos o lesiones en la muñeca que causan la hinchazón, tal como una torcedura o una fractura; hiperactividad de la glándula pituitaria; hipotiroidismo (baja función de la glándula tiroides); artritis reumatoide; problemas mecánicos en el empalme de la muñeca; estrés laboral; uso repetitivo de musculatura del antebrazo (mecánicos, informáticos, masajistas, dentistas, etc.); retención de líquido durante el embarazo o la menopausia, o el desarrollo de un quiste o de un tumor en el túnel carpiano. En algunos casos es imposible determinar las causas.

Existen pocos datos clínicos que confirmen que el realizar movimientos repetitivos y forzados con la mano y la muñeca en actividades laborales o de diversión puede causar el síndrome del túnel carpiano (aunque sí es cierto que hay un alto índice de afectados en trabajos manuales de esfuerzo). Los movimientos repetitivos que se realizan en el curso normal del trabajo u otras actividades diarias pueden dar lugar a trastornos de movimientos repetitivos tales como bursitis (inflamación de una bursa, pequeña bolsa que facilita el movimiento de los músculos y tendones sobre el hueso), tendinitis (inflamación de los tendones) y sobre todo un hipertono, falta de elasticidad y fluidez entre músculos y tendones.
Los puntos posibles de atrapamiento del nervio mediano, son:


El calambre de escritor —una condición causada por una falta en la coordinación motriz, dolor y presión en los dedos, la muñeca o el antebrazo como consecuencia de una actividad repetitiva— no es un síntoma del síndrome del túnel carpiano.

En el año 2006 se aprobó el nuevo cuadro de enfermedades profesionales para España. En él se recogía que la profesión de camarero se encuentran entre las principales actividades capaces de producir esta dolencia.

También es conocida como una "enfermedad de guitarrista" ya que por el constante movimiento del brazo hasta la mano es muy posible que se provoque esta enfermedad.

La semiología clínica es el arma diagnóstica de primera línea tanto para el médico general como para el traumatólogo. La clínica, así como la actividad laboral, son fuertemente sugestivas de esta patología, pero existen algunos signos clínicos y pruebas complementarias que confirman el diagnóstico. Entre los datos que orientan hacia la existencia de un síndrome del túnel del carpo se encuentran una serie de maniobras que deliberadamente disminuyen o aumentan el espacio de tránsito por dicho túnel, comprobando con ello si aumenta o disminuye la sintomatología:

Pero la prueba diagnóstica más sensible y específica y que confirma definitivamente la existencia de compresión del nervio es la Electromiografía. Con esta prueba se establece la velocidad de conducción nerviosa del mediano, manifestándose como un retardo de la conducción nerviosa sensitiva y motora a su paso por el carpo.

Los signos de Phalen y Tinel son supremamente orientativos. En caso de dudas, la electromiografía (EMG) es obligada.

Si se trata de un túnel carpiano secundario a una causa conocida y tratable (diabetes, obesidad, artritis reumatoide, infecciones, hematomas, etcétera) deberá abordarse primero el tratamiento de la causa primaria.

En los casos en los que no existe una causa aparente o aquellos de origen funcional el tratamiento se basa en diferentes medidas:
por una parte la prevención, adoptando, en la medida de lo posible, hábitos de movimiento de la muñeca menos traumáticos o programando períodos alternativos de actividad-descanso. Si esto no es suficiente se inmoviliza la articulación de la muñeca con una férula de descarga y con antiinflamatorios que disminuyan la presión ejercida sobre el nervio mediano. Según las últimas revisiones de la Cochrane el uso de ortesis nocturna es más efectivo que el no tratamiento a corto plazo.Cuando es necesario se realiza incluso la infiltración local de antiinflamatorios (habitualmente esteroideos) apuntándose a esta técnica como una de las más eficaces en el tratamiento agudo del síndrome.

El tratamiento fisioterapéutico incluye el uso de CHC (compresas húmedas calientes), parafina. El tratamiento con movilizaciones y ejercicios no ha demostrado eficacia suficiente en comparación con otras intervenciones no quirúrgicas. En electroterapia es recomendado el uso de ultrasonido o láser, aunque las últimas revisiones científicas no han demostrado eficacia del US en el Síndrome del Túnel Carpiano.
En casos más avanzados o resistentes a tratamiento conservador se propone tratamiento quirúrgico (cirugía), consistente en ampliar el espacio de tránsito del nervio, siendo este el tratamiento más eficaz a largo plazo.

Cuando una de las causas del síndrome sea el uso de teclados, se recomienda el cambio al teclado Dvorak, que está disponible en todos los sistemas operativos. La distribución de las teclas está optimizada en el teclado Dvorak para que sea necesario menos movimiento de dedos para escribir el mismo texto. Con el uso de este teclado, muchos usuarios con trastornos de movimientos repetitivos o síndrome del túnel carpiano han declarado haber notado un alivio o la desaparición de las lesiones.

Además de cambiar por elementos ergonómicos, tanto el teclado, el "mouse" y "mousepad", incluyendo también la silla ergonómica para mantener una buena postura (para prevenir los problemas a largo plazo), se debe apoyar todo el antebrazo sobre el escritorio para utilizar el "mouse" (esta postura reduce la presión sobre la mano).





</doc>
<doc id="37732" url="https://es.wikipedia.org/wiki?curid=37732" title="Aldebarán">
Aldebarán

Aldebarán (Alfa Tauri / α Tau / 87 Tauri / HIP 21421) es la estrella más brillante de la constelación de Tauro («El Toro») y la del cielo nocturno. De magnitud aparente +0,85, es de color rojo anaranjado. Aunque visualmente parece ser el miembro más brillante del cúmulo abierto de la Híades, en realidad no forma parte del mismo y simplemente está en la misma línea de visión. Junto a Sirio (α Canis Majoris) y Arturo (α Bootis), permitió a Edmund Halley descubrir el movimiento propio de las estrellas mediante la comparación de sus posiciones de entonces con las que figuraban en los catálogos antiguos.

El nombre Aldebarán proviene del árabe الدبران, "al-dabarān", cuyo significado es «la que sigue», en referencia a que esta estrella sigue al cúmulo de las Pléyades en su recorrido nocturno a través del cielo. Numerosas fábulas populares la utilizan para designar al hombre y a la mujer perseverante que no acepta derrota.

Ptolomeo la llamó "portador de la antorcha", y en griego también recibió el nombre de "Omma Boos", nombre que más tarde fue traducido literalmente al latín: "Oculus Tauri" (ojo del toro).
En el siglo XVII, el astrónomo Giovanni Riccioli la denominó más específicamente "Oculus Australis" («ojo del sur»). 

En Persia la estrella era conocida como "Satvis" y "Kugard".
El astrónomo persa Al Biruni citaba "Al Fanik" («el camello semental»), "Al Fatik" («el camello gordo») y "Al Muhdij" («el camello hembra») como nombres indígenas árabes para esta estrella.
En la antigua Roma recibía el nombre de "Palilicium", término que proviene de Palilia o Parilia, la fiesta de Pales, divinidad pastoril de la mitología romana.
El título de "Hrusa" designaba a esta estrella en la antigua Bohemia.

En astronomía hindú se identifica con la nakshatra —mansión lunar— de Rohini, y es una de las veintisiete hijas de Daksha y la esposa del dios Chandra.

Situada a 65,1 años luz de distancia, Aldebarán es una estrella gigante naranja de tipo espectral K5III con una temperatura superficial de 4010 K. Al estar cerca de la eclíptica es ocultada por la Luna periódicamente y, gracias a ello, se ha podido medir su diámetro angular, 0,020 segundos de arco, lo que conduce a un radio 44 veces mayor que el radio solar. Situada en el lugar del Sol, se extendería hasta la mitad de la órbita de Mercurio, y en el cielo de la Tierra aparecería como un disco de 20° de diámetro.
Su velocidad de rotación proyectada es de 5,2 km/s, por lo que su período de rotación puede ser de hasta 400 días.

Aldebarán es 425 veces más luminosa que el Sol pero su masa es solamente de 1,7 masas solares; dado el enorme tamaño de esta estrella, su densidad media resulta ser muy inferior a la del Sol.
A diferencia de la mayor parte de las gigantes naranjas, que en su núcleo están fusionando helio en carbono y oxígeno, Aldebarán se hallaría en un estado preliminar en el que su núcleo todavía inerte de helio estaría en el proceso de contraerse y calentarse, provocando que, en conjunto, la estrella se expanda y aumente en brillo.
Está empezando a perder masa por medio de un viento estelar que envuelve la estrella hasta una distancia de 100 ua.
Dentro de solo unos pocos millones de años, la estrella alcanzará una luminosidad 800 veces mayor que la del Sol, momento en el que comenzará la quema del helio y se frenará la contracción del núcleo, lo que conllevará una disminución en su brillo.
Aldebarán está catalogada como una variable pulsante irregular, con una fluctuación del brillo de 0,2 magnitudes.

Aldebarán es un sistema binario: la estrella gigante tiene una compañera lejana y pequeña, Aldebarán B, de magnitud +13,50. Es una enana roja de tipo espectral M2V, cuya masa puede ser tan solo el 15 % de la masa solar y su radio el 36 % del radio solar. Su separación actual de Aldebarán A es de 609 ua.

En la actualidad la sonda Pioneer 10 se dirige hacia Aldebarán, a donde llegará dentro de 1 690 000 años.

En 1997 se anunció el descubrimiento de un planeta gigante, Aldebarán b, de unas 9,5 masas de Júpiter, en órbita alrededor de la estrella principal. El descubrimiento fue dudoso y no fue confirmado hasta 2015. De hecho es difícil detectar la existencia de un compañero subestelar en torno a una estrella muy evolucionada como Aldebarán, ya que las estrellas gigantes, al ser muchas veces estrellas pulsantes, muestran oscilaciones en su velocidad radial análogas a las causadas por la presencia de planetas.



</doc>
<doc id="37734" url="https://es.wikipedia.org/wiki?curid=37734" title="Cuarenta y uno">
Cuarenta y uno

El cuarenta y uno (41) es el número natural que sigue al cuarenta y precede al cuarenta y dos.






</doc>
<doc id="37736" url="https://es.wikipedia.org/wiki?curid=37736" title="Cuarenta y tres">
Cuarenta y tres

El cuarenta y tres (43) es el número natural que sigue al 42 y precede al 44.





</doc>
<doc id="37737" url="https://es.wikipedia.org/wiki?curid=37737" title="Cuarenta y siete">
Cuarenta y siete

El cuarenta y siete (47) es el número natural que sigue al 46 y precede al 48.






</doc>
<doc id="37739" url="https://es.wikipedia.org/wiki?curid=37739" title="Cuarenta y dos">
Cuarenta y dos

El cuarenta y dos (42) es el número natural que sigue al cuarenta y uno y precede al cuarenta y tres.



La descomposición de un número en la suma de tres cubos enteros, toma la forma:

siendo formula_2 un número entero positivo, y formula_3 tres números enteros (positivos o negativos). Se trata de una ecuación diofántica (un tipo de problemas con números enteros que ya se estudiaban en la Grecia clásica). 

Esta sencilla condición (desde el punto de vista aritmético), es extraordinariamente difícil de resolver desde el punto de vista numérico, puesto que no queda más remedio que ir comprobando ingentes combinaciones de números (con la ayuda de algoritmos auxiliares, que permiten descartar a priori determinadas ternas de números). De hecho, el reto se planteó en 1954, y desde entonces se fueron resolviendo todos los casos comprendidos entre 0 y 100, excepto el caso del número 33 (resuelto por Andrew Bookerm en abril de 2019) y el caso del número 42 (solucionado por el propio Bookern y por Andrew Sutherland en septiembre de 2019). El problema se ha resistido a los matemáticos durante 65 años.

La descomposición hallada del número 42 es la siguiente:

Para obtener la gigantesta potencia de cálculo necesaria para abordar el problema, se utilizó un sistema colaborativo en red a través de internet, denominado Charity Engine, que emplea la potencia de cálculo sobrante de 500.000 ordenadores domésticos, desinteresadamente cedida por sus propietarios a través de una aplicación en línea.




Dada su representación binaria, codice_1, es de gran significado por ejemplo para la cultura Geek y "friki", como se puede apreciar en la temática de ciencia ficción ya mencionada ("The X-Files", "The Hitchhiker's Guide to the Galaxy", "Lost"…). El grupo geek "Geeks Without Bounds", utilizaron la fecha de la representación binaria (10 de octubre de 2010) para lanzar su campaña ("101010 +) GWOBorg Hackathon") por el acceso a la información en países desfavorecidos.

En este sentido, incluso Canonical decidió lanzar ("Maverick Meerkat") con eslóganes del tipo "redondo, perfecto..". aunque en realidad la fecha de lanzamiento pueda parecer obvia.

Para otros grupos como los ecologistas también ha sido significativo. Greenpeace, por ejemplo, aprovechó la fecha para lanzar su campaña "Greenpeace 10/10/10 Day of Climate Action" ("Día de Soluciones Climáticas") liderando una acción global también emprendida en el mismo sentido por la ONG 350.org.

Mientras, en Twitter y Facebook también se estableció tal fecha como "El día del porno" (hashtag codice_2) por su representación en numeración romana (XXX).



</doc>
<doc id="37752" url="https://es.wikipedia.org/wiki?curid=37752" title="Parques nacionales de Inglaterra y Gales">
Parques nacionales de Inglaterra y Gales

Los Parques nacionales del Reino Unido son áreas controladas de paisaje sobresaliente donde las actividades de poblamiento y comerciales se encuentran restringidas. Casi todo el terreno en los parques nacionales del Reino Unido es de propiedad y administración privadas. Hay 14 parques nacionales en el Reino Unido en la actualidad, de los cuales 9 están en Inglaterra abarcando el 7% del territorio inglés y 3 en Gales cubriendo alrededor del 20% de la zona terrestre de Gales. 

El control de los parques nacionales en el Reino Unido está descentralizado, con cada condado desarrollando su propia política y disposiciones.

Los dos de Escocia son el de Cairngorms y el del Loch Lomond y los Trossachs. En la actualidad no existen parques nacionales en Irlanda del Norte, pero, al igual que en otras zonas del Reino Unido, existen muchas áreas de extraordinaria belleza natural. Hay movimientos controvertidos para establecer un parque nacional en los montes de Mourne.



</doc>
<doc id="37765" url="https://es.wikipedia.org/wiki?curid=37765" title="(90377) Sedna">
(90377) Sedna

Sedna es el cuerpo menor del sistema solar número 90377; concretamente es un objeto transneptuniano. En 2012 se encontraba aproximadamente tres veces más lejos del Sol que Neptuno. Durante la mayor parte de su órbita está incluso más lejos del Sol, con su afelio estimado en 960 unidades astronómicas (ua) —32 veces la distancia de Neptuno—, por lo que es uno de los objetos más lejanos conocidos del sistema solar, que no sean los cometas de período largo. La órbita excepcionalmente larga y elongada de Sedna, que tarda unos 11 400 años en completarse, y su lejano punto de máxima aproximación al Sol, a 76 ua, han dado lugar a mucha especulación en cuanto a su origen.

Fue descubierto el 14 de noviembre de 2003 desde el observatorio de Monte Palomar. El nombre de Sedna proviene de la diosa de la mitología esquimal del mar y de los animales marinos. Hostil a los hombres y dotada de una altura gigantesca, Sedna estaba condenada a vivir en las frías profundidades del océano Ártico.

La espectroscopía reveló que la composición de su superficie es similar a la de otros objetos transneptunianos, siendo en gran medida una mezcla de hielo y tolina con metano y nitrógeno congelados. Su superficie es una de las más rojas en el sistema solar. No se conoce bien ni su masa ni su tamaño y la Unión Astronómica Internacional no lo ha reconocido formalmente como un planeta enano, aunque varios astrónomos estiman que lo es.

El Minor Planet Center lo coloca en el disco disperso, un grupo de objetos enviados a órbitas muy alargadas por la influencia gravitacional de Neptuno. Sin embargo, esta clasificación es cuestionada ya que Sedna nunca se acerca lo suficiente a Neptuno como para que pueda afectarle, lo que llevó a algunos astrónomos a concluir que en realidad es el primer miembro conocido de la región interior de la nube de Oort. Otros especulan con que podría haber sido empujado a su órbita actual por una estrella en tránsito, tal vez del seno del grupo de nacimiento del Sol, o incluso que fuera capturado de otro sistema estelar. Otra hipótesis sugiere que su órbita puede ser evidencia de otro planeta más allá de la órbita de Neptuno. El astrónomo Michael E. Brown —codescubridor de Sedna y de los planetas enanos Eris, Haumea y Makemake— cree que es el objeto transneptuniano más importante encontrado hasta la fecha, pues el estudio de su inusual órbita puede aportar información valiosa acerca del origen y la evolución temprana del sistema solar.

Mike Brown de Caltech, Chad Trujillo del Observatorio Gemini y David Rabinowitz de la Universidad de Yale el 14 de noviembre de 2003 descubrieron Sedna —designado provisionalmente 2003 VB — como parte de un estudio que comenzó en 2001 con el telescopio Samuel Oschin en el Observatorio Palomar, en el que emplearon la cámara Palomar Quest de 160 megapíxeles de Yale. Observaron un objeto que se movía en 4,6 segundos de arco en 3,1 horas con respecto a las estrellas, indicando que su distancia era de aproximadamente 100 ua. Las observaciones de seguimiento en noviembre-diciembre de 2003 con el telescopio SMARTS en el Observatorio de Cerro Tololo en Chile, así como con el telescopio Tenagra IV del Observatorio W. M. Keck en Hawái, revelaron que el objeto se movía a lo largo de una órbita distante altamente excéntrica. Posteriormente, el objeto fue identificado en imágenes "precovery" realizadas anteriormente por el telescopio Samuel Oschin así como en las imágenes del programa Near Earth Asteroid Tracking. Estas posiciones anteriores ampliaron su arco orbital conocido y permitió un cálculo más preciso de la órbita.

«Nuestro objeto recientemente descubierto es el lugar más frío y más lejano conocido en el
sistema solar», dijo Mike Brown en su página web, «así que siento que es apropiado
nombrarlo en honor de Sedna, la diosa inuit del mar, que se creía que vivía en la parte inferior del frígido océano Ártico». Brown también le sugirió al Minor Planet Center de la Unión Astronómica Internacional que los objetos que fueran descubiertos en la región orbital de Sedna en el futuro también deberían llevar el nombre de las entidades en la mitología del Ártico. El equipo hizo público el nombre «Sedna» antes de que el objeto hubiera sido oficialmente numerado. Brian Marsden, director del Minor Planet Center, dijo que tal acción era una violación del protocolo, y que algunos miembros de la UAI podrían votar en contra. Sin embargo, no hubo objeciones al nombre, y no se propusieron otros. El Comité de la IAU sobre Nomenclatura de Cuerpos Menores aceptó formalmente el nombre en septiembre de 2004, y también consideró que, en casos similares de extraordinario interés, podría permitir en el futuro que se anunciaran los nombres antes de que fueran numerados oficialmente.

Sedna tiene el mayor período orbital de cualquier objeto grande conocido en el sistema solar, calculado en cerca de 11 400 años. Su órbita es extremadamente excéntrica, con un afelio que se calcula en 937 ua y un perihelio de unas 76 ua, siendo el mayor para los objetos conocidos del sistema solar. Cuando fue descubierto se encontraba a aproximadamente 89,6 ua del Sol, y era el objeto más distante observado del sistema solar. Posteriormente el mismo equipo de investigadores descubrió Eris a 97 ua. Si bien las órbitas de algunos cometas de periodo largo se extienden más allá de la de Sedna, son demasiado difusos para ser descubiertos excepto cuando se aproximan a su perihelio en el sistema solar interior. Aunque Sedna alcance su perihelio hacia 2076, el Sol aparecería simplemente como una estrella muy brillante en su cielo, solo cien veces más brillante que la luna llena en la Tierra, y demasiado lejos como para ser visible como un disco a simple vista.

Cuando fue descubierto se pensaba que Sedna tenía un período rotacional inusualmente largo —veinte a cincuenta días—. Inicialmente se especuló con que su rotación era ralentizada por una compañera binaria, como Caronte, la luna de Plutón. En marzo de 2004 el telescopio espacial Hubble buscó ese satélite pero no encontró nada, y las medidas posteriores del telescopio sugirieron períodos de rotación mucho menores —de 10 h aproximadamente—, bastante típico para un cuerpo de su tamaño.

Sedna tiene una magnitud absoluta banda V —H— de aproximadamente 1,8 y se estima que tiene un albedo de alrededor de 0,32, lo que le otorga un diámetro de aproximadamente 1000 km. En el momento de su descubrimiento fue el objeto intrínsecamente más brillante que se encontró en el sistema solar desde Plutón en 1930. En 2004, los descubridores estimaron el límite máximo de su diámetro en 1800 km, pero en 2007 este valor fue revisado y reducido a menos de 1600 km después de ser observado por el telescopio espacial Spitzer. En 2012, las mediciones del Observatorio Espacial Herschel sugirieron que el diámetro de Sedna es de 995 ± 80 km, lo que lo haría más pequeño que Caronte. Como Sedna no tiene satélites conocidos, determinar su masa es imposible en la actualidad sin enviar una sonda espacial. Sin embargo, si además de los cálculos anteriores para su diámetro se toma como referencia la densidad de Plutón de 2,0 g/cm, el rango de masa estimada es aproximadamente 1 x 10 kg.

Las observaciones de los telescopios "SMARTS" muestran que en luz visible Sedna es uno de los objetos más rojos del sistema solar, casi tan rojo como Marte. Se sugirió que el color rojo oscuro de Sedna se debe a una capa superficial de lodo con hidrocarburos, o tolina, formada a partir de compuestos orgánicos más sencillos tras una larga exposición a la radiación ultravioleta. Su superficie es homogénea en color y espectro, lo cual puede deberse a que Sedna, a diferencia de los objetos más cercanos al Sol, raras veces es impactado por otros cuerpos, lo que expondría las partes brillantes de material congelado fresco, como en Asbolo. Sedna y otros dos objetos muy distantes —2000 OO y 2006 SQ— comparten su color con los objetos clásicos del cinturón de Kuiper y el centauro Folo, lo que sugiere un origen en una región similar.

Se establecieron límites superiores a la composición de la superficie de Sedna en 60 % de metano congelado y un 70 % de hielo. La presencia de metano también apoya la existencia de tolinas en la superficie de Sedna, ya que son producidas por la irradiación de metano. El espectro de Sedna fue comparado con el de Tritón y se detectaron bandas de absorción débiles pertenecientes a metano y nitrógeno congelados. A partir de estas observaciones, se sugirió el siguiente modelo de la superficie: 24 % de tolinas tipo Tritón, 7 % de carbono amorfo, un 10 % de nitrógeno, 26 % de metanol y 33 % de metano. La detección de metano y agua congelados se confirmó en 2006 por la fotometría en infrarrojo medio del telescopio espacial Spitzer. La presencia de nitrógeno en la superficie sugiere la posibilidad de que, al menos por un tiempo corto, Sedna pudo poseer una atmósfera. Durante un período de alrededor de doscientos años cerca del perihelio la temperatura máxima de Sedna debió exceder 35,6 K (-237,6 °C), la temperatura de transición entre la fase alfa-sólida de N y la fase beta vista en Tritón. A los 38 K, la presión de vapor de N sería de 14 microbar (0.000014 atmósferas). Sin embargo, su profunda inclinación espectral roja es un indicativo de una alta concentración de materia orgánica en su superficie, y sus bandas débiles de absorción de metano indican que el metano en la superficie de Sedna es antiguo, en lugar de depositarse recientemente. Esto quiere decir que Sedna es demasiado frío para que el metano se evapore de la superficie y luego caiga de nuevo en forma de nieve, como ocurre en Tritón y, probablemente, en Plutón.

Los modelos de calentamiento interno a través de la desintegración radiactiva sugieren que Sedna podría ser capaz de soportar un océano subterráneo de agua líquida.

En el artículo en que anunciaban el hallazgo de Sedna, los descubridores lo describieron como el primer cuerpo observado perteneciente a la nube de Oort, una hipotética nube de cometas que se cree existe a una distancia de aproximadamente un año luz del Sol. Observaron que, a diferencia de objetos del disco disperso como Eris, el perihelio de Sedna —76 ua— está demasiado lejos para haber sido influido por la gravedad de Neptuno. Debido a que está mucho más cerca del Sol de lo esperable para un objeto de la nube de Oort y que tiene una inclinación más o menos similar a la de los planetas y a la de los objetos del cinturón de Kuiper, lo describieron como un «objeto de la nube de Oort interior», situado en el disco que va desde el cinturón de Kuiper a la parte esférica de la nube.

Si Sedna se formó en su ubicación actual, el disco protoplanetario original debió haberse extendido hasta 75 ua desde el Sol. Además, la órbita inicial de Sedna debió ser circular porque de lo contrario no podría haberse formado por la acreción de cuerpos más pequeños, ya que las grandes velocidades relativas entre los planetesimales habrían sido demasiado perjudiciales. Por lo tanto, debió ser una interacción gravitatoria con otro cuerpo la causante de su actual órbita excéntrica. En el artículo inicial, los descubridores sugirieron tres posibles candidatos para el cuerpo perturbador: un planeta oculto más allá del cinturón de Kuiper, una estrella en tránsito o una de las estrellas jóvenes integradas con el Sol en el cúmulo estelar en el que se formó. Concretamente respaldaron esta última hipótesis, aduciendo que el afelio de Sedna, de aproximadamente 1000 ua —relativamente cerca en comparación con el de los cometas de período largo—, no está lo suficientemente lejos como para verse afectado por estrellas en tránsito en sus actuales distancias al Sol. Propusieron que la órbita de Sedna se explica mejor si el Sol se hubiera formado en un cúmulo abierto de estrellas que se disoció gradualmente con el tiempo. Otros astrónomos avanzaron posteriormente en esta hipótesis. Simulaciones por computadora muestran que múltiples tránsitos entre las estrellas jóvenes de dichos cúmulos abiertos podrían provocar en muchos objetos órbitas semejantes a las de Sedna. Un estudio sugiere que la explicación más probable de la órbita de Sedna es que fue perturbada por una estrella que transitaba cerca —a unas 800 ua— durante los primeros 100 millones de años de la existencia del sistema solar aproximadamente.

Varios astrónomos avanzaron en la hipótesis del planeta transneptuniano de varias maneras. Un escenario involucra perturbaciones de la órbita de Sedna por un cuerpo hipotético de tamaño planetario en el interior de la nube de Oort. Simulaciones recientes muestran que las características orbitales de Sedna podrían explicarse por perturbaciones de un objeto de la masa de Neptuno a 2000 ua —o menos—, una masa de Júpiter a 5000 ua, o incluso un objeto de masa terrestre a 1000 ua. Las simulaciones por computadora sugirieron que la órbita de Sedna pudo ser causada por un cuerpo del tamaño de la Tierra, expulsado hacia el exterior por Neptuno, a principios de la formación del sistema solar y que hoy en día se encontraría en una órbita alargada de entre 80 y 170 ua del Sol. Se han realizado varios estudios del cielo sin detectar objetos del tamaño de la Tierra a una distancia aproximada de 100 ua. Sin embargo, es posible que dicho objeto haya sido expulsado fuera del sistema solar después de la formación de la nube de Oort interior.

Algunos astrónomos han sugerido que la órbita de Sedna es el resultado de la influencia de una compañera del Sol situada a miles de unidades astronómicas. Una de esas estrellas hipotéticas es Némesis, una compañera oscura del Sol propuesta como responsable de la supuesta periodicidad de las extinciones masivas en la Tierra por impactos cometarios, el registro de impactos lunares y los elementos comunes orbitales de una serie de cometas de período largo. Sin embargo, no hay hasta la fecha evidencia directa de la existencia de Némesis y muchas líneas de investigación, por ejemplo las que involucran el índice de craterización, han puesto en tela de juicio su existencia. Astrónomos que apoyan esta hipótesis han sugerido que un objeto de cinco veces la masa de Júpiter que se encuentre aproximadamente a 7850 ua del Sol podría provocar en un objeto una órbita como la de Sedna.

Otras hipótesis sugieren que Sedna no se originó en nuestro sistema solar sino que fue capturado por el Sol procedente de un sistema planetario extrasolar en tránsito, específicamente del de una enana marrón con una masa unas veinte veces menor que la del Sol.

La órbita altamente elíptica de Sedna indica que la probabilidad de detectarlo fue de aproximadamente 1 en 80, lo que sugiere que, a menos que su descubrimiento fuese una casualidad, podrían existir en su región entre cuarenta y ciento veinte objetos del tamaño de Sedna. Esto sugiere que Sedna podría ser el primer elemento de una serie de congelados que se ubican entre el cinturón de Kuiper y la nube de Oort denominada «población de Sedna». Otro objeto, (148209), tiene una órbita similar pero menos extrema: cuenta con un perihelio de 44,3 ua, un afelio de 394 ua, y un periodo orbital de 3240 años. Pudo ser afectado por los mismos procesos que Sedna.

Cada uno de los mecanismos propuestos para la órbita extrema de Sedna dejaría una marca distintiva de la estructura y la dinámica de una población más amplia. Si el responsable fue un planeta transneptuniano, todos esos objetos compartirían aproximadamente el mismo perihelio (≈ 80 ua). Si Sedna fue capturado desde otro sistema planetario que gira en la misma dirección que el sistema solar, entonces todos los miembros de la población de Sedna tendrían inclinaciones relativamente bajas y poseerían semiejes mayores que van desde 100 hasta 500 ua. Si rotara en dirección opuesta se formarían dos poblaciones, una con inclinación baja y una con alta. La gravedad de las estrellas perturbadoras produciría una amplia variedad de perihelios e inclinaciones, dependientes del número y el ángulo de tales encuentros.

Obtener una muestra más grande de esos objetos podría ayudar a determinar cuál es el escenario más probable. «Yo diría que Sedna es un registro fósil del sistema solar temprano», dijo Brown en 2006. «Con el tiempo, cuando se encuentren registros de otros fósiles, Sedna nos ayudará a comprender cómo se formó el Sol y el número de estrellas cercanas al Sol cuando se formó». En 2007-2008 Brown, Rabinowitz y Megan Schwamb trataron de localizar otro miembro de la población hipotética de Sedna. Aunque el estudio era sensible a los movimientos a 1000 ua y descubrió al candidato a planeta enano 2007 OR, no se detectaron nuevos cuerpos en órbitas como la de Sedna. Las simulaciones posteriores que incorporan los nuevos datos sugieren que en esta región probablemente existen alrededor de cuarenta objetos del tamaño de Sedna. Otro estudio realizado en 2011 encontró dieciocho objetos del sistema solar exterior, catorce de los cuales eran objetos transneptunianos desconocidos. Varios de estos objetos podrían estar en equilibrio hidrostático y ser por tanto planetas enanos. El estudio concluyó que, en comparación con la población principal del cinturón de Kuiper y para los objetos mayores (H < 4,5 mag), la población del disco disperso parece tener pocas veces más objetos, mientras que la población de Sedna puede ser varias veces mayor.

El Minor Planet Center, que cataloga oficialmente los objetos en el sistema solar, clasifica a Sedna como un objeto disperso. Sin embargo, este grupo está fuertemente cuestionado y muchos astrónomos sugirieron que, junto con algunos otros objetos —por ejemplo, (148209) —, se colocara en una nueva categoría de objetos distantes que podría llamarse «objetos de disco dispersos extendidos» —E-SDO—, «objetos desprendidos», «objetos dispersos a distancia» —DDO— o «dispersos-extendidos» en la clasificación oficial de la Deep Ecliptic Survey.

El descubrimiento de Sedna volvió a plantear el interrogante sobre qué objetos astronómicos deben considerarse planetas y cuáles no, algo que ya se planteó con motivo del descubrimiento de Quaoar. El 15 de marzo de 2004 varias agencias de noticias informaban de que «se había descubierto el décimo planeta». Sin embargo, el 24 de agosto de 2006 la Unión Astronómica Internacional redefinió en Praga lo que debe entenderse por planeta, exigiendo que debía haber despejado la vecindad alrededor de su órbita —dominancia orbital—. Se estima que Sedna tiene un parámetro Stern–Levison mucho menor que 1, y por tanto no se puede considerar que despejó su entorno, aunque no se descubrió ningún otro objeto en su proximidad. Para poder ser calificado como planeta enano, Sedna debía mostrar equilibrio hidrostático, es decir, ser esencialmente esférico. Es lo suficientemente brillante y, por lo tanto, suficientemente grande, por lo cual se espera que este sea el caso.

Sedna alcanzará su perihelio en torno a 2075-2076. Esta aproximación al Sol ofrece una oportunidad de estudio que no volverá a ocurrir en 12 000 años. Aunque Sedna está listado en el sitio web de exploración del sistema solar de la NASA, la agencia no está considerando ningún tipo de misión a Sedna en 2012.



</doc>
<doc id="37771" url="https://es.wikipedia.org/wiki?curid=37771" title="Chuño">
Chuño

El chuño, chuno, voz originaria de los Andes centrales (aimara, quechua: "ch'uñu" que es papa procesada) o tunta, es el resultado de la deshidratación (por lo general por liofilización) de la papa, u otros tubérculos de altura.

La fabricación de chuño es la forma tradicional de conservar y almacenar las papas durante largas temporadas, a veces durante años. Este producto es uno de los elementos centrales de la alimentación indígena y, en general, de la gastronomía de la región altiplánica de América del Sur particularmente de la zona andina de Bolivia y zonas andinas y sur de Perú, región de la cual es originario este producto. También se consume en el Norte de Argentina; el Norte de Chile y en el Sur de Ecuador.

En Bolivia, Perú, Ecuador, Chile y Argentina también puede referirse a la fécula de papa (y a algunos productos preparados con esta), obtenido mediante la molienda de los tubérculos y la decantación de los gránulos de almidón que se mantienen flotando en el jugo. Con él en Argentina ocasionalmente se preparan postres que llegan a ser semejantes a los flanes; en la costa del Perú se utiliza para la elaboración de las mazamorras.

El chuño se originó entre los antiguos pobladores de los Andes, quienes empleaban métodos de deshidratación para conservación de la papa. La evidencia más antigua de su consumo se ha encontrado en una pieza arqueológica hallada en Perú, un batán que aún conservaba restos de chuño molido. 

Las comunidades indígenas de los Andes centrales tradicionalmente han desecado tubérculos para su conservación desde la época precolombina. Se han llegado a encontrar chuños en emplazamientos arqueológicos de la cultura Tiahuanaco que floreció alrededor de la meseta del lago Titicaca y que desapareció en el siglo XII de nuestra era. Esto da una idea de la continuidad de la fabricación tradicional de chuño a través de un prolongado período en los Andes.

Los registros coloniales también mencionan la existencia del chuño, el texto de Bernabé Cobo de 1608, hablando sobre ciertas vasijas del Perú menciona:Del pan ordinario que usar dijeya...ser el maíz, quínua y chuño o papas secas y verdes.Tuestan el maíz en unas cazuelas de barro agujereadasy sírveles de pan y es el más usado matalotaje quellevan cuando caminan, particularmente una harina quedel hacen. Tuestan una especie de maíz hasta que re-vienta y se abre, al cual llaman pasancalla y tienen porcolación y confitura.

El método desecación de los tubérculos consiste en exponerlos a ciclos de congelación y asoleamiento de forma alterna. En cada repetición, el tubérculo pierde agua hasta que, finalmente el calor del sol y cierto "prensado" a pie acaban el trabajo. Por ese motivo, la fabricación de chuño es estacional y está sujeta a condiciones meteorológicas que garanticen la presencia de heladas intensas. Esta época de heladas suele ser característica de los meses de junio y julio tras el solsticio de invierno en el hemisferio Sur.

Cosechados los tubérculos, se clasifican por tamaño, con preferencia por los de pequeño diámetro. Se extienden en suelo plano, cubierto de pajas o pasto seco, dejándolos congelar por la helada, durante tres noches aproximadamente. El proceso completo toma alrededor de 20 días y se realiza de forma comunitaria. Una vez que están congelados, se retiran del lugar donde se congelaron, se dejan al sol y se procede a "pisarlos", método que busca eliminar la poca agua que aún conservan los tubérculos ya congelados. Luego de esto se vuelven a congelar.

En el proceso se elimina al menos el 80% del peso de las papas, facilitando su transporte, almacenaje y conservación.

A partir del proceso básico se obtienen dos variedades: el chuño negro y el chuño blanco.

El chuño negro, es aquel que se obtiene directamente de la congelación, pisado y recongelación. No se somete el producto al agua: concluida la congelación y el pisado se seca al sol, para así convertirse el tubérculo congelado en chuño. Ciertas sustancias presentes en el mismo, en contacto con el aire, se oxidan dándole un característico color que va desde el marrón oscuro hasta el negro.

La "tunta" se obtiene congelando la papa por una noche a la intemperie en helada de invierno: junio-julio, al día siguiente se deshidrata presionando con los pies contra el piso (exprimiendo) y se pone al agua de río o laguna en costales permeables de plástico. Este procedimiento se realiza a la hora de ponerse el sol para mantener el color blanco; las papas normalmente son de color claro pero al contacto con los rayos solares toman el color negro.

El paso final consiste en la extracción del agua después de quince días aproximadamente, el pelado y el secado al sol. El resultado es la "tunta", que en algunos lugares del Perú es conocida como "chuño blanco", en el noroeste de Bolivia es conocida como "tunta". En el Perú también se le conoce como "moraya".

Secado el chuño, y con mínimas exigencias de almacenamiento, el producto puede durar largo tiempo, incluso años.

Su consumo es variado, desde postres hasta platos elaborados, pasando por harina de chuño que es un ingrediente esencial de diversos platos de la gastronomía alto andina.

Cabe destacar que no son los únicos productos que se congelan para su conservación, existe también la "caya" que es una versión congelada de la oca.

Durante la Segunda Guerra Mundial, los nutriólogos de los ejércitos aliados descubrieron el valor del chuño y a partir de éste inventaron los hoy tan comunes purés instantáneos.

Para su preparación, se rehidratan las piezas remojándolas en agua durante varias horas, posteriormente se someten a diferentes tipos de cocción y preparación. Entre los platos que incluyen al chuño como ingrediente principal se encuentra el chairo o el chuño pasi.



</doc>
<doc id="37772" url="https://es.wikipedia.org/wiki?curid=37772" title="Política">
Política

La política es el proceso de tomar decisiones que se aplican a todos los miembros de una comunidad humana. También es el arte, doctrina u opinión referente al gobierno de los Estados. La ciencia política constituye una rama de las ciencias sociales que se ocupa de la actividad en virtud de la cual una sociedad libre, compuesta por seres humanos libres, resuelve los problemas que le plantea su convivencia colectiva. Es un quehacer ordenado al bien común. 

Es la ciencia social que estudia el poder público o del Estado. Promoviendo la participación ciudadana al poseer la capacidad de distribuir y ejecutar el poder según sea necesario para garantizar el bien común en la sociedad.

Una definición más amplia (acuñada de diversas lecturas) nos haría definir la política como toda actividad, arte, doctrina u opinión, cortesía o diplomacia; tendientes a la búsqueda, al ejercicio, a la modificación, al mantenimiento, a la preservación o a la desaparición del poder público.

En esta amplia definición se puede observar claramente al objeto de la ciencia política, entendido como el poder público sustraído de la convivencia humana, ya sea de un Estado, ya sea de un grupo social: una empresa, un sindicato, una escuela, una iglesia, etcétera.

Es por ello que cuando se utiliza la definición más amplia de 'política', se suele aclarar que esta es una actividad de la que es muy difícil sustraerse, por encontrarse en casi todos los ámbitos de la vida humana.

Una definición más estricta, propondría que la política es únicamente el resultado expreso oficialmente en las leyes de convivencia en un determinado Estado.

Definición que restringe a la vida de las agrupaciones y organizaciones no estatales, limitándolas únicamente a las disposiciones legales de sus Estados.

Definiciones clásicas apuntan a definir política como el ejercicio del poder en relación a un conflicto de intereses. Son famosas las definiciones fatalistas de Carl Schmitt de la política como juego o dialéctica amigo-enemigo, que tiene en la guerra su máxima expresión, o de Maurice Duverger, como lucha o combate de individuos y grupos para conquistar el poder que los vencedores usarían en su provecho. También está Max Weber, que define la política estrictamente en función del poder.

Una perspectiva opuesta contempla la política un sentido ético, como una disposición a obrar en una sociedad utilizando el poder público organizado para lograr objetivos provechosos para el grupo. Así las definiciones posteriores del término han diferenciado poder como forma de acuerdo y decisión colectiva, de fuerza como uso de medidas coercitivas o la amenaza de su uso.

Una definición intermedia, que abarque a las otras dos, debe incorporar ambos momentos: medio y fin, violencia e interés general o bien común. Podría ser entendida como la actividad de quienes procuran obtener el poder, retenerlo o ejercitarlo con vistas a un fin que se vincula al bien o con el interés de la generalidad o pueblo.

Gramsci concebía la ciencia de la política tanto en su contenido concreto como en su formación lógica, como un organismo en desarrollo. Al comparar a Maquiavelo con Bodin afirma que este crea la ciencia política en Francia en un terreno mucho más avanzado y complejo que Maquiavelo y que a Bodin no le interesa el momento de la fuerza, sino el del consenso. En la misma página Gramsci opina que el primer elemento, el pilar de la política, "es el que existen realmente gobernados y gobernantes, dirigentes y dirigidos. Toda la ciencia y el arte político se basa en este hecho primordial, irreductible (en ciertas condiciones generales)"

El ejercicio de la política permite gestionar los activos del estado nacional, también resuelve conflictos dentro de las sociedades adscritas a un estado específico lo que permite la coherencia social, las normas y leyes que determine la actividad política se vuelven obligatorias para todos los integrantes del estado nacional de donde proceden tales disposiciones. 

Frank Goodnow hace una especial acentuación sobre la función de la política que corresponde a la voluntad del Estado. Esta se complementa en su ejecución a través del gobierno. La política solo es funcional cuando permite poner reglas entre los gobernantes y los gobernados, los cuales son doblegados a la voluntad de las acciones que se desean orientar con el propósito de alcanzar un determinado fin.

Según Max Weber, Raymond Aron, George Vedel y Maurice Duverger, el objeto de estudio de la política es el "poder".

Política dogmática es concebir el proceso político como mantenimiento de una estrategia considerada ortodoxa, identificada con la "verdad", es decir, que no tiene criterio de discusión.

Conjunto de lineamientos discrecionales que aplican a un proceso, subproceso o unidad para facilitar la toma de decisiones congruentes con lo que los líderes de los procesos y unidades requieren para gestionar las actividades. Una política funcional admite discreción.

El capitalismo es un sistema socioeconómico surgido de las condiciones históricas posteriores al hundimiento del modo de producción feudal. Durante el siglo XIX y XX, el capitalismo se configuró como una ideología política y los partidarios de este sistema socioeconómico sintetizaron y racionalizaron una ideología política justificatoria de este sistema a partir de los principios más refinados sobre los derechos de propiedad privada. Así pues existen actualmente dos grandes vertientes del capitalismo: aquellos que creen en religiones y entidades con autoridad moral sobre el cuerpo humano —conservadores— de aquella más acabada y actual que cree en la propiedad de uno mismo y el principio de no agresión —liberales—.

Cada persona poseerá legítimamente cualquier recurso, "que no tuviera dueño anteriormente", del que se apropie o que provenga del resultado de su trabajo. El sistema de títulos de propiedad está relacionado también con este punto. Este sistema establece el derecho de cada persona a su propia persona, el derecho a donar, dar en herencia (y en consecuencia a heredar) y el derecho al libre intercambio de bienes sobre los que tenga legítima propiedad. Por eso es frecuente, que el capitalismo se identifique con el "libre mercado". En este punto friccionan el capitalismo y las demás ideologías, porque mientras que para un socialista, por ejemplo, lo justo sería que una herramienta pueda ser usado por todos, y que por tanto la propiedad de una herramienta es de todos, para el capitalista lo injusto es que alguien pretenda hacer uso de esa herramienta sin su consentimiento.

El comunismo es un paradigma caracterizado por el control y planificación colectiva de la vida comunitaria, la abolición de la propiedad privada sobre el trabajo y los medios de producción, es decir, la socialización de los medios de producción y no existencia de las clases sociales. El comunismo, cuya base es la colectivización de la propiedad, fue fundada por los pensadores socialistas alemanes del siglo XIX Karl Marx y Friedrich Engels como una interpretación revolucionaria de la historia: un permanente conflicto por el excedente material, cuyo inicio se debe a la aparición de la propiedad que pone fin al comunismo primitivo y separa a la sociedad en clases de acuerdo a su forma de adquisición de recursos. Según el marxismo, las diferentes relaciones sociales de producción que vinculan a los hombres en la sociedad capitalista implican la explotación mediante la apropiación de plusvalía, y estas relaciones generan con el tiempo las condiciones para ser reemplazadas por otras formas de explotación superiores, en una secuencia revolucionaria de nuevas fuerzas productivas. En el capitalismo y en los modos de producción anteriores existen entonces clases con intereses contrapuestos que entran en una constante lucha (la lucha de clases); más precisamente, en el capitalismo, se llega cíclicamente al estancamiento de las fuerzas productivas, y es entonces cuando la lucha de clases se intensifica; los caminos se bifurcan entre una derrota de la clase obrera (y por lo tanto de todo el proletariado) o en un triunfo de esta que puede implicar la modificación de los regímenes capitalistas, la creación de poderes duales o incluso la revolución socialista. Para llegar a este fin debe organizarse un partido comunista que conquiste la hegemonía en la clase obrera para que esta pueda tomar el poder (dictadura del proletariado, es decir la dictadura de las estructuras de poder obrero no de los partidos ni de los burócratas). Esta revolución no puede ser encerrarse en un solo país (utopía reaccionaria) dada la interdependencia de las naciones el comunismo debe ser internacional y la revolución debe ser mundial.

Todas las ideologías políticas se agrupan en torno a dos dimensiones que son la económica y la social. La dimensión económica está integrada por dos ideologías opuestas, izquierda-derecha, que forman una línea horizontal y la dimensión social está integrada por otras dos ideologías opuestas, autoritarismo-libertarismo, que forman una línea vertical. Juntas estas dos dimensiones integran un mapa ideológico en el cual podemos encontrar cuatro grandes sistemas como el totalitarismo, conservadurismo, socialismo y el liberalismo, y el punto en donde se cruzan las dos líneas se considera como el centro político.

Se conoce como totalitarismos a las ideologías, los movimientos y los regímenes políticos donde la libertad está seriamente restringida y el Estado ejerce todo el poder sin divisiones ni restricciones.

Se denomina conservadurismo al conjunto de doctrinas, corrientes, opiniones y posicionamientos, generalmente de centro-derecha y derecha, que favorecen tradiciones y que son adversos a los cambios políticos, sociales o económicos radicales, oponiéndose al progresismo. En un estado conservador, los ciudadanos están sujetos a la autoridad estatal, principalmente en los aspectos sociales de su vida, pero suele haber una gran libertad en el aspecto económico coexistiendo con una gran competitividad individual y empresarial.

En el espectro de cuadrantes es una ideología ubicada entre el libertarismo y la ideología izquierdista. El socialismo cree que la sociedad debe organizarse a lo largo de las líneas sociales en beneficio de todos, en lugar de para lo que se percibe como el beneficio de unos pocos. Sus principales ideas son la oposición al capitalismo, y una creencia en la igualdad, tanto política como económica.

Es una ideología encasillada entre el libertarismo y la ideología derechista. El liberalismo considera a la libertad individual como el más alto valor social y económico. El liberalismo propugna el derecho a disentir de la ortodoxia. La descripción anterior aúna los aspectos sociales del liberalismo de los Estados Unidos con los aspectos económicos del liberalismo europeo.

Para Sandeep Jaitly existen dos grandes variantes: la escuela austríaca, donde los objetos no tienen un valor intrínseco de por sí sino que lo tienen porque estos satisfacen los fines humanos, y el objetivismo, donde se suele argumentar lo contrario, es decir, sostiene que el valor es intrínseco al bien. Así mismo, Jaitly advierte que hay autoproclamados liberalistas en Estados Unidos que confunden las dos variantes.

Para los partidarios de la ideología objetivista liberal, el orden social capitalista descansa sobre la noción de que cada ser humano es dueño de sí mismo y que, en consecuencia, tiene total soberanía sobre su cuerpo. Para los que aceptan esta idea sin reservas, entonces nadie puede invadir, agredir o intervenir de manera legítima el cuerpo de otra persona. Esto ha suscitado enconados debates entre partidarios del capitalismo, conservadores y liberales, en cuestiones como el aborto, la eutanasia o el matrimonio entre personas del mismo sexo. Por ejemplo, Ayn Rand, partidaria del objetivismo, rechazaba las leyes referentes a las uniones entre homosexuales, pero no porque creyese que los homosexuales no tienen derecho a establecer parejas, sino porque no creía que el estado -ni nadie excepto los propios individuos- tuviera la legitimidad de decidir u homologar como deban establecerse las relaciones entre personas. Incluso, algunos defensores capitalistas extremos rechazan frontalmente la democracia como sistema, pues dicen que atentan contra las minorías.

Mientras los socialdemócratas aceptan la idea de la recaudación por medio de impuestos para ser gestionado públicamente, las ideologías ultraliberales abogan por impuestos unipersonales hiperreducidos o se oponen ferozmente al cobro de impuestos (calificándolo de "robo") o imposición de normas morales sobre otros considerando dicha imposición contraria al principio de no agresión que defienden. Sin embargo, en la práctica ninguna organización o partido político de amplia implantación ha sugerido la supresión total de los impuestos.

En el estatus de negación a la acción política de un Estado sobre los individuos, hace que un liberal muchas veces sea definido en ocasiones como "conservador", puesto que un estatista ve a los capitalistas en general como "defensores de las normas tradicionales". Sin embargo, esto no es cierto en todo los casos, ya que muchos liberales no defienden que la tradición se mantenga, sino que se respete que las personas son libres de elegir su camino y que por tanto no deben introducirse normas artificiales destinadas a "inculcar" en la sociedad lo que el planificador económico, en la mayoría de los casos un gobierno democráticamente elegido, considere 'correcto'. También debería distinguirse entre la ideología capitalista liberal, el corporativismo empresarial y el capitalismo de estado (modelo conservador).

Se consideran otras dimensiones aparte de las dos típicas, en función de si se busca el perfeccionamiento de la humanidad o de solo una cultura, nación, sociedad o individuo, si una ideología es progresista o conservadora, individualista o totalista, si hay aceptación o rechazo de la propiedad privada, o en función de si su cultura está influenciada por otras.

Son aquellas ideologías que proponen un finalismo histórico racionalista o positivista basado solo en el perfeccionamiento de la humanidad (antropocentrismo) que va más allá de las decisiones individuales o colectivas. Ven a la historia como un camino de realización y perfeccionamiento. Las ideologías económicas de derecha creen que ya se ha llegado a tal perfeccionamiento después del movimiento ilustrado y de la revolución francesa, mientras que las ideologías económicas de izquierda que surgieron después de la revolución industrial consideran que esta sociedad es injusta y que la plena realización humana sólo se dará cuando sea superada.

Al contrario de las ideologías románticas, que no suelen identificarse con el statu quo, las ideologías evolucionistas se identifican con posturas progresistas o reformistas.

Son las ideologías que por el contrario de las ideologías racionales proponen un finalismo histórico romántico basado en los ideales individuales o colectivos. No ven a la historia como un camino de realización y perfeccionamiento. Algunas ideologías como el libertarismo carecen de un finalismo histórico concreto y niegan todo determinismo histórico que restrinja o atente en contra de la libertad individual. Las ideologías autoritarias ven como el sujeto de la historia a la nación, a la cultura o a la ley del más fuerte (egocentrismo, teocentrismo, etnocentrismo o estatocentrismo). Este dominio no tiene una fundamentación racional sino vital y emocional. A diferencia de las ideologías dentro del espectro progresista que adoptan cambios parciales y graduales propios de posturas reformistas, las ideologías románticas se asocian más a cambios totales propios de posturas revolucionarias, reaccionarias y militaristas.

Los renacimientos son otro tipo de ideologías dinámicas de un marco histórico mucho más amplio que puede adoptar tintes progresistas o románticos ya sea con el fin de mejorar la humanidad o con el fin de cumplir los ideales individuales y colectivos de una cultura o una nación tomando como marco de referencia sociedades, civilizaciones o culturas afines preexistentes más antiguas. Por lo tanto se diferencia del reformismo y las revoluciones, en la que estos solo se basan en cambios que dan soluciones a problemáticas generadas dentro del mismo curso de su historia, mientras que un renacimiento toma de referencia para la solución de sus problemas la forma de proceder de otras culturas muertas más antiguas. En otras palabras el renacimiento es la adopción de una solución ya establecida en otra cultura, mientras que el reformismo y las revoluciones son producto del dinamismo interno y de adaptación al medio dando nuevas soluciones inventadas dentro del curso de la historia de la misma cultura. Ejemplo de un renacentismo reciente es el Partido Nacionalsocialista Obrero Alemán, ya que este mezcló la ideología del partido con aspectos de la cosmogonía germánica precristiana.

El herodianismo, kemalismo o aculturación política es un especie de cambio por imitación que se diferencia del renacimiento en que la solución a problemáticas se toman no de otra cultura antigua sino de una cultura dominante, competidora o paralela en la escala temporal, es decir, adoptan soluciones prestadas tanto en la dimensión social como económica, mientras tanto el reformismo solo suele tomar cambios prestados de la dimensión económica pero guardan características en la dimensión social ajustándola al nuevo contexto con tintes propios. Ejemplos de herodianismo en épocas recientes se podrían citar al Movimiento Nacional Turco de Mustafa Kemal Atatürk durante los años treintas, y al movimiento neoliberal en Latinoamérica entre las décadas de los ochentas y noventas.

El proceso de aculturación política no es un proceso exclusivo de las culturas dominadas, sino también un proceso que suele afectar a la cultura dominante en su afán de abarcar todas las culturas (universalismo y globalización), como actualmente es el caso de occidente mediante unos procesos denominados multiculturalismo y relativismo cultural.

El zelotismo está en contraposición al herodianismo, en la escala de dominancia, y el renacentismo, en la escala temporal, es una ideología que rechaza el proceso de aculturación política tomando una posición normalmente de índole ultra conservador o etnocentrista aceptando solo los parámetros sociales y económicos de su cultura actual. El antagonismo entre herodianismo y zelotismo es análogo al antagonismo entre progresismo y conservadurismo, pero difieren debido a que el plano de antagonismo entre las primeras es en relación a las interacciones con otras culturas, mientras que el antagonismo de las segundas es en relación a interacciones dentro de una misma cultura.

Los primitivistas rechazan desde sociedades agrarias hasta las sociedades más modernas (industriales) argumentando que la mejor sociedad es aquella donde el hombre se encuentra en su estado más natural (caza-recolección). Califican a la civilización y sus derivados como formas de imposición. Aunque los ideólogos principales son occidentales estos ven al progreso como un mito monstruoso basado en la falsa idea de un desarrollo ilimitado sin tomar en cuenta que los recursos son limitados o las técnicas son efímeras, como en el caso del petróleo (sociedades modernas-industriales) o la tierra arable (sociedades agrarias).

Las ideologías que promueven el colectivismo o totalismo son las que dan prioridad al colectivo sobre el individuo argumentando que sin sociedad no hay individuos. Cuando el estado se convierte en el centro de la vida política restringiendo las libertades individuales el totalismo se trasforma en totalitarismo. Mientras que en el totalitarismo es más importante la nación sobre el individuo, en el totalismo es más importante la sociedad, por ello el nombre de socialismo. En el nacionalismo extremo es más importante que el individuo el contexto cultural, las tradiciones, la religión, la lengua e incluso algunas veces la raza (etnocentrismo).

Las ideologías que promueven el individualismo son las que dan prioridad al individuo sobre el colectivo argumentando que sin individuos no hay sociedad. Lo que distingue a las ideologías capitalistas y liberales racionales de las otras ideologías individualistas es el contrato social, que para su ejecución requiere de un estado que lo haga valer en pro de garantizar las libertades individuales. Mientras tanto las ideologías dentro del liberalismo extremo, económico y del socialismo moral son tan celosas de su libertad que niegan al contrato social interpretándolo como una forma de autoritarismo. En el capitalismo extremo y en el económico se considera a la corporación o empresa privada como persona jurídica (distinta de una persona física) que a menudo posee derechos amparados por la ley similares a aquellos de una persona natural o individuo, en donde en la mayoría de las veces, el poder ha sido transferido del estado a las grandes corporaciones o empresas privadas.

Suele darse en sociedades homogéneas (colectivistas) tanto como en sociedades heterogéneas pero que guardan distancia vertical del poder respecto a quien les gobierna. Ejemplos de las primeras pueden ser la sociedad china y ejemplos de la segunda las sociedades árabes y anteriormente latinoamericanas. Normalmente, en estas sociedades no existe una separación clara de funciones judiciales, ejecutivas o legislativas y el poder suele concentrarse en sistemas de partido único, corporaciones, monarquías o dictaduras.

Suelen ser sociedades heterogéneas pero con una distribución horizontal del poder, es decir, en la participación representativa existen varias élites compitiendo por obtener el poder político. Sociedades plurales características se pueden encontrar en las sociedades japonesa, occidental o de la India. En este tipo de sistemas de gobierno suele coexistir una clara separación de los poderes del Estado en poder ejecutivo, legislativo y judicial y múltiples partidos políticos compitiendo por obtener una representación.

Unas ideologías interpretan a la propiedad privada como indispensable para la marcha de la economía y para el ejercicio efectivo de la libertad individual como es en el caso del liberalismo. En el caso del capitalismo y el totalitarismo moral no anulan el derecho a la propiedad privada pero también quieren ponerla al servicio de los intereses de un estado o ideología dominante. Por el contrario, en el capitalismo extremo y económico, el estado está al servicio de los intereses de la empresa privada, el cual el poder se ha transferido desde el estado o sociedad a las grandes corporaciones (corporatocracia) formadas por una sociedad mercantil controlando los medios de producción. El liberalismo extremo y económico a través de la contraeconomía, tratan de suprimir o anular al estado por medio de empresas privadas (empresa agorista) por medio del mercado libre y el anarquismo de mercado, convirtiendo a los trabajadores e individuos en general, en empresarios radicales que controlan los medios de producción. Se oponen a la responsabilidad limitada (despersonalizada) de las corporaciones y a la propiedad inmaterial del capitalismo extremo y económico considerándola un privilegio forzado e ilegítimo. En cambio, piensan que las propiedades materiales (como suelos) sí pueden ser privadamente apropiados.

El totalismo y el socialismo racionales son ideologías que interpretan a la propiedad privada de los medios de producción y a la Acumulación del capital como el origen de todos los males sociales. En cambio, las ideologías socialdemócratas argumentan simplemente que la propiedad privada es un robo si no se ocupa o trabaja, distinguiéndola de aquella propiedad personal legítima producto del trabajo sobre una propiedad natural, aceptando algún grado de privatismo, bajo la concepción de que la humanidad pertenece a la naturaleza y no ésta a la humanidad. La postura ideológica del socialismo radical, a diferencia de las alas más moderadas, defienden el régimen de la propiedad social . La economía del socialismo radical es antagónica con cualquier modelo de organización corporativo mercantil (la empresa privada del capitalismo extremo y económico) o centralista (la empresa estatal del totalitarismo extremo y económico), que controlan los medios de producción, y aboga por el manejo de estos medios por parte de los mismos trabajadores creando corporaciones (empresa autogestionada) formadas por una sociedad civil normalmente de índole sindical. El modelo de empresa de las ideologías socialistas es la cooperativa, considerada a medio camino entre el individualismo y el colectivismo económico, ya que intenta combinar armónicamente propiedad privada, empresa privada, competitividad y economía de mercado junto con democracia directa interna, empresa de autogestión, colaboración mutua, mercados sociales y otras formas de solidaridad voluntaria.

Aunque muestren un finalismo, las ideologías productivistas no deben confundirse con las progresistas pues, mientras las segundas se centran en la evolución cultural y moral viendo como sujeto de la historia a la humanidad, las primeras se centran en el papel de los sistemas en el que predomina el interés por producir bienes materiales apoyándose en un aumento del consumo, la tecnología y el crecimiento económico. Dicho de otra manera, para las ideologías progresistas el fin es la humanidad mientras que para las productivistas el fin último es el crecimiento económico y material.

Desde la perspectiva socialista el productivismo se define como una ética en la que el trabajo cumple un papel crucial al expresar la primacía de la industria en la sociedad moderna. Desde la perspectiva capitalista y de economía de mercado las decisiones de lo que se debe producir, en que cantidad y para quién deben ser tomadas de manera individual. Aunque existen ideologías productivistas, como el socialismo, que siguen viendo a la humanidad como sujeto principal de la historia existen otras ideologías (véase ideologías románticas), como el transhumanismo, que no lo ven así.
Los críticos del productivismo argumentan que el productivismo puede ser como una enfermedad y que puede interferir con los demás procesos que regulan la vida en el planeta, incluso entre las propias relaciones humanas. Para Iván Illich sólo una sociedad que acepte la necesidad de escoger un techo común a ciertas dimensiones técnicas en sus medios de producción tiene alternativas políticas. Así pues, los defensores de estas posturas suelen hablar de un crecimiento moral pero no económico ni material. Aunque existen ideologías anti-productivistas, como el decrecimiento, que siguen viendo a la humanidad como sujeto principal de la historia existen otras ideologías (véase ideologías románticas), como el ecologismo profundo (biocéntrica), que no lo ven así. 

Para que el ecologismo deba ser considerado como ideología política, el activista e investigador ecologista Florent Marcellesi propone el tercer eje «productivismo-antiproductivismo» además de los dos ejes típicos «izquierda-derecha» y «autoritario-libertario» de la clasificación bidimensional conformando así un nuevo sistema tridimensional de la clasificación de ideologías políticas.

El marketing político es el conjunto de técnicas de investigación, planificación, gerenciamiento y comunicación que se utilizan en el diseño y ejecución de acciones estratégicas y tácticas a lo largo de una campaña política, sea ésta electoral o de difusión institucional.

En la actualidad el marketing político presenta dos características adicionales: la mediatización y la videopolítica.

Si bien existen numerosas similitudes técnicas y metodológicas entre el marketing político y el marketing comercial, sus objetivos difieren notablemente. En el mundo comercial la lógica de mercado tiene como objetivo principal la satisfacción de una necesidad. En la esfera política la lógica de mercado tiene como objetivo la elección de una alternativa.





</doc>
<doc id="37783" url="https://es.wikipedia.org/wiki?curid=37783" title="Partido Republicano (Estados Unidos)">
Partido Republicano (Estados Unidos)

El Partido Republicano (GOP) (en inglés, "Republican Party"; también conocido como "GOP", de "Grand Old Party", El Gran Partido Viejo) es un partido político de los Estados Unidos. Al lado del Partido Demócrata, son los dos únicos partidos que han ejercido el poder en ese país desde mediados del siglo XIX. El partido se asocia comúnmente con el conservadurismo. En la actualidad es el partido de gobierno al que pertenece el presidente Donald Trump y el que posee la mayoría en el Senado de los Estados Unidos.

En el año 1854, el Partido Whig de los Estados Unidos terminó por desintegrarse y desapareció; este había sido, desde 1834, el segundo partido más grande del país (el primero era el Partido Demócrata de los Estados Unidos). El 20 de marzo de ese mismo año se fundó el Partido Republicano, en una reunión celebrada en una pequeña escuela de la ciudad de Ripon, Estado de Wisconsin. En concreto, en esa reunión se acordó que, ante la desintegración del Partido Whig, el Comité Local (Municipal) del Partido Whig en Ripon se transformaría en el Comité Local de un nuevo partido que se llamaría Republicano en honor a Thomas Jefferson (quien había sido el fundador del desaparecido Partido Demócrata-Republicano de los Estados Unidos). Pronto el ejemplo de Ripon fue imitado por otros Comités Locales y Estatales del difunto Partido Whig en diferentes regiones del Norte del país; y ya para mediados de año se pudieron celebrar las primeras Convenciones Estatales del nuevo partido. La primera de esas Convenciones Estatales fue la del Estado de Míchigan, que se reunió el 6 de julio de 1854 en la ciudad de Jackson; los Estados del Medio Oeste tomaron la iniciativa en la fundación y organización del nuevo partido, mientras que los del Este tardaron más o menos un año en hacerlo.

Obviamente la mayoría de los miembros fundadores del Partido Republicano habían sido miembros del Partido Whig de los Estados Unidos, pero también algunos eran exmiembros del Partido Demócrata, y otra parte de los fundadores eran independientes o habían formado parte de otros grupos (como el Partido del Suelo Libre).

Lo que tenían en común todos estos grupos que se unieron para formar al nuevo partido era su pertenencia ideológica al poderoso movimiento antiesclavista o abolicionista, que aglutinaba a todas las personas de piel blanca en los Estados del Norte de Estados Unidos que luchaban para abolir la esclavitud de las personas negras en los Estados del Sur del país norteamericano. También estaban de acuerdo en impulsar una política económica basada en dos aspectos fundamentales: una política comercial proteccionista, que disminuyera o impidiera las importaciones por medio de aranceles altos, para proteger la industria nacional de la competencia extranjera; y una política de "mejoras federales", por la cual el Gobierno Federal (Nacional) debía invertir mucho más dinero en obras públicas o de infraestructura (puentes, caminos, etc.) para estimular la economía.

El programa del partido lo hacía muy popular en el Norte del país (sobre todo por la política comercial proteccionista) pero intensamente odiado en el Sur (no solo por la abolición de la esclavitud, sino también por la política proteccionista que perjudicaba a unos Estados como los sureños que no tenían casi industria y que necesitaban el libre comercio para colocar sus productos agrícolas en Europa).

En las elecciones legislativas de noviembre de 1854 el Partido Republicano obtuvo buenos resultados en su primera gran prueba electoral, al ser elegidos Representantes (diputados) un total de 46 políticos republicanos. Estos cuarenta y seis parlamentarios republicanos representaban un 18,3% de los miembros de la Cámara de Representantes del Congreso de los Estados Unidos (en aquella época la Cámara se componía de un total de 252 Representantes o congresistas).

En el año de 1856 el Partido Republicano participó por primera vez en unas elecciones presidenciales, con su candidato John C. Frémont; pero este perdió las elecciones, quedando en un respetable segundo lugar (el ganador fue el Presidente James Buchanan del Partido Demócrata). Sin embargo, el partido dominó las regiones de Nueva Inglaterra, el Estado de Nueva York y el Cercano Oeste norteño, demostrando que sólo necesitaba ganar dos Estados más del Norte para ganar unas presidenciales; además aumentó a 90 el número de sus Representantes en el Congreso (el 38% de la Cámara) y lograron que las Legislaturas Estatales de varios Estados nombraran hasta 20 Senadores republicanos para el Senado de los Estados Unidos (su primera representación en esa Cámara del Congreso). Y en las elecciones legislativas de 1858 el Partido Republicano ganó 116 Representantes, que equivalían al 48,7% de la Cámara y que le daban la mayoría simple en dicha Cámara (que usó para dificultarle las cosas al Presidente Buchanan); y aumentó a 26 el número de sus Senadores en la otra Cámara. En las siguientes elecciones presidenciales la suerte se inclinaría todavía más a favor de los republicanos.

En las elecciones del 6 de noviembre de 1860 el candidato presidencial del Partido Republicano fue Abraham Lincoln, un exrepresentante del Congreso de los Estados Unidos que había sido militante del Partido Whig y que era un gran orador y un antiesclavista moderado.

Aparte de Lincoln había tres candidatos más a la presidencia; el Partido Demócrata se dividió en dos, con los demócratas de los Estados del Norte postulando un candidato y los demócratas del Sur postulando otro. Además había un cuarto candidato del Partido de la Unión Constitucional que quería ser una opción para los que rechazaban tanto a republicanos como a demócratas (norteños y sureños). La elección fue muy reñida y polarizante; en los Estados del Sur la candidatura de Lincoln ni siquiera aparecía en las papeletas de votación. Al final Lincoln ganó, aunque su victoria fue recibida con entusiasmo en el Norte y con indignación en el Sur.

La mayoría de los Estados esclavistas del Sur se separaron de Estados Unidos antes de que Lincoln tomara posesión de la Presidencia (asumió el poder el 4 de marzo de 1861); pero la Independencia de estos Estados (autodenominados Estados Confederados de América) no fue reconocida por Lincoln y su Gobierno, por inconstitucional y delictiva. Así estalló la Guerra Civil o Guerra de Secesión.

Durante la Guerra, la parte sureña del Partido Demócrata, que había impulsado la rebelión separatista, quedó prácticamente fuera de la ley; mientras que la parte norteña (y de los estados sureños que no se habían unido a la rebelión) continuaba siendo legal pero perdía influencia que ganaba el Partido Republicano. Los republicanos lograron hacer realidad todo su programa: decretaron la libertad de los esclavos negros y la abolición perpetua de la institución esclavista, implantaron su política proteccionista y aumentaron el gasto del Estado en proyectos que trajeron mejoras federales. Además, subieron los impuestos federales (creando por primera vez algo parecido a lo que sería más tarde el impuesto sobre la renta).

Lincoln triunfó en la guerra y salvó al país de la división; pero fue asesinado el 15 de abril de 1865, poco después de haber iniciado su segundo período de gobierno (había sido reelecto en 1864 con una amplia ventaja sobre su único rival, el candidato del Partido Demócrata del Norte).

Terminada la Guerra Civil, comenzaba la parte más difícil del proceso histórico conocido como la Reconstrucción, que ayudó a consolidar el dominio de los republicanos.

El segundo Vicepresidente de Lincoln, Andrew Johnson no era republicano sino un demócrata de Tennessee. Muy pronto Johnson entró en conflicto con los republicanos radicales (el sector mayoritario del Partido Republicano) que deseaban castigar a los Estados del Sur por su pasada rebeldía, y que además querían imponer sus reformas radicales en relación a los antiguos esclavos.

Los Estados sureños no querían otorgarle la plena ciudadanía a los negros que habían sido esclavos y Johnson no deseaba obligarlos; el Congreso de los Estados Unidos, dominado por los republicanos radicales, pasó por encima de la autoridad del Presidente y usó al Ejército para imponer Gobiernos provisionales en los Estados ex-rebeldes. Además aprobó las Decimotercera, Decimocuarta y Decimoquinta Enmiendas a la Constitución de los Estados Unidos para garantizar la igualdad entre blancos y negros (incluyendo el derecho al voto para los negros); y por la fuerza obligó a los Estados sureños a ratificarlas. Johnson vetaba las medidas, pero el Congreso rechazaba sus vetos y hasta intentó destituirlo.

Gracias a las reformas, y a la prohibición impuesta a los blancos del Sur para que no pudieran votar hasta que no fuera perdonada su pasada rebeldía y aceptaran los cambios legales; los republicanos ganaron el control de los Estados sureños con los votos de los negros, y eso sumado a su mayoría en los Estados del Norte les garantizo el control del poder (casi como un partido único). Esta situación duró unos cuantos años.

Cuando la ocupación militar del Sur terminó, y los blancos sureños recuperaron su derecho al voto (y los negros lo perdieron en la práctica, porque los demócratas blancos del Sur los agredían para que no votaran); el Partido Republicano casi desapareció en el Sur, pero conservaron la mayoría en el Norte y el Oeste. Pero como la población crecía más en estas dos regiones que en el Sur, el partido se mantuvo en el poder.

Desde 1869 hasta 1933 todos los Presidentes de Estados Unidos fueron republicanos, con sólo dos excepciones: los demócratas Grover Cleveland que gobernó de 1885 a 1889 y de 1893 a 1897, y Woodrow Wilson que gobernó entre 1913 y 1921. Es decir, que restando los 16 años que gobernaron estos dos demócratas, fueron 48 años de gobierno republicano (y un control mayor y casi ininterrumpido del Congreso).

Durante esta etapa de hegemonía casi absoluta del Partido Republicano, el país vivió una gran expansión económica. Los altos aranceles (en el marco de la política proteccionista republicana) permitieron desarrollarse a la industria estadounidense sin competencia extranjera; aunque el enorme tamaño del mercado interno era casi igual al tamaño del mercado formado por el resto del mundo de aquella época. El Estado intervenía poco en la economía, los impuestos eran bastante bajos, las regulaciones casi inexistentes, y no existía un gran número de empresas en manos del Estado.A comienzos del siglo XX varios políticos (entre ellos el Presidente republicano Theodore Roosevelt) lucharon para reducir el poder de las grandes empresas privadas convertidas en monopolios y oligopolios, y las obligaron a renunciar a su posición dominante en favor de la libre competencia.

Pero en la segunda década del siglo XX, a causa de la llamada Gran Depresión, el país se hundió en la peor crisis económica y social de su historia (rayando en una tragedia humana). Las masas hambrientas llevaron al poder al candidato demócrata a la Presidencia Franklin Delano Roosevelt, en las elecciones de 1932, y con ello llegó a su fin la etapa de hegemonía republicana.

Desde 1933 hasta 1953 los republicanos tuvieron que permanecer en la oposición; fueron 20 años en los que perdieron 5 elecciones presidenciales consecutivas (cuatro ganadas por Roosevelt y una por Harry Truman), y en al menos una ocasión (1937) quedaron reducidos a una minoría insignificante en el Congreso.

Durante este tiempo los demócratas introdujeron importantes reformas sociales (pensión de jubilación, salario mínimo, etc.) que les hicieron consagrarse dentro de la socialdemocracia y ganarse el espacio de la izquierda política en Estados Unidos, dejando a los republicanos en la derecha política, cuando en el pasado había sido (más o menos) al revés. Durante este periodo, la popularidad demócrata se incrementó entre las minorías y la clase trabajadora.

Finalmente en 1953 los republicanos volvieron al poder con Dwight Eisenhower.

Al volver al poder en 1953, los republicanos no eliminaron las reformas introducidas por los demócratas en su largo reinado; y lo que hicieron fue continuar las políticas sociales demócratas, pero de forma más moderada.

En la década de los 60, con los demócratas de nuevo en el poder, se intensificó la intervención del Estado en la economía y se aumentó el gasto público en programas sociales. Los republicanos se plegaban a las políticas demócratas y no ofrecían cambios sustanciales.

El cambio que sufrió el Partido Demócrata, que pasó de ser refugio de los racistas del Sur y enemigo de los derechos de los negros, a ser defensor de la igualdad entre blancos y negros y protector de los derechos de estos últimos, causó un cambio en el Partido Republicano. Algunos blancos del Sur empezaron a abandonar al Partido Demócrata y a mudarse al Republicano; el centro geográfico de los republicanos pasó del Noreste del país al Sur, mientras los demócratas ganaban el Noreste. Pero también provocó que la población negra le diera la espalda a los republicanos (a quienes apoyaban por haberlos liberado de la esclavitud); y se cambiaran mayoritariamente a los demócratas. Las elecciones presidenciales de 1964 fueron decisivas en ese sentido, pues fue la primera ocasión que el Sur votó mayoritariamente a los republicanos, lo que no ha dejado de hacer desde entonces. Se había producido un vuelco político histórico: el Sur, antaño feudo demócrata, se había convertido en un fiel bastión del partido de Lincoln. También fue en estas históricas elecciones de 1964 cuando el partido presentó como su candidato a la Casa Blanca a Barry Goldwater. A pesar de obtener un muy mal resultado, Goldwater influyó notablemente en la evolución ideológica que los republicanos harían en las décadas siguientes hacia posiciones mucho más liberales en lo económico y conservadoras en lo social. Así, Goldwater empezó el viraje del GOP hacia la derecha que la "era Reagan" haría irreversible.

El desastroso final del gobierno de Richard Nixon, y el papel gris de Gerald Ford, dejaron al partido debilitado en los años 70; pero entonces el país entró en una grave crisis económica con altos índices de desempleo e inflación bajo el gobierno del demócrata Jimmy Carter. La situación era el resultado de la crisis petrolera de esos años. Era el momento de un cambio radical.

Cuando Ronald Reagan ganó la Presidencia en las elecciones de 1980, comenzó una nueva etapa para el Partido Republicano.

Durante el gobierno de Reagan (1981-1989) el Partido Republicano dio un giro a la política interna de Estados Unidos. Los republicanos hicieron grandes reducciones a los impuestos para intentar estimular el ahorro, esto activó la inversión y con ello se generó un crecimiento económico, que significó más empleo y mayores ingresos. No obstante, aunque Estados Unidos creció económicamente durante la era Reagan, algunos economistas asocian este crecimiento a la época alcista dentro del ciclo económico de la economía estadounidense. Además se implementó una reducción del gasto público y de la burocracia del Gobierno, junto con reducir o eliminar los programas sociales para seguir reduciendo el gasto público, y en algunas ocasiones serían motivos de descontento por parte de algunos ciudadanos. Este giro a la derecha, sofocaría al ala más liberal del partido, reemplazándola por un auge de los sectores más nuevo liberales, integristas religiosos, y libertarios en el seno del partido y de la derecha estadounidense.

Los republicanos propugnaban un regreso al espíritu individualista de los pioneros estadounidenses. Defendían a la economía de libre mercado frente a una intervención del Estado, que según ellos, frenaba el crecimiento económico, la creación de nuevas empresas y por ende la creación de empleo. Su doctrina va en contra del conocido como Estado del Bienestar y de la economía Keynesiana. Según la postura republicana, este modelo era ineficiente, con planes económicos costosos y con altos impuestos.

Esta etapa, denominada "Revolución Conservadora" coexistió con un cierto despegue económico (que también se dio en Europa), pero para los críticos las desigualdades sociales se vieron acentuadas, lo que en su opinión dejó fuera del avance económico a las clases menos pudientes. Por su lado los defensores de esa política económica conservadora, destacan indicadores económicos positivos como el incremento del ahorro, la mayor libertad de gestión y la creación de 20 millones de nuevos empleos durante la administración de Reagan. Aunque, los críticos, suelen achacar esa disminución del desempleo a que Estados Unidos (en teoría) había llegado al máximo histórico de desempleo, por tanto el empleo tendería a crecer.

Después de la Era Reagan, el Partido Republicano continúa albergando una gran facción política favorable al libre mercado y a políticas liberales.

El partido Republicano es el más conservador de los dos grandes partidos del país. Ideológicamente hablando, se podría catalogar de «conservador laico» para distinguirlo de los partidos democratacristianos que encarnan a la derecha en otras naciones occidentales. El partido está afiliado a la Unión Internacional Demócrata (International Democrat Union, IDU) a la que pertenecen otros partidos conservadores democráticos como el Partido Conservador, Los Republicanos o la Unión Demócrata Cristiana.

En el aspecto económico su doctrina es el liberalismo económico; la rama o vertiente económica del liberalismo. En Estados Unidos se suele llamar esta doctrina, defensora del libre mercado y enemiga de la intervención del Estado, conservadurismo fiscal.

Por eso, los republicanos se consideran los más celosos defensores del "laissez faire" estadounidense.

En Estados Unidos hay pocas empresas públicas o del Estado. El Gobierno Federal (Nacional) solo posee una empresa generadora de energía eléctrica (la Autoridad del Valle del Tennessee), participación accionaria dominante en una empresa de ferrocarriles (Amtrak), y el correo público (Servicio Postal de los Estados Unidos); pero las tres deben competir con empresas privadas. Pero los republicanos quieren reducir aún más el Estado privatizando otras áreas. Como ejemplo, muchos republicanos quieren privatizar parcialmente el sistema de pensiones de la Seguridad Social; haciendo que los trabajadores jóvenes destinen una parte de su aportación obligatoria al Seguro Social a fondos de pensiones administrados por empresas privadas que los invertirían en la Bolsa de Valores. Siempre el Partido Republicano apuesta por soluciones de mercado a los problemas nacionales, de ahí las fuertes críticas recibidas cuando apostó por el rescate de grandes empresas y bancos tras la quiebra de Lehman Brothers.

Desde su fundación hasta bien entrado el siglo el Partido Republicano era visto como un partido «progresista»; ya que nació para luchar por una causa muy progresista (la abolición de la esclavitud de los negros) y porque su rival (el Partido Demócrata) era visto como «conservador» en aquel entonces. Aunque ya para aquella época destacaba su defensa de la empresa privada, su rechazo a la injerencia del Estado en la economía y su frontal oposición a socialistas y comunistas.

A medida que el Partido Demócrata se alejaba del conservadurismo y se ubicaba más a la centro-izquierda; el Partido Republicano pasó a ser la fuerza conservadora de Estados Unidos.

Pero es necesario acotar dos puntos: el primero es que debido a la gran libertad de consciencia y la poca disciplina partidista que existe en los partidos estadounidenses los mismos son muy heterogéneos, y dentro de cada uno de ellos conviven personas y grupos con grandes diferencias doctrinales en relación con temas relevantes. Aunque el Partido Republicano es un poco menos heterogéneo que su rival demócrata; no escapa de la existencia de grupos o tendencias internas que tienen visiones diferentes de la filosofía y principios del partido y el segundo es que la escala ideológica en cada país es diferente. En los Estados Unidos las tendencias socialiberalistas se consideran de izquierda cuando en Europa o en Latinoamérica estos estarían clasificados entre el centro y la centro derecha liberal en temas sociales y económicos.

En la mayor parte del siglo se hablaba de tres grandes tendencias en el Partido Republicano: liberales, conservadores y moderados.

Los republicanos liberales (la denominación liberal en Estados Unidos se interpreta como izquierdista, a diferencia de Europa donde se asocia con la centro-derecha) eran el ala del partido más próxima a las ideas que suelen asociarse con los demócratas (de hecho estos republicanos piensan y actúan casi como demócratas). A diferencia de la mayoría de sus compañeros de partido, son menos favorables a reducir los impuestos; están más dispuestos a aumentar el gasto público (sobre todo en programas sociales); son partidarios de un Estado más grande del que están dispuestos a tolerar los otros republicanos; y son más tolerantes en temas sociales (aborto, homosexualidad, etc.)

En el siglo el republicano liberal más famoso fue Nelson A. Rockefeller, que lideraba a esta tendencia del partido (por lo que los liberales eran llamados los «republicanos de Rockfeller»). Durante su mandato como gobernador del Estado de Nueva York aumentó el gasto público del Gobierno, destinando más fondos a la política social y obras públicas.

Los liberales lucharon por el control del partido en los años 60 y 70, pero cuando los conservadores tomaron el control al mando de Ronald Reagan en los 80; los liberales quedaron aislados y fueron perdiendo presencia hasta quedar reducidos a una minoría insignificante y marginada.

Los republicanos conservadores son el ala más radical del partido; son los más derechistas y por tanto, los más duros críticos de los demócratas. En la mayoría de temas adoptan posiciones absolutas, especialmente en los temas sociales o de moral. Así, defienden la pena de muerte. Enemigos acérrimos del «Gobierno Grande», quieren reducir drásticamente el tamaño del Estado; lo que se traduce especialmente en reducciones del gasto público.

Por su parte los republicanos moderados pretendían estar entre liberales y conservadores representando el Centro político del partido; pueden actuar como liberales en algunos temas, y como conservadores en otros.

Sin embargo, en los últimos años esta clasificación ha quedado obsoleta y se habla de un mayor número de tendencias. A grandes rasgos éstas tendencias son:

Derecha Religiosa o Derecha Cristiana (Teoconservadores): desde los años 80 ha cobrado fuerza en el partido este gran movimiento formado por activistas fundamentalistas de diversas iglesias cristianas (principalmente evangélicas). Basan su política en sus conceptos de religión. Se oponen radicalmente al aborto, los matrimonios homosexuales, el libertinaje sexual, etc. Varios pastores cristianos evangélicos se han convertido en dirigentes republicanos; y los feligreses representan una gran parte del electorado del partido. Sus críticos los acusan de ser fanáticos religiosos y de ser una amenaza para la libertad individual y para el principio de la separación entre la Iglesia y el Estado. Los teoconservadores republicanos operan por medio de una organización política llamada National Federation of Republican Assemblies (Federación Nacional de Asambleas Republicanas)que ofrece su apoyo a los republicanos comprometidos con su visión moral y denuncia a aquellos que por moderados o liberales se alejan de ella.

Conservadores Sociales: son personas de mente conservadora en temas sociales, y por eso tienen posiciones parecidas a las de los derechistas cristianos en asuntos como el aborto, el sexo, etc.; pero con la diferencia de que no son necesariamente militantes religiosos como los anteriores y por tanto no necesariamente trabajan bajo la dirección de sus iglesias. Son gente mayormente de clase media que también desean un regreso a los valores morales de antaño (los «valores familiares») pero sin exagerar en la mezcla de religión con política (son un poco más seculares); y simpatizan con la política económica republicana. Sin embargo, algunos observadores prefieren no hacer distinciones entre esta tendencia y la de los derechistas religiosos refiriéndose también a aquellos como conservadores sociales.

Conservadores Fiscales: su principal razón para ser republicanos es su apoyo a la política económica tradicional del partido; el conservadurismo fiscal (llamado liberalismo económico en otros países). Menos gasto público, menos regulaciones y menos impuestos es su objetivo; defienden con pasión el libre mercado y desean un Estado más pequeño y menos intervencionista. También defienden el equilibrio fiscal, y por lo tanto, la reducción o eliminación del déficit fiscal. Promueven el pago de la deuda nacional, la privatización del Seguro Social mediante cuentas individuales y el libre comercio internacional. No están tan interesados en el tema de la moral como los anteriores. Los republicanos conservadores fiscales tienen una poderosa organización política llamada Club for Growth (Club para el Crecimiento); una organización que se dedica a apoyar en las elecciones internas del partido (entre otras cosas recaudando fondos para ellos) a los aspirantes más comprometidos con sus políticas económicas, mientras que al mismo tiempo la organización ataca y denuncia a aquellos republicanos que ellos consideran que se alejan del conservadurismo fiscal al no recortar lo suficiente los impuestos y al apoyar un gasto público muy alto.

Republicanos de Nombre Solamente (RINO por sus siglas en inglés): es el término despectivo con el que los conservadores llaman a los que anteriormente se conocían como "republicanos liberales". Sin embargo, también muchos conservadores usan este término para referirse a los republicanos moderados, ya que para ellos no existe diferencia entre los unos y los otros; y de esta manera descalifican también a los moderados.

Moderados: de los que ya hablamos, intentan representar la moderación entre los extremos del partido. Los republicanos moderados se organizan en grandes grupos de presión, pensamiento y cabildeo; esos grupos son: el Republican Main Street Partnership (Alianza Republicana de la Calle Main)y el Republican Leadership Council (Consejo del Liderazgo Republicano). También hay grupos republicanos pro-abortistas y ambientalistas que son considerados aliados naturales de los grupos republicanos moderados y liberales.

Neoconservadores: o "nuevos conservadores", son defensores de un mayor gasto social y de una política exterior agresiva e intervencionista. Están más dispuestos a gastar dinero del Estado en grandes proyectos y programas dirigidos a crear una sociedad más conservadora en temas sociales (como programas para los pobres financiados por el Gobierno pero administrados por instituciones religiosas); pero sin renunciar a las reducciones de impuestos (lo que puede traer desequilibrios fiscales). Chocan con los conservadores fiscales o tradicionales que se oponen al mayor gasto. En política exterior creen fervientemente en el papel de "pueblo elegido" de Estados Unidos y que por lo tanto este país debe extender la democracia por el mundo; están convencidos de que sólo una política exterior agresiva puede proteger a la nación de sus numerosos enemigos extranjeros, y para ello Estados Unidos debe usar su fuerza en cualquier parte del mundo, de forma unilateral sí es necesario. Piensan que el país debe ejercer de forma activa su papel de única superpotencia en un mundo unipolar sin sacrificar los intereses nacionales por la búsqueda de consensos internacionales.

Paleoconservadores: son un grupo minoritario que sigue defendiendo la política comercial proteccionista (a pesar de que el partido abandonó ésta política desde, al menos, las últimas décadas del siglo XX). El paleoconservadurismo está en desacuerdo con el libre comercio que defiende la mayoría del partido. También son fuertes opositores de la política exterior porque son aislacionistas (partidarios de que el país se aísle de los problemas del resto del mundo). Rechazan la inmigración ilegal y la acción afirmativa a favor de las minorías étnicas.

Republicanos Gays y Lesbianas (Log Cabin Republicans, nombre de su organización): a pesar de que la mayoría del partido se opone a aceptar el matrimonio entre homosexuales, y los miembros de la derecha cristiana los critican sin piedad; muchos gays y lesbianas pertenecen al Partido Republicano y han articulado un movimiento para cambiar al partido desde adentro. Pero no han tenido mucha suerte en imponer sus objetivos, aunque varios dirigentes republicanos los respetan.

Es necesario acotar que las líneas que separan a estos grupos o facciones a veces no son tan claras; y existen republicanos que tienen características que los hacen pertenecer a más de una de dichas tendencias. Por poner un ejemplo, el Ex-Secretario de Estado Colin Powell se definía a sí mismo como "un conservador en temas fiscales (conservador fiscal) y un moderado en temas sociales y políticos"; aunque sus críticos dentro del partido lo califican de RINO (liberal). Otro caso es el del Senador John McCain, que es considerado muy conservador; pero cuyos puntos de vista en algunos temas lo hacen lucir a veces como un moderado.

También el factor geopolítico juega un papel destacado en relación a estas tendencias ideológicas internas. Así los políticos republicanos de los estados conservadores (aquellos donde la gran mayoría de la opinión pública es claramente conservadora ideológicamente hablando) tienden a ser más conservadores y menos moderados para atraer a los votantes. En cambio, en aquellos estados que tienen fama de progresistas (donde la mayoría de la opinión pública se inclina a la centroizquierda) los políticos republicanos tienden a ser más moderados para no perder votos frente a los demócratas. En otras palabras, en aquellos estados que por ser conservadores son sus feudos naturales los republicanos suelen ser más conservadores mientras en los estados que por progresistas son los feudos de sus rivales demócratas los republicanos se ven forzados a ser más moderados; todo por razones de conveniencia electoral. De tal manera que, por poner un ejemplo, en el Partido Republicano de Texas predominan los conservadores sobre los moderados mientras en el Partido Republicano de Nueva York es a la inversa. Al Partido Demócrata le afecta el mismo factor pero a la inversa en cuanto a la polaridad conservador-progresista.

A diferencia de sus rivales demócratas que tienden a ser más pro-Tíbet, los republicanos mantienen una posición pro-China, a pesar del hecho de contar este país con un sistema comunista. Muchos republicanos tienen una visión favorable de China por ser un estratégico socio de negocios y un mercado abundante. Aun así, el ala religiosa del partido conformada por cristianos evangélicos suelen ser más críticos de Pekín y exigir mayor presión para el respeto a los derechos humanos, en particular de la minoría cristiana.

Otro tema en que se diferencian gravemente de los demócratas es en el de Medio Oriente, el Partido Republicano está casi enteramente unificado en torno a su apoyo a Israel; el 75% de los republicanos afirman sentir simpatía y respaldo por Israel, a diferencia de los demócratas donde la mayoría (si bien por un margen más estrecho de 40 sobre 33%) dice sentir mayor simpatía y respaldo hacia Palestina. Solo un 6% de los republicanos afirma simpatizar con los palestinos o darles su apoyo en el conflicto, con el restante porcentaje dividido entre quienes afirman no apoyar a ninguno o apoyar a ambos por igual.

Según estudios estadísticos los republicanos reciben el respaldo principal de hombres más que de mujeres. En cuanto a estratos socioeconómicos la clase obrera y de bajo ingreso tiende a ser demócrata, mientras los republicanos reciben el apoyo de los sectores de clase alta, de la pequeña, mediana y gran empresa, de los sectores vinculados a la industria y de los militares de carrera o que han ejercido servicio militar. Étnicamente, como se explica mejor más adelante, los republicanos reciben el apoyo mayoritario de la población blanca de origen europeo ya que el 60% de los blancos no hispanos son republicanos, mientras que negros (en un 90%), latinos (alrededor de 70%) y judíos (alrededor del 70%) son principalmente demócratas. De este porcentaje se exceptúa a los cubano-estadounidenses que son el único grupo hispano que vota mayoritariamente por los republicanos.

Religiosamente los protestantes blancos en un 67% (al 2010) y los mormones (73% al 2010) apoyan abrumadoramente al Partido Republicano, mientras que los protestantes negros, los católicos de todas las etnias y los musulmanes son mayormente demócratas. Solo 27% de los ateos y agnósticos estadounidenses son republicanos frente al 64% demócrata.

La sociedad estadounidense tradicionalmente ha sido muy conservadora, y hasta hace poco se vivía un auge del conservadurismo; esto le daba al Partido Republicano una relativa ventaja sobre su rival, el Partido Demócrata. Pero la posición relativamente ventajosa de los republicanos está perdiéndose por la realidad demográfica de los Estados Unidos.

La mayoría de los republicanos pertenecen al grupo étnico de los (Non-Hispanic Whites), una subcategoría reconocida por la Oficina del Censo de los Estados Unidos con fines estadísticos oficiales para identificar a todos los estadounidenses blancos descendientes de europeos que no tienen ascendencia latina o hispana; esta subcategoría constituye junto con la de los blancos hispanos o latinos (las personas de piel blanca que sí son de sangre o ascendencia latina o hispana) la categoría mucho más amplia de los blancos estadounidenses. Más aún, muchos republicanos son de los llamados W.A.S.P. (Blancos Anglo-Sajones y Protestantes, según sus siglas en inglés) el tipo racial y religioso que durante mucho tiempo fue el más poderoso de Estados Unidos. Los blancos no hispanos son todavía la mayoría de la población estadounidense, pero gradualmente van reduciendo su tamaño mientras las minorías étnicas aumentan el suyo.

Como ya vimos el Partido Republicano contó con las simpatías de los habitantes de raza negra estadounidenses hasta la década de los 60 del siglo XX; pero cuando en esa década los Presidentes demócratas John F. Kennedy y Lyndon B. Johnson lograron imponer una serie de reformas legales que hicieron realidad las normas de la Constitución que garantizaban la igualdad de razas, los blancos del Sur (enemigos de los afroamericanos) se sintieron traicionados por el Partido Demócrata por el que siempre votaban, y muchos de ellos comenzaron a votar por los republicanos. El Sur demócrata se convirtió en el Sur republicano, lo que trajo como consecuencia que el Partido Republicano dejara de defender los derechos de los afroamericanos, y que estos se convirtieran en demócratas.

La minoría negra afroamericana (llamada así, entre otras razones, para diferenciarla de los o latinos en las estadísticas oficiales) es el grupo étnico más comprometido con un solo partido; en todas las elecciones, sobre todo en las presidenciales, más del 90% vota por los candidatos del Partido Demócrata. Los afroamericanos republicanos, aunque existen, son una minoría casi insignificante.

El Presidente George W. Bush en un intento por cambiar esta situación, se rodeó de varios colaboradores negros afroamericanos, como Colin Powell, el primer Secretario de Estado afroamericano de la historia de los Estados Unidos y Condoleezza Rice, la primera mujer afroamericana en ocupar el mismo cargo, quien había sido antes también la primera en ocupar el puesto de Asesora de Seguridad Nacional; Rice fue la consejera más importante durante su presidencia. Pero más allá de los intentos de Bush, el Partido Republicano no ha logrado que dicha minoría se acerque al partido.

Aun así, en el 2010 se dio un nuevo paso en el acercamiento del Partido Republicano a los afroamericanos cuando por primera vez desde finales del siglo XIX dos afroamericanos republicanos ganaron en las elecciones escaños a la Cámara de Representantes del Congreso por distritos ubicados en el Sur profundo de Estados Unidos, tradicionalmente la región más racista del país. Fueron ellos Allen West, que ganó en un distrito de Florida, y Tim Scott, que ganó en un distrito de Carolina del Sur; la victoria de este último además es más importante ya que previamente había derrotado en las elecciones primarias internas del Partido Republicano de su distrito al hijo del antiguo dirigente segregacionista Strom Thurmond. Ellos fueron 2 de los 32 candidatos al Congreso afroamericanos que presentó el Partido Republicano en esas elecciones. El 17 de diciembre de 2012 la gobernadora republicana de Carolina del Sur, Nikki Haley (una estadounidense hija de inmigrantes Sijes de la India), nombró a Tim Scott senador por ese estado al Senado de los Estados Unidos para cubrir la vacante dejada por Jim DeMint, que renunció a su escaño para ser presidente de la Fundación Heritage; con su toma de posesión el 2 de enero de 2013 Scott se convirtió en el primer afroamericano en el Senado estadounidense desde que Roland Burris (sucesor de Barack Obama como senador) terminó su mandato en 2010, y un hito al ser un afroamericano republicano.Pero al mismo tiempo que Scott entraba al Senado, Allen West abandonaba la Cámara de Representantes porque perdió en su intento de ser reelegido, por lo que actualmente no hay ningún afroamericano republicano en esa Cámara del Congreso.

El caso de los hispanos o latinos es más importante aún. Desde hace unos años la población hispana o latina es ya la minoría más numerosa del país, y seguirá creciendo para ser un porcentaje cada vez más importante de la población total estadounidense; así que para ganar las elecciones un partido requiere de su apoyo.
Tradicionalmente la gran mayoría de los mexicano-estadounidenses (los latinos más numerosos en USA), puertorriqueños, dominicanos, y otros grupos latinos votan siempre por los demócratas. Solo los cubano-estadounidenses votan mayoritariamente por los republicanos.

Pero George W. Bush logró cambiar esa tendencia en sus elecciones. Ya en su época de Gobernador de Texas logró una hazaña, al conseguir el 49% del voto hispano o latino en su reelección en la Gobernatura (en un Estado donde la mayoría de los hispanos son mexicanos). Y como Presidente consiguió establecer otro récord al obtener aproximadamente el 40% de los votos de los hispanos en todo el país en su reelección a la Presidencia en el año 2004, un hecho inimaginable para un republicano hacía solo unos años.

En su Gobierno no hubo una figura hispana de la talla de Rice, pero sí algunos Secretarios (Ministros) de origen latino. Entre ellos, el octogésimo primer Secretario de Justicia, Alberto R. Gonzales y el exsenador en el Congreso de Estados Unidos, el cubano-estadounidense, Mel Martínez (Ex-Secretario de Vivienda y Desarrollo Urbano).

Cada vez hay más funcionarios electos (Representantes federales y estatales, senadores estatales, concejales, etc.) del Partido Republicano que son de origen Latino; sobre todo en Estados de fuerte presencia hispana como Florida, California y Texas. En las elecciones legislativas, estatales y locales de mitad de período realizadas en el año 2010, varias prominentes figuras latinas del Partido Republicano obtuvieron victorias que los proyectaron al primer plano nacional. Los más famosos fueron Susana Martínez, electa Gobernadora de Nuevo México; Brian Sandoval, electo Gobernador de Nevada, y Marco Rubio, electo Senador por Florida al Senado de los Estados Unidos. Además otros cinco latinos republicanos fueron elegidos para su primer período al Congreso, y sumados ellos y Rubio a los dos hispanos republicanos que ya eran congresistas (Ileana Ros-Lehtinen y Mario Díaz-Balart) sumaban ocho latinos republicanos en el Congreso estadounidense frente a veinte congresistas hispanos que pertenecían al Partido Demócrata.En las elecciones legislativas celebradas simultáneamente con las presidenciales del 6 de noviembre de 2012 fueron elegidos siete latinos republicanos al Congreso de los Estados Unidos, uno menos que en los comicios anteriores; pero ahora hay otro latino o hispano republicano en el Senado de los Estados Unidos, el senador por Texas Ted Cruz, con lo que junto a Marco Rubio son ya dos los senadores latinos republicanos del Congreso estadounidense (de un total de sólo tres senadores latinos, el otro es el demócrata Bob Menendez). Los otros cinco latinos republicanos son congresistas de la Cámara de Representantes de los Estados Unidos (hubo 16 candidatos republicanos latinos o hispanos a la Cámara de Representantes, pero solo resultaron elegidos en sus respectivos distritos esos cinco); sin embargo los siete legisladores republicanos de etnia latina todavía son bastante menos que los 24 legisladores latinos demócratas que fueron electos en los comicios del 2012.

Los valores morales tradicionales y conservadores (los llamados "valores familiares") de los republicanos son muy populares entre personas como los latinos, provenientes de sociedades relativamente conservadoras y muy religiosas, y es uno de las principales “armas” utilizadas por los republicanos para obtener el voto hispano. Por otro lado, la reducción del dinero destinado a programas sociales (planteada siempre por este partido) suele ser impopular entre muchos Latinos que dependen de estas ayudas del Estado.

El principal obstáculo para los esfuerzos de Bush y otros líderes para atraer militantes y simpatizantes Latinos al Partido Republicano, es el discurso fuertemente anti-inmigrante de varios representantes partidarios radicales e influyentes. Las medidas que pretenden tomar estos políticos contra los extranjeros indocumentados (la mayoría provenientes de América Latina) les hacen ganar votos entre algunos sectores (los de los ciudadanos estadounidenses que lo son desde hace varias generaciones y que creen que los inmigrantes solo ocasionan problemas), pero alejan a los hispanos del partido. De hecho, algunas derrotas electorales de los republicanos en comicios estatales y locales se han debido en buena parte al enfado de los Latinos con políticos republicanos que han tomado demagógicas medidas anti-inmigrantes.
Otras minorías étnicas (judíos, asiáticos, etc.) son también mayoritariamente demócratas.
La idea a futuro del Partido Republicano, poder atraer a las minorías, hasta hoy claramente inclinadas hacia el Partido Demócrata, sin perder el apoyo de sus aliados tradicionales y manteniendo su condición de "fuerza conservadora".

En otros aspectos, la presidencia de George W. Bush marcó el comienzo de una etapa difícil para el partido; su política económica fue duramente criticada por los sectores mayoritarios del Partido Republicano por ir en contra de los postulados del conservadurismo fiscal.Aunque Bush hizo enormes rebajas a los impuestos federales, algo ajustado al conservadurismo fiscal, por otro lado también fue responsable de masivos aumentos del gasto público, y por lo tanto rompió con la disciplina fiscal, causando gigantescos déficits. Esto tanto ocasionó el rechazo y el enfado de la mayoría de los republicanos. Otros aspectos de su gestión, como la política exterior, también causaron críticas en el interior del partido, por lo que al final de su gobierno todos los precandidatos presidenciales republicanos guardaban distancia de él y el candidato electo, John McCain, era su mayor rival dentro del partido.

Pero McCain no pudo convencer al electorado de que representaba una ruptura con Bush y fue finalmente derrotado por el 44º Presidente, el demócrata Barack Obama; en su nuevo rol como partido opositor, el Partido Republicano se hallaba confundido y carente de liderazgo. Posteriormente se desató una polarización interna entre moderados y conservadores; estos últimos han revitalizado la oposición con su dura y radical campaña contra Obama, simbolizada en el Tea Party Movement, al precio de ocasionar un amargo enfrentamiento con los moderados que temen que esa radicalización a la derecha, aunque sirva para movilizar a la opinión pública contra Obama, al final pueda alejar a los electores independientes de centro.
Sin embargo, el creciente descontento popular con la gestión de Obama y el éxito del Tea Party para canalizarlo, consiguieron que el Partido Republicano obtuviera la victoria en las elecciones de mitad de período del 2010. Los republicanos recuperaron el control de la Cámara de Representantes del Congreso, redujeron la mayoría demócrata en el Senado y ganaron la mayor parte de las gobernaciones de los estados que convocaban elecciones estatales. Aunque esta serie de victorias aumentó el poder del Partido Republicano y le infundió renovadas fuerzas; el presidente Barack Obama logró ganar la reelección apenas dos años después, en gran parte por el apoyo de las minorías, a quienes el Partido Republicano no pudo convencer y por el contrario terminó alejándolas.

Las Primarias Republicanas de 2012 es el proceso de selección en la cual el Partido Republicano de los Estados Unidos seleccionará a delegados para que asistan a la Convención Nacional Republicana de 2012, en la cual será nominado el candidato único a la Presidencia para las elecciones de 2012.
El exgobernador de Massachusetts Mitt Romney, quién también se postuló en las primarias presidenciales de 2008, obtuvo un apoyo al principio por parte de los votantes republicanos, siendo el favorito para la nominación presidencial de su partido. Sin embargo, su ventaja sobre el campo republicano ha sido precario, debido a la entrada de nuevos candidatos que atrajeron la atención considerable de los medios entre abril y agosto de 2011. El apoyo entre los republicanos por el Gobernador Rick Perry de Texas, lo impulsó a unirse a la carrera en agosto de 2011, que teniendo un fuerte desempeño en las encuestas, instantáneamente convirtiéndose en un serio contendiente. El 10 de abril, Santorum suspendió su campaña, dejando a Mitt Romney, como el indiscutible favorito para la nominación presidencial. A fines de agosto, en la Convención Nacional Republicana de 2012, habrá de ser proclamado candidato a la Presidencia. Ya anunció a Paul Ryan como su candidato a vicepresidente. El día 6 de noviembre de 2012, el exgobernador y actual candidato por el partido republicano perdió las elecciones ante el candidato y presidente Barack Obama, sumando este una mayoría de electores superior a los 270 necesarios para asegurar la presidencia, quedando en segundo lugar con la mayor votación.
Pocos días después de perder la elección, en medio de reproches cruzados entre los republicanos, se desató una sarta de críticas a Romney por sus polémicas declaraciones.

En las siguientes elecciones presidenciales, las de noviembre de 2016, el partido eligió al controvertido millonario Donald Trump como su candidato a la Casa Blanca. Su victoria en las primarias originó tensiones en el seno de la formación, al tratarse de un candidato recién llegado al partido y a la política, que había mantenido agrias polémicas con veteranos republicanos como McCain o sus rivales en esas primarias Jeb Bush y Marco Rubio y que por su estilo de vida rompía con las condiciones que se solían atribuir a un candidato republicano. El rechazo público de miembros prominentes del GOP a Trump, incluso siendo ya este presidente, es un caso inédito que muestra la fractura interna del partido.

El futuro del Partido Republicano causa incertidumbre entre sus propias filas, ante esta situación de ruptura y extremismo internos, o ante el cobro de un protagonismo cada vez mayor en el seno de la sociedad estadounidense de colectivos como los negros, los hispanos o las minorías sexuales muy alejados de los republicanos. Un antiguo colaborador de George W. Bush, David Frum, se preguntaba en un libro intitulado "Por qué Romney perdió" y publicado con motivo de las elecciones de 2012 (que él daba por perdidas) por qué «vemos tanto extremismo [entre nosotros, los republicanos]». Frum apuntaba también varios datos elocuentes: «El Partido Republicano está cada vez más aislado de la América moderna. En el cuarto de siglo que ha pasado desde 1988 ha habido seis elecciones presidenciales. Sólo en una de ellas el candidato republicano consiguió la mayoría del voto popular, y por un miserable 50,73 por ciento. Como contraste, de las seis elecciones que hubo entre 1968 y 1988, los republicanos ganaron cinco. La media de porcentaje de voto conseguida, incluyendo la derrota de 1976, fue del 52,5 por ciento».








</doc>
<doc id="37795" url="https://es.wikipedia.org/wiki?curid=37795" title="Compra de Luisiana">
Compra de Luisiana

La compra de Luisiana fue una transacción comercial mediante la cual Napoleón Bonaparte, entonces Primer Cónsul francés, faltando al acuerdo de retroventa a España, vendió a Estados Unidos en 1803, () de posesiones francesas en América del Norte ( cedidas por España) a un precio de alrededor de 3 centavos por acre (7 centavos por ha); un precio total de 15 millones de dólares u 80 millones de francos franceses. Con los intereses, el territorio de Luisiana costó 23.213.568 dólares. 

La vasta extensión objeto del tratado comprendía los territorios de los actuales estados de Arkansas, Misuri, Iowa, Oklahoma, Kansas, Nebraska, Minnesota al sur del río Misisipi, gran parte de Dakota del Norte, casi la totalidad de Dakota del Sur, el noreste de Nuevo México, el norte de Texas, una sección de Montana, Wyoming, Colorado al este de la divisoria continental, y Luisiana a ambos lados del río Misisipi, incluyendo la ciudad de Nueva Orleans. Además, la compra comprendía partes de las provincias actuales de Alberta y Saskatchewan, en el actual Canadá. Este territorio representa el 23% de la superficie actual de los Estados Unidos. 

La compra era importante para la presidencia de Thomas Jefferson, que se enfrentó a cierta resistencia interna a la compra. Aunque existían dudas acerca de la constitucionalidad de la adquisición del territorio, decidió comprar Luisiana porque no le gustaba la idea de que Francia y España tuvieran el poder de bloquear el acceso de comerciantes estadounidenses al puerto de Nueva Orleans. Esta negociación abrió a Estados Unidos el acceso al océano Pacífico e incrementó de forma espectacular su territorio, por lo que constituye uno de los acontecimientos históricos de mayores consecuencias en la Historia de los últimos dos siglos de los Estados Unidos de América.

En 1803, la Luisiana era un territorio de aproximadamente 2 millones de km², que lindaba al norte con las posesiones británicas, al este con el río Misisipi y al sur y oeste con los territorios de España. Estaba poblada por alrededor de 35.000 personas de origen europeo (la tercera parte de ellas en Nueva Orleans), y un número indeterminado de nativos norteamericanos.

Los Estados Unidos buscaban la manera de controlar la navegación sobre el río Misisipi, hecho que le llevó a iniciar negociaciones con el régimen de Napoleón Bonaparte, por entonces Primer Cónsul francés.

Por su parte Francia tenía prisa por deshacerse de esta colonia, ya que la había obtenido sólo dos años atrás (por medio del Tratado de San Ildefonso (1800) con España, y por el que Francia estaba obligada a vender preferentemente a España el territorio que adquiró de España, y que obtuvo tras la Guerra de los siete años), y debido a que la revolución negra de la colonia de Haití había declarado su independencia de Francia, traía como consecuencia que la Luisiana perdiese interés estratégico para los franceses.

Napoleón prefirió entonces vender la colonia a los estadounidenses, antes que a España, o de correr el riesgo de perderla ante los ingleses. Como dijo él mismo: «"Esta venta no es un gran negocio para Francia, pero lo importante es que le daremos a los ingleses un competidor nuevo en su monopolio marítimo"». Y de esta manera además Napoleón evitaba la reunificación de las colonias españolas en Norteamérica.

Los límites de la cuenca del río Misisipi, definidos por las cabeceras de todos sus afluentes de la margen derecha, serían considerados los del nuevo territorio, por parte de los Estados Unidos. Su extensión se estimaba basándose en las exploraciones de Robert de La Salle. 

Enseguida se generó una disputa en España por esta cuestión. Los límites del territorio no habían sido fijados en el Tratado de Fontainebleau (1762), por el cual Francia lo cedía a España, ni en el Tratado de San Ildefonso (1800), de retrocesión en Francia, ni tampoco en el definitivo acuerdo de compra de 1803. Los Estados Unidos reclamaban que la Luisiana incluía toda la parte occidental de la cuenca del río Misisipí hasta la divisoria de aguas de las Montañas Rocosas y también la tierra que se extendía al sudeste hasta el río Bravo. España insistía en que comprendía sólo la mitad occidental de la cuenca del Misisipi, con las ciudades de Nueva Orleans y San Luis. La relativamente estrecha franja de la Luisiana incluida en Virreinato de la Nueva España fue una provincia especial bajo la jurisdicción de la Capitanía General de Cuba, mientras que la vasta región que se extendía hacia el oeste en 1803 todavía era considerada como una parte de la Comandancia General de las Provincias Internas. La Luisiana quedó bajo el gobierno de la Capitanía de Cuba, como provincia independiente del virreinato de Nueva España, pero no formando parte de las Provincias Internas.

Del mismo modo, el límite norte del territorio adquirido lo constituía el igualmente mal definido límite con las posesiones británicas de la Tierra de Rupert, actualmente parte de Canadá. La compra, en principio, se extendía más allá del paralelo 50º norte. Aun así, el territorio al norte del paralelo 49º norte, incluyendo las cabeceras del río Milk y del río Poplar, fue cedido al Reino Unido a cambio de una parte de la cuenca del río Rojo al sur del paralelo 49º norte por la Convención Angloamericana de 1818. 

El límite oriental de la Luisiana comprada era el río Misisipi, desde su nacimiento hasta el paralelo 31º norte, si bien las fuentes del río Misisipi (el lago Itasca) todavía no se conocían. Pero el límite este al sur del paralelo 31º no estaba claro; los Estados Unidos reclamaban toda la tierra hasta el río Perdido, mientras que España declaraba que la frontera de su colonia de Florida estaba fijada en el río Misisipi. A primeros del 1804, el Congreso acordó la Mobile Act, que consideraba la Florida occidental integrada a los Estados Unidos (actualmente, es la salida al mar de los estados de Misisipi y Alabama). El Tratado de Adams-Onís, firmado con España en 1819, resolvió definitivamente esta cuestión, y actualmente el paralelo 31º norte y el río Perdido constituyen las fronteras norte y oeste, respectivamente, de la Florida continental u occidental. 

El límite meridional respecto de la Nueva España al principio tampoco estaba nada definido; el Tratado de Tierra Neutral de 1806 creó una franja neutral a lo largo del río Sabine durante un tiempo, hasta que el Tratado de Adams-Onís, en 1819, empezó a fijar la línea divisoria.

El límite occidental era desconocido pero el tratado Adams-Onís lo fijó como sigue: siguiendo el río Sabine hasta el paralelo 32, luego dirigiéndose al norte hasta el río Rojo, desde el río Rojo hasta el meridiano 100, sigue hasta el río Arkansas, después hasta la cabecera del Arkansas, sigue hacia el norte hasta el paralelo 42, y continúa hasta el océano Pacífico.

El presidente estadounidense Thomas Jefferson envía a sus emisarios a Francia por primera vez en 1801 dando inicio a las conversaciones.

Los estadounidenses, que sólo buscaban comprar la parte ribereña, se vieron sorprendidos cuando los franceses les ofrecieron el terreno en su totalidad. La compra se llevó a cabo sin que nadie supiera exactamente las características del terreno. Se trataba de zonas inexploradas y no existía ningún tratado con España que determinara el límite entre los dos territorios.

Es verdad que la suma de dinero fue relativamente muy poca, pero para un joven país como era Estados Unidos, fue una suma considerable e incluso debió endeudarse para cubrirla. Sin embargo puede considerarse el negocio más rentable de la Historia.

La adquisición del territorio de Luisiana no estuvo exenta de polémica en el ámbito local. La coherencia de los principios de Jefferson fue cuestionada a causa de su estricta interpretación de la Constitución. Muchos veían que Jefferson se comportaba de forma hipócrita, haciendo algo que habría criticado de haberlo hecho Alexander Hamilton. Los Federalistas se opusieron de forma férrea a la adquisición, apoyando establecer relaciones más estrechas con Gran Bretaña frente a Francia. Según ellos la compra era inconstitucional y creían que EE. UU. había pagado una gran cantidad de dinero sólo para declarar la guerra a España. La Cámara de Representantes también se opuso a esta compra. El líder de la mayoría John Randolph lideró la oposición y se realizó una votación que aprobó la compra por un estrecho margen (59-57). Los Federalistas incluso intentaron probar que la tierra era posesión española, no francesa, si bien los documentos indicaban lo contrario. Los federalistas temían que el poder político de los estados de la costa este fuese amenazado por los nuevos ciudadanos del oeste, produciéndose un conflicto entre los banqueros de Nueva Inglaterra y la población de las granjas del oeste. Además se temía que los nuevos estados esclavistas que surgieran aumentasen la tensión entre el norte y el sur. Un grupo de federalistas liderados por el senador por Massachusetts Timothy Pickering incluso fue más lejos ideando un plan para segregar el norte ofreciéndole a Aaron Burr la presidencia de la nueva confederación si conseguía que Nueva York se uniera a ella. La relación de Burr con Alexander Hamilton que terminó con dicho movimiento separatista se deterioró durante ese período, acabando con un duelo de pistola que acabaría con la vida del segundo en 1804.

El sábado 30 de abril de 1803 el tratado que establecía los términos para la adquisición de Luisiana fue firmado por Robert R. Livingston, James Monroe y Barbé Marbois en París. Jefferson anunció a su país el tratado el 4 de julio. Después de la firma de este acuerdo Livingston declaró: ""Aunque hemos vivido mucho este es la más noble tarea de nuestras vidas... Desde este día los Estados Unidos ocupan su lugar entre las potencias de primer rango"". El Senado ratificó la compra por un amplio margen (24-7) el 20 de octubre del mismo año. Al día siguiente se autorizaba al presidente Jefferson a establecer un gobierno militar interino. En la legislación promulgada el 31 de octubre el Congreso ordenó que las autoridades civiles locales continuasen en sus puestos tal y como se habían mantenido bajo el mandato francés y autorizó al presidente a usar el ejército para mantener el orden. Asimismo se dispusieron planes para mandar misiones a explorar y cartografiar el territorio, la más célebre de las mismas fue la expedición de Lewis y Clark.

Francia abandonó Nueva Orleans el 20 de diciembre de 1803 en El Cabildo. El 10 de marzo del año siguiente una ceremonia en San Luis representó formalmente la transferencia del territorio a las nuevas autoridades.
Con fecha de 1 de octubre de 1804 el nuevo territorio adquirido fue subdividido en Territorio de Orleans (que formaba parte del estado de Luisiana) y el Distrito de Luisiana que paso a formar parte del Territorio de Indiana.

Tras la expedición de Lewis y Clark, el gobierno de los Estados Unidos se esforzó por controlar efectivamente toda la región, puesto que el comercio a lo largo de los ríos Misisipi y Misuri todavía estaba controlado por comerciantes británicos y franceses y sus aliados indios, especialmente los sauk. Fort Bellefontaine fue reconvertido en un establecimiento militar estadounidense cerca de San Luis en 1804. En 1808 se construyeron dos nuevos fuertes militares que incorporaban sendas factorías comerciales, Fort Osage en el río Misuri y Fort Madison en el alto Misisipi. Durante la Guerra anglo-estadounidense de 1812-1815, los británicos y sus aliados indios derrotaron a las fuerzas norteamericanas en el alto Misisipi; tanto Fort Osage como Fort Madison fueron abandonados, al igual que varios fuertes construidos por los estadounidenses durante la guerra, incluyendo Fort Johnson y Fort Shelby, en Wisconsin. Tras que fuera confirmada la posesión estadounidense de la región por el Tratado de Gante, se edificaron o bien ampliar fuertes a lo largo de los ríos Misisipi y Misuri, incluidas la ampliación de Fort Bellefontaine y la construcción de Fort Armstrong (1816) y de Fort Edwards (1816) en Illinois, Fort Crawford (1816) en Prairie du Chien, Fort Snelling (1819) en Minnesota, y Fort Atkinson (1819) en Nebraska.

El gobierno de los Estados Unidos utilizó tres millones de dólares en oro para empezar a pagar la compra a Francia, y el resto en bonos del Tesoro. Debido a la inminente guerra contra el Reino Unido, los bancos franceses no querían comprar ni negociar con bonos estadounidenses. Por lo tanto, los diplomáticos estadounidenses Livingston y Monroe recomendaron las casas Baring, de Londres, y Hope, de Ámsterdam, para que garantizasen la transacción ante el gobierno francés. Como que tenían la reputación de ser las dos firmas financieras más estables de Europa, y puesto que Napoleón quería recibir su dinero con la mayor brevedad posible, el ministro francés de Finanzas, Barbé-Marbois, negoció con los dos bancos por convertir los bonos que Francia quería recibir en metálico. Nada más recibir los bonos estadounidenses, el gobierno francés los vendió a Baring y a Hope con descuento. 

El documento original de la compra de la Luisiana estuvo expuesto en el vestíbulo de entrada de la sede del Barings Bank, en Londres, hasta que esta entidad quebró en 1995, y ahora lo guarda la entidad ING Group, que compró Barings.




</doc>
<doc id="37799" url="https://es.wikipedia.org/wiki?curid=37799" title="VIII legislatura de España">
VIII legislatura de España

La legislatura de España comenzó el 2 de abril de 2004 cuando, tras la celebración de las elecciones generales, se constituyeron las Cortes Generales, y terminó el 15 de enero de 2008, con la disolución de las mismas. Le precedió la legislatura y le sucedió la legislatura. 

El Partido Socialista Obrero Español obtuvo mayoría simple en el Congreso de los Diputados. José Luis Rodríguez Zapatero fue investido presidente del Gobierno y formó su primer Gobierno. El Gobierno se caracterizó por la creación de más incremento de los servicios sociales creando la Ley de Dependencia entre otras, cuyos objetivos se irían incrementando años a años con modificación de la política económica del gobierno saliente, además de más derechos como el matrimonio homosexual o una nueva regularización de emigrantes, el inicio de un denominado proceso de paz con ETA y la reforma de Estatutos de Autonomía. En el terreno internacional, se retiraron las tropas de Irak y se promovió la Alianza de Civilizaciones. 

El Partido Popular, por su parte, se quedó aislado en los acuerdos sobre la reforma de la justicia o en las conclusiones de la Comisión del 11-M. Incluso un socialista fue elegido presidente de la cámara del Senado, en donde el Partido Popular contaba con mayoría simple. El Gobierno sustituyó también todos los cargos públicos de designación directa procedentes de la etapa del PP en todos los medios e instituciones de titularidad pública. Se nombró a Cándido Conde-Pumpido como Fiscal General del Estado.

En esta legislatura se sucedieron diversos sucesos polémicos que intensificaron el debate político y provocaron la ruptura del entendimiento entre los dos grandes partidos, quedándose el PP en muchas ocasiones aislado del resto de formaciones, por no cumplir lo pactado anteriormente en asuntos como el terrorismo de ETA. Las reformas de los estatutos de autonomía, el alejamiento del Gobierno y la Iglesia, el interés del capital extranjero por las empresas españolas y el enfrentamiento entre grupos mediáticos por la manipulación de "El Mundo" a cuenta de la investigación y juicio del 11-M, fueron algunos de los asuntos centrales de la legislatura.

Las elecciones generales se celebraron el 14 de marzo de 2004, tres días después de los atentados del 11-M en Madrid. El Partido Socialista Obrero Español (PSOE) ganó las elecciones al obtener una mayoría simple de 164 escaños en el Congreso de los Diputados, 39 diputados más que en las anteriores elecciones. El Partido Popular (PP) obtuvo 148 escaños, 39 menos que en las anteriores. Los resultados de las elecciones dieron la vuelta a las encuestas que pronosticaban una victoria del PP. 

En cuanto a los partidos minoritarios, Convergència i Unió (CiU) obtuvo 10 escaños, Esquerra Republicana de Catalunya (ERC) obtuvo 8 escaños, el Partido Nacionalista Vasco (PNV) obtuvo 7 escaños, Izquierda Unida (IU) obtuvo 5 escaños, Coalición Canaria (CC) obtuvo 3 escaños, el Bloque Nacionalista Galego (BNG) obtuvo 2 escaños, Chunta Aragonesista (CHA) obtuvo 1 escaño, Eusko Alkartasuna obtuvo 1 escaño y Nafarroa Bai (NaBai) obtuvo 1 escaño.

Las Cortes Generales se constituyeron el 2 de abril de 2004 y se designó a los miembros de las Mesas de las cámaras. El socialista Manuel Marín fue elegido presidente del Congreso de los Diputados gracias a los votos del PSOE y del resto de partidos excepto el PP que votó en blanco. El socialista Javier Rojo fue elegido presidente del Senado.

Los días 16 y 17 de abril de 2004 se celebró la sesión de investidura del candidato a presidente del Gobierno, José Luis Rodríguez Zapatero. En la votación de investidura, Zapatero consiguió 183 votos a favor, 149 en contra y 19 abstenciones. Al obtener mayoría absoluta en la primera votación se declaró otorgada la confianza del Congreso de los Diputados al candidato. Seis fuerzas parlamentarias dieron su apoyo al candidato socialista, el mayor registro hasta la fecha. También fue esta la única votación de investidura en la que solamente un partido, en este caso el PP, votó en contra del candidato electo.

Actualmente, España se sitúa en el puesto 26 en desarrollo educativo, por debajo de muchos de los países de la UE, según el informe "Educación para todos" elaborado por la UNESCO. El 14 de mayo de 2004 se aprueba la mejora de becas para el curso 2004-2005 en 66 millones de euros. El Gobierno aprueba también la nueva Ley Orgánica de Educación [LOE], cuyo proyecto fue contestado en varias manifestaciones con participaciones multitudinarias. También en cuanto a educación, se ha observado un incremento de la alarma social acerca de las agresiones o situaciones violentas de acoso escolar en los centros de enseñanza. A este respecto, uno de los episodios más controvertidos se produjo a raíz del suicidio por acoso homófobo de Jokin, un estudiante de 4º de ESO del IES Talaia de Fuenterrabía el 21 de septiembre de 2004 ().
En cuanto a la religión, se ha observado un enfrentamiento entre el Gobierno y la jerarquía eclesiástica durante toda la legislatura. Como precedente, diferentes miembros del Gobierno se habían manifestado a favor de la supresión de la asignatura de Religión (católica) o la equiparación de otras religiones en la educación pública. A finales de 2004, la Conferencia Episcopal Española muestra su oposición con el gobierno en asuntos como los matrimonios entre personas del mismo sexo y la reforma educativa de la asignatura de religión. En noviembre la Conferencia Episcopal lanza una campaña, con siete millones de folletos, contra la eutanasia, y anuncia que seguirán otras sobre otros asuntos que considera de interés público. El enfrentamiento se ha mantenido en diversos frentes durante toda la legislatura, tanto el económico (en el que finalmente se llegó a un nuevo acuerdo de financiación con la Iglesia, a aplicar desde 2007) como el educativo, especialmente en lo referente a la asignatura de Educación para la Ciudadanía, o el de los derechos y libertades (especialmente en lo referente al matrimonio homosexual).

En clave socio-educativa, desde el Gobierno se ha propuesto una "recuperación de la memoria histórica", concretada en un principio en la devolución a Cataluña de los denominados "papeles de Salamanca" (documentación que permanecía en el Archivo General de la Guerra Civil) y al sindicato UGT del patrimonio incautado tras la Guerra Civil, o en la retirada de las estatuas ecuestres de Francisco Franco de las vías públicas. Finalmente y como colofón de ese proyecto se aprobó la llamada Ley de la memoria histórica (Ley 52/2007), que incluye un reconocimiento de la ilegitimidad de tribunales y jurados durante la Guerra Civil, medidas para la recuperación de los cadáveres sepultados en fosas comunes durante la guerra y la retirada de símbolos franquistas. Algunas de esas medidas han sido duramente criticadas por la oposición, principalmente por el Partido Popular, puesto que consideran a la nueva ley un regreso al "guerracivilismo" o una lectura parcial de la historia.

En el terreno deportivo, Madrid no resultó elegida como sede de los Juegos Olímpicos del 2012 a los que se había presentado como candidata. La alcaldesa en funciones de Madrid, Ana Botella, culpó de la eliminación al gobierno de José Luis Rodríguez Zapatero, ya que tras la eliminación de Nueva York en la segunda ronda, ninguno de sus votos fue a Madrid (Madrid fue eliminada en la tercera ronda por dos votos). Dichos votantes, afirma Botella, estarían molestos por las supuestas actuaciones "antiamericanas" del presidente del Gobierno (véase más abajo la sección de política exterior). El alcalde, Alberto Ruiz-Gallardón, desautorizó dicha interpretación. Meses más tarde, sin embargo, un representante del COI que participó en las votaciones reveló que Madrid resultó eliminada por un error humano de uno de los presentes.

El gobierno también prometió mayores subvenciones a la producción artística. Desde sectores próximos al Partido Popular se ha criticado que, entre otros, esto beneficie a los cineastas, muy significados en la oposición a la guerra de Iraq ("No a la guerra") durante la anterior legislatura.

También en clave cultural, el 8 de mayo se inauguró el Forum de las Culturas Barcelona 2004, clausurado el 26 de septiembre con unos resultados bastante discretos en cuanto a visitantes (2.6 millones) frente a las sobredimensionadas expectativas de la propia alcaldía de Barcelona (que llegaron en algún momento a los 7 millones.

En los primeros días tras las elecciones generales, Rodríguez Zapatero prometió una gestión independiente de Radio Televisión Española, y aunque había prometido que el director general sería nombrado por el Congreso de los Diputados, el 23 de abril el Consejo de Ministros nombró a Carmen Caffarel directora del ente público. Álex Grijelmo fue elegido como presidente de la Agencia EFE. 

Se creó un "comité de sabios" para estudiar la dramática situación económica del ente público RTVE, nunca solventada por gobierno alguno, y que concluye que su enorme deuda debe ser asumida por el Estado. El 24 de junio se comunica que RTVE pasará a ser una sociedad anónima de capital público con lo que, entre otras cosas, tendrá que tener un mayor control sobre sus gastos. Se estudian también los contenidos de la televisión con el fin de proteger a la infancia de programas inapropiados. 

Se procedió al relevo de presentadores como Alfredo Urdaci (único caso conocido en España de periodista condenado judicialmente por manipulación informativa), Carlos Dávila, Fernando Sánchez Dragó, Jenaro Castro o Baltasar Magro. Regresó el humorista El Gran Wyoming, que había combatido activamente contra el PP, y cuyo programa fue rápidamente cancelado debido a la baja audiencia que tuvo. También se incorporó Julia Otero, que presenta el programa "Las cerezas". Más éxito de crítica y público tuvieron nuevos programas de debate (59 segundos) y de divulgación sexual, "Dos rombos". La audiencia, subió por encima de Antena 3 y Tele 5.

TVE dedicó una amplia cobertura a la muerte de Juan Pablo II, lo que generó críticas en Izquierda Unida por considerarla impropia de un "estado aconfesional". Por su parte, el PP exigió el 15 de junio el cese de Lorenzo Milá, presentador de la segunda edición del "Telediario" de TVE 1, por sus declaraciones, no en el Telediario, en contra de las manifestaciones que el PP apoya. Tras el tratamiento informativo de las elecciones gallegas en Informe Semanal, el PP anunciaba su intención de no enviar a sus representantes políticos a "59 segundos"

Desde el Gobierno se sugirió una moratoria para la Televisión Digital Terrestre y nuevas concesiones para la emisión analógica, lo que provocó la oposición del PP y de todos los medios privados (Paolo Vasile, consejero de Telecinco y amigo de Berlusconi: "Esto es un golpe) exceptuando al Grupo Prisa. Se concedieron dos nuevas licencias para la emisión en abierto en analógico: canal Cuatro y la Sexta. Según el PP nos estaríamos alejando del pluralismo ideológico.

La COPE es uno de los medios audiovisuales críticos con los Gobiernos de España y Cataluña. Por su parte, desde el Gobierno y ERC aumentaron las críticas y acciones contra la cadena crítica argumentando que desde esa emisora se propagaría el odio entre los españoles. Tanto es así que en Cataluña tras la creación del Consejo Audiovisual de Cataluña criticada no solo por la COPE sino también por el AERC, Uteca, Telecinco, PP, CGPJ, FAPE o APM, se abrió expediente a dicha cadena. Esta actitud principalmente del ministro José Montilla así como los reajustes en la asignación de frecuencias radiofónicas en Cataluña llevaron a Luis Herrero y otros periodistas españoles a presentar ante la Unión Europea un manifiesto firmado por más de setecientas mil personas para que la institución comunitaria tomara cartas en lo que consideran una operación política "destinada a desposeer a la segunda cadena de radio más importante de España de todas sus emisoras en Cataluña". Su amplio enfrentamiento es tratado de un modo más completo en el artículo de la cadena COPE.

El Consejo General del Poder Judicial (CGPJ) y otros órganos de gobierno judicial, dominados por conservadores, nombrados durante los ocho años de gobierno del PP, venían tratando de paralizar o de frenar algunas de las reformas que el nuevo gobierno trataba de efectuar. Debido a que en el 2005 se iban a producir nuevos nombramientos de juristas y magistrados por parte del CGPJ el gobierno decidió actuar antes planteando la Reforma de la Ley Orgánica del Poder Judicial aprobada con el consenso de todos los partidos menos el PP el 1 de diciembre del 2004, que preveía la necesidad de mayores mayorías para proceder al nombramiento de jueces y magistrados. Según el gobierno, esta nueva ley trata de evitar que uno de los dos sectores (conservador o progresista) domine sobre el otro aunque goce de mayoría absoluta por lo que tal medida, según ellos, debería fomentar el consenso y los acuerdos entre sus miembros. Hasta entonces la mayoría de los nombramientos se efectuaban por mayoría absoluta excepto los del Tribunal Constitucional que se ordenaban con una mayoría de tres quintos. Con la nueva ley no solo estos últimos serán elegidos por los tres quintos sino que también lo serán los Presidentes de Sala y Magistrados del Tribunal Supremo así como los Presidentes de los Tribunales Superiores de Justicia. La medida fue duramente criticada por el PP aduciendo que pretendían hacerse con el control de la Justicia, a lo que el Gobierno replicó que pretendían acabar con el dominio conservador que según ellos se habría promovido durante la etapa del PP.

Aunque previamente el denominado Plan Ibarretxe ya inició el debate sobre el modelo territorial del Estado, es con el inicio del proceso de reforma del Estatuto de Autonomía de Cataluña en su Parlamento cuando se intensifica el debate. La Comunidad Valenciana fue la primera en presentar una reforma de su estatuto de autonomía al Congreso de los Diputados. En Baleares, Andalucía y Galicia los principales partidos políticos han iniciado conversaciones para reformar también sus respectivos estatutos de autonomía.

No obstante, en el seno del PP y del PSOE se muestran públicamente sus discrepancias internas con las reformas de los estatutos en los que participan sus partidos. Así, por ejemplo, sobre la propuesta de reforma del estatuto de Autonomía de la Comunidad Valenciana. 

El 2 de noviembre de 2005 el Estatuto Catalán es admitido a trámite en el Parlamento Español tras más de doce horas de debate con el único rechazo del Partido Popular, quien ese mismo día presentó el recurso de inconstitucionalidad en el Tribunal Constitucional. El texto, defendido por el tripartito catalán (PSC, ERC y ICV) y el gobierno de Zapatero, se sometió a la Comisión Constitucional del Congreso y fue modificado. Finalmente se aprobó con los votos del 54% del congreso y el 48% del senado, con el voto contrario de ERC en el congreso y la abstención en el senado. La oposición al estatuto por parte del PP se basa en su opinión de que es lesivo para los intereses de los catalanes y el resto de españoles, insolidario, rupturista e inconstitucional. Para ERC el estatuto es insuficiente, aunque inicialmente se inclinaron por el voto nulo, haciendo caso de sus bases solicitan a los catalanes el voto negativo en el referéndum. Afirman que las modificaciones introducidas tras el pacto Mas-Zapatero han dejado el texto totalmente "descafeinado" y que se pierde una oportunidad de conseguir algo mucho mejor para Cataluña.

En enero de 2006 el Consejo de Estado, a petición del gobierno, elaboró un informe respecto del Estatuto aprobado por el Parlamento catalán. Las conclusiones fueron terminantemente contrarias al mismo.

El 25 de abril de 2006, el Presidente del Partido Popular, Mariano Rajoy presentó en el Congreso de los Diputados más de 4 millones de firmas para que el gobierno someta a referéndum la pregunta: "¿Considera conveniente que España siga siendo una única Nación en la que todos sus ciudadanos sean iguales en derechos, obligaciones, así como en el acceso a las prestaciones públicas?" por la definición de Nación del Estatuto Catalán y por considerar que establece "distinciones entre ciudadanos".

El Pleno del Congreso aprobó el 24 de mayo la toma en consideración del proyecto de reforma del estatuto de Andalucía con el 57% de los votos y la única oposición del Partido Popular.

El 18 de junio de 2006 el Estatuto Catalán fue definitivamente aprobado en referéndum con el 73,9% de los votos, frente a un 20,7% de votos negativos, con una participación del 49,42%.





</doc>
<doc id="37801" url="https://es.wikipedia.org/wiki?curid=37801" title="Estatua de la Libertad">
Estatua de la Libertad

La libertad iluminando el mundo (en inglés: "Liberty Enlightening the World"; en francés: "La Liberté éclairant le monde"), conocida como la Estatua de la Libertad, es uno de los monumentos más famosos de Nueva York, de los Estados Unidos y de todo el mundo. Se encuentra en la isla de la Libertad al sur de la isla de Manhattan, junto a la desembocadura del río Hudson y cerca de la isla Ellis. La Estatua de la Libertad fue un regalo del pueblo francés al pueblo estadounidense en 1886 para conmemorar el centenario de la Declaración de Independencia de los Estados Unidos y como un signo de amistad entre las dos naciones. Fue inaugurada el 28 de octubre de 1886 en presencia del presidente estadounidense de la época, Grover Cleveland. La estatua es obra del escultor francés Frédéric Auguste Bartholdi y la estructura interna fue diseñada por el ingeniero Alexandre Gustave Eiffel. El arquitecto francés Eugène Viollet-le-Duc se encargó de la elección de los cobres utilizados para la construcción de la estatua. El 15 de octubre de 1924, la estatua fue declarada como monumento nacional de los Estados Unidos y el 15 de octubre de 1965 se añadió la isla Ellis. Desde 1984 es considerada Patrimonio de la Humanidad por la Unesco.

La Estatua de la Libertad, además de ser un monumento importante en la ciudad de Nueva York, se convirtió en un símbolo en Estados Unidos y representa, en un plano más general, la libertad y emancipación con respecto a la opresión. Desde su inauguración en 1886, la estatua fue la primera visión que tenían los inmigrantes europeos al llegar a Estados Unidos tras su travesía por el océano Atlántico. En términos arquitectónicos, la estatua recuerda al famoso Coloso de Rodas, una de las siete maravillas del mundo antiguo. Fue nominada para las nuevas siete maravillas del mundo, donde resultó finalista. El nombre asignado por la Unesco es Monumento Nacional Estatua de la Libertad. Desde el 10 de junio de 1933 se encarga de su administración el Servicio de Parques Nacionales de los Estados Unidos.

El jurista y político francés, autor de "Paris en Amérique", Eduardo Laboulaye, tuvo la idea de que Francia ofreciera un regalo a Estados Unidos como un obsequio para la conmemoración del centenario de la independencia estadounidense, como recuerdo de la larga amistad entre ambos países y para garantizar la alianza franco-estadounidense. En una conversación con Laboulaye, su amigo el joven escultor alsaciano Frédéric Auguste Bartholdi le había dicho:
En esa época, Estados Unidos acababa de salir de la guerra civil que duró de 1861 a 1865 y el país estaba en medio de la reconstrucción. Bartholdi fue contratado para diseñar una estatua, que debería acabarse en 1876, fecha del centenario de la independencia estadounidense. En 1870, Bartholdi talló el primer esbozo en terracota y un modelo que no sirvió, que actualmente se encuentra en el Museo de Bellas Artes de Lyon. Ese mismo año, Francia entró en guerra con Prusia y tuvo que parar el proyecto. El 10 de mayo de 1871, Francia tuvo que ceder el territorio de Alsacia-Lorena al Imperio Alemán. La opinión pública y el gobierno francés se sintieron decepcionados por la simpatía de los Estados Unidos hacia los alemanes, que contaban con un número importante de residentes en suelo estadounidense. El proyecto volvió a ser parcialmente paralizado a causa de los problemas políticos de la Tercera República, que todavía era considerada por muchos como un arreglo "temporal" y que tenían la esperanza de un retorno de la monarquía. La idea de ofrecer una representación de libertad en una república hermana para Francia, al otro lado del Atlántico, desempeñó un papel importante en la lucha por el mantenimiento de la república francesa.

En junio de 1871, Bartholdi viajó a Estados Unidos. Durante el viaje, escogió la isla de Bedloe, (llamada posteriormente la isla de la Libertad) como ubicación de la estatua y también trató de conseguir seguidores al otro lado del Atlántico. El 18 de julio de 1871, se reunió con el presidente de aquel entonces Ulysses S. Grant, en Nueva York.

Existen diversas hipótesis de los historiadores sobre el modelo que pudo haberse utilizado para determinar la cara de la estatua, aunque ninguna de ellas es realmente definitiva hasta el momento. Definitivamente existe una influencia del arte clásico griego, pero en escala colosal. Los recientes descubrimientos arqueológicos han determinado comparaciones casi exactas entre la antigua diosa griega Hécate y la figura de la estatua, como muy certera inspiración, a partir de las simbologías utilizadas, que son entre otras, la corona de rayos y la antorcha. 

Para el rostro se utilizó un modelo de Isabella Eugenie Boyer, viuda del inventor millonario Isaac Singer, aunque según otras fuentes, Bartholdi se habría inspirado en el rostro de su madre, Charlotte Bartholdi (1801-1891), y es la hipótesis más considerada hasta el presente. La revista "National Geographic" apoyó esta posibilidad, indicando que el escultor nunca explicó ni desmintió esta semejanza con su madre. Otras versiones sostienen que Bartholdi habría querido reproducir el rostro de una niña encaramada en una barricada sosteniendo una antorcha, al día siguiente del golpe de estado de Napoleón III. Tal vez simplemente realizó una síntesis de varias caras femeninas, con el fin de dar una imagen neutra e impersonal de la Libertad, inspirada en el concepto realista del arte helénico antiguo.
Durante una visita a Egipto, Bartholdi tuvo que hacer un trabajo en el canal de Suez. Este proyecto se inició bajo la dirección del empresario y diplomático francés Ferdinand de Lesseps, que posteriormente se convirtió en uno de sus mejores amigos. Bartholdi imaginó un gran faro, que se encontraría en la entrada del canal, el cual señalaría las rutas. El faro fue ideado como la imagen con apariencia clásica (estola, sandalias, expresión facial) de la diosa Libertas de la mitología romana, divinidad de la libertad. Se pretendía que la luz del faro brillara a través de una venda colocada alrededor de la parte superior del faro, y surgió la idea de una antorcha mantenida en el aire, hacia el cielo. Bartholdi presentó el proyecto al "jedive" Ismail Pachá en 1867 y nuevamente en 1869, pero el proyecto nunca fue aprobado. Los dibujos del proyecto titulado "Egipto lleva la luz a Asia", se asemejan en gran medida a la Estatua de la Libertad, aunque Bartholdi aseguró que el monumento de Nueva York no era una reutilización, sino una obra original.

En cuanto a la coronación de la cabeza, Bartholdi optó por una diadema de rayos solares, en lugar del característico gorro frigio con el que siempre se había ataviado a la diosa "Libertas". La utilización de este símbolo en este tipo de representaciones tiene como precedentes dos obras del escultor español Ponciano Ponzano, situadas en Madrid —en el Congreso de los Diputados y en el Panteón de Hombres Ilustres— y realizadas en 1848 y 1855, respectivamente.

Por mutuo acuerdo entre Francia y Estados Unidos, este último llevaría a cabo la construcción de la base del monumento, mientras que Francia se encargaría de la construcción de la estatua y de su posterior ensamblaje una vez que las piezas se transportaran a suelo estadounidense. Sin embargo, surgieron problemas financieros a ambos lados del Atlántico.

En Francia, la campaña para la promoción de la estatua comenzó en otoño de 1875. Fue la fundación en 1874 de la llamada Unión Franco-Estadounidense, la que se hizo cargo de organizar la recaudación de fondos para la construcción del monumento. Todos los medios de la época se utilizaron para ese propósito: artículos en la prensa, espectáculos, banquetes, impuestos, loterías, etc. Varias ciudades francesas, el Consejo General, la cámara de comercio, el Gran Oriente de Francia y miles de particulares hicieron donaciones para la construcción de la estatua. Hubo un número total de 100 000 donantes. Antes de que finalizara el año 1875, los fondos ascendían a un total de 400 000 francos, pero el presupuesto se incrementó posteriormente hasta 1 000 000 de francos de la época. No fue hasta 1880 que se recogieron el total de los fondos en Francia. Mientras tanto, en Estados Unidos, se hicieron presentaciones teatrales, exposiciones de arte, subastas, así como combates de boxeo profesional para recaudar fondos para la construcción.
Mientras tanto, en Francia, Bartholdi buscó un ingeniero para que se encargara del diseño de la estructura interna de la estatua, en cobre. Gustave Eiffel fue contratado para llevar a cabo dicha labor, además de crear una torre interna que soportara la estatua y diseñar un esqueleto secundario interno que permitiera que la "piel" de cobre se mantuviera en posición vertical. Las piezas de cobre fueron construidas en los talleres de la empresa «Gaget, Gauthier et Cie», en 1878. Las planchas de cobre fueron una donación de Pierre-Eugene Secrétan. Los trabajos de precisión se encargaron al ingeniero Maurice Koechlin, hombre de confianza de Eiffel, con el que también había trabajado en la construcción de la Torre Eiffel.

Bartholdi tenía la esperanza de que la estatua estaría completada y montada para el 4 de julio de 1876, fecha del centenario de la independencia de Estados Unidos. Hubo una demora en el inicio de la construcción y, a continuación, algunos problemas durante el período de construcción retrasaron la obra: el yeso de la mano se rompió en marzo de 1876. Esta última, con parte del brazo, se expuso en septiembre de 1876 en la Exposición del Centenario de Filadelfia. Los visitantes pudieron subir una escalera que conducía al balcón situado en torno a la antorcha pagando tan solo 50 centavos. Fotografías, carteles y modelos de la estatua fueron vendidos durante la exposición. El dinero recaudado se usó para completar los trabajos. Dos años más tarde, en junio de 1878, la cabeza de la estatua fue mostrada al público en los jardines del Campo de Marte con ocasión de la Exposición Universal de París, donde los visitantes podían entrar en la cabeza y subir hasta la corona usando una escalera de 43 metros.

El 18 de febrero de 1879, Bartholdi obtuvo en Estados Unidos la patente para el monumento, con el número D11.023.

Esta patente la describía en los siguientes términos:
La patente también especificaba que el rostro de la estatua tenía "rasgos clásicos, pero al mismo tiempo se encuentra seria y tranquila" …, y ligeramente inclinada hacia la izquierda para descansar sobre la pierna izquierda, con toda la figura que permanece en equilibrio.

La estatua se encuentra situada en la isla de la Libertad en el puerto de Nueva York. Originalmente a la isla se la conocía como isla de Bedloe, y sirvió como una base militar. En ella se alojó el fuerte Wood, un antiguo bastión de artillería construido en granito y cuyos cimientos en forma de estrella de once puntas, sirvieron de base para la construcción del zócalo de la estatua. La elección del terreno y su obtención requirieron de varios pasos. En 1887, el Congreso de los Estados Unidos dio su aprobación para la construcción de la estatua y el general W. T. Sherman fue nombrado para designar el terreno donde se construiría el monumento. Éste escogió como emplazamiento la isla de Bedloe. Quince años antes de la inauguración, Bartholdi ya había previsto la construcción del monumento en la isla de Bedloe, fascinado por la juventud y promesas de libertad de esa nación y la imaginó orientada hacia su continente de origen, la Europa que acogía e iba a continuar acogiendo a los inmigrantes. No fue sino hasta 1956, que el Congreso de Estados Unidos decidió un cambio de nombre de la isla Bedloe por el de Liberty Island o isla de la Libertad.

El embajador de los Estados Unidos en Francia, Levi P. Morton, colocó el primer remache de la construcción de la estatua en París el 24 de octubre de 1881.

La realización de la inmensa base de la estatua había sido confiada por Bartholdi a los estadounidenses, mientras que los franceses asumieron la construcción de la estatua y su correspondiente montaje.

La recaudación de fondos para llevar a cabo la construcción de la base en Estados Unidos, se encontraba bajo la responsabilidad del Fiscal General, William M. Evarts. Dado que la construcción avanzaba muy lentamente, Joseph Pulitzer (famoso por el premio que lleva su nombre) aceptó poner a disposición de los responsables de la construcción las primeras páginas del "New York World", y llevó a cabo una gran campaña de publicidad para recaudar fondos. El diario también fue utilizado para criticar a las clases altas, mostrando su incapacidad para recaudar los fondos necesarios, así como las clases medias, que contaban con los más ricos para hacerlo. Las duras críticas del periódico tuvieron un impacto positivo, fomentando a los donantes privados a incrementar sus aportaciones y proporcionando al mismo tiempo publicidad al periódico, ya que se registraron unos 50 000 nuevos abonados durante este período.

Los fondos necesarios para la construcción del sótano diseñado por el arquitecto estadounidense Richard Morris Hunt y realizado por el ingeniero Charles Pomeroy Stone, se reunieron en agosto de 1884. La primera piedra del pedestal fue colocada el 5 de agosto de 1884, mientras que la base, en su mayoría compuesta por piedra de Kersanton, fue construida entre el 9 de octubre de 1883 y el 22 de agosto de 1886.

Cuando se colocó la última piedra del monumento, los albañiles cogieron varias monedas de sus bolsillos, y las echaron en el mortero. Los participantes en la ceremonia dejaron sus tarjetas de visita, medallas y periódicos en un pequeño cofre de bronce, y lo depositaron en el zócalo.

En el corazón del bloque que compone la base, dos series de vigas la unen directamente con la estructura interna diseñada por Gustave Eiffel de manera que la estatua forme un todo con su pedestal. La piedra que compone la base de la Estatua de la Libertad proviene de las canteras de una aldea de Francia, Euville en el departamento de Mosa, famosa por la blancura de su piedra y por sus cualidades de resistencia a la erosión y al agua de mar.

Las distintas partes de la estatua fueron terminadas en Francia en julio de 1884. La estatua recibió hasta entonces múltiples visitas, como la del presidente de la república francesa Jules Grévy y el escritor Victor Hugo. El desmontaje comenzó en enero de 1885.

La estatua fue enviada a Ruan en tren, luego bajó el Sena en barco, antes de llegar al puerto de El Havre. El monumento llegó a Nueva York el 17 de junio de 1886, a bordo de la fragata francesa "Isère", y recibió una acogida triunfal por parte de los neoyorquinos. Para hacer posible la travesía por el Atlántico, la estatua fue desmantelada en 350 piezas, divididas en 214 cajas, teniendo en cuenta que el brazo derecho y su llama estaban ya presentes en suelo estadounidense, donde habían sido expuestos en la Exposición del Centenario de Filadelfia y luego en Nueva York. 36 cajas fueron reservadas para las tuercas, los remaches y los pernos necesarios para la ensambladura. Una vez llegada a su destino, la estatua fue ensamblada en cuatro meses, sobre su nuevo pedestal. Las diferentes piezas fueron unidas por remaches de cobre y el vestido permitió resolver los problemas de dilatación.

El 28 de octubre de 1886, la Estatua de la Libertad fue inaugurada en presencia del presidente estadounidense de la época, Grover Cleveland, antiguo gobernador del estado de Nueva York, delante de 600 invitados y millares de espectadores. Frédéric Desmons, por entonces vicepresidente del Senado, representó a Francia durante la inauguración. Ferdinand de Lesseps y numerosos francmasones estaban también presentes. El monumento representaba así un regalo que celebraba el centenario de la independencia estadounidense, aunque entregado con diez años de retraso. El éxito del monumento creció rápidamente: en las dos semanas que siguieron a la inauguración, cerca de 20 000 personas se habían presentado para admirarlo. La frecuentación del sitio pasó de 88 000 visitantes al año, a un millón en 1964 y a tres millones en 1987. Actualmente recibe en torno a 3,5 millones de personas al año.

La estatua funcionó como faro entre la fecha de su montaje y 1902. En aquella época, la US Lighthouse Board estaba encargada de asegurar su funcionamiento. Se había asignado un guardián del faro a la estatua y el poder de su haz luminoso era tal, que era visible a una distancia de 39 kilómetros. Se instaló un generador eléctrico en la isla con el fin de suministrar potencia a la estructura.

Desde su inauguración en 1886, el monumento ha experimentado múltiples renovaciones y reformas:

El sistema de iluminación original ha sido sustituido varias veces por un equipamiento más moderno. En 1916 el presidente Woodrow Wilson inauguró el primer sistema que cumplía con las expectativas iniciales de iluminación de la estatua, consistente en doscientos cuarenta y seis proyectores, utilizando lámparas incandescentes de 250 W, situados en las puntas de la estrella de la base del monumento y otros puntos de la isla, y quince de 500 W en la antorcha. Posteriormente, en 1931 y 1945 se intensificó la iluminación anterior, se añadieron efectos más vistosos y se eliminaron sombras.

Junto con el proyecto de mejora de la iluminación de 1916, la antorcha, que era inicialmente de cobre, fue reformada utilizando un total de 600 piezas individuales de vitral de color amarillo para realzar y embellecer los efectos luminosos al encenderse. Este trabajo fue ejecutado por Gutzon Borglum, conocido por sus colosales esculturas de monte Rushmore. También se aumentó la potencia luminosa de la antorcha. El 30 de julio de ese mismo año, debido a un acto de sabotaje conocido como explosión Black Tom, el acceso a la antorcha fue oficialmente clausurado.

Aunque cuando se inició la construcción de la base ya se había previsto la instalación de un ascensor en su interior, el primero no fue instalado hasta 1908-9. Con posterioridad se sustituyó por uno más moderno en 1931.

En 1949 se instaló un sistema de calefacción en la base de la estatua. Antes de esta mejora, durante los meses de invierno, la enorme masa de la base (unas 48 000 toneladas) se hizo cada vez más fría, y cuando en marzo el aire exterior se hacía más caliente se producía una condensación que impregnaba las paredes y que perjudicaba la estructura y sus instalaciones. Por otra parte, la calefacción, además de solucionar los problemas de enfriamiento y condensación en la base, añadía un mayor confort a los empleados y visitantes.

En 1937 se sustituyeron varias plataformas y escaleras en el pedestal de la estatua. Se procedió a una inspección de la estructura y las planchas de cobre de la estatua, desde la antorcha hasta las vigas sobre las que se apoya la estructura. El soporte de hierro fue sustituido en las secciones donde se había oxidado, y los remaches que se habían soltado se cambiaron por unos nuevos y los marcos de la corona de la cabeza fueron reconstruidos con unos nuevos de hierro. No se efectuaron reformas en la escalera en espiral del interior de la estatua.

La Estatua de la Libertad fue uno de los primeros monumentos que se beneficiaron de lo que en Estados Unidos se conoce como una campaña de "cause marketing". En efecto, en 1983, el monumento fue el centro de una operación de promoción llevada a cabo por American Express, que pretendía recaudar fondos para mantener y renovar el edificio. Se acordó que cada compra realizada con una tarjeta "American Express" conllevaría una donación de un centavo de dólar por parte de la empresa bancaria. La campaña permitió así reunir 1.7 millones de dólares. En 1984, la estatua fue cerrada con el fin de realizar trabajos, por un importe de 62 millones de dólares, llevados a cabo con ocasión de su centenario. El presidente de Chrysler, Lee Iacocca, fue nombrado por el presidente Ronald Reagan para encabezar la comisión encargada de la supervisión de las obras (aunque fue destituido posteriormente para «evitar cualquier duda sobre un posible conflicto de intereses»).

Los obreros encargados de los trabajos levantaron un andamiaje alrededor del edificio, ocultando a la vista el monumento hasta la ceremonia del centenario, el 4 de julio de 1986. Los trabajos en el interior de la estructura comenzaron con el empleo de nitrógeno líquido con el fin de quitar las diferentes capas de pintura aplicadas al armazón de cobre durante varias décadas. Una vez eliminadas estas capas de pintura, no quedó más que la base original de alquitrán que servía para tapar agujeros y evitar la corrosión. El alquitrán fue a su vez eliminado utilizando bicarbonato sódico, sin que la estructura de cobre sufriera ningún daño. Los agujeros más grandes en el cobre fueron pulidos, antes de ser recubiertos por nuevas plaquetas.
Cada una de las 1350 costillas de hierro que sostenían la «piel» debió ser retirada y luego reemplazada. El hierro había sufrido una fuerte corrosión galvánica en todas aquellas partes donde estaba en contacto con la piel de cobre, perdiendo hasta la mitad de su grosor. Bartholdi había anticipado este fenómeno y previsto una combinación de amianto y brea para separar ambos metales, pero el aislamiento se había deteriorado décadas antes. Las barras de hierro fueron reemplazadas por nuevas barras modeladas de acero inoxidable, con una película de teflón que las separan del cobre para obtener un mejor aislamiento y reducción de la fricción.

La estructura interna del brazo derecho (el que se mantiene erguido sosteniendo la antorcha) fue reconstruida. En el momento de la construcción de la estatua, el brazo había sido desplazado 46 centímetros a la derecha y hacia adelante con relación a la estructura central de Eiffel, mientras que la cabeza había sido desplazada 0,61 cm a la izquierda, lo que había estado poniendo en peligro el armazón. Se cree que Bartholdi habría tomado esta decisión sin consentimiento de Eiffel después de ver que el brazo y la cara estaban demasiado próximos. Los ingenieros consideraron los trabajos de refuerzo de 1937 como insuficientes, y añadieron una sujeción diagonal en 1984 y 1986 para hacer el brazo estructuralmente más sólido.

La antorcha que sostiene en su mano la estatua no es la que enarbolaba en el momento de su inauguración en 1886. Durante estas obras de restauración fue reemplazada por una nueva antorcha recubierta con láminas de oro, que es iluminada por lámparas colocadas sobre el balcón que la rodea. En 1985, para renovar la antorcha de la estatua, los Estados Unidos acudieron a una empresa de Bezannes, cerca de Reims, donde trabajan artesanos expertos en artesanía del hierro de obras de arte. Un equipo de Reims reemplazó la vieja antorcha, corroída por el óxido, por una nueva. La antigua antorcha se expone actualmente en el museo situado en el vestíbulo del monumento.

Además de la sustitución de la mayor parte del hierro del armazón interno con acero inoxidable y el refuerzo estructural de la propia estatua, la restauración de mediados de los años 1980 también incluyó la renovación de la escalera de hierro interna, la sustitución del ascensor situado en el interior de la base por uno más moderno y la mejora de los sistemas de control de ventilación.

La Estatua de Libertad se reabrió al público el 5 de julio de 1986, durante la celebración del "Liberty Weekend".

La Estatua de la Libertad fue inaugurada el 28 de octubre de 1886, declarada Monumento Nacional estadounidense el 15 de octubre de 1924, y confiada su administración al Servicio de Parques Nacionales el 10 de junio de 1933. En 1986, la celebración del centenario de la Estatua de la Libertad consistió en cuatro días de festividades que se denominaron Liberty Weekend. Las celebraciones comenzaron el 3 de julio con una ceremonia de apertura en Governors Island y finalizaron el 6 de julio en el Giants Stadium de Nueva York. Estos cuatro días de fiestas marcaron el fin de las obras de restauración del monumento llevadas a cabo desde principios de los años 1980, bajo la tutela de la fundación Statue of Liberty-Ellis Island. Estas obras, en las cuales las aportaciones de la empresa Chrysler tuvieron un papel predominante, terminaron justo a tiempo para la ceremonia del centenario del monumento, donde se rindió homenaje a la estatua a lo largo del Liberty Weekend.

La ceremonia inaugural, celebrada el jueves 3 de julio, tuvo como invitado de honor al por entonces presidente de la república francesa François Mitterrand y atrajo a numerosos personajes del espectáculo, como Gregory Peck o Elizabeth Taylor. El presidente estadounidense Ronald Reagan pronunció un discurso donde destacó la amistad entre Francia y los Estados Unidos e hizo una mención hacia los trabajadores que llevaron a cabo los trabajos de restauración. A continuación descubrió de nuevo la estatua (tapada desde el inicio de las obras), y pronunció un segundo discurso en el momento de encender la antorcha de la estatua, tras el despliegue de unos fuegos artificiales. Después se llevaron a cabo varias actuaciones a cargo de Neil Diamond, Frank Sinatra y Mikhail Baryshnikov, entre otros. Por último se entregó la Medalla de la Libertad a destacadas personalidades que, aunque no nacieron en el país, adquirieron la ciudadanía estadounidense: Henry Kissinger, Ieoh Ming Pei, Irving Berlin, Hanna Holborn Gray, Kenneth Bancroft Clark, Elie Wiesel, Albert Sabin, James Reston, An Wang, Itzhak Perlman, Franklin Chang-Díaz y Bob Hope.

El 4 de julio, día de la fiesta nacional estadounidense, durante la mañana se celebró, con el presidente Reagan en la cubierta del acorazado USS Iowa, una revista naval de barcos de guerra y grandes veleros en el río Hudson, compuesta por 33 buques de 14 naciones. Por la noche se celebró un concierto con la participación del compositor John Williams. A la mañana siguiente, la esposa del presidente, Nancy Reagan pronunció un discurso que marcaba la reapertura oficial de la estatua al público, y por la tarde, se celebró una ópera en Central Park. El 6 de julio, la ceremonia de clausura se celebró en el Giants Stadium situado en Nueva Jersey (que, geográficamente, se encuentra más próximo a la estatua).

Inicialmente era posible visitar el interior de la estatua. Los visitantes llegaban por transbordador a la isla de la Libertad, generalmente desde Battery Park, y tenían la oportunidad de ascender a través de la única escalera de caracol en el interior de la estructura metálica. Cuando la estatua estaba expuesta al sol, era frecuente que la temperatura dentro del monumento fuera muy elevada. Unas treinta personas podían ascender los 354 escalones que conducen a la cabeza de la estatua y a su corona. Desde allí era posible apreciar unas vistas del puerto de Nueva York, aunque no el panorama urbano de Manhattan, contrariamente a la creencia popular. Esto se explica por el hecho de que la cara de la estatua está orientada hacia el océano Atlántico y Europa, no hacia el Oeste. Además, la vista fue restringida parcialmente dado que las 25 ventanas de la corona son más bien pequeñas y la mayor de ellas sólo tiene 46 centímetros de altura. No obstante, esto no desanimaba a los turistas, que debían esperar tres horas por término medio para acceder al recinto de la estatua, sin contar la espera en el transbordador y la ventanilla de venta de billetes.

Después de los atentados del 11 de septiembre de 2001, se prohibió el acceso a la isla de la Libertad hasta diciembre del mismo año y el acceso del público al monumento no se permitió hasta el 3 de agosto de 2004. El acceso a la corona estuvo cerrado hasta 2009. Durante ocho años, solo la base de la estatua y los diez primeros pisos estaban abiertos a los visitantes, a condición de que éstos estuvieran en posesión del pase de acceso al monumento. Generalmente es posible obtenerlo tras una reserva con, al menos, dos días de antelación antes de la visita, y mostrarlo antes de acceder al transbordador. Sin embargo, aunque el interior de la estatua sea inaccesible, una nueva plataforma con techo de vidrio permite ver, alzando la vista, la estructura interna realizada por Gustave Eiffel. Todos los visitantes que deseen acceder a la isla de la Libertad son controlados antes de subir al transbordador y nuevamente al acceder a la base del monumento, de forma similar que en los aeropuertos.

El 4 de agosto de 2006, Fran P. Mainella, directora del Servicio de Parques Nacionales, en una carta al congresista por Nueva York Anthony D. Weiner, declaró que la corona y el interior de la estatua permanecerían cerrados indefinidamente. La carta indicaba que «la actual reglamentación de los accesos refleja una estrategia de dirección responsable con los mejores intereses de todos nuestros visitantes». Sin embargo, en los últimos años se están produciendo múltiples iniciativas para reabrir la estatua al público y permitir el acceso a la corona. Así, en el mismo año 2006, un proyecto de ley (S. 3597) se tramitó en el Senado proponiendo su acceso al público, en julio de 2007 se propuso una medida similar en la Cámara de Representantes, y en julio de 2008 surgieron noticias sobre estudios del Servicio Nacional de Parques para reformar los accesos a la estatua que permitan la entrada al público. El 23 de enero de 2009, Ken Salazar, Secretario del Interior bajo la presidencia de Barack Obama, declaró que estaba considerando la reapertura del acceso a la corona de la Estatua a los turistas, y el 8 de mayo Salazar anunció en una entrevista en un programa televisivo que la corona se volvería a abrir al público el 4 de julio de 2009. Finalmente el interior de la estatua y el acceso a la corona se reabrieron al público el día anunciado, aunque por motivos de seguridad el acceso quedó restringido a un máximo de 240 turistas al día en grupos de un máximo de 10 personas cada vez.

El acceso al público volvió a cerrarse por completo el 9 de agosto de 2010 para acometer unas obras de instalación de nuevas escaleras a prueba de incendios, ascensores y salidas de emergencia, con un presupuesto de 26 millones de dólares. Esta intervención, que ya estaba planificada, se aceleró tras una falsa alarma de incendio en julio, que puso de manifiesto la insuficiencia de la escalera de caracol como única salida de emergencia existente. La Estatua reabrió el 12 de octubre de 2011, pocos días antes del 125.º aniversario de su inauguración.

La estatua representa a una mujer en posición vertical, vestida con una especie de estola amplia y en su cabeza tiene una corona con siete picos, que simbolizan los siete continentes y los siete mares. Hay 25 ventanas en la corona que representan gemas encontradas sobre la tierra y los rayos del cielo que brillan sobre el mundo. La diadema recuerda a la que portaba Helios, personificación del Sol en la mitología griega. Bartholdi optó por la corona, y no se decidió por el gorro frigio, símbolo de libertad desde la Antigüedad. La estatua blande en su mano derecha una antorcha encendida, mantenida en alto. La antorcha nos remite al siglo de las luces, aunque algunos lo consideran un símbolo francmasón. En su mano izquierda sostiene una tablilla, que sujeta cerca de su cuerpo. La tablilla evoca la ley o el derecho, y tiene grabada la fecha de la firma de la Declaración de Independencia de los Estados Unidos, escrita en números romanos: JULY IV MDCCLXXVI.
La estructura está recubierta con una fina capa de cobre, que reposa sobre un gran armazón de acero inoxidable (que inicialmente era de hierro), a excepción de la llama que está recubierta con láminas de oro. La estructura reposa sobre una base de forma cuadrada, que a su vez se apoya sobre un primer zócalo en forma de estrella irregular de once puntas. La altura de la Estatua de la Libertad es de 46 metros, y alcanza los 93 metros desde el suelo hasta la antorcha. Al pie de la estructura se encuentran cadenas rotas que simbolizan la libertad. La estatua está orientada hacia el Este, es decir hacia Europa, con la que los Estados Unidos comparten pasado y valores.

La coloración verde de la estatua es a causa de reacciones químicas, que produjeron sales de cobre y le dieron su actual color. La mayor parte de las estatuas de cobre situadas en exteriores, salvo que se tomen medidas adicionales, acaban adquiriendo finalmente este tono tras un proceso llamado patinación.

En la base del monumento, una placa de bronce lleva grabada una parte (el final) del soneto de la poetisa estadounidense Emma Lazarus titulado "The New Colossus" ("El nuevo coloso"). La placa de bronce no estaba cuando se inauguró, sino que se añadió en 1903. A continuación se muestra la parte del poema que está inscrito en la placa, y su traducción al español:

Debido a la situación de la isla de la Libertad en la porción correspondiente a Nueva Jersey del río Hudson, la titularidad de la estatua no ha estado exenta de controversia.

En 1987 el representante demócrata por Nueva Jersey, Frank J. Guarini, y Gerald McCann, que fuera alcalde de Jersey City, interpusieron una demanda contra la ciudad de Nueva York, afirmando que Nueva Jersey debería ejercer dominio sobre la isla de Libertad dada su situación en la parte de Nueva Jersey del río de Hudson. La isla, bajo jurisdicción federal, se encuentra aproximadamente a 600 metros de distancia de la ciudad de Jersey y más de tres kilómetros de la ciudad de Nueva York. La Corte Suprema decidió no oír el caso, por lo que el estatus legal existente de las partes de la isla que están sobre el agua no se alteró. Sin embargo, los derechos ribereños sobre toda la tierra sumergida que rodea la estatua pertenecen a Nueva Jersey. Las islas del puerto de Nueva York han sido parte de la ciudad de Nueva York desde la emisión en 1664 de los atípicos estatutos coloniales que crearon Nueva Jersey, que no fijaron, como es habitual, un límite en medio del río Hudson —aunque la frontera para los derechos acuáticos fue fijada más tarde al medio del canal—.

El servicio federal de parques establece que la Estatua de Libertad está en la Isla de Libertad, que es una propiedad federal administrada por el Servicio de Parques Nacionales y que, oficialmente, la isla está localizada dentro de la jurisdicción territorial del estado de Nueva York debido a un pacto entre los gobiernos estatales de Nueva York y Nueva Jersey que emitió una resolución sobre esta cuestión y que fue ratificada por el Congreso en 1834.

Debido a su consideración de monumento universal, la Estatua de la Libertad ha sido copiada y reproducida a diferentes escalas y en diversos lugares a lo largo de todo el mundo. Estas copias van desde las simples miniaturas vendidas como "souvenir" en la tienda del museo situado en la base de la estatua, a las reproducciones a gran escala situadas en ciertas ciudades, bien porque forman parte de la historia del monumento o de alguno de sus creadores, o bien porque el original constituye un importante símbolo de la Libertad a través del mundo.

Las primeras miniaturas de la estatua, realizadas por la empresa Gaget, Gauthier & Cie (hecho popularmente asociado al nacimiento de la palabra "gadget") fueron comercializadas y distribuidas entre las numerosas personalidades presentes durante la ceremonia de inauguración del 28 de octubre de 1886. Estas primeras reproducciones sirvieron como modelo para diversas réplicas construidas posteriormente. La mayoría de ellas se encuentran hoy en Francia o en los Estados Unidos, sin embargo también las podemos ver en numerosos países, como Argentina, Austria, Alemania, Italia, Japón, China, o incluso Vietnam, antigua colonia francesa.

Entre las principales réplicas francesas del monumento, encontramos la de la Île aux Cygnes (isla de los Cisnes) en París, con una altura de 11,50 m, que se erige en el extremo situado río abajo de la isla (situada en el Sena), a la altura del "pont de Grenelle", cerca del antiguo taller de Frédéric Auguste Bartholdi. Hay también una réplica en Colmar, inaugurada en 2004 en la entrada norte de la ciudad, conmemorativa del centenario de la muerte de Bartholdi. Con sus tres toneladas de peso y doce metros de altura, la Miss Liberty de Colmar sobrepasa en cincuenta centímetros a su hermana mayor parisina de la isla de los Cisnes, hasta entonces la más grande de Francia, lo que la convierte también en la réplica auténtica más grande del mundo de la Estatua de la Libertad. Existe también en Barentin, en el Sena Marítimo, una copia en poliéster de 13,5 m y 3,5 t de peso, utilizada en la película de 1965 "Le Cerveau" ("El cerebro"), de Gérard Oury. Una réplica de la antorcha de la estatua, la Flamme de la Liberté, regalada por los Estados Unidos a París, está instalado en la place de l'Alma parisina.

En otras partes del mundo, las réplicas más famosas de Lady Liberty o la Dama de la Libertad son las del New York-New York Hotel & Casino en Las Vegas y la de la isla artificial de Odaiba en la bahía de Tokio. En Argentina, se encuentran al menos dos réplicas: una en la plaza Barrancas de Belgrano de la ciudad de Buenos Aires, realizada por Bartholdi en hierro rojo y adquirida por encargo de la Municipalidad de Buenos Aires a Francia e inaugurada días antes que su par de Nueva York, el 3 de octubre del mismo año, la segunda, realizada posteriormente a la muerte del autor, se halla en la ciudad de Pocito.

La Estatua de la Libertad se convirtió muy pronto en un icono popular, apareciendo en numerosos anuncios e imágenes y en películas y libros. En 1911, el escritor estadounidense O. Henry hacía dialogar a "Miss Liberty" con otra estatua. En 1918, el monumento figuraba sobre el anuncio de "Victory Loan" (préstamo de la victoria) concedido por los Estados Unidos a Europa. En los años 1940 y 1950, numerosas revistas "pulp" de ciencia ficción mostraban a la estatua rodeada de ruinas y restos de otras épocas. Durante la Guerra Fría, la estatua aparecía a menudo en los carteles de propaganda como símbolo de la libertad o de los Estados Unidos (lógicamente en un sentido u otro, dependiendo del bando en cuestión). Los dibujantes estadounidense la mostraron como la representación de Nueva York tras los atentados del 11 de septiembre de 2001. La publicidad también la utilizó para dar a conocer productos tales como Coca-Cola o una goma de mascar. La estatua también inspiró a pintores del , como a Andy Warhol.

En el cine, la estatua aparece en un gran número de películas, en algunas de ellas con un gran protagonismo. Ya en 1917, en "The Immigrant", Charlie Chaplin admira la estatua mientras su barco llega al puerto de Nueva York. En 1942 aparece en la película de Alfred Hitchcock "Sabotaje" durante el desenlace final. Al final de la primera versión de la película "El planeta de los simios", la muestra en un sorprendente final sepultada en parte bajo la arena de una playa. En la película "Cazafantasmas 2" logran que la Estatua de la Libertad «cobre vida», recorriendo las calles de Nueva York. En la filmografía más reciente también hace su aparición en películas como "X-Men" en la que la batalla final se desarrolla en la estatua, en "Titanic" en donde aparece en una de las escenas finales de la película cuando el "Carpathia", el barco con los supervivientes del hundimiento, los lleva a Nueva York, en "Inteligencia artificial" donde aparece sumergida, en "Hombres de negro II" donde el agente K "neuraliza" a todo Nueva York con su antorcha, en "The Day After Tomorrow" en la que aparece en varias escenas o en "National Treasure" donde la estatua hermana situada en París proporciona al cazador de tesoros Ben Gates (Nicholas Cage) una pista para descubrir el tesoro secreto. En la pequeña pantalla también aparece con regularidad, como en la serie de televisión "Fringe", donde la estatua es la sede del Departamento de Defensa de los Estados Unidos en un universo paralelo.
En 1979, la estatua formó parte de una novatada imaginada en la Universidad de Wisconsin-Madison. Varios estudiantes reprodujeron la parte superior de la estatua y las colocaron en un lago helado de la región, lo que daba la impresión de estar sumergida. El monumento figura en las placas de matrícula del estado de Nueva York y en las de Nueva Jersey.

En el mundo del deporte, la Dama de la Libertad sirve de logotipo para el equipo de la NHL de los New York Rangers y para el equipo de baloncesto femenino de New York Liberty, que compite en la WNBA. Para celebrar el centenario del monumento, el servicio postal francés creó en 1986 un sello que representa la cara de la estatua titulado «Libertad». En el año 2000, el monumento formó parte de las propuestas para designar las nuevas maravillas del mundo, donde resultó finalista. El logotipo de la Universidad de Nueva York recupera la antorcha de la Estatua de la Libertad para mostrar que está al servicio de la ciudad de Nueva York. La antorcha aparece a la vez sobre el sello y sobre el logotipo de la universidad, diseñado por Ivan Chermayeff en 1965. El famoso ilusionista David Copperfield hizo «desaparecer» el monumento en un programa en directo en la televisión en 1983, en uno de sus trucos más memorables.

La imagen de la estatua también ha sido trasladada al mundo de los videojuegos. El lanzamiento del polémico título de Rockstar Games "GTA IV" en 2008 incluía una fiel representación de Nueva York (Liberty City en la historia del juego) y de la Estatua de la Libertad, parodiada como «Estatua de la Felicidad».

Fuente: "Statue Statistics", National Park Service "(16/8/2006). Consultado el 18 de agosto de 2008."







</doc>
<doc id="37803" url="https://es.wikipedia.org/wiki?curid=37803" title="Yesterday">
Yesterday

«Yesterday» es una canción compuesta por Paul McCartney, cuya versión original fue grabada en 1965 para el álbum "Help!", de la agrupación británica "The Beatles". Dicha versión es una balada melancólica de dos minutos y tres segundos que supuso la primera composición oficial de The Beatles donde un solo integrante de la banda grababa un tema: Paul McCartney, que fue acompañado por un cuarteto de cuerdas. La composición se diferencia de los trabajos anteriores del grupo, por lo que el resto de los integrantes no aceptaron que se editara como sencillo en el Reino Unido.

Según el Libro Guinness de los récords es una de las canciones más versionadas en la historia de la música popular hasta 2017, La Broadcast Music, Inc. (BMI) afirma que en el siglo XX, fue interpretada cerca de siete millones de veces. Además, a pesar de que los datos de las discográficas y las editoriales musicales son dispares, el Guiness es tajante: ‘Yesterday’ es la canción más rentable de todos los tiempos ya que hasta 2015 había aportado a McCartney 490 millones de euros.

En 2002, McCartney preguntó a Yoko Ono (la viuda de Lennon) si podía invertir los créditos de la composición, de tal manera que se leyera "McCartney/Lennon". Ono se negó.

De acuerdo con los biógrafos de McCartney y The Beatles, McCartney compuso la melodía completa en un sueño que tuvo en la casa de su novia Jane Asher y su familia en Londres, en la calle Wimpole. Al despertarse, fue al piano, puso en funcionamiento una grabadora de cintas y la tocó, para no olvidarla junto con su sueño. Al principio, temió haber plagiado una composición existente:

Después de convencerse de que no había tomado la melodía de otra composición, McCartney empezó a hacerse cargo de la letra que la acompañaría. Tenía el título "Scrambled Eggs" (en español: "Huevos revueltos"), mientras encontraba la letra y un título más adecuado. En su biografía, "Many Years From Now", McCartney dijo:

Durante el rodaje de la película "Help", se colocó un piano en uno de los estudios en donde se filmaba. McCartney aprovechó para cantar "Scrambled Eggs" acompañándose con el piano. Richard Lester, el director de la película, se irritó mucho al oírla por lo que le dijo a McCartney que terminara de escribir la canción o se llevaría el piano. La paciencia de los otros Beatles también llegó al límite por el empeño que ponía McCartney para progresar en la canción, George Harrison dijo al respecto: "¡Caray, él siempre estaba hablando sobre esta canción. Se podría pensar que era Beethoven o alguien parecido!".

McCartney afirmó haber compuesto «Yesterday» durante los conciertos de The Beatles en Francia en 1964; sin embargo, la canción no fue lanzada hasta el verano de 1965. Durante ese lapso el grupo publicó dos discos, "A Hard Day's Night" y "Beatles for Sale", por lo que en cualquiera de ellos pudo haberse incluido la canción «Yesterday». Aunque McCartney no ha comentado nunca lo que pasó, es probable que la razón de este largo retraso pudiera haber sido un hipotético desacuerdo entre McCartney y George Martin respecto al arreglo de la canción, o pudo deberse al rechazo de los otros miembros del grupo.

Lennon comentó que el tema estuvo yendo y viniendo durante algún tiempo:

El 27 de mayo de 1965, McCartney y Asher viajaron a Lisboa para pasar unas vacaciones en Albufeira, Algarve, y fue ahí donde McCartney terminó de escribir la letra de la canción. Ambos se hospedaban en la casa de Bruce Welch, y McCartney tomó prestada una de sus guitarras acústicas, para así completar el trabajo de «Yesterday».

En 1980 Lennon explicó que a menudo se le daba erróneamente el mérito de haber escrito la canción.
McCartney sólo toca una guitarra sajona acompañado por un cuarteto de cuerdas. En uno de los primeros usos de The Beatles de los músicos de sesión, «Yesterday» cuenta con dos secciones contrastantes, que difieren en la melodía y el ritmo, produciendo una sensación de separación.

La primera sección (""Yesterday, all my troubles seemed so far away"...") abre con un acorde en Fa mayor, cambia a Mi menor, siguiendo con un La mayor y finalizar en Re menor. En este sentido, el acorde de apertura es un cimbel, como el musicólogo Alan Pollack señala, la tonalidad de inicio (Fa mayor) tiene poco tiempo para establecerse antes de que "tome dirección hacia Re menor". Señala que esta desviación es un recurso de composición utilizado por Lennon y McCartney, que él describe como "gratificación diferida".

La segunda sección (""Why she had to go I don't know"...") es, según Pollack, menos sorprendente de lo que parece en el papel del sonido. Comenzando con Mi menor, la progresión armónica se mueve rápido a través de La mayor, Re menor, y (más cercano a Fa mayor) Si bemol, antes de volver a Fa mayor, y al final de este, McCartney se mantiene en Fa mientras que las cuerdas descienden para restablecer la tonalidad introductoria de la primera sección, antes de una breve frase de cierre tarareada.

Pollack dijo que la orquestación era "realmente una inspiración", la mencionó como un ejemplo del "talento [de Lennon y McCartney] para crear híbridos estilísticos"; en particular, elogia "la irónica tensión entre el contenido sensiblero de lo que el cuarteto interpreta y la naturaleza austera y contenida del medio en el que se interpreta".

La clave tónica de la canción es Fa mayor (aunque, dado que McCartney afinó su guitarra durante la grabación, parece que las cuerdas están tocadas en Sol mayor), posteriormente la canción cambia en la tonalidad de Re menor. La progresión armónica en II-V7 (acordes La séptima en Mi mayor) conduce toda la canción, dándole un aura melancólico. El acorde La séptima es un ejemplo de un dominante secundario, específicamente en el acorde V/vi. El acorde Sol séptima en el puente es otro dominante secundario, en este caso un acorde en V/V, pero en lugar de resolverlo espera a la cuerda, como en el verso La séptima, en su lugar, McCartney sigue con el acorde IV, Si bemol mayor. Este movimiento crea una línea cromática descendente de Do, Si, Si bemol y La para acompañar al título de la canción.

Los arreglos de cuerdas en la canción dan una aire de tristeza, especialmente en la melodía del violonchelo gimiendo en nota de blues que conecta las dos mitades del puente (en la línea, "I don't know" / "she wouldn't say") así como la línea descendente de la viola que sigue al coro en los versos. Esta idea sencilla es sorprendente, McCartney imita esto con su voz en la segunda parte del coro. Esta línea de la viola y el violín en La en los versos finales son los únicos arreglos de cuerda que McCartney atribuye a George Martin.

La canción fue grabada en los estudios de grabación de EMI (después de la grabación de "I'm Down") el 14 de junio de 1965. Hay versiones contradictorias sobre cómo fue esa grabación; la más citada es que McCartney grabó la canción sin la participación de los otros miembros del grupo. 

Otras fuentes afirman que en un principio McCartney y los Beatles probaron una gran variedad de instrumentos, incluyendo la batería y un órgano, y que más tarde George Martin los convenció para que McCartney solo tocara una guitarra sajona, e incluir un cuarteto de cuerdas para acompañar a McCartney. De todos modos, ninguno de los otros miembros fue incluido en la grabación final. Sin embargo, la canción fue tocada con los demás miembros de la banda en sus conciertos de 1966.

McCartney hizo dos tomas de «Yesterday», el 14 de junio de 1965. La toma dos se consideró la mejor y fue utilizada para la masterización final. Un cuarteto de cuerdas fue añadido en la toma dos y esta versión de la canción fue la publicada. La toma uno, que no contiene ninguna mezcla, más tarde fue lanzada en el álbum recopilatorio "Anthology 2". En la toma dos se incorporaron dos líneas más que en la primera toma: "There's a shadow hanging over me"/"I'm not half the man I used to be".

George Martin dijo:

Aunque McCartney había quedado fascinado con la canción, no pudo convencer a los demás integrantes de la banda para que pudiera aparecer en un álbum, ya que la canción no correspondía con su imagen, considerando que tenía muy poco parecido con las demás canciones del grupo en ese momento. Este parecer fue tan fuerte que Lennon, Harrison y Starr no permitieron que «Yesterday» se publicara como sencillo en el Reino Unido. Esto no evitó que Matt Monro hiciera una versión del tema, siendo este el primero de muchas otras versiones de «Yesterday», las cuales se harían tiempo más tarde. La versión de Monro apareció entre los diez primeros lugares en las listas inglesas, poco después de su lanzamiento en el otoño de 1965.

La influencia de The Beatles sobre su sello discográfico en Estados Unidos, Capitol, no fue tan fuerte como con la británica de EMI, Parlophone. Un sencillo fue lanzado en los EE. UU., el cual incluía a «Yesterday» y «Act Naturally», un tema que contaba con Starr en la voz principal. El sencillo fue lanzado el 29 de septiembre de 1965, y encabezó las listas durante todo un mes, comenzando el 9 de octubre. La canción permaneció durante un total de 11 semanas en las listas estadounidenses, vendió un millón de copias en cinco semanas. «Yesterday» fue la más tocada en la radio americana durante ocho años consecutivos, por lo que su popularidad era innegable.

El 4 de marzo de 1966, la canción «Yesterday» fue lanzada como un EP en el Reino Unido, junto con "Act Naturally" en el lado A y "You Like Me Too Much" y "It's Only Love" en el lado B. Hacia el 12 de marzo, el EP había hecho su aparición en las listas. El 26 de marzo de 1966, el EP llegó al número uno, posición en la que permaneció durante dos meses. Más tarde ese año, «Yesterday» fue incluida en el álbum de lanzamiento estadounidense "Yesterday and Today".

El 8 de marzo de 1976, fue lanzada por Parlophone como un sencillo en el Reino Unido, con "I Should Have Known Better" en el lado B. Apareció en las listas de éxitos el 13 de marzo y permaneció durante siete semanas, pero no alcanzó un número superior al 8. El lanzamiento fue producto de la expiración del contrato de The Beatles con EMI, sello discográfico al que pertenecía Parlophone.

«Yesterday» ganó el Premio Ivor Novello por 'Mejor Canción de 1965', y ocupó el segundo lugar por 'El trabajo con más actuaciones del año', perdiendo contra otra composición de McCartney, «Michelle». La canción ha recibido una justa aclamación en los últimos tiempos. Así, ocupa el puesto número 13 en la lista de "Rolling Stone" de las 500 mejores canciones de todos los tiempos. En 1999, la Broadcast Music Incorporated (BMI) colocó a «Yesterday» en la tercera posición en su lista de las canciones más interpretadas del siglo XX en radio y televisión, con aproximadamente siete millones de actuaciones. «Yesterday» fue superada solo por "Never My Love" de The Association y "You've Lost That Lovin' Feelin'" de Righteous Brothers. En 1999, «Yesterday» fue elegida como la mejor canción del siglo XX en una encuesta realizada por la BBC Radio.

«Yesterday», sin embargo, también ha sido criticada por ser mundana y empalagosa. Bob Dylan tuvo una marcada aversión por la canción, afirmando que "si usted entra en la Biblioteca del Congreso, puede encontrar algo mucho mejor que eso. Hay millones de canciones como 'Yesterday' y 'Michelle' escritas en Tin Pan Alley'". Irónicamente, en última instancia, Dylan grabó su propia versión de «Yesterday» cuatro años más tarde, pero esta versión nunca fue publicada.

Los Beatles tocaron «Yesterday» por primera vez en directo el 1 de agosto de 1965 en el ABC Theatre de Blackpool en el marco del European Tour 1965. Se mantuvo en el repertorio hasta el concierto en Candlestick Park, San Francisco, el 29 de agosto de 1966, último de la historia del grupo, ya que ese año decidieron dejar de hacer giras musicales.

Posteriormente, Paul McCartney, tanto con los Wings como en solitario, ha seguido tocando la canción en sus conciertos hasta la actualidad.

La información respecto a los reconocimientos atribuidos a «Yesterday» está adaptada de AcclaimedMusic.net

«Yesterday» ha sido reconocida como una de las canciones con más grabaciones en la historia de la música popular según el Libro Guinness de los récords, por una ecléctica mezcla de artistas como Marianne Faithfull, The Mamas and the Papas y Barry McGuire, The Seekers, Joan Baez, John Denver, Luis Miguel, Michael Bolton, Bob Dylan,Tom Jones,Liberace, Frank Sinatra, Elvis Presley, Ray Charles, Marvin Gaye, Daffy Duck, Jan & Dean, Wet Wet Wet, Plácido Domingo, The Head Shop, Billy Dean, En Vogue, Muslim Magomayev y Boyz II Men. En 1976, David Essex hizo una versión de la canción para el documental musical "All This and World War II". En los , McCartney tocó la canción en vivo como un "mashup" con la canción de Linkin Park y Jay-Z "Numb/Encore".

También Marianne Faithfull interpretó la canción en su sencillo publicado en 1965 y recogido en su álbum "Love in a Mist" (1967).

En 2001, Ian Hammond especuló que McCartney pudo haberse basado inconscientemente en la versión de Ray Charles de "Georgia On My Mind", pero cerró su artículo diciendo que a pesar de las similitudes con «Yesterday» es "completamente original e individual [el trabajo]".

En julio de 2003, musicólogos británicos encontraron importantes similitudes en la letra y los esquemas rítmicos de «Yesterday» y la canción "Answer Me, My Love" de Nat King Cole (originalmente una canción alemana de Gerhard Winkler y Fred Rauch llamada "Mütterlein"), lo que dio lugar a especulaciones de que McCartney había sido influido por esa canción. Publicistas de McCartney negaron cualquier parecido entre «Answer Me» y «Yesterday». Sobre la hipótesis:

En 2006, el productor y escritor italiano Lilli Greco aseguró que «Yesterday» es una versión de la canción napolitana del siglo XIX llamada "Piccerè Che Vene a Dicere". Sin embargo, tras las investigaciones se concluyó que la canción no tiene ninguna similitud.

También se puso en evidencia por parte de un estudioso mexicano de la música, Jorge Seymour Perkins, de que la célebre canción de McCartney tenía varias coincidencias con el bolero mexicano «Bésame mucho», de Consuelo Velázquez, en lo que parece ser un genial caso de «imitación creativa inconsciente». Seymour había detectado una exagerada serie de coincidencias fonéticas y estructurales entre ambos temas. Según él, toda la estructura de «Yesterday» seguía la del bolero mexicano. Entre «Yesterday» y «Bésame mucho» había en total 153 coincidencias fonéticas. Además, ambas canciones tenían una estructura idéntica de 12 renglones y de cuatro segmentos por renglón en los que los cambios armónicos se daban en paralelo, provocando la clara impresión de que McCartney había utilizado el bolero de Velázquez como un patrón sobre el que había calcado el esquema de «Yesterday». «Bésame mucho» ya había entrado a principios de los años sesenta en el repertorio musical en directo de los Beatles. Fue una de las 14 canciones grabadas durante la audición para la casa discográfica Decca en enero de 1962, volviéndola a interpretar para EMI en junio de ese mismo año. La grabación para EMI apareció finalmente en el recopilatorio "Anthology 1".




</doc>
<doc id="37806" url="https://es.wikipedia.org/wiki?curid=37806" title="Colegio Comunitario Sir Arthur Lewis">
Colegio Comunitario Sir Arthur Lewis

El Colegio Comunitario Sir Arthur Lewis (en inglés: "Sir Arthur Lewis Community College") es una institución educativa ubicada en la ciudad de Castries, capital de la isla de Santa Lucía, en el Caribe.

Su nombre rinde tributo al académico santaluciano que ganó el Premio en Ciencias Económicas en memoria de Alfred Nobel en 1979 y cuyos restos descansan en la capilla de la institución. La escuela fue fundada en 1985 y desde esa fecha se especializa en ofrecer educación vocacional y técnica en el área de agricultura, salud, pedagogía y administración de negocios. Entre sus directivos más destacados sobresale Leton Felix Thomas, el autor de la partitura del himno nacional de Santa Lucía.




</doc>
<doc id="37808" url="https://es.wikipedia.org/wiki?curid=37808" title="Thriller (álbum)">
Thriller (álbum)

Thriller es el sexto álbum de estudio del artista estadounidense Michael Jackson, publicado el 30 de noviembre de 1982 por Epic Records después del éxito crítico y comercial del álbum "Off the Wall" del mismo artista lanzado en 1979. La obra está compuesta por un balance de varios géneros musicales: entre disco y rock, funk y balada, R&B y pop. El propio Jackson escribió cuatro de las nueve canciones del álbum.

Las sesiones de grabación se iniciaron en abril de 1982 en los estudios de grabación Westlake de Los Ángeles, con un presupuesto de producción de . El producto final fue lanzado en el mes de noviembre siguiente, y tras poco más de un año se convirtió en el de la historia a nivel mundial, llegando a superar el listón de sesenta y seis millones de copias comercializadas en 2017. En los Estados Unidos, se mantuvo como el más vendido de ese país hasta agosto de 2018, cuando fue superado por "Their Greatest Hits (1971-1975)" de la banda Eagles. Siete de las nueve canciones del álbum fueron lanzadas como sencillos y todos ellos llegaron al "top" diez en el "Billboard" Hot 100. El álbum ganó un en la .

"Thriller" estableció el estatus de Jackson como una de las estrellas del pop por excelencia, permitiéndole superar algunas barreras raciales. Además, con este proyecto, Jackson se convirtió en uno de los primeros artistas en utilizar los vídeos musicales como herramientas de promoción, destacando los clips de «Thriller», «Billie Jean» y «Beat It», que se transmitieron de manera continua en la cadena de televisión MTV. En 2001, se publicó una reedición especial del álbum que contenía entrevistas adicionales de audio, un "demo" y la canción «Someone in the Dark», que fue ganadora de un Grammy por su aparición en el audiolibro de la película "E.T., el extraterrestre". En 2008, el álbum fue reeditado como "Thriller 25", conteniendo remezclas y canciones inéditas.

Desde 2008, una copia de "Thriller" permanece en el Registro Nacional de Grabación de la Biblioteca del Congreso de Estados Unidos debido a su «gran significado cultural».
En la revisión del 2012 de la lista de de la revista "Rolling Stone" se colocó en el número 20,mientras que en la actual revisión del 2020 de la lista logró la posición número 12,siendo el mejor album pop de la lista. 

El disco contó con la colaboración de varios integrantes de la banda de rock Toto, entre ellos Jeff Porcaro (batería), Steve Porcaro y David Paich (teclados), Steve Lukather (guitarra) y el teclista Greg Phillinganes.

El anterior álbum de Jackson "Off the Wall" (1979) recibió críticas generalmente favorables. También fue un éxito comercial, con ventas finales de más de 20 millones de copias en todo el mundo. Los años que transcurrieron entre "Off the Wall" y "Thriller" fueron un período de cambio para el cantante, un momento en el que aumentó su independencia. Durante ese período el cantante manifestó sentirse infeliz: «Incluso en casa, me siento solo. Me siento en mi habitación y algunas veces me pongo a llorar. Es tan difícil hacer amigos [...] a veces me paseo por el barrio en la noche, con la esperanza de encontrar a alguien con quien hablar. Pero siempre termino volviendo a casa.» Cuando Jackson cumplió 21 años en agosto de 1979, despidió a su padre Joseph Jackson como su mánager y lo reemplazó por John Branca.

Cuando Jackson consideró que "Off the Wall" (en 1981, el álbum más vendido por un artista negro) había sido menospreciado en los , decidió crear un nuevo proyecto manifestado que sería aún mejor. También se sentía infravalorado por la industria del entretanimiento estadounidense ya que en 1980, cuando Jackson pidió al publicista de la revista "Rolling Stone" si estarían interesados en hacer un reportaje sobre él, este se negó, a lo que Jackson respondió: «Me han dicho una y otra vez que la gente negra en la portada de las revistas no vende copias [...] Solo hay que esperar. Algún día, las revistas van a venir a mí para pedirme una entrevista».

Jackson se reunió con el productor de "Off the Wall" Quincy Jones para grabar su sexto álbum de estudio. Ambos trabajaron en alrededor de 30 canciones, nueve de las cuales fueron incluidas en la configuración final del álbum. "Thriller" fue grabado entre abril y noviembre de 1982, con un presupuesto de . Varios miembros de la banda Toto también participaron en la grabación del álbum como músicos de sesión. Jackson escribió cuatro canciones para el disco: «Wanna Be Startin' Somethin'», «The Girl Is Mine» (con Paul McCartney), «Beat It» y «Billie Jean». Jackson no escribió estas canciones en papel, sino que las recitaba en un magnetófono de casete; cuando fue momento de grabarlas pudo cantarlas de memoria.

La relación entre Jackson y Jones se volvió tensa durante la grabación del álbum. Jackson pasó mucho de su tiempo ensayando los pasos de baile de manera independiente. Cuando se terminaron las nueve canciones, tanto Jones y Jackson estaban inconformes con el resultado y volvieron a mezclar cada canción, llevándose una semana en cada una.

Jackson se inspiró para crear un álbum donde «cada canción era un asesinato», y desarrolló "Thriller" bajo ese concepto. En la reedición de 2001 del álbum, Jones y el compositor Rod Temperton hablaron detalladamente sobre la grabación del álbum. Jones explicó porqué «Billie Jean» era tan personal para Jackson, quien al momento de concebir el tema, luchaba por mantenerse a salvo de los fanes obsesionados. Por su parte, Jones quería que la larga introducción que había en la canción se acortara, sin embargo, Jackson insistió en que permaneciera, porque esto hacía que le dieran ganas de bailar.

El declive de la música disco hizo necesario moverse en otra dirección musical. Jones y Jackson estaban decididos a hacer una canción de rock que fuera llamativa para todos los gustos musicales y pasaron semanas para poder encontrar un guitarrista adecuado para la canción «Beat It», una canción escrita por Jackson. Posteriormente encontraron a Eddie Van Halen de la banda de rock Van Halen.

Por otra parte, cuando Temperton escribió la canción «Thriller», quería originalmente llamarla «Starlight» o «Midnight Man», pero finalmente se dejó «Thriller», porque consideraban que el nombre tenía un gran potencial comercial. Siempre quisieron encontrar una persona que fuera ideal para recitar las letras finales de la canción; Jones invitó al actor Vincent Price, que era un conocido de su esposa, quien completó su parte en sólo dos tomas. Temperton escribió la parte final de la canción en un taxi cuando se dirigía hacia el estudio de grabación.

El crítico musical Stephen Thomas Erlewine de Allmusic señala que "Thriller" de 1982 fue un álbum que tuvo algo para todo el mundo, basándose en el proyecto básico de "Off the Wall", pero añadiendo "funk" más duro" (Billie Jean" o "Thriller"), "hard rock" ("Beat It"), baladas más suaves ("The Girl Is Mine" o "Human Nature"), y "soul" más fluido ("Baby Be Mine"), ampliando el enfoque de tener algo para cada público.

El álbum incluye las baladas «The Lady In My Life», «Human Nature» y «The Girl Is Mine»; las piezas "funk" de «Billie Jean» y «Wanna Be Startin 'Somethin'», y música disco en «Baby Be Mine» y «P.Y.T. (Pretty Young Thing)», y tiene un sonido similar al material de "Off The Wall". «Wanna Be Startin 'Somethin'» está acompañada de un bajo y la percusión en la parte central de la canción, y culmina con un canto en suajili. «The Girl Is Mine» cuenta la lucha de dos amigos por una mujer, argumentando sobre quién la ama más y concluye con un rap hablado.

A pesar del sonido pop de estas dos grabaciones, "Thriller", más que "Off The Wall", muestra presagios de las temáticas contradictorias que vendrían a caracterizar los trabajos posteriores de Jackson. Con "Thriller" comenzaría su relación con lo subliminal de la paranoia y temas más oscuros. Esto es evidente en la canción «Billie Jean» que comienza con el largo sonido de un bajo y batería en la introducción, y en la que Jackson habla sobre una fan obsesiva que alega haber tenido un hijo con él. Así mismo, en «Wanna Be Startin' Somethin'», el artista argumenta estar en contra de los cotilleos y los medios de comunicación. Además, Jackson incluye metáforas sobrenaturales en la pista que da título al álbum, donde pueden ser escuchados distintos efectos de sonido, como una puerta chirriante, un trueno, pies caminando sobre tablones de madera, vientos y ladridos de perros.

La lucha contra la violencia de pandillas en «Beat It» se convirtió en un homenaje a "West Side Story", y fue la primera pista "rock" de éxito de Jackson. Jackson dijo después de «Beat It», «el punto es que nadie tiene que ser el tipo duro, se puede caminar lejos de una pelea y aun así ser un hombre duro. No se tiene que morir para demostrar que eres un hombre.» «Human Nature» es temperamental e introspectiva, según se indica en su letra como, «mirando hacia fuera, a través de la mañana, el corazón de la ciudad comienza a latir, extendiéndome, toco su hombro, sueño con las calles.»

La revista "Rolling Stone" comparó la voz de Jackson a la «jadeante y maravillosa voz» de Stevie Wonder. Su análisis fue que el «ligero tono de Jackson es de extraordinaria belleza. Se desliza suavemente en un sorprendente falsete que es utilizado de manera muy atrevida.» Aunque Jackson podía cantar en un tono débil —por debajo de do— prefería cantar más alto porque de esa manera los tenores pop tienen más alcance para lograr el estilo. La opinión de "Rolling Stone" fue que Jackson estaba cantando en una «voz totalmente adulta» que se «teñía de tristeza». «P.Y.T. (Pretty Young Thing)», acreditada a James Ingram y Quincy Jones, y «The Lady In My Life» de Rod Temperton, le dieron al álbum una dirección más fuerte de R&B. El cantante ya anteriormente había adoptado una tono vocal llamado «hipo», y este se siguió aplicando en "Thriller". El propósito del «tono de hipo» —algo así como tragando aire o jadeando— ayuda a provocar cierta emoción para el receptor, ya sea la excitación, la tristeza o el miedo.

La portada de "Thriller" presenta a Jackson con un traje blanco en un toma del fotógrafo Dick Zimmerman. La manga desplegable revela un cachorro de tigre en la pierna de Jackson, que, según Zimmerman, mantuvo alejado de su rostro, temiendo que le rasguñase. Otra imagen de la sesión, con Jackson abrazando al cachorro, se utilizó para la edición especial de 2001 de Thriller.

"Thriller" fue lanzado el 30 de noviembre de 1982, y vendió un millón de copias a nivel mundial durante su primera semana. Siete sencillos fueron extraídos del álbum, comenzando por la canción «The Girl Is Mine» que fue seguida por el sencillo «Billie Jean», que hizo a "Thriller" un éxito en las listas. El éxito continuó con el sencillo «Beat It», que incluyó a los guitarristas Eddie Van Halen y Steve Lukather. La canción «Thriller» fue lanzada como sencillo y también se convirtió en un éxito internacional.

El álbum fue bien recibido por los críticos. La revista "Rolling Stone" le dio una calificación de cuatro estrellas, y en la revisión de Christopher Connelly lo describió como «un LP vigoroso» con «un desgarrador y oscuro mensaje». A pesar de la respuesta positiva, la canción homónima fue objeto de fuertes críticas. "Rolling Stone" expresó un sentimiento negativo en ella, criticándola fuertemente. La revista explicó que hay un desorden terrible en la parte final, donde Vincent Price recita las últimas palabras de la canción. Por su parte, "The New York Times" hizo una revisión positiva del álbum, destacando particularmente la canción «Human Nature» que describió como una de las «canciones más sorprendentes». Concluyó su revisión agregando, «Lo mejor de todo, es la confianza generalizada que se infunde en todo el álbum, "Thriller" demuestra que la evolución del señor Jackson como un artista está lejos de terminar.»

Robert Christgau clasificó al álbum como positivo (A) unos pocos días antes de su lanzamiento. Reconoció que hubo «relleno» en el álbum, pero aun así lo denominó como «casi clásico». Expresó la opinión de que «Beat It» fue el mejor tema del álbum, calificándolo como el triunfo del álbum, pero criticó a «The Girl Is Mine» como «la peor canción de Michael desde 'Ben'». Era de la opinión de que la colaboración no funcionó bien, pero lo elogió por el «amor interracial que se consiguió en la radio». Un año después del lanzamiento del álbum, "Time" resumió los tres principales sencillos del álbum, diciendo: «América y gran parte del resto del mundo se mueve de forma irregular, al compás de los difíciles puntales de 'Billie Jean', el aria de 'Beat It' y los escalofríos sumamente frescos de 'Thriller'.» Por el contrario, en una revisión de "Melody Maker", Paolo Hewitt declaró que «este no es un buen LP», en su opinión, había sólo «dos temas dignos de mención». «Wanna Be Startin 'Somethin'» fue elogiada como una «emocionante, moderna canción de electro funk», tal y como lo fue «Billie Jean». La opinión de Hewitt fue que el álbum sólo puede ser descrito como «ligero», particularmente en el cierre de las pistas. Resumió: «Parece que Jackson ha perdido su talento para convertirlo en cifras brutas en oro.»

Durante la celebrada el 28 de febrero de 1984 en reconocimiento a los logros discográficos alcanzados durante 1983, "Thriller" ganó un total de ocho premios Grammy, entre ellos el de y , este último otorgado a Bruce Swedien. Esa misma noche, Jackson se hizo acreedor en la categoría por su contribución en la banda sonora "E.T. the Extra-Terrestrial". Ese mismo año, Jackson ganó ocho American Music Awards, el Premio Especial al Mérito y tres MTV Video Music Awards. Además es uno de sólo tres álbumes en haber permanecido en el "top" diez del "Billboard" 200 durante un año completo, y pasó 37 semanas no consecutivas en el número uno. El álbum también fue el primero de tres que sus siete sencillos aparecieron en los diez primeros del "Billboard" Hot 100,

"Thriller" fue reconocido como el el 7 de febrero de 1984, cuando fue incluido en el "Libro Guinness de los récords", siendo además el único "bestseller" en dos años consecutivos (1983-1984) en los Estados Unidos. El álbum encabezó las listas en varios países, vendió 3,8 millones copias en el Reino Unido, 2,5 millones en Japón y fue 16 veces Platino en Australia.

El álbum alcanzó el número dos del Catalog charts en febrero de 2003 y el número 39 en el Reino Unido en marzo de 2007. Después de la muerte de Jackson en junio de 2009, "Thriller" experimentó una nueva fase de popularidad. En EE.UU. vendió más de 100.000 copias en una semana, situándose en el número dos en la lista del Top Pop Catalog Albums. Las canciones de "Thriller" también ayudaron a Jackson en convertirse en el primer artista en vender más de un millón de descargas de canciones en una semana, y según Nielsen SoundScan, ese año fue el 14º álbum más vendido en los Estados Unidos con 1,27 millones de copias comercializadas. También en 2009 el álbum fue certificado de Platino en Europa por la IFPI, a modo de acreditación de ventas legales de 1 millón de copias sólo en ese año.

El "Libro Guinness de los récords" aseguró en 2017 que "Thriller" había excedido la marca de sesenta y seis millones de copias vendidas en el mundo. En diciembre de 2015, "Thriller" obtuvo la certificación de 30 discos de platino por la Recording Industry Association of America, al alcanzar ventas superiores a las 30 millones de copias en los Estados Unidos; menos de dos meses después, llegó a los 32 discos de platino, en representación a las 32 millones de unidades.

El álbum "Thriller" cambió la forma en que funcionaba la industria musical haciéndola más rentable. En un mercado impulsado por sencillos, "Thriller " planteó la importancia que podría tener un álbum, y sus múltiples éxitos cambiaron la forma de percibir cuantos sencillos exitosos podrían extraerse de un mismo álbum. La época vio la llegada de novedades como el muñeco de Michael Jackson, que apareció en las tiendas en mayo de 1984. "Thriller" mantiene una posición importante en la cultura estadounidense. El biógrafo J. Randy Taraborrelli, explica, en cierto momento, «"Thriller" dejó de venderse como un elemento de ocio —como una revista, un juguete o entradas para una película de éxito— y empezó a venderse como un elemento básico del hogar».

Al mismo tiempo que se publicó el álbum, un comunicado de prensa de A&M Records, dijo que, «Toda la industria obtiene un beneficio de este éxito.» La revista "Time" especuló que «el éxito de "Thriller" le dio a los negocios [musicales] uno de sus mejores años desde los embriagadores días de 1978, cuando había un estimado de ingresos totales de 4,1 mil millones de dólares.» "Time" resumió el impacto de "Thriller" como una «recuperación de confianza» para una industria que se encontraba en «las ruinas del punk y los elegantes sintetizadores pop». La publicación describió a la influencia de Jackson de ese momento como« Una estrella de discos, radio y videos de rock. Un hombre que rescató al negocio de la música. Un compositor que pone el ritmo de una década. Un bailarín con los pies más elegantes en la calle. Un cantante que atraviesa todas las fronteras del gusto y del estilo».

En enero de 1984, "The New York Times" manifestó que Jackson era un «fenómeno musical», y que «en el mundo de la música pop, no hay nadie igual a Michael Jackson.» En este sentido, el artista obtuvo un récord de beneficios en las ventas de discos compactos, y de la venta de copias del documental "The Making of Michael Jackson's Thriller", financiado parcialmente por la cadena de televisión MTV.

En 1983, el presidente de CBS Records presionó a la cadena de televisión MTV para que emitiese los videoclips de Jackson: "Yo no voy a darles más videos y voy a hacer público lo que han hecho, de no desear transmitir la música de un hombre negro". Esta postura contribuyó para que la cadena transmitiera el videoclip de "Billie Jean" y posteriormente el de "Beat It", situación que más tarde ayudó a otros artistas negros en obtener el reconocimiento general de la prensa musical. Eventualmente, la popularidad de estos vídeos contribuyó a aumentar la presencia mediática de la cadena de televisión, y el enfoque de MTV cambió a favor del pop y el Rhythm and blues. Cuando a finales del mismo año se presentó el cortometraje "Michael Jackson's Thriller" de 14 minutos, MTV comenzó a transmitirlo dos veces por hora para satisfacer la demanda. La popularidad del vídeoclip hizo que el álbum "Thriller", que había caído de la posición número 1 casi seis meses antes, saltase al primer lugar en esa Navidad y permaneciendo allí hasta entrado el año nuevo.

Con este álbum, Jackson transformó el medio del videoclip en una forma de arte y una herramienta de promoción mediante el uso de historias, rutinas de baile y efectos especiales. Cortometrajes como el de "Thriller" siguieron siendo utilizados en gran medida por Jackson, mientras que la secuencia de baile grupal en "Beat It" ha sido frecuentemente imitado. La coreografía de "Thriller" se ha convertido en parte de la cultura popular mundial, siendo replicada en todas partes de Bollywood y las cárceles de Filipinas.

Para un artista negro en la década de 1980, el éxito de Jackson no tenía precedentes. Según "The Washington Post", "Thriller" allanó el camino para otros artistas afroamericanos como Prince. "The Girl Is Mine" fue acreditada para promover el amor interracial en la radio. La revista "Time" señaló que "Jackson es lo más grande que ha sucedido desde The Beatles. Él es el fenómeno más enérgico desde Elvis Presley. Es el único cantante negro que puede llegar a ser el más popular en la historia".

En marzo de 1983, durante el especial de televisión , Jackson interpretó la canción "Billie Jean". Luego de tres minutos y 36 segundos de heber iniciado la interpretación, Jackson hizo el paso de baile conocido posteriormente como Moonwalk. Apenas dura un par de segundos y Jackson no había ejecutado el paso durante los ensayos. A pesar de que el cantante aseguró que simplemente dejó que “la canción crease los movimientos”, Jackson ya había visto el Moonwalk en 1979 en el programa de televisión Soul Train. Sin embargo, al artista renegó de esta versión cuando en su autobiografía Moonwalker aseguró que lo había aprendido observando niños negros que lo practicaban en un gueto.

Para la revista Vanity Fair, el impacto del Moonwalk no radica en la originalidad o la propiedad intelectual, “sino a cómo engendra la propia identidad de Michael Jackson como artista”.

El álbum sigue siendo materia de análisis por parte de los críticos. Stephen Thomas Erlewine de Allmusic le dio al álbum el máximo de cinco estrellas y escribió que la grabación sigue teniendo el mismo intereses para todos. Opina que mostró más "funk" y "hard rock" sin dejar de ser «indudablemente divertido». Felicitó a «Billie Jean» y "«Wanna Be Startin' Somethin'», y dijo, «las dos mejores canciones del disco: "Billie Jean"... y la delirante "Wanna Be Startin' Somethin'", el álbum tiene un "funk" más fresco, [pero] el más claustrofóbico». Erlewine opinó que "Thriller" mostraba un claro progreso a diferencia del anterior álbum en Jackson, aunque Allmusic criticó a la canción homónima, describiéndola como "ridícula". "Slant Magazine" le dio al álbum cinco estrellas y al igual que la revisión de Allmusic y la revisión original de "Rolling Stone", explicó que la mejor canción del álbum es "«Wanna Be Startin' Somethin'».

En base al éxito cosechado en masa hasta hoy en día, "Thriller" fue colocado en el puesto número 20 de los 500 mejores álbumes de todos los tiempos por la revista "Rolling Stone" en el 2003, que mantuvo el ranking en una lista revisada de 2012 hasta su actual revision del año 2020, en el que se colocó en el puesto número 12, siendo el álbum pop mejor clasificado de la lista. En 2007 la National Association of Recording Merchandisers (NARM) lo colocó en el puesto número 3 en su lista de los 200 discos «definitivos» que «todo amante de la música debería tener». En 2008, 25 años después de su lanzamiento, el álbum galardonado con el Premio del Salón de la Fama de los Grammy y, unas semanas después, una copia del álbum fue preservado en la Biblioteca del Congreso de los Estados Unidos por su «significado cultural». En 2009, críticos de MTV Base y VH1 eligieron a "Thriller" como el mejor álbum lanzado desde 1981. Ese mismo año, "Thriller", fue elegido como el mejor álbum de la historia en una encuesta realizada por MTV Generation.

El álbum fue reeditado en octubre de 2001 en un conjunto más amplio titulado "Thriller: Special Edition". Los temas originales fueron remasterizados, y el álbum incluyó un nuevo folleto y material extra, incluyendo las canciones "Someone In the Dark", "Carousel", y el demo original de "Billie Jean", así como entrevistas de audio con Quincy Jones y Rod Temperton comentando sobre la grabación del álbum. Sony también contrató al ingeniero de sonido y mezclador Mick Guzauski para trabajar con Jackson en la creación de mezclas surround en 5.1 canales de "Thriller", así como todos sus otros álbumes, para lanzarlo en el entonces nuevo formato Super Audio CD. A pesar de numerosos intentos, el artista no aprobó ninguna de las mezclas. En consecuencia, "Thriller" se publicó el SACD sólo en una versión estéreo.

En 8 de febrero de 2008, Epic Records lanzó "Thriller 25", en el que Jackson trabajó como productor ejecutivo. El álbum apareció en CD, USB y en vinilo con siete canciones extra, una nueva canción llamada «For All Time», un fragmento de la voz en off de Vincent Price, y cinco remezclas con artistas contemporáneos como Fergie, Akon, Will.I.Am y Kanye West. También incluye un DVD con tres videosclips, la presentación de "Billie Jean" en "", y un folleto con un mensaje de Jackson. La balada "For All Time" supuestamente data de 1982, pero se considera que fue grabada durante las sesiones de "Dangerous".

"Thriller 25" fue un éxito comercial alcanzando la posición número uno en ocho países, la número dos en los Estados Unidos, la número tres en el Reino Unido y alcanzó el "Top" 10 en más de 30 listas nacionales. En los Estados Unidos, "Thriller 25" fue el segundo álbum más vendido en su primera semana de publicación, alcanzando ventas de ciento sesenta y seis mil copias, sólo catorce mil menos para alcanzar la posición número uno. Fue inelegible para el "Billboard 200" por ser un relanzamiento, pero entró en las listas del Pop Catalog alcanzando el número uno (donde permaneció durante diez semanas no consecutivas), consiguiendo las mejores ventas en esta lista desde diciembre de 1996. Con la llegada de Halloween en noviembre, "Thriller 25" pasó su undécima semana no-consecutiva en la cima del Pop Catalog. Esto le trajo al álbum ventas de 688.000 copias en EE.UU., por lo que fue el mejor álbum de la lista del Pop Catalog en 2008. Este fue el mejor lanzamiento de Jackson desde "Invincible" en 2001, vendiendo tres millones de copias en todo el mundo en 12 semanas.




</doc>
<doc id="37810" url="https://es.wikipedia.org/wiki?curid=37810" title="RadiOS">
RadiOS

RadiOS es un sistema operativo basado en un micronúcleo libre. Comparte muchas ideas y técnicas de implementación con VSTa y QNX

El micronúcleo de RadiOS (RMK) tiene un conjunto de llamadas al sistema compatible con QNX Neutrino 6.1. El Administrador de Tareas (Task Manager, equivalente al administrador de procesos en QNX) tiene también el formato de los mensajes compatibles. Por último, RadiOS C y las librerías se basan también en el código de libc QNX 6.1.

La mayoría del código de RadiOS (unas 45000 líneas actualmente) está escrito en lenguaje ensamblador. La única parte en Lenguaje C es el módulo enlazador en tiempo de arranque (boot-time module linker o BTL), que será re-escrito en otro Lenguaje una vez finalice el desarrollo del compilador.

Actualmente RadiOS es el único proyecto de código abierto que busca compatibilidad binaria con QNX Neutrino.



</doc>
<doc id="37824" url="https://es.wikipedia.org/wiki?curid=37824" title="Museo Louisiana de arte moderno">
Museo Louisiana de arte moderno

El museo Louisiana de arte moderno está situado en la costa norte de la isla de Zelanda, cerca de Copenhague, Dinamarca, en lo que antes era un parque. Fue fundado en 1958 por Knud W. Jensen como una institución pública independiente.

Uno de sus puntos clave es que combina las muestras de arte moderno con la arquitectura del museo y el paisaje alrededor, encontrándose muchas de las esculturas al aire libre, así como su situación estratégica junto a la costa, desde donde puede verse Suecia a la otra orilla.
El nombre del museo proviene del primer propietario de la propiedad, Alexander Brun, quien nombró a la villa así por sus tres esposas, todas llamadas Louise. El museo fue creado en 1958 por Knud W. Jensen, el propietario en ese momento. Se puso en contacto con los arquitectos Vilhelm Wohlert y Jørgen Bo, quienes pasaron unos meses paseando por la propiedad antes de decidir cómo una nueva construcción se ajustaría mejor al paisaje. Este estudio dio como resultado la primera versión del museo que consta de tres edificios conectados por corredores de vidrio. Desde entonces, se ha ampliado varias veces hasta alcanzar su actual forma circular en 1991. Partiendo de la villa preexistente, en la que se sitúa la entrada del museo, se hace un recorrido que conduce al visitante a través de los árboles y el paisaje, a la vez que disfruta de la contemplación de la obras expuestas.

Desde el punto de vista arquitectónico la sección, y altura, de los corredores y las salas están dimensionadas más según una tipología doméstica que como un museo público. Se pueden apreciar influencias de la obra residencial de Richard Neutra, Marcel Breuer o Craig Ellwood. El Louisiana puede considerarse una obra de arquitectura total en la que el edificio, lo que contiene y su entorno se relacionan de una manera totalmente satisfactoria. 
A fines de noviembre de 2012, el Louisiana Museum of Modern Art lanzó Louisiana Channel, un canal de televisión en línea que contribuyó al desarrollo del museo como una plataforma cultural.

En 2013, el departamento de música del museo lanzó Louisiana Music, una página web dedicada a los vídeos musicales producidos por el museo en colaboración con músicos de fama mundial.

El museo ocasionalmente también celebra exposiciones de las obras de grandes artistas impresionistas y expresionistas, p.e. Claude Monet fue el foco de una gran exposición en 1994.

El museo está incluido en el libro de Patricia Schultz "1,000 Places to See Before You Die" y ocupa el lugar 85 en una lista de los museos de arte más visitados del mundo (2011).

Tiene una amplia gama de pinturas, esculturas y videos de arte moderno que datan desde la Segunda Guerra Mundial hasta nuestros días, incluyendo obras de artistas como Roy Lichtenstein, Andy Warhol, Anselm Kiefer, Alberto Giacometti, Pablo Picasso, Yves Klein, Robert Rauschenberg, David Hockney y Asger Jorn. Encaramado sobre el mar, hay un jardín de esculturas entre las dos alas del museo con obras de artistas como Henry Moore, Alexander Calder y Jean Arp.

Además de la colección de arte moderno, Luisiana también muestra una colección de arte precolombino. Compuesta por más de 400 objetos, la colección fue una donación de la Fundación Wessel-Bagge en 2001. Es la colección personal que dejó Niels-Wessel Bagge, bailarina, coreógrafa y coleccionista de arte danesa radicada en California, que murió en 1990.
La sala de conciertos fue construida en 1976 en conexión con el ala oeste que se construyó entre 1966 y 1971. Su acústica la hace especialmente adecuada para la música de cámara, pero también se utiliza para otros géneros musicales, así como para una amplia gama de otros eventos y actividades como debates, conferencias y simposios. Las sillas están diseñadas por Poul Kjærholm y la pared posterior está decorada con pinturas creadas para el sitio por Sam Francis.

En 2007 comenzó un proyecto para producir la filmación de conciertos y clips musicales dirigidos por Stéphan Aubé. Todas las películas están disponibles de forma gratuita en el sitio web de Louisiana Music.
Los jardines alrededor del museo contienen un jardín de esculturas. Consiste en una meseta y un terreno que se inclina hacia el Øresund y está dominado por enormes y antiguos árboles y vistas panorámicas del mar.

Contiene obras de artistas como Jean Arp, Max Ernst, Max Bill, Alexander Calder, Henri Laurens, Louise Bourgeois, Joan Miró y Henry Moore. Las esculturas se colocan para que puedan verse desde dentro, en patios de esculturas especiales o de forma independiente alrededor de los jardines, formando una síntesis con el césped, los árboles y el mar. También hay ejemplos de arte específico del sitio de artistas como Enzo Cucchi, Dani Karavan y George Trakas.
El festival de literatura de Luisiana es un festival anual que se lleva a cabo en el Museo de Arte Moderno de Luisiana. Lanzado en 2010, el festival presenta cada año a unos cuarenta escritores de todo el mundo. Se presentan en escenarios alrededor del museo y en el parque de esculturas, y atraen a más de 10.000 personas cada año.

1958 -: Knud W. Jensen

1995 -: Lars Nittve

1998 -: Steingrim Laursen

2000 -: Poul Erik Tøjner

El museo está ubicado junto a la costa de Øresund, en la región de North Zeeland, a unos 30 km al norte del centro de Copenhague y a 10 km al sur de Elsinore. Desde la estación de tren regional en Humlebæk, se tarda 10-15 minutos en llegar al museo.


</doc>
<doc id="37826" url="https://es.wikipedia.org/wiki?curid=37826" title="Nayarit">
Nayarit

Nayarit, oficialmente llamado Estado Libre y Soberano de Nayarit, es uno de los treinta y un estados que, junto con la Ciudad de México, conforman México. Su capital y ciudad más poblada es Tepic. 

Está ubicado en el oeste del país, limitando al norte con Sinaloa y Durango, al este con Zacatecas, al este y sur con Jalisco y al oeste con el océano Pacífico, donde posee también las islas Marías, la isla Isabel, las Tres Marietas y el Farallón La Peña. Con 1 181 050 habs. en 2015 es el cuarto estado menos poblado —por delante de Campeche, Baja California Sur y Colima—, con 27 815 km², el noveno menos extenso —por delante de Tabasco, Estado de México, Hidalgo, Querétaro, Colima, Aguascalientes, Morelos y Tlaxcala— y con 39,01 hab/km², el noveno menos densamente poblado, por delante de Quintana Roo, Coahuila, Zacatecas, Sonora, Campeche, Chihuahua, Durango y Baja California Sur. Fue fundado el 26 de enero de 1917.

Se divide en 20 municipios. Su capital es Tepic. Otras localidades importantes son Nuevo Vallarta, Valle de Banderas (ambas en el municipio de Bahía de Banderas), Ixtlán del Río, San Blas, Santiago Ixcuintla, Acaponeta, Compostela, Jala, Santa María del Oro, Rosamorada, Xalisco, San Pedro Lagunillas, Tecuala y Huajicori.

La palabra Nayarit proviene del cora, que es el etnónimo con el que se nombran al pueblo cora Naáyarite (singular: Naáyari). Nayarit significa: "Hijo de Dios que está en el cielo y en el Sol".

Nayarit fue uno de los últimos territorios admitidos como Estado de la federación mexicana, lo cual ocurrió el 1 de enero de 1917.

Por Decreto, el 13 de marzo de 1837 surge el "Departamento de Tepic", de conformidad con el Artículo 8 de las "Bases y Leyes Constitucionales de la República Mexicana" de 23 de octubre de 1835; el cual contaba con una superficie de 1868 leguas cuadradas y su población era de 62 620 habitantes. Se dividía en 2 Ayuntamientos: Tepic y Ahuacatlán.
En 1838 el Departamento de Tepic contaba con una población de 67 180 habitantes. Anteriormente se le conocía como el 7º Cantón de Jalisco, nombre que retomó el 18 de septiembre de 1846 una vez que el estado de Jalisco se integró al Pacto Federal, constituyéndose en 5 Departamentos: Acaponeta, Ahuacatlán, Sentispac (hoy en día Santiago Ixcuintla), Compostela y Tepic. 

En el año de 1858 el 7º Cantón de Jalisco registró una población de 74 538 habitantes. Miguel Miramón decretó la creación del Territorio de Tepic el 24 de diciembre de 1859. Poco después el departamento de Nayarit contaba con 97 000 habitantes y se dividió en 6 distritos: Tepic, Ahuacatlán, Compostela, Acaponeta, Santiago y San Luis. Restaurado el Federalismo, pasó a ser el Distrito Militar de Tepic, el cual se erigió por Decreto del 7 de agosto de 1867 emitido por el Presidente Benito Juárez García; se constituía en 11 Ayuntamientos: Ahuacatlán, Jala, Villa de Ixtlán, Compostela, San Pedro Lagunillas, Santiago Ixcuintla, Acaponeta, Tepic, Tuxpan, San Blas y Xalisco. 

En el año de 1877 el Distrito Militar de Tepic tenía una población de 95 000 habitantes, registrándose así un decremento poblacional ocasionado sin lugar a dudas por los constantes disturbios de la época. Posteriormente, el 18 de diciembre de 1884, se elevó al rango de territorio federal de Tepic con el nombre de Tepic. Dicho territorio fue administrado por el gobierno federal hasta 1917, fecha en la que el territorio, se constituyó como estado soberano. Tepic es la ciudad más grande y la capital del estado, la cual está conurbada con el Municipio de Xalisco. Le siguen en importancia el área conurbada de Bahía de Banderas, Santiago Ixcuintla, Tuxpan, Ixtlán del Río, Acaponeta, Compostela, Tecuala, Estación Ruiz, Villa Hidalgo.

Se ubica en el noroeste del territorio de México. Colinda con los estados de Sinaloa, Durango y Zacatecas hacia el norte, y con el estado de Jalisco hacia el este y sur. Hacia el poniente tiene una importante franja costera en el Océano Pacífico, donde posee también las islas Marías, la isla Isabel, las Tres Marietas y el Farallón La Peña.

Se divide en 20 municipios. Su capital es Tepic. Otras localidades importantes son Nuevo Vallarta, Valle de Banderas (ambas en el municipio de Bahía de Banderas), Santiago Ixcuintla, Acaponeta, Compostela, Santa María del Oro, Rosamorada, Xalisco, San Pedro Lagunillas y Huajicori.

El estado se divide en 20 municipios:

La temperatura media anual del estado es de 24 °C, las temperaturas mínimas promedio son alrededor de 12 a 14 °C en el mes de enero y las máximas promedio puede ser ligeramente mayores a 28 °C durante los meses de mayo y junio.

Las lluvias se presentan en el verano durante los meses de mayo a septiembre, la precipitación media del estado es de anuales.

El clima cálido, semicálido y templado es algo favorable para el cultivo de: maíz, frijol, sorgo, tabaco, arroz, sandía, cacahuate, jitomate, chile seco, caña de azúcar, café, mango, plátano, aguacate, nopal, limón, pepino, cebolla, jícama, nanchi, chile serrano.

Las corrientes hidrológicas de Nayarit desembocan en el Océano Pacífico. Estas son: Río Ameca, río Grande de Santiago, río San Pedro Mezquital, río Acaponeta, río Cañas y otros de menor importancia como el río Bolaños, el río Huaynamota y el río Chapalagana; y además el estero de Teacapán y el estero de Cuautla.

Existen también las aguas estuarinas de laguna de Agua Brava, laguna de Mexcaltitán y laguna de El Pescadero y en la parte sur se encuentran la laguna de Santa María del Oro, la laguna de Tepetiltic y la laguna de San Pedro.

El clima predominante en la entidad es cálido, concentrándose principalmente a lo largo de una franja que va de norte a sur, situada precisamente en la zona de transición entre la Llanura Costera del Pacífico y la Sierra Madre Occidental.

En menor grado se distribuyen climas de tipo templado principalmente en las sierras.

Los climas muy cálidos se restringen a las áreas de la costa y en las zonas bajas de los valles del río Huaynamota y río San Pedro Mezquital.

La vegetación se ve distribuida en un mosaico irregular, cuya formación depende de los factores imperantes del clima y del suelo que se encuentra en la región.

En Nayarit, existen los siguientes tipos de vegetación:

Nayarit está comprendido, desde el punto de vista geológico, dentro de cuatro provincias, que son:


Según los datos que arrojó el "II Censo de Población y Vivienda" realizado por el Instituto Nacional de Estadística y Geografía (INEGI) con fecha censal del 12 de junio de 2015, el estado de Nayarit contaba hasta ese año con un total de 1 284 979 habitantes; de dicha cantidad, 581 007 (49.9 %) eran hombres y 583 972 (50.1 %) eran mujeres. La tasa de crecimiento anual para la entidad durante el período 2005-2010 fue del 2,7 %.

Se ha iniciado una desaceleración del crecimiento demográfico en gran medida por la emigración hacia el norteño estado de Baja California y al vecino Jalisco y, en gran medida hacia EE.UU.

Desde 2003 un gran sector de la población dedicada a las actividades primarias vé con buenos ojos la emigración a Canadá, país al que recientemente se han trasladado una importante cantidad de trabajadores agrícolas debido a un convenio establecido entre ese país y México, hace ya algunos años.

De acuerdo a los resultados que presentó el II Conteo de Población y Vivienda en el 2005, en Nayarit el 93.3 % de la población mayor de 5 años profesa la religión católica; el 2.0 % la religión evangélica; el 0.1 % la religión judaica; el 1.5 % otro tipo de religión y el 3.1 % declaró no profesar ninguna religión o ninguna específica.

En Nayarit, registró el censo de 2000 una población de 37 206 personas que hablaban alguna lengua indígena, dispersándose como lo detalla la tabla:

Cabe destacar que, la etnia huichol ha ido desplazando a la cora, que era la que más peso demográfico tenía en Nayarit, hasta antes de 1995.

Los principales grupos étnicos que registró el censo de 2000 en el estado son los huicholes (16 932 personas), los coras (15 389) y los tepehuanos (1422); el náhuatl (1422) ha iniciado un aumento propiciado por la inmigración proveniente del estado de Guerrero y otras entidades del centro del país. Sin embargo, existe un asentamiento ancestral en el municipio de Acaponeta que comúnmente se le denomina mexicaneros.

Más del 60 % del PIB estatal está conformado por el sector servicios. En primer lugar están los servicios comunales, sociales o personales; le siguen el comercio, hoteles y restaurantes, así como servicios financieros, de alquiler y seguros.

Es gran productor de derivados del ganado bovino, porcino, ovino, caprino, y de aves de corral; se consume cotidianamente el conejo, la codorniz, el pato, el pollo y el guajolote, así como el avestruz.

Nayarit es el primer estado en donde se produce el camarón, y se pesca principalmente en los municipios de Santiago Ixcuintla, San Blas y Bahía de Banderas (en el Poblado de La Cruz de Huanacaxtle), Tecuala. También se pescan huachinango, salmón, róbalo y lisa.

La minería fue una actividad importante a finales del siglo XIX y principios del siglo XX, destacándose en esta actividad los municipios de La Yesca y Amatlán de Cañas, aunque también se pueden encontrar vestigios mineros en otros municipios tal es el caso de "El Real Mineral del Zopilote" en el municipio de Ruiz. Recientemente se ha retomado la explotación de los yacimientos localizados en el municipio de La Yesca.

Las industrias que más sobresalen en Nayarit son la del azúcar que se produce en "El Molino de Menchaca" y "El Ingenio de Puga", Industria Tabaquera que se produce en British American Tobacco México S.A. de C.V., Tabacos Desvenados, S.A. de C.V., Tabacos del Pacífico Norte, S.A. de C.V., y la industria del turismo.

En la ciudad de Tepic se encuentra el "Molino de Menchaca" y en la localidad de Francisco I. Madero se localiza el "Ingenio de Puga", siendo grandes fuentes de empleo para familias campesinas de todo el estado. El "Ingenio de Puga" es el molino con mayor capacidad de molienda de caña de azúcar del país, con más de 1,5 millones de toneladas anuales.

En materia de energía eléctrica destaca la Presa de Aguamilpa en el municipio de Tepic donde se práctica la pesca deportiva de lobina existen hoteles como Coras Lodge que ofrecen servicio de pesca y renta de lanchas. Además se realizan torneos de pesca deportiva de lobina, y la Presa del Cajón en Santa María del Oro. Además de la Presa La Yesca en el municipio de La Yesca inaugurada el día 6 de noviembre de 2012.

En Tepic se encuentra la Embotelladora del Nayar S.A. de C.V. (Coca Cola), propiedad de la empresa nayarita Alica. También tienen presencia la empresa Salsa "Huichol", "Aga", Embotelladora del Pacífico (Pepsi), Cervecería Corona Pacífico, Grupo Bimbo, entre muchas otras sucursales de empresas localizadas en la Ciudad Industrial, colonia de la Ciudad de Tepic. 
Además, se encuentran pequeñas industrias de origen nayarita impulsadas por el gobierno del Estado que fabrican productos manufacturados (alimentos, artesanías, utensilios, etc.) que se comercializan dentro del estado y a nivel nacional.

Lo primordial en el turismo del estado de Nayarit es su costa, que se le ha denominado para aumentar la promoción turística a nivel nacional e internacional como la Riviera Nayarit, que es la costa de Nayarit desde el municipio de San Blas (Compostela y Bahía de Banderas) hasta llegar a Puerto Vallarta, Jalisco, pero no son menos las playas ubicadas en Santiago Ixcuintla, Tecuala y Acaponeta.

La otra parte que destaca del turismo en Nayarit son sus bosques la parte más visitada es "La Noria" que son cabañas de descanso. Este lugar está situado a las afueras de su capital, Tepic. Otros bosques que destacan entre estos son los bosques que se expanden desde Acaponeta hasta Amatlan de Cañas.

Existen varios, entre ellos: Mexcaltitán, Santa María del Oro, Tepetiltic, El Pescadero, Francisco Villa y San Pedro Lagunillas.

Uno de los balnearios fluviales más conocidos de Nayarit es "El Manto", ubicado en la localidad de "El Rosario", municipio de Amatlán de Cañas; El Manto es un cañón natural por el cual corre el arroyo que emana de una manantial, que se ha ido adaptando como centro recreativo.

El estado cuenta con lugares como El Cora, municipio de San Blas; en ese lugar hay unas cascadas llamadas como el pueblo, cascadas El Cora.
Es un majestuoso lugar con cuerpos de agua y vegetación exuberante, entre el bosque y la selva con un clima tropical, es un destino lleno de vida perfecto para relajarse y conectarse con la naturaleza.

Un importante destino a donde alba te va a llevar turístico durante el temporal de lluvias es el El Venado en el municipio de Ruiz, ya que dicha población es punto de partida para tres importantes balnearios naturales, "El Salto", "El Malpaso" y "El Tenamache" y otro menos conocido "El Pozolillo", ellos toman su nombre de los arroyos que captan el agua pluvial de la región y la incorporan al río "San Pedro", estos destinos ofrecen un contacto amable y directo con la naturaleza la cual presenta allí en la zona rasgos selváticos con el característico clima cálido y húmedo.

Se puede acceder a ellos por la ciudad de Estación Ruiz la cual es cabecera municipal del municipio de Ruiz, los balnearios son visitados por turistas que se desplazan desde el centro y norte de la entidad e inclusive se ha observado que también los prefieren turistas de la zona sur del estado vecino de Sinaloa.

Otro destino el cual ofrece uno de los paisajes más bellos del estado es "La Tovara" manantial próximo a la bahía de Matanchén en el municipio de San Blas, en este lugar se puede disfrutar de un nacimiento cristalino de agua dulce la cual conforme escurre a los esteros de la zona se mezcla con el agua salobre.

Cañada del Tabaco. En Bahía de Banderas, en la comunidad de San Juan de Abajo, se encuentra el río Huicicila que se forma el lo alto de la Sierra de Vallejo y desemboca en el río Ameca. Forma un escenario espectacular de vegetación y algunas cascadas pequeñas.

En el municipio de Acaponeta se encuentra el manantial de aguas sulfurosas llamado "San Dieguito", con temperaturas de 38 °C. "El Manto", así como también las "Aguas Termales", estas últimas ubicadas a un costado del río Chiquito, ambos balnearios se encuentran situados en el Municipio de Amatlan de Cañas. En la comunidad de Amado Nervo (El Conde), se encuentra un balneario con aguas cálidas llamado "El Agua Caliente", un excelente lugar para visitar en invierno con la familia, también en las afueras de Tepic rumbo a Francisco I. Madero (Puga) existe un balneario también llamado "El agua caliente", en el cual se encuentra una gran alberca de agua caliente y 8 cálidas fosas de agua caliente.

Existe una zona arqueológica dentro del municipio de Ixtlán del Río. La zona arqueológica se llama "los Toriles" y se encuentran varias construcciones prehispánicas ceremoniales de la antigua tribu que habitaba la región y una de las principales construcciones es la pirámide circular, que está dedicada al Sol y la Luna. El asentamiento arqueológico se encuentra aproximadamente a 80 kilómetros de la ciudad de Tepic.

Existen además diseminadas por todo el estado ricas zonas arqueológicas las cuales han sido poco estudiadas, entre ellas la zona arqueológica de "Coamiles" en las proximidades de la localidad del mismo nombre la cual se sitúa en el municipio de Tuxpan; en esta zona es posible apreciar inscripciones en roca datadas de la era paleolítica.

Así mismo se puede nombrar a las inscripciones del mismo tipo localizadas en un pequeña localidad del municipio de Compostela denominada "Puerta de la Lima", en el arroyo próximo a esta población se localizan rocas con inscripciones que desafortunadamente están perdiéndose debido al poco cuidado que reciben.

Catedrales originarias de los siglos XVIII Y XIX como lo es la Catedral de Tepic, edificaciones coloniales, museos de arte, cultura, historia entre otras cosas de atractivo cultural en el Estado de Nayarit y otras más atractivos de cultura.

Cuenta con servicios de correos, telégrafos, teléfonos, radiodifusoras, periódicos, Internet y varios canales de televisión digital locales y repetidoras; TV por cable y satélite, telefonía celular, comunicación privada, onda corta y banda civil permisionada.

Las carreteras en el Estado de Nayarit son:

Actualmente se construyen las carreteras Tepic-Aguascalientes, Ruiz-Zacatecas, Tepic-Compostela.
El Estado cuenta con Centrales de Autobuses en los principales poblados del Estado como las cabeceras municipales, la Capital y otras localidades en las cuales se sitúan compañías de autobuses con destinos a nivel estatal y nacional.
Cada municipio cuenta con su propio servicio de transporte público y de taxis como ACASPEN, TUNAY, TRANSPORNAY entre otras.

En Nayarit hay estaciones del ferrocarril operadas por Ferromex.

En transporte marítimo cuenta con el Puerto de San Blas que es el principal puerto en el Estado de Nayarit ente otros.
En materia aérea cuenta con un aeropuerto que es el Aeropuerto Internacional Amado Nervo ubicado en la ciudad de Tepic que próximamente será internacional, actualmente se hacen vuelos a toda la República Mexicana y a unas ciudades en Estados Unidos.
También cuenta con Aeropistas que hacen vuelos dentro del Estado y la Sierra.

En todo el estado existen instituciones públicas y privadas: de preescolar, primaria, secundaria, telesecundaria, bachillerato, telepreparatoria, técnico, media superior, superior. Dentro de las instituciones de educación superior se encuentran:

En materia de salud, Nayarit cuenta con diversos centros de salud operados por la Secretaria de Salud del estado de Nayarit, IMSS, ISSSTE, Cruz Roja Mexicana y el DIF.

Dentro de la infraestructura de salud se encuentran:





La Constitución Política de Nayarit establece la forma de gobierno republicana, representativa y popular; dividiendo el poder público en tres: Ejecutivo, Legislativo y Judicial. Establece también que no podrán reunirse dos o más poderes en una sola persona ni el poder legislativo depositarse en un solo individuo.

El Poder Ejecutivo se deposita, por voto popular directo, en un ciudadano que se denomina "Gobernador del Estado Libre y Soberano de Nayarit".

Según la Ley Orgánica del Estado de Nayarit las Dependencias Estatales son las siguientes:


En cuanto a Entidades Paraestatales se cuenta con:


El Poder Legislativo está formado por el H. Congreso del Estado de Nayarit, integrado por 18 Diputados Locales, electos por mayoría relativa, y 12 por representación proporcional, por un periodo de 3 años.
El Poder Judicial se deposita en un Tribunal Superior de Justicia, Consejo de la Judicatura y en 42 Juzgados de Primera Instancia.

La base de la división territorial, de la administración pública y de la organización política estatales es el Municipio Libre, administrado por un Ayuntamiento de elección popular, cuyo primer regidor es el Alcalde o Presidente Municipal; radicando este en la Cabecera Municipal, localidad que también alberga a los síndicos y regidores del cabildo municipal, un secretario, un tesorero y varios servidores públicos más; con una duración en el cargo de 3 años.

Según la Constitución Política de Nayarit, el estado se integra por 20 municipios libres y soberanos con su Presidente y cabildos correspondientes.

Las mayores aportaciones musicales del estado, son provenientes de los grupos indígenas que habitan la región, como lo son los huicholes, coras y tepehuanes.

En la música huichola se emplean varios instrumentos autóctonos: el raberi, un instrumento de cuatro cuerdas similar al violín; el kanari, un tipo de guitarra; idiófonos de sacudimiento,como son los sartales de semillas que amarran en la parte inferior de morrales y las maracas de jícara; además de otros artefactos como cinturones de pezuña de venado y sartales de semillas o espinas.

Los coras emplean un instrumento musical llamado arco cara o tounamaci, que consiste en una calabaza hueca colocada boca bajo en cuya cúspide se sostiene la curva de un arco que el ejecutante fija con el pie. Un género musical de función ceremonial es el minuete cora, tocado con violines y guitarra, pero también existen los sones de tarima, interpretados con violín y guitarra.

Las artesanías de los grupos étnicos huicholes y coras han cobrado fama a nivel mundial debido a su elaboración, entre ellos podemos destacar las artesanías hechas con chaquiras y los bordados, los cuales son famosos por sus llamativos colores y patrones geométricos. Estas artesanías tienen una fuerte influencia religiosa, y son inspirados en animales, plantas, y otros elementos del entorno de Nayarit.




</doc>
<doc id="37828" url="https://es.wikipedia.org/wiki?curid=37828" title="La Gioconda">
La Gioconda

El Retrato de Lisa Gherardini, esposa de Francesco del Giocondo, más conocido como La Gioconda (La Joconde en francés) o La Mona Lisa, es una obra pictórica del polímata renacentista italiano Leonardo da Vinci. Fue adquirida por el rey Francisco I de Francia a comienzos del siglo XVI y desde entonces es propiedad del Estado francés. Se halla expuesta en el Museo del Louvre de París, siendo, sin duda, la «joya» de sus colecciones.

Su nombre, "La Gioconda" ("la alegre", en castellano), deriva de la tesis más aceptada acerca de la identidad de la modelo: la esposa de Francesco Bartolomeo de Giocondo, que realmente se llamaba Lisa Gherardini, de donde viene su otro nombre: "Mona" ("señora", en el italiano antiguo) "Lisa". El Museo del Louvre acepta el título completo indicado al principio como el título original de la obra, aunque no reconoce la identidad de la modelo y tan solo la acepta como una hipótesis.

Es un óleo sobre tabla de álamo de 77 × 53 cm, pintado entre 1503 y 1519, y retocado varias veces por el autor. Se considera el ejemplo más logrado de "sfumato", técnica muy característica de Leonardo, si bien actualmente su colorido original es menos perceptible por el oscurecimiento de los barnices. El cuadro está protegido por múltiples sistemas de seguridad y ambientado a temperatura estable para su preservación óptima. Es revisado constantemente para verificar y prevenir su deterioro.

Por medio de estudios históricos se ha determinado que la modelo podría ser una vecina de Leonardo, que podrían conocerse sus descendientes y que la modelo podría haber estado embarazada, por la forma de esconder que tienen sus manos. Pese a todas las suposiciones, las respuestas en firme a los varios interrogantes en torno a la obra de arte resultan francamente insuficientes, lo cual genera más curiosidad entre los admiradores del cuadro.

La fama de esta pintura no se basa únicamente en la técnica empleada o en su belleza, sino también en los misterios que la rodean. Además, el robo que sufrió en 1911, las reproducciones realizadas, las múltiples obras de arte que se han inspirado en el cuadro y las parodias existentes contribuyen a convertir a "La Gioconda" en el cuadro más famoso del mundo, visitado por millones de personas anualmente.

Leonardo da Vinci nació en el caserío de Anchiano del municipio de Vinci, en Italia. Fue fruto de la relación ilegítima del notario ser Piero y de su sirvienta, Catarina Vacca. A los 14 años entró en el prestigioso taller del pintor florentino Andrea Verrocchio, donde se formó como artista junto a Sandro Botticelli y Perugino. Desarrolló el estudio de las matemáticas, la geometría, la arquitectura, la perspectiva y todas las ciencias de la observación del medio natural, las cuales se consideraban indispensables en la época. Como educación complementaria, también estudió arquitectura e ingeniería. Leonardo fue un humanista renacentista, destacado en múltiples disciplinas. Sirvió a personas tan distintas e influyentes como a Lorenzo de Médici, al duque de Sforza, a los soberanos de Mantua y al rey Francisco I de Francia.

Debido a la minuciosidad de su técnica y también a sus muchas otras ocupaciones como inventor y diseñador, la producción pictórica de Leonardo es extremadamente escasa: los expertos reducen las obras de autoría relativamente segura a apenas una veintena, y de ellas muy pocas cuentan con pruebas documentales concluyentes. Entre sus pinturas más destacadas están "La Virgen de las Rocas", "La dama del armiño", el mural de "La última cena" y, la más famosa de todas: "La Gioconda" o "La Mona Lisa".

"La Gioconda" ha sido considerada como el cuadro más famoso del mundo. Su fama se debe probablemente a las múltiples referencias literarias, a las diversas hipótesis sobre la identidad de la protagonista y al espectacular robo del que fue objeto el 21 de agosto de 1911.

Es además la última gran obra de Da Vinci. Después de terminar el cuadro, Leonardo llevó su obra a Roma y luego a Francia, donde la conservó hasta su fallecimiento en su residencia del castillo de Clos-Lucé.

Tras la muerte del rey, la obra pasó a Fontainebleau, luego a París y más tarde al palacio de Versalles. Con la Revolución francesa llegó al Museo del Louvre, lugar donde se trasladó en 1797. En 1800 Napoleón Bonaparte ordenó sacar el cuadro del museo y colocarlo en su dormitorio del palacio de las Tullerías hasta que lo devolvió al museo en 1804. Allí se alojó definitivamente, salvo un breve paréntesis durante la Segunda Guerra Mundial, cuando el cuadro fue custodiado en el castillo de Amboise y posteriormente en la abadía de Loc-Dieu.

Hasta 2005 se ubicó en la "Sala Rosa" del Louvre, y desde entonces se encuentra en el "Salón de los Estados". Es pertinente decir que la mayoría de datos acerca del cuadro se conocen gracias al trabajo biográfico del pintor Giorgio Vasari, contemporáneo de Leonardo.

Leonardo dibujó el esbozo del cuadro y después aplicó el óleo diluido en aceite esencial. La técnica empleada, conocida con el término italiano de "sfumato", consiste en prescindir de los contornos netos y precisos típicos del "quattrocento" y envolverlo todo en una especie de niebla que difumina los perfiles y produce una impresión de inmersión total en la atmósfera, lo que da a la figura una sensación tridimensional.

El cuadro se pintó sobre una tabla de madera de álamo recubierta por varias capas de enlucido. Se conserva en una urna de cristal de 40mm de espesor a prueba de balas, tratada de manera especial para evitar los reflejos. La cámara que alberga el cuadro está diseñada para mantener una temperatura constante de 20 °C y 50 % de humedad relativa, con lo que se busca garantizar las condiciones óptimas para la estabilidad de la pintura.

La pintura tiene una grieta vertical de 12 centímetros en la mitad superior, tal vez debida a la eliminación del marco original, si bien un estudio actual con rayos infrarrojos revela que la grieta puede ser tan antigua como la misma tabla. Dicha grieta fue reparada entre mediados del siglo XVIII y principios del XIX mediante dos piezas metálicas en forma de mariposa fijadas por el reverso. De ellas, una se soltó posteriormente. En la actualidad, se ha determinado que la fisura es estable y no ha empeorado con el tiempo.

Sin embargo, para descartar cualquier peligro, en 2004 se constituyó un equipo de curadores franceses, que vigilan permanentemente el estado de la pintura, previniendo cualquier alteración provocada por el tiempo.

En este retrato la dama está sentada en un sillón y posa sus brazos en los brazos del asiento. En sus manos y sus ojos puede verse un ejemplo característico del dolor y del juego que el pintor hace con la luz y la sombra para dar sensación de volumen.

Aparece sentada en una galería, viéndose a los lados, cortadas, las bases de unas columnas.

La galería se abre a un paisaje tal vez inspirado en las vistas que Leonardo pudo divisar en los Alpes, durante su viaje a Milán, aunque una última investigación reveló que el fondo podría corresponder a la ciudad de Bobbio, en la región de Emilia-Romaña. Anteriormente, se pensaba que el paisaje, que posee una atmósfera húmeda y que parece rodear a la modelo, estaba en Arno o en una porción del lago de Como, sin haber llegado a conclusiones definitivas.

Se ha intentado muchas veces compaginar las dos mitades del paisaje que aparece tras la modelo, pero la discordancia entre ambos lados es tan grande que no permite diseñar una imagen continuada. El lado izquierdo parece estar más bajo que el derecho, entrando en conflicto con la física, puesto que el agua no puede permanecer quieta si existe desnivel en el terreno. A este respecto el historiador de arte E.H. Gombrich escribe:

En medio del paisaje aparece un puente, conocido en Bobbio como puente Gobbo o el puente Vecchio, y que muestra un elemento de civilización que podría estar señalando la importancia de la ingeniería y la arquitectura. La ubicación geográfica del puente fue posible gracias a un códice que dejó Leonardo da Vinci, en el que se muestra la escena en la que se pintó. Una crecida, ocurrida años más tarde en el río Trebbia, destruyó el puente, que posteriormente fue reconstruido.
La modelo carece de cejas y pestañas, posiblemente por una restauración demasiado agresiva en siglos pasados, en la cual se habrían eliminado las veladuras o leves trazos con que se pintaron. Vasari, en efecto, sí habla de cejas: «En las cejas se apreciaba el modo en que los pelos surgen de la carne, más o menos abundantes y girados según los poros de la piel; no podían ser más reales». Según otros expertos, las cejas depiladas eran habituales en las damas de alcurnia florentinas; o Leonardo evitó pintar las cejas y las pestañas para dejar su expresión más ambigua, o tal vez porque nunca llegó a terminar la obra.

La dama dirige la mirada ligeramente a su izquierda y muestra una sonrisa considerada enigmática. Cuenta Vasari que: 

Sobre la cabeza lleva un velo, signo de castidad y atributo frecuente en los retratos de esposas.

El brazo izquierdo descansa sobre el de la butaca. La mano derecha se posa sobre la izquierda. Esta postura transmite una impresión de serenidad y de que el personaje retratado domina sus sentimientos.

La técnica de Leonardo da Vinci se aprecia con más facilidad gracias a la «inmersión» de la modelo en la atmósfera y el paisaje que la rodean, potenciada además por el avance en la «perspectiva atmosférica» del fondo, que sería el logro final del Barroco, donde los colores tienden al azulado y la transparencia, aumentando la sensación de profundidad.

La conservación de la obra es mediana, con un craquelado bastante evidente en toda la superficie y una fisura bastante importante que, desde el borde superior, desciende en vertical sobre la cabeza del personaje. Esta grieta se mantiene estable y no es previsible que empeore, gracias a que la obra se conserva en un espacio climatizado. La deficiencia de conservación más criticada es la suciedad que enmascara los colores; la pintura está tapada por capas de barniz que han amarilleado con el tiempo, efecto habitual en las sustancias de origen natural. En siglos pasados, cuando no existían los disolventes, la opacidad de las pinturas antiguas se paliaba o disimulaba aplicando nuevas capas de barniz. El cuadro de Leonardo acumula varias, y los responsables del Louvre se resisten a eliminarlas por miedo a alterar el aspecto de la obra. La hipótesis de una próxima restauración de "La Gioconda" se ve ahora todavía más remota, tras una polémica suscitada en 2011 por la limpieza de otra obra del artista en el Louvre, "La Virgen, el Niño Jesús y Santa Ana", una intervención considerada abusiva por algunos expertos y que provocó la dimisión de dos técnicos contrarios a ella.

Mediante un programa informático se ha recreado el colorido que debería tener la obra si se eliminasen las capas de suciedad. La restauración en 2011-2012 de la copia conservada en el Museo del Prado (Madrid), pintada simultáneamente en el taller del maestro, puede ayudar a imaginar el aspecto que originalmente tuvo la obra del Louvre.

Durante varios siglos los interrogantes sin respuesta acerca de la obra de Leonardo han ido creciendo, originando apasionadas polémicas en muchos autores e investigadores. Frente a la gran cantidad de preguntas, las respuestas no suelen ser demasiado convincentes, por lo que los debates siguen abiertos. Especialmente durante los siglos XIX y XX, las teorías acerca del origen de la modelo, la expresión de su rostro, la inspiración del autor y otras tantas, han tomado gran protagonismo y obligan a un análisis histórico y científico profundo.

En el siglo XVI Leonardo da Vinci pintó a "Mona Lisa" buscando el efecto de que la sonrisa desapareciera al mirarla directamente y reapareciera solo cuando la vista se fija en otras partes del cuadro. El juego de sombras refuerza la sensación de desconcierto que produce la sonrisa. No se sabe si en verdad sonríe o si muestra un gesto lleno de amargura. Sigmund Freud interpretó la sonrisa de la Gioconda como el recuerdo latente que había en Leonardo de la sonrisa de su madre.
Margaret Livingstone, experta en percepción visual, desveló en el Congreso Europeo de Percepción Visual que se celebró en 2005 en La Coruña, que la enigmática sonrisa es «una ilusión que aparece y desaparece debido a la peculiar manera en que el ojo humano procesa las imágenes». Livingstone recalca que los artistas llevan mucho más tiempo estudiando la percepción visual humana que los mismos médicos especialistas en el tema.

El ojo humano tiene una visión fotópica, retiniana o directa, y otra escotópica o periférica. La primera sirve cuando se trata de percibir detalles, pero no es apta para distinguir sombras, que es la especialidad de la segunda. Leonardo pintó la sonrisa de "Mona Lisa" usando unas sombras que se ven mejor con la visión periférica. Como ejemplo para ilustrar el efecto, uno puede concentrar la mirada en una sola letra sobre una página impresa y comprobar lo difícil que le resulta reconocer el resto de las letras.

En otro orden de cosas muy diferente, y tratando de averiguar el estado de ánimo de la modelo durante el posado, se utilizó un software especializado en la "medición de emociones", el cual fue aplicado a la pintura para obtener datos relevantes acerca de su expresión. La conclusión alcanzada por el programa, es que Mona Lisa está un 83 % feliz, un 9 % disgustada, un 6 % temerosa y un 2 % enfadada. El software trabaja sobre la base de analizar rasgos tales como la curvatura de los labios o las arrugas producidas alrededor de los ojos. Tras obtener las mediciones, las compara con una base de datos de expresiones faciales femeninas, de la que obtiene una expresión promedio.

Un grupo de investigadores del Consejo Nacional de Investigaciones de Canadá que examinó la obra en 2004 utilizó un escáner de infrarrojos en tres dimensiones, cuyos resultados, de escasa entidad, fueron publicados el 26 de septiembre de 2006.

El uso de dicha técnica, que permite una resolución 10 veces más fina que el cabello humano, permitió a los investigadores apreciar detalles hasta ahora desconocidos. Han opinado que el velo de gasa fina y transparente, enganchado al cuello de la blusa, era una prenda que solían llevar las mujeres embarazadas o que habían dado a luz recientemente. Entre sus peculiares conclusiones, el estudio consideró que el peso de la modelo era de 63 kilos y su estatura de 1,68 metros así como que llevaba el pelo recogido en un moño cubierto por un bonete detrás de la cabeza, y que no aparece ningún mensaje secreto en ninguna de las capas de la pintura, como se contaba en la novela "El código Da Vinci".

Por su parte, el doctor Julio Cruz Hermida, de la Universidad Complutense de Madrid, afirma que la modelo padecía bruxismo (rechinar de los dientes), alopecia (caída del cabello) y principios de la enfermedad de Parkinson.

Diversas hipótesis se han generado en torno a la identidad de la modelo.

El pintor y biógrafo Giorgio Vasari escribió en 1550:
En 1625, Cassiano dal Pozzo vio la obra en Fontainebleau y escribió sobre ella:

Tomando como base estos testimonios se ha identificado a la modelo con Lisa Gherardini, la esposa del acaudalado comerciante Francesco del Giocondo.

Sin embargo, en 1517, antes del escrito de Vasari, Antonio de Beatis visitó a Leonardo en el castillo de Cloux y mencionó tres cuadros suyos, uno de ellos de una dama florentina hecho del natural a petición de Juliano II de Médicis.

Aunque Antonio de Beatis podría haber visto una tabla distinta, este testimonio parece discrepar con los de Vasari y Cassiano del Pozzo, por lo que algunos han supuesto que la modelo fue en realidad una amiga o amante de Juliano II de Médicis.
Algunas otras teorías poco difundidas afirman que podría tratarse de Isabel de Aragón, a quien Leonardo dibujó a lápiz para luego hacer un óleo; o de Constanza d'Avalos, duquesa de Francaville, mencionada en un poema de la época, donde se lee que Leonardo la pintó «bajo el hermoso velo negro»; o de Isabella Gualanda, una dama napolitana. Según esta última teoría, Leonardo habría pintado el retrato en Roma por encargo de Juliano de Médicis y habría reciclado para ello un retrato inconcluso que había hecho a Lisa Gherardini.

Otras propuestas han sido que la modelo pudo ser una amante del propio Leonardo, un adolescente vestido de mujer, un autorretrato del autor en versión femenina o incluso, una simple mujer imaginaria. A este respecto, Sigmund Freud sugirió que la pintura reflejaba una "preocupante masculinidad". Estudios que apoyan la teoría de la identidad masculina del modelo lo identifican como Gian Giacomo Caprotti, conocido como "Il Salai".

Hay estudiosos que creen que el tema de la pintura es la madre de Leonardo, Caterina (1427-1495).

En el año 2005, Armin Schlechter descubrió una nota de Agostino Vespucci en el margen de un libro de la colección de la biblioteca de la Universidad de Heidelberg, que confirmaba con certeza la creencia tradicional de que la modelo del retrato era Lisa. En esta acotación, Vespucci, quien era un amigo cercano de Leonardo da Vinci, compara a Apeles, gran pintor de la Antigüedad, con Leonardo, y hace referencia a tres obras en las que estaba trabajando en esas fechas: el retrato de Lisa del Giocondo, otro de Santa Ana y el mural de "La batalla de Anghiari". Esta pequeña anotación data de octubre de 1503, aproximadamente 47 años antes de las referencias realizadas por Giorgio Vasari. Además, el libro donde se realizó el comentario sobre "“Mona Lisa”" pertenece al autor Marco Tulio Cicerón, y particularmente esta edición fue publicada en 1477.

Por otra parte, en los archivos de impuestos de 1480 puede verificarse la identidad, paradero y lugar de nacimiento de la modelo. Nació el 15 de junio de 1479 y murió el 15 de julio de 1542, a los 63 años, en el convento de Santa Úrsula de Florencia. Según el historiador Giuseppe Pallanti, que trata el tema en su libro "La historia de Mona Lisa", Gherardini ingresó en el convento cuatro años después de quedar viuda, donde ya era monja su hija Marietta.

Basándose en estos datos, el investigador genealogista italiano Domenico Savini asegura que existen descendientes de Gherardini; se trata de Natalia e Irina Strozzi, hijas del príncipe Girolamo de Toscana. En el supuesto de que la modelo de Leonardo fuera la mujer que falleció en el convento, el médico forense Maurizio Seracini se ha ofrecido para buscar el cadáver y hacer un análisis de ADN para establecer el parentesco de los Strozzi con Gherardini.

Unido a dichos elementos, documentos oficiales del censo de la época confirman que el padre de Leonardo da Vinci vivía exactamente enfrente de la familia de Gherardini. El historiador supone, sin mayores pruebas, que el retrato fue un regalo de Giocondo a su esposa por motivo de su segundo embarazo, a los veinticuatro años de edad. Existen detractores de las teorías expuestas por Pallanti, pero sus opiniones son mayoritariamente aceptadas.

Para saciar la curiosidad histórica acerca de la veracidad de las teorías vertidas, en 1987 se realizaron los primeros estudios, superponiendo un autorretrato de Leonardo a la pintura de la Mona Lisa; el resultado fue una gran similitud en las dimensiones y rasgos físicos. Los detractores de dicha investigación alegan que, dado que el autor es el mismo, los trazos son similares y por eso generan confusión. Lillian Swartz y Gerald Holzman, los directores de dicha prueba, aseguran que el autor se autorretrató, dándose apariencia de mujer.

Tanta ha sido la obsesión por esclarecer la identidad de la retratada, que el doctor Matsumi Suzuki, investigador japonés, reconstruyó el cráneo de la Gioconda mediante un análisis óseo, y a partir de dicho cálculo generó la posible voz de la modelo. El investigador asegura que la reproducción de la voz es fiable en un noventa por ciento. También ha realizado la misma simulación para el autor de la obra, de la cual desconfía un poco porque la barba reflejada en los autorretratos esconde algunos detalles importantes.

El título oficial de la obra, según el Museo del Louvre, es "Retrato de Lisa Gherardini, esposa de Francesco del Giocondo", aunque el cuadro es más conocido como "La Gioconda" o "Mona Lisa".

Este título aparece documentado por primera vez mucho después de la muerte de su autor. Con respecto al nombre de "Mona Lisa", más usado en fuentes anglosajonas, "Monna" es el diminutivo en italiano de "Madonna", que quiere decir "Señora".

El 21 de agosto de 1911, el carpintero italiano Vincenzo Peruggia (exempleado del Museo del Louvre) llegó al Museo del Louvre a las 7 de la mañana, vestido con un blusón de trabajo blanco como los utilizados por el personal de mantenimiento del museo, descolgó el cuadro y a continuación, en la escalera Visconti, separó la tabla de su marco, abandonando este último. A continuación salió del museo con el cuadro escondido bajo su ropa, que colocó posteriormente en una valija.

Unos años antes el museo había sufrido el robo de otras varias piezas, lo cual hizo suponer a la policía que ambos acontecimientos estaban relacionados. Guillaume Apollinaire y Pablo Picasso se convirtieron en sospechosos puesto que se los había relacionado con la desaparición de unas piezas de escultura del museo, además de por unas declaraciones en las que Apollinaire apoyaba la propuesta formulada por el futurista Marinetti de quemar los museos para dejar paso al nuevo arte. Posteriormente se demostró que ambos eran inocentes. Al mismo tiempo que se realizaban las investigaciones sobre el robo, se capturó al aventurero belga Honoré-Joseph Géry Pieret, quien confesó ser el autor de otro robo acaecido en 1906, pero no del de "La Gioconda".

Durante la ausencia de la obra, se batió el récord de visitantes al museo; acudían a apreciar el hueco dejado en la pared por el cuadro que había sido hurtado.
La pintura fue recuperada dos años y ciento once días después del robo, tras la captura de Peruggia. El detenido intentó vender el cuadro original al director de la Galleria degli Uffizi de Florencia, Alfredo Geri, quien se hizo acompañar de la policía. Peruggia alegó que su intención era devolver la obra a su verdadera patria, y que él solo era víctima de un estafador; los tribunales de justicia lo condenaron a un año y quince días de prisión que luego redujeron a siete meses y nueve días. Antes de regresar al museo, la pintura se exhibió en Florencia, Roma y Milán.

En 1932, el periodista Karl Decker publicó una información según la cual el autor intelectual del robo habría sido un comerciante argentino llamado Eduardo Valfierno, que habría fallecido en 1931, con el fin de vender seis copias falsas, e incluso proporcionó los nombres de los presuntos coleccionistas estafados, pero la veracidad de este relato no pudo ser probada.

El 30 de diciembre de 1956, el boliviano Ugo Ungaza Villegas arrojó una piedra contra la "Mona Lisa" durante una exhibición en el Louvre. Lo hizo con tanta fuerza, que destrozó la vitrina, y desprendió un trozo de pigmento del codo izquierdo. La pintura estaba protegida por un cristal, ya que pocos años antes, un hombre que declaró estar enamorado de la misma, la había cortado con una cuchilla y la había intentado robar.Desde entonces, se instaló un cristal a prueba de balas para proteger a la pintura de nuevos ataques.

Posteriormente hubo otros incidentes. El 21 de abril de 1974, mientras el cuadro estaba expuesto en el Museo Nacional de Tokio, una mujer le arrojó pintura roja, como protesta por la ausencia de accesos al museo para personas discapacitadas. Y el 2 de agosto de 2009, una mujer rusa, aturdida por la denegación de su solicitud de ciudadanía francesa, arrojó una taza de cerámica comprada en la tienda del Louvre, haciéndose añicos contra el cristal. Afortunadamente, el cuadro no sufrió desperfectos en ambos casos.

La "Mona Lisa" ha adquirido un estatus de icono cultural. Son numerosas sus reproducciones y utilización en la publicidad, objetos cotidianos y también como referencia cultural. Algunas incluyen:






</doc>
<doc id="37835" url="https://es.wikipedia.org/wiki?curid=37835" title="Unión Postal Universal">
Unión Postal Universal

La Unión Postal Universal (en francés: "Union postale universelle"; UPU) es un organismo especializado de las Naciones Unidas. Tiene como objetivo 
afianzar la organización y mejorar los servicios postales, participar en la asistencia técnica postal que 
soliciten los países miembros y fomentar la colaboración internacional en materia postal. La UPU fija tarifas, 
límites máximos y mínimos de peso y tamaño, así como las condiciones de aceptación de la correspondencia, 
establece reglamentos aplicables a ésta, y a objetos cuyo transporte requiere preocupación especial, como 
sustancias infecciosas y radiactivas. 
Su lengua oficial es el francés y su sede se encuentra en 
la ciudad de Berna, Suiza. Actualmente cuenta con 191 países miembros.

Durante los siglos XVII y XVIII, el intercambio de correspondencia entre los diferentes países era regularizado por acuerdos entre cada par de naciones, pero en el siglo XIX esta red de acuerdos se hizo tan compleja que impedía que los envíos se entregasen con rapidez. A causa de esto, se empezaron a implementar varios proyectos, entre los cuales destacó el de Sir Rowland Hill, creador de la estampilla, quien introdujo un sistema para uniformizar el tamaño de las cartas. 

En 1863, a solicitud del General Montgomery Blair, de la administración postal de Estados Unidos, se convocó a una conferencia en París a quince delegados europeos y de América para acordar los principales tratados postales, pero solo llegaron a algunos acuerdos separados y no pudieron establecer un sistema postal universal. Para hacer esto se le dejó la tarea a Heinrich von Stephan quien convocó una nueva conferencia en Berna el 15 de septiembre de 1874, en la cual propuso hacer una organización de regularización del correo a escala mundial. Gracias a esto, el día 9 de octubre del mismo año, en virtud del Tratado de Berna, nacía la Unión Postal General y esa fecha es hoy en día el día mundial del servicio postal. El nombre fue cambiado a Unión Postal Universal en 1878. Posteriormente pasó a ser un organismo especializado de las Naciones Unidas, por un acuerdo que entró en vigor el 1 de julio de 1948.

La UPU mantiene relaciones cercanas con varios programas, como el Programa de las Naciones Unidas para el Control de Drogas (UNDCP), el Programa de las Naciones Unidas para el Desarrollo (PNUD) y el Programa de las Naciones Unidas para el Cuidado del Medio Ambiente (UNEP).
La organización también coopera con diferentes organizaciones, como la Unión Internacional de Telecomunicaciones (ITU), la Organización Internacional de Aviación Civil (ICAO), la Organización Internacional del Trabajo (ILO), la Organización Mundial del Comercio (WTO) y el Banco Mundial.
Para facilitar el traslado de los mensajes, la UPU trabaja en conjunto con la Organización Internacional del Transporte Aéreo (IATA), la Organización Internacional para la Estandarización (ISO), la Organización Mundial de Aduanas (WCO) y la Organización Internacional de Policía Criminal (INTERPOL).

El director y el subdirector de la UPU es elegido todos los años por el consejo administrativo, sin posibilidad de hacer dos mandatos sucesivos. Este consejo está constituido por dos personas de América, dos de África, dos de Asia, Oceanía e islas del Pacífico, dos de Europa y una del este de Europa.

Este consejo se reúne cuatro veces al año. Las funciones de este grupo, además de la elección del presidente, son:


El Fondo de Calidad del Servicio (también conocido por su sigla en inglés, QSF) se encarga de mejorar el servicio postal especialmente en los países en vías de desarrollo y financiar proyectos con este fin. 

En el congreso de la UPU en el año 2009, en Bucarest, la QSF dividió a los países miembros en diferentes grupos dependiendo del desarrollo de estos:


Además, se decidió aumentar la inversión en los países subdesarrollados y reducirla en los NCCs.

El Servicio Postal Universal (SPU) es el conjunto de servicios postales básicos mínimos de calidad disponibles a todos los habitantes de un territorio nacional en todo momento, en cualquier lugar y a un valor accesible.

El SPU está contenido en la resolución C 103/1999 aprobada por el XXII Congreso de la unión postal universal (UPU) realizado en Pekín (China) en 1999. Esta resolución define el SPU como la garantía de presentación de un servicio postal universal, que permite a los clientes enviar y recibir mercaderías y mensajes desde y hacia cualquier parte del mundo. Para ello los países miembros de la UPU deben procurar el acceso a SPU que corresponda a una oferta de servicios postales básicos de calidad y a precios rentables y accesibles.

Para facilitar el desarrollo de una norma de seguridad y las mejores prácticas, la UPU ha establecido el Grupo de Seguridad Postal (PSG).

Durante los últimos 15 años, este grupo estratégico se ha guiado por el lema de Seguridad Postal. El grupo está compuesto por expertos en seguridad de los 77 países miembros de PSAG y se carga con el desarrollo de la seguridad mundial y regional de estrategias para ayudar a todo el mundo puestos en sus misiones de seguridad. A través de iniciativas de formación, misiones de consultaría y los programas de prevención, el PSG se esfuerza por proteger a los empleados y activos.

La Oficina Extraterritorial de Intercambio es una oficina que trabaja con operarios fuera de su territorio nacional y está establecido para tratamientos comerciales y negocios en mercados de otro país.

El papel del sector postal en la sociedad de la información, y particularmente su contribución a reducir la divisoria digital, ha sido el tema de discusión en curso de la Unión Postal Universal desde la primera fase de la convención del WSIS en Ginebra en 2003. Conforme al plan de Ginebra de acción adoptado durante la primera fase del WSIS en 2003, los objetivos del UPU son:


En respuesta a la globalización, la liberalización, la desregulación, la apertura de la competición y los avances tecnológicos, los operadores postales han intentado las nuevas oportunidades que van más allá de sus actividades centrales y de sus mercados geográficos tradicionales. Se han reestructurado los negocios, se han formado alianzas, se han hecho adquisiciones estratégicas, actividades comerciales y se han modernizado los productos, las instalaciones y las operaciones.

El sector postal está experimentando reformas importantes, en los países industrializados (IC) y en los países en vías de desarrollo (DCS). Esto pide una comprensión de los desafíos económicos que se presentan de tales reformas, para hacer un servicio postal universal, proporcionando un servicio de calidad con precios rentables y accesibles a todos.

Aunque la reforma postal en países industrializados haya sido el tema de muchos estudios económicos profundizados, ha habido poca investigación económica en los países en vías de desarrollo. El nuevo programa postal de economía de la UPU, por lo tanto, ha realizado una investigación económica dirigida a la comprobación de las razones del desarrollo postal desigual en el sector y ha presentando los modelos viables para el crecimiento en el sector postal de estos países.

Totalmente con el fin de realizar grandes cambios en los últimos años se ha observado un crecimiento continuo en las figuras relacionadas con el comercio electrónico. Mientras que la gente hace más cómoda las compras y la penetración de la banda ancha continúa aumentando, el servicio postal debe progresar constantemente en el porvenir.

Al estar presente en cada país del mundo, el servicio postal está en una posición ideal para asistir a las pequeñas y medianas empresas y a los países menos desarrollados para desarrollar sus actividades del comercio electrónico.




</doc>
<doc id="37838" url="https://es.wikipedia.org/wiki?curid=37838" title="ABC">
ABC

El acrónimo ABC hace referencia a varios artículos:









</doc>
<doc id="37842" url="https://es.wikipedia.org/wiki?curid=37842" title="Colegio de Profesores de Santa Lucía">
Colegio de Profesores de Santa Lucía

El Colegio de Profesores de Santa Lucía (en inglés: "Saint Lucia Teachers College") era una institución educativa fundada en 1963 en "San Souci" y posteriormente trasladada a "Morne Fortune" en 1968. En 1985 se unió a otras dos instituciones técnicas y vocacionales para formar el Colegio Comunitario Sir Arthur Lewis de la ciudad de Castries, Santa Lucía.

Entre sus graduados más destacados sobresalen Pearlette Louisy, Gobernadora General de Santa Lucía y Kenneth Anthony, Primer Ministro de la isla.



</doc>
<doc id="37849" url="https://es.wikipedia.org/wiki?curid=37849" title="Sonora">
Sonora

Sonora , es uno de los treinta y un estados que, junto con la Ciudad de México, conforman México. Su capital y ciudad más poblada es Hermosillo. Está ubicado en la región noroeste del país, limitando al norte con Arizona (Estados Unidos), este con Chihuahua, al sur con Sinaloa y al oeste con el mar de Cortés o golfo de California (océano Pacífico) y con Baja California. Con 179 503 km² es el segundo estado más extenso —por detrás de Chihuahua— y con 14,83 hab/km², el quinto menos densamente poblado, por detrás de Campeche, Chihuahua, Durango y Baja California Sur. Fue fundado el 10 de enero de 1824. 

El territorio está conformado por cuatro provincias fisiográficas: la Sierra Madre Occidental, las Sierras y Valles Paralelos en el centro, el desierto y la costa del golfo de California. Está compuesto principalmente por desiertos semiáridos y praderas, en donde solo en las elevaciones más altas se presenta suficiente lluvia para sostener otros tipos de vegetación. 

Es hogar de ocho pueblos indígenas, entre ellos los mayos, los yaquis y los seris. Ha sido económicamente importante por su agricultura, su ganadería (en especial de res) y su minería desde el periodo colonial, y por su situación como estado fronterizo desde la Invasión estadounidense en México. Después de la venta de La Mesilla, Sonora perdió más de un cuarto de su territorio. Desde el siglo XX hasta el presente, la industria, el turismo y los agronegocios han dominado la economía, lo que ha atraído migración de otras partes de México.

Se divide en 72 municipios. Su capital es Hermosillo. Otras localidades importantes son Ciudad Obregón, Navojoa, Caborca, Guaymas, Puerto Peñasco, Nogales, Agua Prieta, San Luis Río Colorado, Nacozari de García y Cananea.

El estado de Sonora cuenta con una superficie territorial, forma parte de los Estados Unidos Mexicanos, se encuentra ubicado en su región noroeste y ocupa el segundo lugar en extensión de entre todas las entidades federativas de la República mexicana, con una porción de 9.2% del total de la superficie.
Su situación geográfica, se sitúa entre los 32°29' y los 26°14' de latitud Norte y entre los 108°26' y los 105°02' de longitud Oeste del meridiano de Greenwich. Limita al norte con los Estados Unidos de América, al sur con el estado de Sinaloa, al este con Chihuahua y al oeste con el golfo de California y la Baja California. Su fisiografía está constituida en su mayoría por llanuras y sierras. 
La costa con el golfo de California se formó por la separación de la península de Baja California entre diez y doce millones de años atrás. El golfo de California, en sí, se formó entre 5.5 y 6 millones de años atrás. El estado tiene 816 kilómetros de costa, todas en el golfo, son aguas relativamente poco profundas y tranquilas. Hay playas en la mayor parte de la costa, algunas de arena fina y blanca.

La región del desierto está principalmente compuesta de matorrales, a excepción de la zona de Altar donde es arenoso. Constituye el desierto más rico y variado en vida y comunidades bióticas de toda América. La zona del desierto de Altar alberga el campo volcánico El Pinacate.

Las sierras y valles paralelos del centro del estado tienen entre 50 y 120 kilómetros de anchura, esparcidos entre la Sierra Madre y el golfo de California. Los patrones climáticos traen humedad y nubes hacia al este desde el océano Pacífico, lo que forma ríos y arroyos que cruzan el área de los valles y se vacían en el golfo. Estos ríos han traído sedimento de la roca volcánica desde la Sierra Madre y este ha enterrado la mayoría de las montañas y cerros del centro del estado, convirtiéndolas en planicies. Estos suelos son ricos en minerales y tienen cientos de metros en grosor en algunos lugares, lo que hace muy fértil a la región, solo que carece de agua. 
La Sierra Madre Occidental domina el este del estado, ésta cuenta con temperaturas menos extremas y, debido a la altitud, más lluvia. A medida que las masas de aire húmedo entran a la región desde el océano y llegan a las montañas, se enfrían y esto lleva a que haya precipitación, en su mayoría lluvia pero a veces nieve en las regiones más altas como [[Yécora (Sonora)|[[Yécora]], [[Cananea]] y [[Nogales]]. Este proceso elimina la mayoría de la humedad del aire y la lleva a varios ríos y arroyos que se vacían en el Golfo así como en mantos acuíferos subterráneos en las llanuras.

[[Archivo:Valle_de_Sonora.jpg|thumb|250px|left|Valle de Sonora.]]

Durante el [[Plioceno]], la separación de Baja California y el desarrollo del golfo de California redujeron de manera drástica la humedad que entraba a Sonora lo que llevó a una aridez regional severa tanto en Sonora como en Baja California. Esto creó comunidades de [[matorral xerófilo]] y el desarrollo de especies únicas para esta región. 
Noventa por ciento del estado tiene condiciones desérticas o áridas. Los otros tipos de clima están restringidos a las áreas del estado con altitudes mayores como el área de Yécora, las montañas al norte de Cananea, y una franja a lo largo del sureste del estado con la frontera con Chihuahua. 
La temperatura alta promedio varía entre en [[Yécora (Sonora)|Yécora ]] a los en el municipio de Navojoa. La temperatura baja promedio varía entre 5.9 en Yécora a en el municipio de Hermosillo. En invierno llegan masas de aire frío del norte al estado, esto puede producir temperaturas congelantes y vientos fuertes por la noche en los lugares más elevados, pero la temperatura puede volver a ser hasta durante el día. Casi nunca ocurren temperaturas congelantes en las tierras bajas. En febrero de 2011, el gobierno mexicano registró una temperatura baja histórica en Yécora de . 
La [[Precipitación (meteorología)|precipitación]] es por temporada y ocurre casi siempre en las elevaciones más altas. En las tierras calientes áridas o semiáridas, la evaporación supera por mucho la precipitación. La zona más árida de México, el [[gran desierto de Altar]], se encuentra en este estado. El este del estado, dominado por la [[Sierra Madre Occidental]], tiene temperaturas menos extremas y con relativamente más lluvia debido a la altitud.

[[Archivo:Sonoran_desert_sunset.jpg|250px|right|thumb|[[Ocaso|Atardecer]] sobre el [[desierto de Sonora]].]]

El estado cuenta con una gran variedad de especies de flora, predominan los matorrales en la planicie costera, al noroeste y región central. Hay selvas en la parte sureste del estado, seguidas por el matorral [[Clima subtropical|subtropical]]; los pastizales se ubican al norte y en los límites con Chihuahua se localizan los bosques templados. 
En las regiones secas se encuentran una gran gama de plantas xerófilas, como los sahuaros, y árboles como el mezquite, el palo blanco, el palo fierro, el palo verde y el torote, ya que tienen sistemas de adaptación como crecer a la orilla de riachuelos y en las faldas de los cerros, no ser muy altos para contrarrestar la fuerza del viento y tener la madera muy dura y raíces largas que penetran en la tierra hasta encontrar un depósito de agua.

La mayoría de los bosques se localizan en el noreste del estado y cubren cerca de 6.4% del estado Esta área es la que tiene la temperatura más fría. La deforestación es un problema significativo, en especial después de 1980, debido al incremento en la tasa de corte de árboles. Muchos árboles de mezquite también han desaparecido por la demanda de combustibles locales y el mercado del carbón de mezquite en México y los EUA. 
La mayor parte del norte de México sufre de una de las tasas de desertificación más altas del mundo debido a la degradación de la tierra en las áreas áridas y semiáridas, lo que conlleva la pérdida de la productividad biológica y económica, pero el proceso es más grave en Sonora que en Sinaloa, por ejemplo. La degradación de la tierra ocurre por la limpieza de tierra para la agricultura, la plantación de pasto no nativo de la región para pastar, la tala de bosques, el sobre-consumo de vegetación natural y la salinización de suelo por la irrigación. 
La [[fauna]] de Sonora es rica y variada, pudiéndose contar como animales principales los siguientes, agrupados por especies:





[[Archivo:ColoradoRiverDelta_ISS009-E-09839.jpg|thumb|250px|right|Delta del Río Colorado.]]

Con la excepción del [[río Colorado]], los sistemas de ríos y acuíferos en Sonora son el resultado de la lluvia proveniente de las nubes sobre la Sierra Madre Occidental. Esta agua corre río abajo hacia el oeste de las montañas a lo largo de los cañones y valles hasta las praderas y la costa del Golfo de California. Sonora tiene siete ríos principales, el Colorado, el Concepción, el San Ignacio, el [[Río Sonora|Sonora]], el Mátape, el [[Río Yaqui|Yaqui]] y el [[Río Mayo (México)|Mayo]]. Presas, como la [[Presa Álvaro Obregón|Álvaro Obregón]] (Oviáchic), la [[Presa Mocúzari|Adolfo Ruíz Cortines]] (Mocúzari), la [[Presa El Novillo|Plutarco Elías Calles]] (Novillo), la Abelardo L. Rodríguez y la [[Presa La Angostura (Sonora)|Lázaro Cárdenas]] (La Angostura) han sido construidas a lo largo de algunos de estos ríos, en al menos dos de ellos donde ya existían lagos naturales. Algunas de las presas formaron grandes deltas, como la del Río Mayo. Los acuíferos más grandes se encuentran principalmente entre Hermosillo y la costa, el valle de Guaymas y el área alrededor de Caborca. Muchos de estos han tenido problemas debido a la sobre explotación para el riego en la [[agricultura]].

[[Archivo:Infiernillo_Tiburon_Island.JPG|thumb|250px|right|Vista de la Isla del Tiburón desde el [[Estrecho del Infiernillo|canal del Infiernillo]].]]

Sonora tiene de áreas de vida salvaje protegidas. Las áreas naturales protegidas en el estado de tipo federal son siete, de tres tipos: [[reservas de la biosfera de México|reservas de la biosfera]] (3), [[áreas de protección de flora y fauna de México|áreas para la protección de flora y fauna]] (2) y áreas para la protección de los recursos naturales (2). De forma resumida son:








Además, hay tres áreas protegidas de nivel estatal: 

Otras áreas son el Cañón Las Barajitas es un área natural protegida municipal establecida en 1993 que consiste de tres ecosistemas diferentes, localizado a al norte de [[San Carlos (Sonora)|San Carlos]]. Tiene un kilómetro de playas y un cañón que tiene dos microclimas distintos, uno árido y parecido al desierto, y otro subtropical. El área tiene una amplia variedad de fauna incluidas ballenas, delfines y mantas raya que pueden verse desde la costa dependiendo de la temporada. Algunas actividades para los visitantes son el uso del [[kayak]], [[buceo]] y [[pesca]]. También hay cuevas así como un observatorio solar.
La unidad de conservación del área Mesa el Campanero-Arroyo El Reparo es una zona sujeta a conservación ecológica, que forma parte del corredor biogeográfico de la Sierra Madre Occidental y de la Cuenca del Río Yaqui y Mayo y se encuentra en el municipio de [[Yécora (Sonora)|Yécora]]. Es una meseta con montañas que cubren , contiene pinos y bosques tropicales, ríos, arroyos, formaciones de roca y caminos de tierra. Debido a su altitud entre 700 y 2100 [[Altitud|msnm]], su temperatura es baja con respecto a la del estado. Es parte de la biorregión de la [[Sierra Madre Occidental]] y del nacimiento de los ríos [[Río Yaqui|Yaqui]] y [[Río Mayo (México)|Mayo]].

Existen algunas conjeturas sobre el origen del nombre "Sonora". Una es que proviene de la palabra "Señora", que fue el primer nombre dado a la región por los exploradores españoles, encabezados por Diego de Guzmán, debido a que arribaron al río Yaqui el [[7 de octubre]] de [[1533]], día de Nuestra Señora del Rosario. Como los nativos no podían pronunciar la letra 'ñ', "Señora" derivó en "Sonora". Una segunda conjetura dice que los indígenas vivían en chozas de cañas llamadas en su lengua "sonot". Los españoles cambiaron esa palabra a "sonora" y luego extendieron el nombre a toda la provincia.

Algunos de los más antiguos sitios arqueológicos encontrados en Sonora están relacionados con la [[cultura cochise]], que se desarrolló en el período [[Paleoamericano]]. En el noroeste del estado, algunos sitios en las cuencas de los ríos Altar, Magdalena y Concepción muestran una evolución de la cultura Cochise a la [[cultura de Trincheras]], caracterizada por ciertos tipos de cerámicas y por la construcción de terrazas y muros en las laderas de los cerros. Entre los sitios más característicos de esa cultura se encuentra [[Cerro de Trincheras]].

[[Archivo:Yaqui_indians.jpg|thumb|left|220px|La [[Pueblo yaqui|tribu Yaqui]], es conocida por su resistencia constante a los extranjeros, incluidos los europeos.]]
Aunque hay poca información sobre lo que pasó en el territorio después de la llegada de los españoles a México en el Siglo XVI, se sabe que hubo exploraciones españolas pero no asentamientos, en gran parte por la resistencia de los pueblos originarios, del [[pueblo yaqui]] principalmente. Los primeros asentamientos fueron fundados por los [[jesuitas]] y su [[Misión (religión)|sistema de misiones]] en la [[Nueva España]]. 

El misionero más famoso de Sonora, así como en gran parte de lo que hoy es el [[Suroeste de Estados Unidos|suroeste de los Estados Unidos]], es el jesuita [[Italia|italiano]], [[Eusebio Francisco Kino]], mejor conocido como El Padre Kino. Llegó a Sonora en 1687 y comenzó su trabajo en el área de la [[Pimería Alta]] de Sonora y Arizona. Fundó su primera misión en Cucurpe, después estableció iglesias y misiones en otras villas tales como Los Remedios, [[Imuris|Ímuris]], [[Magdalena de Kino|Magdalena]], Cocóspera, San Ignacio, [[Tubutama]], [[Caborca]] y otras. Además enseñó técnicas europeas para la agricultura a los indígenas que predicaba, para permitir el desarrollo de una economía para el beneficio de los nativos.

Sin embargo, durante el Siglo XVIII el sistema español y las misiones generaron descontento entre la población indígena debido a la expulsión de muchos de ellos de sus tierras, en especial las cercanas a las minas que eran de principal interés para la corona española; esto generó ataques esporádicos en su contra y a pesar de que los españoles habían construido [[Presidio|presidios]] para resguardarse, los asentamientos españoles estuvieron en desarreglo. En [[1767]], el rey de España, [[Carlos III de España|Carlos III]], [[Expulsión de los jesuitas del Imperio Español de 1767|expulsó a los jesuitas de los territorios controlados por los españoles]], terminando así el sistema de misiones. A finales del [[Siglo XVIII]], Sonora junto con Sinaloa formaron parte de la [[Intendencia de Arizpe]] en la Nueva España, que fue la división territorial que antecedió al [[Estado de Occidente]].

El periodo colonial terminó en Sonora con la [[independencia de México|Guerra de Independencia mexicana]] de [[1810]] a [[1821]]; sin embargo, Sonora no estuvo directamente involucrada en la guerra. La independencia llegó en forma de decreto. Un resultado positivo de la independencia fue que permitió el desarrollo económico. La antigua [[Estado de Occidente|provincia de Sonora y Sinaloa]] fue dividida en [[1823]] para formar los estados de Sonora y [[Sinaloa]], estableciéndose la capital sonorense en [[Ures]]. Sin embargo, se reunificarían otra vez en [[1824]] bajo el [[Estado de Occidente]] y permanecerían así hasta [[1830]], a pesar del hecho de que Sonora fue declarado como estado en la [[Constitución Federal de los Estados Unidos Mexicanos de 1824|constitución mexicana de 1824]]. Sonora se separó de Sinaloa nuevamente en [[1831]] cuando escribió su primera constitución estatal, que puso la capital en [[Hermosillo]]. Posteriormente siguió un periodo de inestabilidad política causado por la disputa entre liberales y conservadores por el tipo de gobierno que debía tener el nuevo país. 

La [[Intervención estadounidense en México|guerra de Estados Unidos-México]] trajo solo una confrontación militar importante entre las fuerzas [[México|mexicanas]] y [[Estados Unidos|estadounidenses]], pero las consecuencias serían graves para el estado. En octubre de [[1847]], el buque de guerra Cyane asedió la bahía de [[Guaymas]], lo que resultó en el control estadounidense de esta parte de la costa desde entonces y hasta [[1848]]. Cuando terminó la guerra, Sonora perdió 339,370 [[Hectárea|hectáreas]] de su territorio a los Estados Unidos a través del [[tratado de Guadalupe Hidalgo]]. Además de eso, la guerra arruinó la economía estatal. Sonora perdería más territorio en [[Años 1850|los 1850]], a través del [[Venta de La Mesilla|Tratado de La Mesilla]]. Antes de la guerra, Sonora era la entidad más grande de México. La debilidad del área en la etapa posterior a la guerra la hizo susceptible a bucaneros tales como [[William Walker]], [[Gaston de Raousset-Boulbon]] y Henry Alexander Crabb que atacaron los puertos sonorenses tales como Guaymas y [[Caborca]]. Sin embargo, la mayoría de los ataques fueron repelidos. La economía no se volvería a recuperar de la guerra hasta finales de [[1850]], cuando [[Ignacio Pesqueira]] se convirtió en gobernador y atrajo la inversión extranjera al estado, en especial en el sector minero, así como trabajó para crear un mercado exterior para los productos agrícolas de Sonora. 

Durante la [[Segunda intervención francesa en México|intervención francesa en México]], Sonora fue invadido por tropas [[Francia|francesas]] como parte de su esfuerzo para instalar una monarquía en México bajo [[Maximiliano I de México|Maximiliano I]]. El puerto de Guaymas fue atacado por fuerzas bajo Armando Castagny, lo que forzó a las fuerzas mexicanas al mando de Pesqueira y el General Patoni a retirarse al norte de la ciudad. Las tropas francesas atacaron a los mexicanos de nuevo en un lugar llamado La Pasión, resultando nuevamente en la derrota de la resistencia mexicana. Los franceses no fueron derrotados en el estado hasta la Batalla de Guadalupe de Ures en 1866 por Pesqueira, Jesús García Morales y Ángel Martínez. Poco después de esto, se escribió la constitución actual del estado en [[1873]], y su capital sería movida permanentemente a Hermosillo. 
[[Archivo:Cajeme2.jpg|thumb|right|250px|Cajeme, líder de la resistencia yaqui.]]
Durante el régimen de [[Porfirio Díaz]] a finales del siglo XIX y a principios del [[Siglo XX|XX]], se promovieron cambios económicos significativos. Estos cambios generaron un rápido crecimiento económico, que tuvo profundas consecuencias políticas y sociales. Sonora, junto con el resto de los estados fronterizos del norte incrementaron rápidamente en importancia. El desarrollo del sistema de [[ferrocarril]] integró la economía estatal con la nacional, y también tuvo un efecto de mayor control federal en todo el territorio de México. Después de 1880, el sistema de rieles atravesó el norte hacia los Estados Unidos, el cual sigue siendo una parte importante de las relaciones económicas entre ambos países. A pesar de eso, los cambios también permitieron a extranjeros y a ciertos mexicanos apoderarse de grandes zonas de terrenos en México. En Sonora, Guillermo Andrade controlaba 1,570,000 hectáreas, Manuel Peniche y el estadounidense [[William Cornell Greene]] tenían cerca de 500,000. Los dueños extranjeros de la industria también tendían a traer a trabajadores extranjeros, incluso de [[Asia]]. La [[Inmigración china en México|inmigración china]] a Sonora comenzó en este periodo, y los chinos pronto se convirtieron en una fuerza económica a medida que construyeron pequeños negocios que se esparcirían donde sea que hubiera desarrollo económico en el estado. 

La apropiación de la tierra tanto para la agricultura como la minería, puso nueva presión sobre los Yaquis y otros pueblos nativos de Sonora. La resistencia yaqui hasta este punto les había dado un control bastante autónomo de una porción del estado, y mantenían su sistema de agricultura a lo largo del Río Yaqui. 

La invasión de esta tierra llevó a levantamientos y a una guerra de guerrillas por parte de los yaquis después de 1887. En 1895, los gobiernos federal y estatal comenzaron a reprimir violentamente a los yaquis y comenzaron a expulsar a los yaquis capturados a plantaciones en el sureste de México, especialmente las plantaciones de [[Agave fourcroydes|henequén]] en la [[península de Yucatán]]. La resistencia yaqui continuó bien entrado el siglo XX, y las expulsiones alcanzaron un pico entre 1904 y 1908, en cuyo punto, cerca de un cuarto de esta población había sido mandada fuera del estado. Algunos más fueron forzados a escapar a [[Arizona]].

[[Archivo:Cananea.jpg|thumb|right|250px|Imagen de la huelga de mineros de Cananea de 1906.]]
Las políticas del gobierno de Porfirio Díaz no solo causaron resentimiento entre los yaquis, sino también en el resto de Sonora y el país. Uno de los antecedentes de la [[Revolución mexicana]] fue la [[Huelga de Cananea|huelga de Cananea de 1906]], que buscaba negociaciones con el dueño minero estadounidense William Greene, pero este se negó a reunirse con los cerca de 2,000 huelguistas. La huelga se volvió violenta rápidamente cuando los mineros trataron de tomar control de la mina e intercambiaron disparos. Cuando las tropas federales mexicanas llegaron dos días después, pusieron un fin brutal a todo, con la ejecución de los sospechosos de liderar la huelga. De tal suerte que la huelga hizo crecer el resentimiento hacia Díaz, no disminuir las huelgas en otras zonas del país.

A finales de [[1910]], estalló la [[Revolución mexicana]] y Díaz fue rápidamente removido del cargo, el resto de la guerra determinaría quién se quedaría en el poder después de esto. El entonces gobernador de [[Coahuila]], [[Venustiano Carranza]], buscó refugio en Sonora, y se convirtió en uno de los principales protagonistas durante el resto de la guerra, con su base de operaciones principal en Hermosillo. Después de que Díaz fue removido del cargo, Carranza estaba en disputa por el poder en contra de [[Álvaro Obregón]] y otros.
[[Archivo:Obregón_Salido,_Álvaro.jpg|thumb|left|220px|[[Álvaro Obregón]], importante partícipe de la [[Revolución mexicana]] originario de Sonora.]]
A pesar de que Carranza obtuvo la presidencia en 1920, el conflicto con Obregón y su resistencia no cesó, por lo que Carranza intentó suprimir la oposición política en Sonora. Esto ocasionó que Álvaro Obregón y sus aliados (principalmente [[Abelardo L. Rodríguez]], [[Benjamín Hill (militar)|Benjamín Hill]] y [[Plutarco Elías Calles]]) firmaran el [[Plan de Agua Prieta]] por el cual desconocían al gobierno carrancista, este movimiento pronto vino a dominar la situación política mexicana, pero causó inestabilidad política generalizada. Obregón triunfó en quitar a Carranza del cargo y convertirse el siguiente presidente de México. Para las elecciones presidenciales de 1924, Obregón escogió a Plutarco Elías Calles como sucesor, que también era un líder revolucionario de Sonora. Esto terminó de manera efectiva la guerra, pero las hostilidades habían destruido nuevamente la economía sonorense. De 1920 a principio de los 1930, cuatro sonorenses ocuparían la presidencia de la república, [[Adolfo de la Huerta]], Álvaro Obregón, Plutarco Elías Calles y Abelardo L. Rodríguez. 

Los esfuerzos de modernización y desarrollo económico iniciados en el [[Porfiriato]] continuarían a lo largo de la Revolución y por el resto del [[Siglo XX]]. A finales del siglo XIX y a principios del XX, el proceso de llevar electricidad incrementó ampliamente la demanda de cobre, lo que trajo consigo un gran incremento en la minería de Sonora. Cananea creció muy rápido de una villa de 900 a una ciudad de 20,000 habitantes. También ocasionó una red de caminos, vías de tren y otras conexiones a lo largo de la frontera. Sin embargo, el desarrollo organizado de la agricultura estatal se detuvo por la Revolución, la [[Gran depresión]] y otros problemas políticos. 

En los 1930, Sonora se benefició de varias políticas nacionales enfocadas al desarrollo de ciudades en la frontera con los Estados Unidos y por la construcción de varias presas para ayudar al desarrollo de la agricultura y la demanda de agua general. En los 1940 comenzaron reformas agrícolas importantes en el área del [[Río Mayo (México)|Río Mayo]], cuando se limpió el delta de vegetación natural y se convirtió en tierras de cultivo. Se aseguró el agua para estas granjas con la construcción de la [[Presa Mocúzari]] a cerca de 24 kilómetros de [[Navojoa]]. Cuando se terminó en 1951, había un sistema de canales, pozos y carreteras para apoyar la agricultura a gran escala para su exportación.

En la última mitad del siglo XX, la población del estado ha crecido y se ha incrementado la inversión extranjera debido a su localización estratégica cerca de la frontera y su puerto de [[Guaymas]]. Esto ha permitido el desarrollo de infraestructura moderna como carreteras, puertos y aeropuertos, lo que hace del estado uno de los mejores conectados del país. En 1964 se construyó un puente sobre el [[río Colorado]] para enlazar a Sonora con la vecina [[Baja California]]. Un sector importante de la economía ha sido la industria, que trajo consigo la [[Hermosillo Stamping & Assembly|planta Ford]] en [[Hermosillo]] y varias plantas de ensamble llamadas [[Maquiladora|maquiladoras]] en la frontera con los [[Estados Unidos]]. Uno de los sectores de la economía con mayor crecimiento ha sido el turismo, en especial en la costa, esto ha llevado al surgimiento de infraestructura hotelera, en especial en [[Puerto Peñasco]].

De acuerdo con los resultados del Censo de Población y Vivienda 2010 del [[Instituto Nacional de Estadística y Geografía]] (INEGI), el estado de Sonora contaba con 2,882,628 habitantes, que representó el 2.4% de la población de [[México]]. Del total, 50.3% eran hombres y 49.7% eran mujeres. La tasa de crecimiento poblacional anual para la entidad durante el período 2005-2010 fue del 2.1%. El crecimiento de la población ha sido constante en desde [[1940]]. El censo también indicó que [[Mediana (estadística)|la mitad]] de la población tiene 26 años o menos. 

La [[densidad de población]] en Sonora es de 14.8 habitantes por kilómetro cuadrado, lo que convierte a Sonora una de las entidades menos densamente pobladas de México, en el lugar 29 de 32; resaltando que el estado es el segundo más grande del país, después de [[Chihuahua]]. 

En la entidad 60,310 personas hablan una [[Lenguas indígenas de América|lengua indígena]], lo que representa un 3% de la población. Esta población se localiza principalmente en el sur del Estado. Las lenguas indígenas más frecuentes son el [[idioma mayo|mayo]] (46.4%) y el [[Idioma yaqui|yaqui]] (26.6%).

A continuación se presenta una lista de las ciudades más pobladas de Sonora de acuerdo con el último censo de población (2010), no confundirse con los Municipios más poblados.

Algunos de los festivales culturales más importantes del estado son el [[Festival del Pitic]] en [[Hermosillo]], el Festival Alfonso Ortiz Tirado en Álamos, las Fiestas de San Francisco en Magdalena de Kino y el Carnaval de [[Guaymas]]. Sonora cuenta además con dos [[Pueblo Mágico|pueblos mágicos]], los anteriormente mencionados [[Álamos (Sonora)|Álamos]] y [[Magdalena de Kino]]. 

Los museos más importantes de Sonora son el [[Museo Costumbrista de Sonora]] en Álamos, el [[Museo Casa General Álvaro Obregón (Huatabampo)|Museo Casa del General Álvaro Obregón]] en Huatabampo, el [[Museo de los Yaquis]] en Cócorit, [[Cajeme (municipio)|Cajeme]]; el [[Museo Comcaác (Bahía de Kino)|Museo Comca'ac (de los seris)]] en [[Bahía de Kino]], Hermosillo; el [[Museo Costumbrista Regional (Ures)|Museo Costumbrista Regional]] en [[Ures]], el [[Casa Museo Silvestre Rodríguez (Nacozari de García)|Museo Silvestre Rodríguez]] en [[Nacozari de García]] y el [[Museo de la Lucha Obrera]] en [[Cananea]]. 

Las danzas indígenas más importantes son la [[danza del venado]], la de la páscola y la de los matachines, que se presenta principalmente en [[Huatabampo]]. Todavía se practica la medicina con plantas medicinales en especial en las zonas rurales. 

Desde la época colonial, gran parte de la economía estatal ha estado relacionado con la ganadería, siendo los [[Cowboy|vaqueros]] una parte importante de la identidad estatal. Hoy en día, la mayoría de ellos trabajan en la industria y el turismo, pero la indumentaria y el folclor del vaquero sigue siendo importante. Los [[Pantalón vaquero|pantalones de mezclilla]] y los sombreros de vaquero siguen siendo muy populares, en especial entre los hombres. El estilo de vida vaquero está asociado con las camionetas [[Pickup]]. Esta influencia se extiende a la música popular. 

El estilo musical más popular del área es la [[norteña]], que incluye la [[Banda sinaloense|Banda]]. La música norteña se desarrolló desde finales del siglo XIX hasta el comienzo del siglo XX en toda la región fronteriza del norte de México y el suroeste de los Estados Unidos, con la influencia de los [[Vals|valses]], [[Polka|polkas]], las [[Ranchera|rancheras]], [[Mazurca|mazurcas]] y los [[Corrido (México)|corridos]]. Un instrumento importante del género es el [[acordeón]], traído por primera vez a la región por inmigrantes [[Alemania|alemanes]]. Las versiones sonorenses de este tipo de música se desarrollaron de los 1920s a los 1960s. Muchas de las primeras composiciones exitosas son de compositores anónimos. En los 1950s, con la expansión de la [[Radio (medio de comunicación)|radio]], la popularidad del género se incrementó pues se comenzó a escuchar música norteña de [[Nuevo León]], [[Durango]] y otros estados. Estas versiones regularmente incluían canciones escritas por compositores sonorenses tales como Amor de Madre de Jesús "El Chito" Peralta, Cuatro Milpas, Mundo Engañoso, El Venadito, La Higuerita y El Tarachi de Aristeo Silvas Antúnez y La Barca de Guaymas de José López Portillo. La juventud que trabajaba en los campos y ranchos se identificaba de manera particular con esta música.

El primer grupo norteño formal fue [[Los Cuartetos de Sonora]], formado por los hermanos Carvajal. En contraste con bandas de otros estados, que eran duetos, las bandas sonorenses eran tríos antes de volverse cuartetos y quintetos con la adición de más instrumentos musicales. La letra casi siempre habla de momentos importantes del día a día que se celebran y embellecen. Solo de manera reciente la música norteña ha sido aceptada por otras clases sociales fuera de aquellas en las que se desarrolló. Los grupos norteños de Sonora, a veces llamados taca-tacas, pueden ser escuchados en eventos sociales de todos los niveles socio-económicos. 

La región ha sido un área de estudio para los arqueólogos, antropólogos e historiadores; quienes han trabajado con las ruinas [[prehispánicas]] y los huesos fosilizados. Sin embargo, gran parte de la investigación en esta zona sigue en su etapa descriptiva inicial y hay muchas preguntas básicas aún sin responder. Sonora es considerada una zona cultural separada de [[Mesoamérica]], aunque hay algo de influencia mesoamericana. Las principales diferencias entre las culturas sonorenses y las de Mesoamérica son el cultivo en climas secos, aunque se produce de igual manera el maíz, la calabaza y los frijoles. También hay mayor dependencia de los recursos no cultivados por el hombre. Aunque vale la pena resaltar que lo más importante es la carencia de ciudades como tales en la historia prehispánica de esta región, habiendo únicamente poblaciones pequeñas cerca de las fuentes de agua y un débil sistema jerárquico. Las culturas aquí también comparten algunas características con aquellas del [[Suroeste de Estados Unidos|suroeste de los Estados Unidos]], pero también son notablemente distintas. 
[[Archivo:Cerro_de_Trincheras.jpg|thumb|250px|Cerro de Trincheras.]]
El [[Cerro de Trincheras]] es un sitio arqueológico importante localizado en el [[Trincheras (Sonora)|municipio homónimo]], con [[petroglifos]], plazas y observatorios astronómicos. Se disputa el propósito exacto del área, pero se sabe que alcanzó su esplendor entre [[1300]] y [[1450]] de nuestra era, cuando tuvo una población de cerca de mil habitantes, que vivieron del cultivo del maíz, la calabaza, el algodón y el agave. Su estructura más grande se llama La Cancha, que está en la base al lado norte del cerro. Es un patio rectangular caracterizado por sus rocas apiladas en las orillas, que miden 51 por 13 metros. Algunos investigadores creen que era un tipo de cancha para el juego de pelota y otros creen que era una especie de teatro al aire libre. En el cerro mismo hay un observatorio, que da una vista del lugar. La mayoría de los artefactos del área de piedra y concha fueron encontrados ahí. La Plaza del Caracol se caracteriza por su pared de piedra de metro y medio en espiral abierta, muy probablemente utilizada para ceremonias.

Como en otras partes de México, la cocina sonorense es básicamente una mezcla de las influencias indígenas y [[España|españolas]]. Cuando los españoles avanzaron al norte del [[Valle de México]], encontraron que la dieta del lugar era más simple, con lo básico de maíz, frijoles y calabaza, pero sin la variedad extra que existía en el sur. Por esta razón, esos españoles influenciaron la manera en la que se desarrolló esa dieta. Trajeron a Sonora cosechas europeas de [[harina]], [[Bos primigenius taurus|res]], [[Lácteo|productos lácteos]], [[Sus scrofa domestica|puerco]] y más, así como platillos e ingredientes del centro y sur de México, tales como [[tortillas]], más variedades de [[Chile (pimiento)|chiles]] y [[tamales]]. La cultura del vaquero ha sido un aspecto importante de la cultura de Sonora desde la [[Colonización europea de América|época colonial]] y gran parte de la cocina está basada en lo que los vaqueros comieron en sus inicios, incluso aunque la mayoría de los sonorenses ya no trabajan al aire libre. La cocina sonorense no está limitada a su geografía actual. [[Arizona]], en especial en el área sur cerca de la frontera, tiene una cocina que también es rica en harina, queso y res; así fue llevada por los españoles, a lo que alguna vez fue [[La Mesilla|parte]] de Sonora. Ambos estados continúan con la tradición del vaquero. Debido a la situación de Sonora como un estado fronterizo, su cocina también ha recibido una influencia significativa de los [[EUA]]. 
[[Archivo:Parrillada_Carne_asada.jpg|thumb|250px|Asando carne.]]
Dos componentes importantes de la dieta del estado son los mariscos y la res, esta última juega un papel más importante en Sonora que en el resto de México. La res normalmente se cocina a la parrilla, y los sonorenses prefieren cortes robustos como pecho y falda. La [[machaca]] o carne seca todavía se disfruta, aunque la refrigeración ha hecho desaparecer la necesidad de secar la carne para su conservación. Sonora también tiene la reputación de producir cortes de res finos, pero el ganado español más delgado ha sido sustituido por [[Aberdeen angus|Angus]], [[Hereford (raza bovina)|Herefords]] y [[Holstein (ganado)|Holsteins]]. Entre los platillos que se basan en res o que la contienen se incluyen la carne deshebrada, la carne con rajas verdes, los burros de carne asada, la carne con chile colorado, el chorizo de res, la carne seca, la machaca, el menudo, las gorditas y las albóndigas. La comida del mar o mariscos también es parte importante de la cocina, en especial cerca de la costa pues hay una gran variedad de peces y almejas en el [[Golfo de California]]. Los mariscos se cocinan generalmente en platillos muy sencillos, tales como tacos de pescado, sopas de mariscos, arroz con camarón, o pulpo y albóndigas de camarón. Algunos de los peces más consumidos son las [[Serranidae|cabrillas]], el [[marlín]] y la [[mantarraya]].
[[Archivo:Large_Tortilla.jpg|thumb|250px|Tortilla de harina grande.]]
Las tortillas son parte fundamental de la dieta sonorense, pero son de harina, en vez de maíz. En Sonora, estas tortillas son más grandes y delgadas que las preparadas en otras partes. Los productos lácteos son comunes en los platillos y los chiles juegan un rol más pequeño. Aun así, un chile nativo importante es el [[chiltepín]], que la gente continúa cultivando en las zonas secas del noroeste de México. 

La cocina contiene varias sopas y caldos, que combinan ingredientes regionales con elementos de todo México, con res, puerco y maíz. Las sopas de mariscos son populares a lo largo de la costa. Entre estos platillos se incluye el pozole de trigo, el pozole de res, el menudo con pata, la sopa de elote, el caldo de calabazas y la sopa de camarón. Los tamales se hacen con masa de maíz seco y con rellenos; como carne con chile, aceitunas, papas, chile colorado y otros ingredientes, envueltos en hojas de maíz secas y hechos al vapor. Los rellenos de los tamales varían en el estado pero los más populares incluyen maíz fresco y crema, tiras de chile verde y queso, res con chile colorado y a veces mariscos. 

Las culturas varias que han llegado al estado han influenciado el desarrollo de panes, postres y dulces. La mayoría de los dulces se hacen con [[leche de vaca]], [[caña de azúcar]], [[almendras]], [[Bertholletia excelsa|nueces]], [[arroz]], [[Semilla de girasol|semillas de girasol]] y [[piloncillo]]. Entre estas están las pipitorias, el jamoncillo, el cubierto de biznaga, los cubiertos de calabaza, los cubiertos de camote, calabaza en miel, piloncillo, nieve de pitahaya, naranjas y limones cristalizados.

El [[Bacanora (licor)|bacanora]] es un licor regional con [[denominación de origen]] que desde hace décadas se produce en el [[Bacanora (Sonora)|pueblo del mismo nombre]], localizado en el centro del estado. Su base es la planta de agave ([[Agave vivipara]]) como el [[mezcal]] y el [[tequila]] y viene en varios estilos, incluido añejo. Tiene un sabor distintivo. Se estima que medio millón de plantas son cosechadas de la naturaleza para hacer esta bebida cada año, lo que ha llevado a preocupaciones sobre su sobre-explotación.

[[Archivo:Centro de Gobierno Sonora.jpg|left|thumb|250px|Centro de Gobierno del Estado de Sonora, en [[Hermosillo]].]]

La forma de gobierno de Sonora es descrita en su Constitución, que data de 1917. De acuerdo con la [[Constitución Política de los Estados Unidos Mexicanos]], el Estado de Sonora adopta para su régimen interior la forma de gobierno republicano, representativo, democrático, laico y popular, teniendo como base de su división territorial y organización política y administrativa, el Municipio Libre, según la presente Constitución y sus respectivas leyes. El gobierno del Estado se divide en tres poderes: Ejecutivo, Legislativo y Judicial.

El Poder Ejecutivo se deposita en una sola persona denominada Gobernador del Estado de Sonora, electo por voto directo cada seis años sin posibilidad de reelección. Toma posesión el día 13 de septiembre del año de su elección. La actual Gobernadora por el período 2015-2021 es [[Claudia Pavlovich Arellano|Claudia Pavlovich]] del Partido Revolucionario Institucional ([[Partido Revolucionario Institucional|PRI]]).

El Poder Legislativo es unicameral y se deposita en 21 diputados electos de manera directa y 12 diputados electos por representación proporcional para un período de tres años sin posibilidad de reelección; estos conforman el [[Congreso del Estado de Sonora]]. La actual Legislatura (2018-2021) es la LXII. El Congreso del Estado se instala el día 16 de septiembre del año de su elección. 
[[Archivo:PoderJudicialDelEstado.JPG|right|thumb|250px|Poder Judicial del Estado de Sonora, en [[Hermosillo]].]]
El Poder Judicial se deposita en el Supremo Tribunal de Justicia, en Tribunales Regionales de Circuito, en Juzgados de Primera Instancia y en Juzgados Locales. Existe, además, el Consejo del Poder Judicial del Estado de Sonora, como un órgano permanente de la administración de la justicia. El Supremo Tribunal de Justicia está compuesto por siete Magistrados Propietarios
y siete Suplentes y funcionará en Pleno, en Salas o en Comisiones. Desde [[octubre]] de [[2015]] el Licenciado Francisco Gutiérrez Rodríguez es el presidente del Supremo Tribunal de Justicia del Estado. 

A nivel federal, Sonora cuenta con tres senadores y siete diputados (los que representan a los siete [[Distritos electorales federales de México#Sonora|distritos electorales federales del estado]]) en el [[Congreso de la Unión]] de México. Los senadores son elegidos por elección popular cada seis años y los diputados cada tres. La actual legislatura federal es la [[LXIII Legislatura del Congreso de la Unión de México|LXIII Legislatura]]. 

El [[7 de junio]] de [[2015]] se llevaron a cabo las [[elecciones estatales de Sonora de 2015]] donde se eligió nuevo Gobernador, Presidentes municipales, Diputados locales y federales. El [[15 de junio]] de 2015 el Instituto Estatal Electoral de Sonora, a través de la Consejera Presidenta Guadalupe Taddei Zavala, realizó la entrega de la constancia de mayoría a [[Claudia Pavlovich Arellano]], que la acredita como Gobernadora electa para el período 2015 -2021. Esto después de que se declarara la validez de la elección en la sesión permanente del cómputo estatal de la elección de Gobernador del Estado que concluyó el mismo día.

[[File:Mapa_Municipios_Sonora_2018_-_2021.png|thumb|220px|right|Mapa de los municipios de Sonora de acuerdo al partido político gobernante para el periodo 2018-2021.]]

El Estado de Sonora se encuentra dividido políticamente en [[Municipio|municipios]] que representan el tercer nivel de gobierno en [[México]], debajo del nivel estatal y federal. El estado de Sonora está compuesto por 72 municipios que cuentan con un gobierno propio que radica en el [[Ayuntamiento]], el cual es dirigido por un Presidente Municipal elegido por elección popular cada 3 años. Los municipios son reconocidos como las partes integrantes del Estado. 

La Constitución sonorense declara que el Estado tiene una composición pluricultural, basada originalmente en los pueblos indígenas, y que conservan sus propias instituciones sociales,
económicas, culturales y políticas, o parte de ellas. De manera que en la Constitución se garantiza el derecho de los pueblos y las comunidades indígenas a la libre determinación dentro de su territorio y además, garantiza el respeto y la igualdad de oportunidades para su desarrollo integral.

Según el censo de población y vivienda 2010, en Sonora la tasa de alfabetización de las personas de entre 15 y 24 años es de 98.3% y la de las personas de 25 años o más es de 95.6%. 

La asistencia escolar para las personas de 3 a 5 años es del 43.5%; de 6 a 11 años es del 97.1%; de 12 a 14 años es del 94.3% y de 15 a 24 años es del 45.8%.

[[Archivo:UNISON.jpg|thumb|250px|left|[[Universidad de Sonora|UNISON]], Unidad Regional Centro.]]

La principal institución de educación superior es la [[Universidad de Sonora]] (UNISON), fundada en [[1942]] por decreto estatal, la universidad ha crecido manteniendo su identidad. Está dividida en seis unidades a lo largo del Estado, con campus en Nogales, Santa Ana, Caborca, Hermosillo, Navojoa, y en Ciudad Obregón. Hermosillo, como sede principal, alberga a la mayoría de los estudiantes y de la oferta educativa. La institución ofrece programas de estudio en más de cuarenta especialidades a través de seis divisiones. Las maestrías y doctorados se ofrecen principalmente en ciencia y tecnología. La UNISON es actualmente una de las instituciones públicas de educación superior más importantes de [[México]].

El [[Instituto Tecnológico de Sonora]] (ITSON) tiene cerca de 17,000 alumnos y ofrece veintitrés licenciaturas, ocho programas de maestría, y tres programas de doctorado dentro de sus seis campus. La institución fue formada por iniciativa de la sociedad cajemense en Ciudad Obregón en 1955, pero recibió su nombre actual en 1962. Originalmente era una escuela de entrenamiento técnico, pero se reorganizó como universidad en 1973. Actualmente es la institución tecnológica más grande del estado.

Además, la entidad cuenta con otras instituciones públicas y privadas, como el [[Instituto Tecnológico y de Estudios Superiores de Monterrey]], Campus Sonora Norte (ITESM-CSN), el [[Instituto Tecnológico de Hermosillo]] (ITH), la [[Universidad Estatal de Sonora]] (UES, antes CESUES), la Universidad Tecnológica de Hermosillo (UTH), la [[Universidad del Valle de México]] (UVM), el [[Instituto Sonorense de Administración Pública, A.C.|Instituto Sonorense de Administración Pública]] (ISAP), entre otras.

A pesar del terreno [[Aridez|árido]] y el clima extremoso, Sonora, como el resto del [[Región Norte (México)|norte de México]], es rico en recursos naturales. Esto ha llevado a una historia de [[autosuficiencia]] y muchos sonorenses se ven a sí mismos como herederos de esta tradición pionera. Una gran parte de esto está relacionado con la cultura y tradición del campo, pues gran parte de la economía estatal estuvo relacionada con la ganadería. 

El estado tiene una fuerza laboral altamente cualificada y fuertes lazos con la economía de [[Estados Unidos]] debido en su mayoría a la frontera común con [[Arizona]]. Este enlace afecta a varios sectores de la economía estatal. Sonora tiene un crecimiento de [[Producto interno bruto|PIB]] que generalmente es mayor que el resto del país, con un crecimiento de 4.0% en 2014, comparado con el promedio nacional de 3.3%. Aunque este crecimiento era mucho mayor antes de la [[Crisis económica de 2008-2015|crisis económica de 2008]], por ejemplo, en 2006, el crecimiento fue de 8.4% con respecto al nacional de 4.8%. El éxito económico del estado, en especial en sus sectores industriales y de agricultura, así como en la frontera, ha atraído a muchos inmigrantes del centro y sur de México al estado. 

La mayor parte de la industria estatal está relacionada con la [[agricultura]] y la [[Pesca comercial|pesca]], en el [[Procesado de los alimentos|procesamiento de comida]] y su empaquetamiento. En los [[años 1980]], se instalaron en el estado un gran número de plantas industriales llamadas "[[Maquiladora|maquiladoras]]", la mayoría situadas a lo largo de la frontera y en la capital, Hermosillo. Estas plantas de ensamblaje eran controladas en su mayoría por compañías estadounidenses a las que se les dieron facilidades en responsabilidades e [[Impuesto|impuestos]]. Hacia finales del siglo XX, estas empresas tenían una gran influencia en la expansión y la modernización del [[Frontera entre Estados Unidos y México|área fronteriza de México]], incluido Sonora. No solamente trajeron nuevas fuentes de empleo, sino además el estilo de gestión estadounidense tuvo influencia en los negocios del estado y del resto del norte de México. Sin embargo, las maquiladoras llegaron a un tope en [[2001]], cuando muchas compañías de Estados Unidos movieron su producción a [[República Popular China|China]]. El número de maquiladoras disminuyó, pero el valor de su producto final ha incrementado, de la misma manera que aquellos que prefieren bienes con mayor valor añadido y automatización. Además, muchas plantas abandonadas por las compañías de Estados Unidos fueron adquiridas por empresas mexicanas. A pesar del decremento en el número de maquiladoras, sus exportaciones han aumentado. 

Además de la ganadería, la [[minería]] es otro elemento tradicional de la economía de Sonora, que comenzó con un hallazgo importante cerca de la ciudad de [[Álamos (Sonora)|Álamos]]. Aunque la [[plata]] del área casi se acaba, Sonora sigue jugando un papel importante en México como uno de los quince productores de minerales más importantes del mundo, liderando en plata, [[Celestina (mineral)|celestina]] y [[bismuto]]. Sonora es el productor líder en [[oro]], [[cobre]], [[grafito]], [[molibdeno]] y [[wollastonita]]. Todavía hay depósitos de plata en la Sierra Madre Occidental. Sonora además tiene una de las reservas de [[carbón]] más grandes del país. El estado tiene la superficie para la minería más grande de México, y tres de las minas más importantes del país: La Caridad, Cananea y Minería María. También es hogar de la mina de cobre más vieja de América del Norte, localizada en [[Cananea]]. [[Grupo México]], con operaciones de minería principalmente en Cananea, es el tercer productor de cobre a nivel mundial. 

La industria de la minería en México estuvo dominada principalmente por los españoles durante el periodo colonial, y por empresas extranjeras después de la independencia. En los [[años 1960]] y [[Años 1970|1970]], el gobierno expulsó la mayoría de los intereses extranjeros en la minería mexicana, comenzando con mayores restricciones de propiedad en las compañías mineras mexicanas. Estas restricciones fueron tranquilas cuando comenzaron en 1992, con la única restricción de que la compañía operadora fuera mexicana. A tan solo tres años del cambio, más de setenta compañías extranjeras, en su mayoría [[Estados Unidos|estadounidenses]] y [[Canadá|canadienses]], abrieron oficinas en [[Hermosillo]]. 

Las operaciones mineras principales han tenido un severo [[impacto ambiental]], especialmente en las áreas cercanas, Cananea siendo el ejemplo principal. La minería ha lleva funcionado ahí más de un siglo, y los desechos mineros han contaminado los ríos San Pedro y [[río Sonora|Sonora]] cerca a la mina, amenazando ambas cuencas. Las operaciones mineras también destruyen bosques cercanos debido a la demanda de materiales de construcción y combustible. Quedan pocos árboles viejos cerca de la ciudad de Cananea y el pueblo de [[San Javier (Sonora)|San Javier]] en el centro de Sonora. En [[agosto]] de [[2014]], Grupo México fue responsable de lo que es considerado el peor desastre ambiental en la historia del sector minero en México, cuando más de 40,000 metros cúbicos de sulfato de cobre fueron derramados en el río Sonora, afectando a al menos 20,000 personas, miles de hectáreas de cultivo y cabezas de ganado.

En el estado la [[Carretera Federal 15]] tiene 2,711 kilómetros en el Estado, sumadas al resto de las carreteras estatales de 4,591 kilómetros; en total el estado cuenta con 7,302 kilómetros de carreteras. Sonora cuenta también con 2,008 kilómetros de vías férreas, 5 aeropuertos internacionales (el principal es el [[Aeropuerto Internacional General Ignacio Pesqueira García]]) y 2 puertos marítimos (el principal es [[Guaymas]]).

Operan en el estado 150 radio-difusoras (53 de [[amplitud modulada]] y 97 de [[frecuencia modulada]]) así como 90 estaciones de televisión. La entidad cuenta también con 466 establecimientos de diversas categorías para el hospedaje.

[[Archivo:Aeropuerto_HMO_1.JPG|thumb|250px|right|[[Aeropuerto Internacional General Ignacio Pesqueira García]] en [[Hermosillo]].]]

Sonora yace en un corredor que ha conectado las tierras altas del centro de [[México]] con el norte hacia [[Estados Unidos]] por la costa del [[Océano Pacífico|Pacífico]] al menos desde el [[Virreinato de Nueva España|periodo colonial]], y hay evidencia de que este corredor existió en el [[América precolombina|periodo prehispánico]] también. Hoy, aún es un corredor importante para los viajes y los envíos, con vías de [[ferrocarril]] y la [[Carretera Federal 15]] que las sigue. El estado tiene un total de 24,396 km de [[Carretera|carreteras]]. Las líneas de ferrocarril consisten en aquellas que van hacia Estados Unidos. El [[puerto]] comercial más importante está en [[Guaymas]], y algunos más pequeños para el turismo en [[San Carlos (Sonora)|San Carlos]], [[Puerto Peñasco]] y [[Bahía de Kino]]. El estado tiene cuatro aeropuertos en las ciudades de Hermosillo, Puerto Peñasco, Ciudad Obregón y Nogales. Estos aeropuertos conectan el estado con 112 localidades tanto en México como en el extranjero. 

[[Archivo:Zona_Hotelera_Puerto_Peñasco.png|thumb|250px|left|Zona hotelera en [[Puerto Peñasco]].]]
El principal atractivo [[turístico]] de Sonora son sus playas, poblaciones, además del desierto que se une al mar y a la sierra. La diversidad de sus ecosistemas hace posible que en el Estado puedan realizarse gran variedad de actividades de recreación y turismo como [[buceo]], [[Pesca deportiva|pesca]], [[nado]], [[senderismo]], [[ciclismo de montaña]], [[caza|turismo cinegético]] y [[turismo ecológico]].

Las playas más conocidas son [[San Carlos (Sonora)|San Carlos]], [[Puerto Peñasco]] y [[Bahía de Kino]]. San Carlos, y su playa Los Algodones es una de las zonas más visitadas en la costa sonorense. San Carlos tiene una gran variedad de vida marina en sus costas, lo que la convierte en un lugar popular para la pesca deportiva y el buceo. Algunos yaquis y seris cerca de [[Guaymas]] y el Cerro del Tetakawi viven la de pesca. 

[[Puerto Peñasco]] se localiza en el extremo noroeste del estado en el Golfo Superior algo cerca del desemboque del [[Río Colorado]]. Tiene 110 kilómetros de playas en aguas tranquilas, cerca del [[Gran desierto de Altar|Desierto de Altar]] y de la [[Reserva de la biosfera El Pinacate y Gran Desierto de Altar|reserva de la biosfera de El Pinacate]], con uno de los climas más secos de México. Desde la década de 1990, ha experimentado un desarrollo a gran escala a lo largo de sus extensas playas.

En el municipio de Hermosillo se encuentra [[Bahía de Kino]], las playas de la bahía son de arena blanca y agua calmada y cálida. En las islas cercanas se pueden observar leones marinos. Cerca de ésta costa está [[Isla Tiburón]], la isla más grande de México y una reserva natural con borregos y venados salvajes. Ahí viven los [[Pueblo seri|seris]], una comunidad indígena con especial importancia en [[Punta Chueca]], que todavía practica la cacería, la pesca y la recolección de recursos naturales, además de la venta de manualidades a los turistas.
[[Archivo:Plaza_de_Alamos,_Son.jpg|thumb|250px|right|Plaza principal en [[Álamos (Sonora)|Álamos]].]]
Para promover el turismo en áreas fuera de las playas principales, el estado de Sonora ha creado varias rutas turísticas para su promoción así como el estado de "[[Pueblos Mágicos|Pueblo mágico]]" para algunas de sus ciudades más pequeñas. La [[ruta de las Misiones]] cubre las paradas principales del misionero [[Compañía de Jesús|jesuita]] [[Eusebio Francisco Kino]], entre éstas se encuentran iglesias y misiones en [[Caborca]], [[Pitiquito]], [[Oquitoa]], [[Atil (Sonora)|Átil]], [[Tubutama (municipio)|Tubutama]], [[Imuris|Ímuris]], [[Cucurpe]] y [[Magdalena de Kino]]. En Magdalena, los restos del padre Kino se encuentran en el mausoleo en la Plaza Monumental. 

La [[ruta del río Sonora]] sigue una serie de pueblos a lo largo del [[río Sonora]]. La ruta incluye los asentamientos de [[Ures]], [[Baviácora (municipio)|Baviácora]], [[Aconchi (municipio)|Aconchi]], [[San Felipe de Jesús (Sonora)|San Felipe de Jesús]], [[Huépac]], [[Banámichi]], [[Sinoquipe]], [[Arizpe]], [[Bacoachi (municipio)|Bacoachi]] y [[Cananea]]. La ruta incluye atracciones como viejas haciendas, plazas, arroyos, bosques y otros atractivos naturales.

[[Archivo:Estadio_Sonora_2.JPG|thumb|250px|Los Naranjeros de Hermosillo disputando un partido de la Liga Mexicana del Pacífico en el [[Estadio Sonora]].]]

El estado cuenta con múltiples infraestructuras deportivas, principalmente en Hermosillo y Ciudad Obregón, en las que se incluyen el Centro de Usos Múltiples (CUM), el [[Estadio Héroe de Nacozari]], el [[Estadio Sonora]], el [[Gimnasio del Estado de Sonora|Gimnasio del Estado]], la [[Arena ITSON]], así como múltiples unidades deportivas.

El estado cuenta además con los siguientes equipos deportivos:
El deporte más popular de Sonora es el béisbol, con casi todas las ciudades principales con al menos un equipo de béisbol que juega en la liga regional. Aunque el fútbol no es tan popular en el estado se mantiene como una práctica recreativa por la facilidad del desarrollo del juego en múltiples instalaciones deportivas.





[[Categoría:Estado de Sonora]]

</doc>
<doc id="37852" url="https://es.wikipedia.org/wiki?curid=37852" title="Organización Mundial del Turismo">
Organización Mundial del Turismo

La Organización Mundial del Turismo (OMT) es un organismo internacional creado en 1975 que tiene como propósito promover el turismo. Formalmente vinculada a las Naciones Unidas desde 1976 al transformarse en una agencia ejecutiva del PNUD. En 1977 se firmó un convenio que formalizó la colaboración con las Naciones Unidas, siendo un organismo especializado del sistema de las Naciones Unidas desde 2003, dependiente del Consejo Económico y Social de las Naciones Unidas. Tiene su sede en Madrid (España) y cuenta con 158 Estados miembros, 6 territorios y más de 500 miembros del sector privado, instituciones educativas, asociaciones de turismo y autoridades locales de turismo (en 2018). El día mundial del turismo se celebra el 27 de septiembre, coincidiendo con la fecha de aprobación de sus estatutos.

Los siguientes son los órganos de la
La "Asamblea General" es el órgano supremo de la Organización. En la Asamblea se reúnen en sesión ordinaria, cada dos años, los delegados de los Miembros Efectivos y de los Miembros Asociados, mientras que como observadores actúan los Miembros Afiliados y representantes de otras organizaciones internacionales.

Es la reunión más importante de altos funcionarios y de eminentes representantes del sector privado del mundo entero.

Reuniones anteriores:

Las seis "Comisiones Regionales", fueron creadas en 1975, como órganos subsidiarios de la Asamblea General, se reúnen normalmente una vez por año. Les permiten a los Estados Miembros mantener contacto entre ellos y con la Secretaría entre dos reuniones de la Asamblea General a las que someten propuestas y, expresan sus preocupaciones. Cada Comisión elige su propia mesa para un mandato de dos años que cubre el período entre dos reuniones de la Asamblea General.

El Consejo Ejecutivo tiene por misión adoptar, en consulta con el Secretario General, todas las medidas necesarias para el cumplimiento de sus propias decisiones y de las resoluciones de la Asamblea General, informando a ésta de su actuación. El Consejo se reúne por lo menos dos veces al año.

El Consejo Ejecutivo se compone de "Miembros Efectivos", elegidos por la Asamblea a razón de un miembro por cada cinco Miembros Efectivos, de conformidad con el reglamento establecido por la Asamblea, a fin de obtener una distribución geográfica justa y equitativa.

El mandato de los miembros elegidos en el Consejo es de cuatro años y cada dos años se procede a la renovación de la mitad de los Miembros del Consejo. España es Miembro Permanente del Consejo.

El Consejo elige a su propia mesa entre sus miembros, para un mandato de un año.

La oficina de la Secretaría de la OMT se encuentra en su sede institucional de Madrid (España). La
Secretaría está dirigida por el Secretario General, habiendo sido nombrado para este cargo Zurab Pololikashvili (desde el uno de enero de 2018), procedente de Georgia, y se organiza en
forma de programas que cubren temas tales como la
sostenibilidad, la educación, las tendencias del turismo y el
marketing, el desarrollo sostenible, las estadísticas y la cuenta
satélite de turismo (CST), la gestión de destinos, la ética y la gestión
de riesgos y de crisis. El Programa de Cooperación Técnica y
Servicios, por su parte, lleva a cabo proyectos de desarrollo en más
de 100 países del mundo, mientras que los Programas Regionales
para África, las Américas, Asia y el Pacífico. Miembros.
El Programa de Miembros Afiliados, por último, representa a los
más de 500 miembros de la OMT del sector privado. No obstante, cabe señalar que la modificación estatutaria realizada en 2005 suprimió la clase de "Afiliados", cuyos miembros adquieren desde entonces la calidad de "Asociados".





</doc>
<doc id="37857" url="https://es.wikipedia.org/wiki?curid=37857" title="Prevalencia">
Prevalencia

En epidemiología, se denomina prevalencia a la proporción de individuos de un grupo o una población (en medicina, persona), que presentan una característica o evento determinado (en medicina, enfermedades). Por lo general, se expresa como una fracción, un porcentaje o un número de casos por cada 10.000 o 100.000 personas.

Podemos distinguir dos tipos de prevalencia: puntual y de periodo.

Es un parámetro útil porque permite describir un fenómeno de salud, identificar la frecuencia poblacional del mismo y generar hipótesis explicatorias. La utilizan normalmente los epidemiólogos, las personas encargadas de la política sanitaria, las agencias de seguros y en diferentes ámbitos de la salud pública.

La prevalencia de una enfermedad se calcula dividiendo el número total de los individuos que presentan un atributo o enfermedad en un momento o durante un periodo (evento), entre la población en ese punto en el tiempo o en la mitad del periodo (número de individuos). Cuantifica la proporción de personas en una población que tienen una enfermedad (o cualquier otro suceso) en un determinado momento y proporciona una estimación de la proporción de sujetos de esa población que tenga la enfermedad en ese momento.

formula_1

Por ejemplo, la prevalencia de obesidad entre los adultos estadounidenses en 2001 fue estimada por los Centros para el Control de Enfermedades de los EE. UU . (CDC) en aproximadamente el 20,9%.

La prevalencia es un término que significa estar extendido y es distinto de la incidencia . La prevalencia es una medida de "todos los" individuos afectados por la enfermedad en un momento determinado, mientras que la incidencia es una medida del número de nuevos individuos que contraen una enfermedad durante un período de tiempo particular. La prevalencia es un parámetro útil cuando se habla de enfermedades duraderas, como el VIH , pero la incidencia es más útil cuando se habla de enfermedades de corta duración, como la varicela .

formula_2


Según la National Library of Medicine de EE.UU., la seroprevalencia es la manifestación general de una enfermedad o una afección dentro de una población definida en un momento dado, medida con análisis de sangre (pruebas serológicas).



</doc>
<doc id="37858" url="https://es.wikipedia.org/wiki?curid=37858" title="Paprika Steen">
Paprika Steen

Paprika Kirstine Steen, actriz danesa nacida en Copenhague el 3 de noviembre de 1964.

Sus papeles en "Los idiotas" (1998) de Lars von Trier o "La Celebración" (1998) de Thomas Vinterberg le han dado fama internacional.

En el año 2004 estrenó como directora con la película "Lad de små børn..." ("Dejales a los niños") que ganó premios en varios festivales de cine.

En 2011 co-protagonizó "Superclásico", una película danesa/argentina.


</doc>
<doc id="37860" url="https://es.wikipedia.org/wiki?curid=37860" title="Jalapa">
Jalapa

La palabra Jalapa se refiere a las siguientes localidades:

En Estados Unidos:

En México:

En Guatemala:

En Nicaragua:
También es el nombre de una especie vegetal:


</doc>
<doc id="37863" url="https://es.wikipedia.org/wiki?curid=37863" title="Factor de confusión">
Factor de confusión

En una investigación científica, una variable de confusión o factor de confusión es una variable o factor que distorsiona la medida de la asociación entre otras dos variables. El resultado de la presencia de una variable de confusión puede ser el surgimiento de un efecto donde en realidad no existe o la exageración de una asociación real (confusión positiva) o, por el contrario, la atenuación de una asociación real e incluso una inversión del sentido de una asociación real (confusión negativa).


</doc>
<doc id="37865" url="https://es.wikipedia.org/wiki?curid=37865" title="Guadalajara (México)">
Guadalajara (México)

Fue capital de la Provincia de Nueva Galicia, actual Nayarit y Jalisco, en el Reino de Nueva Galicia entre su fundación y 1786, y de la Independencia de Guadalajara de 1786 a 1821.. En la Independencia de México, Guadalajara desempeñó un papel importante, siendo la sede en la que se declaró la abolición de la esclavitud por el cura Miguel Hidalgo. Al terminar la guerra de independencia, y con la proclamación de estado libre y soberano de Jalisco, Guadalajara se convirtió en capital del estado.

Guadalajara se constituye como el , después de Ciudad de México y Monterrey, con un PIB de 80,656 millones de dólares en 2014. Fue catalogada como ciudad "gamma" en el 2016 y como una de las 90 ciudades más productivas del mundo, con una puntuación de 56.3.

La economía de la ciudad se basa en servicios e industria, especialmente en tecnología de la información, con un gran número de firmas internacionales que tienen oficinas en la región e instalaciones de fabricación en el Área Metropolitana de Guadalajara, y a su vez varias empresas nacionales de dicha actividad con sede en la ciudad. Otras industrias más tradicionales, como el calzado, los textiles y el procesamiento de alimentos también son factores importantes que contribuyen. Asimismo, es un importante centro cultural en México, considerada la cuna del mariachi y sede de varios acontecimientos culturales de renombre internacional, como el Festival Internacional de Cine de Guadalajara y la Feria Internacional del Libro de Guadalajara, que atraen multitudes nacionales e internacionales. Guadalajara fue nombrada Capital Americana de la Cultura en el 2005 y sede de los Juegos Panamericanos de 2011.

Su nombre proviene de la ciudad homónima en Castilla-La Mancha, el cual procede a su vez del vocablo andalusí وادي الحجارة ("wādi al-ḥiŷara"), que significa ‘valle de la piedra’, aunque la traducción tradicional es ‘río de piedras’, ‘río que corre entre piedras’ o ‘valle de las fortalezas’. El fundador, Cristóbal de Oñate, nombró así a la ciudad en honor al conquistador del occidente de México, Nuño de Guzmán, que nació en Guadalajara, España.

En México también se le conoce "como La Perla de Occidente", "La Perla Tapatía", o "La Ciudad de las Rosas". Su abreviatura oficial es "Guad" pero también es común el uso de las letras "GDL" para referirse a la ciudad"."

Su gentilicio es "guadalajarense", aunque popularmente (y tal vez de manera más común) también se conoce a las personas oriundas de Guadalajara como tapatías y tapatíos.

Tras la victoria de Tonalá (acaecida el 25 de marzo de 1530), Nuño de Guzmán goza del tributo y homenaje de todas comunidades del valle de Atemajac, donde hoy se asienta la capital jalisciense. Inclusive, el conquistador aspira a ser nombrado por Carlos V como el primer marqués del Valle de Tonalá.

La ciudad tuvo cuatro asentamientos antes de establecerse su estancia en dicha capital, en un principio estuvo en Nochistlán en el paraje conocido como el zapote hoy conocida como San Juan. La fundó Cristóbal de Oñate el 5 de enero de 1532, quien al efecto había sido comisionado por Nuño de Guzmán. Este deseaba contar con una ciudad que le sirviera para asegurar sus conquistas. Entre La Villa de Guadalajara la fundaron 42 vecinos; el nombre de Guadalajara lo tomaron en recuerdo de la ciudad española homónima, cuna de Nuño de Guzmán. Poco duró la Villa en este sitio, con la anuencia de Guzmán, Juan de Oñate (hijo de Cristóbal de Oñate), Miguel de Ibarra y Sancho Ortiz, el 19 de mayo de 1533, proyectaron mudarla de lugar. Así, para el día 8 de agosto de 1533, Guadalajara se encontraba en su segundo asiento.

La nueva ciudad fue atacada el 28 de septiembre de 1535 por los aborígenes que habían participado en la Guerra del Mixtón. Cristóbal de Oñate, entonces gobernador de la ciudad organizó una batalla contra los aborígenes de la que resultaron victoriosos los nuevos habitantes de Guadalajara. Pensaron entonces trasladarla al valle de Atemajac, por este valle corría el río San Juan de Dios y era un sitio más seguro para ser defendido de cualquier ataque de los naturales.

Unos se trasladaron de Tlacotán a Tonalá y otros a Tetlán en donde el 9 de octubre de 1541 se pregonó el padrón de los nuevos vecinos. Cristóbal de Oñate, el 5 de febrero de 1542, nombró a los integrantes del nuevo ayuntamiento que regiría los destinos de la nueva ciudad. Finalmente, el 14 de febrero de 1542 se fundó la ciudad de Guadalajara en el sitio donde actualmente se encuentra; asentándose, además de Cristóbal de Oñate, 63 familias españolas (incluida por aquel entonces Portugal). Se instaló el primer ayuntamiento de la actual Guadalajara, presidido por el vizcaíno Miguel de Ibarra.

Hasta el mes de agosto de 1542, llegaron a su destino las reales cédulas expedidas por el emperador Carlos I de España y V del Sacro Imperio Romano, en noviembre de 1539, en las cuales concedía a Guadalajara el título de ciudad y escudo de armas. Ese mismo mes se pregonaron ambas cédulas en la plaza mayor de la novel y definitiva Guadalajara. 

En 1560 el papa Paulo III autorizó establecer en Guadalajara el obispado de la Nueva Galicia, y en ese mismo año la audiencia del reinado fue trasladada también para esa ciudad, que por la época abrigaba a unos 500 españoles, a otros tantos esclavos negros y unas 2200 familias indígenas esparcidas en un área de cinco km alrededor de los primeros cimientos de la Catedral.

El 18 de noviembre de 1791, se dispuso la fundación de la Universidad de Guadalajara en la ciudad del mismo nombre, capital del Nuevo Reino de Galicia. La inauguración de este centro cultural fue el 3 de noviembre de 1792, teniendo como sede el excolegio de Santo Tomás.

Para la guerra de Independencia, Guadalajara desempeñó un papel importante, ya que fue en esta ciudad donde el cura Miguel Hidalgo y Costilla, declaró la abolición de la esclavitud. Fue también aquí donde publicó el periódico "El Despertador Americano" donde publicaba sus ideas. En las cercanías del lugar, en el puente de Calderón, tuvo lugar la batalla donde fueron vencidos los insurgentes. Guadalajara también fue testigo de la muerte del insurgente José Antonio “El Amo Torres”, quien ayudó a Hidalgo a tomar la ciudad. Al terminar la guerra de independencia, y con la proclamación de estado libre y soberano de Jalisco, Guadalajara se convirtió en la capital del estado.

El Porfiriato había terminado y estallaba la revolución mexicana. Para ese entonces Guadalajara reinaba la calma aparente (ya que el conflicto se concentraba en la capital). Después del conflicto cristero, la paz regresó a Guadalajara. Durante un largo periodo la ciudad floreció y que comenzó a crecer desde la colonia, nacían así los nuevos conceptos arquitectónicos que decorarían la ciudad con estilos desde los años 1920 hasta los años de 1980.

Guadalajara creció aceleradamente hasta ocupar un sitio como metrópolis mexicana industrial, turística y de servicios y como la segunda economía en México después de la Ciudad de México. Con la Revolución de 1910, Guadalajara pasó a ser la segunda ciudad más poblada del país, pero durante las décadas siguientes sobrevinieron guerras de carácter regional, en los estados de Jalisco, Michoacán y Guanajuato. Asimismo, las secuelas del crac del 29 repercutieron mucho más de lo deseado. La década de 1940 fueron de tranquilidad social y política, y de crecimiento marcado en el comercio, la industria y la demografía.

Durante cada periodo gubernamental, la ciudad pasó por planes estructurales con los que nacieron nuevas zonas y núcleos comerciales y con los que empresas transnacionales e industrias internacionales llegaron a la ciudad. La ciudad comenzó a expandirse hasta encontrar la unión territorial con el municipio de Zapopan. Entre los desarrollos creados durante este periodo se cuentan la Expo Guadalajara, el tren ligero, la expansión de calles y avenidas. Surgieron en la ciudad el primer centro comercial en Latinoamérica, el primer sistema de tren eléctrico urbano en América Latina, y la primera universidad autónoma en México.

Durante las explosiones de Guadalajara de 1992 cientos de casas, avenidas, calles, empresas e infraestructura en la colonia Analco quedaron seriamente dañadas, "sin que a la fecha exista un claro deslinde de información y responsabilidades", en uno de los acontecimientos más trágicos en la historia de Guadalajara. Este suceso, aunado a la crisis económica mencionada, dio como resultado la pérdida del poder industrial de Guadalajara; la investigación de los hechos duró más de 11 años en los que no se encontraron pruebas suficientes para nombrar un responsable, las investigaciones ahora están cerradas atribuyendo los acontecimientos a un accidente.

La ciudad de Guadalajara se encuentra en el Estado de Jalisco y se asienta en el Valle de Atemajac, que en náhuatl significa "lugar donde el agua se bifurca", en el Eje Neovolcánico. Tiene una altitud promedio de 1570 msnm, en su mayoría son lomas bajas, cuyo punto más alto es el Cerro del Cuatro (). El municipio cuenta con el Río San Juan de Dios, que se encuentra entubado, al norte con el Río Santiago y el Arroyo Atemajac y al sur del municipio se encuentran los manantiales del Agua Azul. El municipio de Guadalajara es el más poblado en el estado de Jalisco, su extensión territorial es de 187,91 km² (municipio), y de más de 850 km² en aglomeración. El suelo es de origen volcánico y del periodo Cuaternario y Terciario de la Era Cenozoica, de uso urbano en su mayoría. La actividad sísmica es de moderada a intensa y la actividad volcánica se reduce al Volcán de la Primavera, en la Sierra Primavera.

El clima de la ciudad es templado húmedo subtropical con lluvias en verano de humedad media e invierno mayormente seco (Cwa) según la clasificación climatica de Köppen.

Los días de primavera comprenden los más secos y cálidos del año. Siendo en todo mayo y principios de junio, los más calurosos de esta época y por ende del año, con máximas qué ocasionalmente superan los 35 °C, y con mínimas que varian entre los 13°C y 20°C en las noches más cálidas.

Posteriormente (en verano) llega la temporada de lluvias que tiene lugar después de la primera quincena de junio hasta finales de octubre, presentándose tormentas con intensa actividad eléctrica, fuertes vientos y en ocasiones granizadas, como consecuencia de esto las temperaturas máximas bajan a un promedio de 26.5 °C en esta época del año.
De esta temporada destaca el mes de julio por ser el más húmedo, lluvioso y con más cantidad de días nublados en todo el año, por esto mismo sule ser el más representativo de la temporada.

Hacia otoño e invierno las lluvias se reducen y dan paso a los días soleados y vientos fríos del norte. En invierno la temperatura mínima promedio es de 5°C, pero pueden ocurrir ocasionales heladas, especialmente en las afueras de la ciudad cerca del municipio de Zapopan, con temperaturas que pueden descender por debajo de los –2 °C durante las noches más frías. Aun así, es relativamente común que la temperatura dentro de Guadalajara descienda en el amanecer (alrededor de las 8:00 a. m.) hasta caer a 1 °C o 0°C, en al menos cuatro ocasiones, entre diciembre, enero y febrero. Las temperaturas diurnas (en invierno) pueden variar, entre 12 °C y 26 °C, según sean días lluviosos, soleados o haya algún frente frío. Sin embargo los días cálidos son numerosos, pudiendo registrarse incluso en febrero tardes con 28-30 °C.

En la temporada de lluvias se han llegado a presentar tornados. El más destructivo fue el llamado "Tornado de Talpita" registrado el 27 de junio del 2018 en punto de las 20:30, <formado por una una nube embudo con intensa lluvia acompañada de granizo y vientos de hasta 120km/h> ocasionó daños en la zona oriente de la ciudad, pero sobre todo en la colonia homónima (Talpita), en donde hubo una destrucción casi total, y como consecuencia aparte, se estimaron más de 200 árboles caídos en la ciudad. 

La temperatura máxima histórica de 38.7 °C, fue registrada el 6 de mayo de 1994, aunque se registraron 38.5 °C el 9 de mayo de 1998 y el 4 de mayo de 2003. El 31 de mayo de 2018 se registró una temperatura de 37.7 °C convirtiéndose en la más alta que se ha presentado desde el 2004 hasta la actualidad y la noche más calurosa de su historia fue el 5 de junio de 2020 registrando una mínima de 23.3 °C. 

Por contraparte, la temperatura más baja registrada fue de –7.0 °C el 14 de diciembre de 1997, cuándo por segunda vez nevó en la ciudad, hecho inusual después de 116 años de la primera nevada el 8 de febrero de 1881.

Debido a su posición geográfica y a su elevación, Guadalajara tiene el 50% del año (de abril a septiembre) un índice UV (radiación ultravioleta) extremo; alcanzando este su punto mínimo dentro de la ciudad de noviembre a enero (UV 5).
La riqueza natural de Guadalajara está representada por el Bosque de la Primavera, Los Colomos, y la Barranca de Huentitán. La flora se destaca por pinos michoacanos, diferentes especies de encinos, liquidámbar, fresnos, sauces; y árboles introducidos como tabachines, jacarandas y ficus, además de orquídeas, rosas y varías especies de hongos. La fauna se reduce a la típica fauna urbana, además de 106 especies de mamíferos, 19 especies de reptiles y seis especies de peces.

La Barranca de Huentitán (también conocida como Barranca de Oblatos) se localiza al norte del municipio de Guadalajara.
Mide aproximadamente 1.136 hectáreas y tiene una profundidad promedio de 600 metros de diferencia. La diferencia en altitudes de la curva de nivel más alta (1520 msnm) y la más baja (1000 msnm) es de 520 metros en el punto del riel del fonicular. Este cañón es nombrado también como Oblatos-Huentitán debido a que atraviesa 2 áreas de la ciudad llamadas Oblatos y Huentitán respectivamente.

La Cascada Cola de Caballo: está ubicada en la carretera Guadalajara a Zacatecas (km 15) a unos pocos kilómetros del Periférico norte justo después de pasar el poblado de San Esteban. La cascada está formada por una corriente proveniente del Valle de Atemajac pero ahora, por estar tan cerca de Guadalajara y de una colonia con muy poco desarrollo, se encuentra muy contaminada.

Bosque urbano o bosque de Colomos, donde se encuentra el jardín japonés, ubicado en la parte noroeste de Guadalajara. Se encuentra en una de las zonas de mayor plusvalía, resultando imposible conservar su superficie original. Fue una de las principales fuentes de abastecimiento de la ciudad, y en la actualidad sigue brindando ese vital líquido a algunas colonias aledañas. Actualmente, este bosque cuenta con 92 hectáreas de masa forestal en la que destacan los pinos, eucaliptos y cedros. Posee interesantes y variados atractivos como: pistas de trote, jardines, estanques, lago de aves, áreas para día de campo, de juegos infantiles, de campamento, de caballos.

Entre otros lugares están también: parque natural Acuático Camachos. Barranca de Huentitlán La nogalera. Barranca de Oblatos, Barranca Colimilla o Río grande, Jardín Japonés en el bosque de colonos.

Al igual que el resto de los municipios en México, Guadalajara es regida por un presidente municipal, quien ejerce el poder ejecutivo durante tres años consecutivos.

El poder legislativo lo tiene el cabildo, formado por la planilla escogida por el candidato a la alcaldía, compuesto por regidores, quienes no son elegidos por la ciudadanía por voto directo o indirecto, sino que la planilla pasa en automático si gana el alcalde.

El municipio está dividido en cinco distritos electorales para fines de elección de los representantes de la ciudad en el poder legislativo federal. Dichos distritos son el VIII, IX, XI, XIII y XIV del estado de Jalisco.

Debido al crecimiento de la mancha urbana, el área metropolitana se compone de los siguientes municipios: Guadalajara, Zapopan, Tonalá, Tlaquepaque, Tlajomulco de Zúñiga, El Salto, Juanacatlan, Ixtlahuacán de los Membrillos y Zapotlanejo. Cada municipio es gobernado por un alcalde de elección popular y con un mandato de tres años. Los alcaldes, junto con un grupo de regidores de nombre Cabildo, forman un Ayuntamiento. En este sentido cada municipio urbano y conurbano es autónomo, y lo que los une en pos del área metropolitana es el Consejo Metropolitano, el que está formado por los municipios ya mencionados y está regido por el gobernador del estado. Este grupo tiene por función ver los problemas y soluciones del área urbana y resolverlos en conjunto principalmente. Cada municipio se divide en zonas, estas tienen por función el ordenamiento del planeamiento urbano en general, Guadalajara cuenta con las siguientes siete zonas: Centro, Minerva, Huentitán, Oblatos, Olímpica, Tetlán y Cruz del Sur.

Igualmente la zona metropolitana, está dividida en cuatro sectores: Juárez, Hidalgo, Libertad y Reforma.

Según las cifras más actuales por parte del INEGI, corroboradas en el 2010, el municipio de Guadalajara tiene una población aproximada de con una población en el área metropolitana de habitantes, siendo la ciudad más poblada del estado de Jalisco, la de mayor conurbación dentro de la zona metropolitana de Guadalajara, y la segunda ciudad más poblada de México; la primera es la Ciudad de México.

En el 2007, la ONU listó a las cien aglomeraciones urbanas más pobladas del mundo. México destacó con tres ciudades en la lista: Ciudad de México, Guadalajara y Monterrey. Guadalajara ocupó el 66º lugar de estas ciudades. En la lista de Latinoamérica, Guadalajara ocupó el 10º lugar.

El municipio de Guadalajara se localiza al centro del Estado, un poco hacia el oriente, en las coordenadas 20° 36’ 40" a los 20° 45’ 00" de latitud norte y 103° 16’ 00" a los 103° 24’ 00" de longitud oeste, a una altura de 1700 metros sobre el nivel del mar.

El municipio de Guadalajara se encuentra delimitado al norte con Zapopan e Ixtlahuacán del Río, al oriente con Tonalá y Zapotlanejo, al sur con Tlaquepaque y al poniente con Zapopan.

El crecimiento de la ciudad se debe a que Guadalajara (cabecera) ha crecido y se adjudicó las comunidades más cercanas. Así ocurrió con las antiguas comunidades Atemajac, Huentitán, Tetlán, Analco, Mexicaltzingo, Mezquitan y San Andrés, entre otras, que fueron absorbidas por el crecimiento de la cabecera municipal y que forman ahora parte integral de la conurbación, quedando totalmente urbanizadas.

Actualmente las comunidades más cercanas a Guadalajara son:

La segunda zona metropolitana más poblada del país. Lo integran 6 municipios centrales y 3 exteriores. Los centrales son Guadalajara, Zapopan, San Pedro Tlaquepaque, Tonalá, Tlajomulco de Zúñiga y El Salto; y los externos son Ixtlahuacán de los Membrillos, Juanacatlán y Zapotlanejo en los Altos de Jalisco.

El incremento de la población del municipio fue de un 0,80% desde 1995 al 2000, representado con un incremento de 13103 habitantes aproximadamente. La tasa de crecimiento anual en la ciudad del período 1995 al 2000 fue de 0,0%. Esta tasa se redujo considerablemente desde 1990 a 1995 en la que la tasa de aumento era de –0,2%.

Por grupos de edad, la población en el 2000 reflejaba que la población de 15 a 64 años es la que forma mayoría con 1 049 545 habitantes y el grupo con menor cantidad es el de los mayores de 65 años con 97 134 habitantes. En el 2000 la población se mantenía de esta manera:

La densidad de población del municipio de Guadalajara no ha cambiado mucho desde el año 1980, casi siempre se ha mantenido por encima de los 8000 habitantes por kilómetro cuadrado.

La mayoría de la población en el municipio es urbana, debido a que gran parte del municipio es de uso urbano, la única área sin urbanizar corresponde a la barranca de Huentitan, debido a sus características y por ser un patrimonio natural del municipio. La población se concentra más si es de tipo urbano que rural; a pesar de esto todavía se encuentra un grado de población rural.

La ciudad es sede de la segunda arquidiócesis católica más importante de México, la Arquidiócesis de Guadalajara, solo detrás de la Arquidiócesis Primada de México. Posee además el seminario con mayor número de estudiantes, no solo de México sino a nivel mundial con 622 seminaristas en etapa de seminario mayor. Cuenta con una fuerte jerarquía presidida por el arzobispo cardenal José Francisco Robles Ortega y tres obispos auxiliares. La Arquidiócesis de Guadalajara reportó en 2009 los siguientes números: 432 parroquias, 1100 sacerdotes diocesanos, 341 sacerdotes religiosos, 25 ordenaciones sacerdotales diocesanas y 21 ordenaciones sacerdotales de las congregaciones religiosas. Actualmente hay en la Arquidiócesis de Guadalajara Un sacerdote diocesano por cada 6454 habitantes del territorio pastoral.

Guadalajara alberga un gran número de seguidores cristianos de otras filiaciones; protestantes, anglicanos, ortodoxos, mormones y testigos de Jehová. También cuenta con una comunidad judía establecida. Dicha presencia se remonta a las primeras décadas del siglo XX, cuando la comunidad estaba dividida entre judíos de origen Ashkenazí y judíos de origen Sefaradí (español). A mediados de los años 60, ambas comunidades deciden fusionarse y trabajar de manera conjunta. Bajo dicha unión, los judíos de la ciudad vivieron alrededor de 40 años, contando con la presencia en la ciudad de varios rabinos conservadores. El nuevo siglo trajo a la vida judía de la ciudad la ruptura de la organización unificada, generándose la división entre la Comunidad Israelita de Guadalajara, con rito ortodoxo, y la Comunidad Hebrea de Guadalajara, con rito conservador.

También hay un importante número de creyentes en las doctrinas del budismo, hinduismo, judaísmo y un fuerte movimiento de "nuevas religiones" como el movimiento de paz mundial (RAP), rainbow, rastafari, nueva era ("new age"), y otras. La ciudad es centro mundial de la Iglesia La Luz del Mundo donde también se celebra la “Santa Cena”, un evento religioso internacional. 

Si bien los Santos Patronos de Guadalajara son san Miguel Arcángel, Santiago Apóstol y la Virgen de Guadalupe, el día que puede ser señalado como de la fiesta religiosa más grande de Guadalajara es el 12 de octubre, día en que la imagen de la Virgen de Zapopan recorre el trayecto que va desde la catedral Metropolitana de Guadalajara hasta la basílica de Nuestra Señora de la Expectación de Zapopan. Lo que en Guadalajara se conoce como “La Romería”. Aproximadamente dos millones de personas salen a las calles que constituyen el trayecto que recorre la venerada imagen, en una verdadera fiesta de pueblo, en el mejor de los sentidos.

La Romería es el punto álgido de una fiesta religiosa que dura prácticamente todo el año, y que consiste en la visita que hace la imagen de Nuestra Señora de la Expectación de Zapopan a prácticamente todas las capillas y parroquias de la ciudad de Guadalajara.

Oficialmente las visitas de la imagen inician en mayo, justo antes de que inicie el temporal de lluvias, y se prolongan hasta el día 12 de octubre, visitando dos parroquias por día. En promedio trescientas parroquias en este período al año. Casas y calles son adornados con papel picado, alfalfa y arreglos florales por donde pasa la imagen antes de llegar a la parroquia o capilla de cada barrio.

El último templo en ser visitado es la catedral Metropolitana, regularmente llega allí el día 9 de octubre y permanece hasta el día 12 por la mañana momento en que inicia su regreso a la basílica de Zapopan, su santuario, en lo que los tapatíos llaman la Romería.

Esta devoción es muy particular de Guadalajara y no tiene paralelo en México, ni en sus formas ni en su historia, historia que está íntimamente ligada a la fundación misma de Guadalajara en 1542, pues ya en 1531 fray Antonio de Segovia, recorría el valle de Atemajac y Zapopan, evangelizando a los originales de estas tierras, acompañado de la imagen de la Virgen de la Expectación, que es la imagen original que hace el recorrido cada 12 de octubre.

Hay una fuerte comunidad tapatía en la ciudad de Los Ángeles (EE.UU.) que ha llevado esta devoción y tienen una réplica de la imagen de la virgen de Zapopan que igualmente visita varias capillas y parroquias de aquella ciudad.


Guadalajara cuenta con una extensa gama de servicios públicos dedicados a la ciudadanía. La mayoría son solventados por el gobierno estatal de Jalisco, y otros por la iniciativa privada.


El tema de salud es atendido por la Secretaría de Salud del gobierno estatal, el Instituto Mexicano del Seguro Social (IMSS), el Instituto de Seguridad y Servicios Sociales de los Trabajadores del Estado (ISSSTE), el Hospital Civil de Guadalajara, la Cruz Verde, la Cruz Roja Mexicana, además de un gran número de clínicas y hospitales particulares, así como gabinetes de radiodiagnóstico.

El bienestar social es atendido en sus diferentes vertientes por el Sistema para el Desarrollo Integral de la Familia (DIF), a través del Comité Municipal y algunos otros organismos asistenciales públicos y privados. 79 es el número de clínicas, hospitales, sanatorios y unidades médicas que se encuentran actualmente en Guadalajara. Actualmente, Guadalajara cuenta con una extensa red de servicios de salud, tanto públicos como privados.

La traza urbana dentro del municipio de Guadalajara puede considerarse variada, ya que dentro de él se sitúan varias formas de calles, avenidas, colonias y fraccionamientos que tienen diferentes trazos como el ortogonal, es decir, con líneas horizontales y verticales que se cruzan, e irregular, es decir, calles y avenidas sin sentido, pero en general podemos decir que Guadalajara cuenta con una traza de estrella, que son cinco salidas, con varios anillos viales a su alrededor y cruzando en ellos.

Al inicio de Guadalajara, esta contaba con una traza ortogonal, es decir, con líneas horizontales y verticales que se cruzan, sin embargo al paso de los años y al ir creciendo hacia el río San Juan de Dios, esta traza se inclinó aunque se siguió con la misma tendencia, a diferencia del lado norte, oeste y sur, donde se siguieron las mismas líneas de calles.

Esto cambiaría cuando se introdujo el ferrocarril a Guadalajara en 1888, ya que en el siglo XX al urbanizarse en el sur, sufrió otra inclinación, se da el mismo caso que en San Juan de Dios. Con el paso del tiempo y la adjuntación de pueblos, primeramente Analco, Mexicaltzingo, Mezquitan y San Juan de Dios, y posteriormente han hecho que Guadalajara tenga una variada traza, a esto se suma su crecimiento acelerado en el Siglo XX, lo que generó la traza mencionada en el primer párrafo.

Durante el gobierno de José de Jesús González Gallo, entre 1947 y 1953, Guadalajara fue objeto de grandes obras públicas que cambiaron en parte el paisaje urbano del centro histórico de la urbe.

Destacan las ampliaciones de las avenidas Alcalde-16 de septiembre y Juárez, mismas que ya no eran suficientes para el creciente número de automóviles que, día a día, circulaban por el centro de la ciudad. Así pues, para agrandar las avenidas fue necesario derrumbar edificios y emparejar el trazo de la calle. Aquellas demoliciones no han dejado de generar controversia, pues, aunque se procuró modernizar y agilizar al centro de Guadalajara, no deja de ser lamentable la pérdida irreparable de muchas construcciones antiguas con valor arquitectónico e histórico.

Algunos otros edificios del entorno de la catedral de Guadalajara fueron derribados con el propósito de dejar grandes espacios abiertos en los cuatro costados de la sede episcopal, dando forma a una gran cruz latina en cuyo centro sobresale la Catedral.

La Cruz de Plazas se conformó a partir de la ya existente Plaza de Armas, en el costado sur de la iglesia. En este espacio no fueron necesarias las demoliciones. En cambio, al frente de la fachada principal de la catedral sí fue necesario abrir el espacio para la plaza del Ayuntamiento (posteriormente conocida como plaza de la Fundación, plaza de los Laureles y Plaza Guadalajara, sucesivamente). Asimismo, al norte, una antigua iglesia del siglo XVII fue reemplazada por la plaza de la Rotonda de los Hombres Ilustres (mausoleo en el que están depositados los restos de jaliscienses destacados). Finalmente, para completar la cruz en su parte más larga, fueron arrasadas por completo las antiguas mansiones que ocupaban las dos manzanas entre la fachada posterior de la catedral y la fachada frontal del Teatro Degollado, dando lugar a la plaza de la Liberación (llamada también Plaza de los Tres Poderes, y mejor conocida como Plaza de las Dos Copas).

En su empeño por hacer de Guadalajara una ciudad sustentable se ha construido el corredor México (COME) desde la avenida López Mateos hasta Juan Palomar y Arias, con la intención de ampliar la infraestructura de la ciclovía por la avenida de México con 1,8%km y el mejoramiento y embellecimiento con jardines y el establecimiento de un Punto Limpio.

Existen actualmente 17.200 calles y avenidas, muchas de las cuales se han ido trazando según la necesidad de la población; algunas calles dejan de funcionar como tales para convertirse en avenidas, expandiendo su trazado vial. Las modificaciones, las reparaciones y las hechuras corren a cuenta del gobierno municipal de Guadalajara.

Guadalajara está conformada por más de 2300 colonias en las que se extiende la Zona Metropolitana, el primer cuadro de la ciudad lo conforman casas en su mayoría de más de 2 niveles con estilos arquitectónicos que van desde el churrigueresco, barroco y estilos europeos del siglo XIX, el primer cuadro de la ciudad lo conforman las zonas céntricas y sus alrededores, como el barrio del Santuario, Mezquitan, Analco, San Juan de Dios y la Colonia Centro.

Hacia el poniente del primer cuadro comienzan a levantarse las casonas del siglo XIX, residencia de distinguidos personajes en la historia de la ciudad, estructuras neoclásicas y casonas del Porfiriato, este cuadro lo conforman colonias como Lafayette, Americana, Moderna, Arcos Vallarta, en las cuales sus respectivas expansiones corresponden a construcciones de los años 1920,1930, 1940 y 1950. A sus alrededores Guadalajara se expande en un segundo cuadro donde el florecimiento de las nuevas tendencias arquitectónicas de los años 1960 y 1970 dejaran la huella de colonias como la Colonia Americana, Vallarta Poniente, Moderna, Providencia, Vallarta San Jorge, Jardines del Bosque, Chapalita, etc. Entre las cuales se encuentran desde las líneas posmodernistas, el Art déco hasta el legado arquitectónico de uno de los iconos mundiales de la arquitectura Mexicana: Luis Barragán.

La ciudad cuenta con los desarrollos residenciales y comunidades privadas más distinguidos del occidente del país. Estas colonias se ubican tanto en el municipio de Guadalajara, como en su municipio vecino de Zapopan y algunas en el sur de la ciudad, en el municipio de Tlajomulco. Algunas de estas colonias son: Colinas de San Javier, Puerta de Hierro, Providencia, Chapalita, Jardines de San Ignacio, Ciudad del Sol, Valle Real, Lomas del Valle, Santa Rita, Monraz, Santa Anita Club de Golf, El Cielo, Santa Isabel, Virreyes, Bugambilias, Las Cañadas, La Estancia, etc.

Los límites de la ciudad son conformados en su mayoría por colonias de clase media y conjuntos habitacionales desarrollados como parte de planes gubernamentales. El poniente de la ciudad, es en conjunto el área que representa el índice económico más elevado, mientras que el oriente muestra un nivel más bajo. La ciudad se extiende hacia el poniente en colonias como Pinar de la Calma, Las Fuentes, Paseos del Sol, El Colli Urbano, La Estancia, anexando su zona metropolitana al municipio de Zapopan; mientras que hacia el oriente lo hace en colonias como San Juan Bosco, San Andrés, Oblatos, San Onofre, Insurgentes, Jardines de la Paz, Jardín de los Poetas, por mencionar algunas.

Los parques y bosques son importantes en Guadalajara. Por eso de las tres ciudades más importantes de México, es la que tiene más áreas verdes y plantas. Cabe destacar que no todos se encuentran en Guadalajara, sino que están distribuidos en la Zona Metropolitana de Guadalajara (ZMG).
"Los parques más importantes son:"

Jardines

Parques

Bosques

Zoológicos

En Guadalajara existe una creciente inversión, tanto pública como privada, lo que ha provocado que se desarrollen varios proyectos distribuidos en la Zona Metropolitana, principalmente hacia el lado Oeste del municipio de Guadalajara y Zapopan. Estos son los principales proyectos, sin embargo, algunos encuentran problemas de financiamiento, su construcción está en suspenso e incluso posible cancelación:


La economía de Guadalajara está activa en los tres sectores económicos (actividades económicas) que son el Primario, Secundario y Terciario. Las actividades primarias se basan en el tránsito y comercio de ganado bovino, porcino, ovino, caprino, equino, avícola. Las actividades secundarias se basan en las industrias textil y metalmecánica. Guadalajara es la capital industrial en el occidente de México. La industria alimentaria exporta la mayoría de sus productos (jugos, productos enlatados, dulces, salsas y alimentos en general). En la industria farmacéutica, Guadalajara juega el papel más importante en la producción nacional, solo superada por el Distrito Federal, y es uno de los mayores distribuidores en el país.

Guadalajara es conocida como “El Valle del Silicio” mexicano, debido al desarrollo de la industria electrónica: es la principal fabricante de software en el país, y la mayor fabricadora de componentes electrónicos y digitales para aparatos de vanguardia, albergando compañías como, General Electric, IBM, Kodak, Intel, Hewlett-Packard, Siemens, Flextronics, Foxconn, Gateway, Sanmina-SCI, Dell, Solectron y BlackBerry.

La ciudad también es pionera en la producción y exportación textil a nivel nacional, y una de las mayores distribuidoras de ropa en México. Diseñadores de moda, fotógrafos, agencias, coordinadores, modelos, y gente alrededor de este sector son apoyados por la Cámara de la Industria del Vestido (CAINVE) y la Cámara de la Industria del Calzado (CAIC) a través de la Cámara de Comercio, la ciudad alberga el evento de moda más importante en México; Intermoda

El equipamiento de la ciudad en servicios básicos aunque es completo, se requiere de renovar en aspectos tan vitales como el sistema de agua y el sistema de drenaje y alcantarillado, así como definir una fuente alterna de abastecimiento de Agua para dejar de depender del Lago de Chapala, con una propuesta que involucre a los tapatíos y sin visiones de negocio como lo es la propuesta de la Presa de Arcediano que como ha quedado demostrado solo se ha prestado para la especulación inmobiliaria en un lugar donde la calidad de sus aguas deja mucho que desear por la gran cantidad de contaminantes y materiales pesados dañinos para la salud que corren a lo largo del Río Santiago.

La ciudad cuenta con quince pasos a desnivel, siete nodos viales, nueve pisos elevados y más de once viaductos distribuidos en la zona metropolitana. La Zona Metropolitana de Guadalajara cuenta con cuatro salidas principales, estas unen las zonas centro, norte, noreste y la costa del pacífico del país. Otras salidas son las carreteras principales y secundarias, que por lo general unen poblados cercanos a la ciudad, hay cuatro salidas de este tipo.

La Zona Metropolitana de Guadalajara, cuenta con más de 5 millones de habitantes, por lo que es necesario contar con un sistema de transporte público colectivo tanto masivo como convencional, para satisfacer las necesidades de la ciudad, formado por diversos sistemas de transporte, tanto Gubernamentales, Federales, Estatales, Municipales, iniciativa privada, hasta medios sin autorización oficial.

Diez compañías prestan el servicio de transporte urbano de pasajeros en Guadalajara: Alianza de Camioneros de Jalisco, Servicios y Transporte, Sistecozome, TUTSA, Línea Tapatía, Transporte Vanguardista de Occidente A.C. (TVO), Transporte Vanguardista de Jalisco A. C. (TVJ), Movilidad Corporativa de Guadalajara (MC), Alianza de Tlaquepaque A.C., Transportes Vanguardistas de Guadalajara A.C. (TVG).

En los últimos años el transporte público en Guadalajara ha sido criticado por la alta cantidad de accidentes y de muertes que ha provocado.


Cuenta actualmente con 60 trenes de 2 coches bi-articulados, para el transporte de personas. Con 29 estaciones de servicio al usuarios distribuidas en toda su red (Línea 1 y Línea 2) de 25 km de extensión. La forma de peaje es: Transvales, Tarjetas Inteligentes de prepago. Fue implantada el 1 septiembre de 1989, para satisfacer las necesidades escasas de transporte urbano, y hacer un transporte público urbano ecológico.

Actualmente el tren ligero ha creado una extensión de su línea 2 con un sistema de Alimentador de autobuses que se conocen con el nombre de Pre-tren que ciculan desde Av. Aviación y Carretera a Nogales hasta la estación de Juárez en Av. Federalismo y Av. Juárez para así conectar el PreTren y el Tren Ligero y cubrir los cuatro puntos cardinales.

Ferrocarril Mexicano es una empresa privada de transporte de carga, comercial e industrial a gran escala así como de transporte turístico regional (Tequila Express y José Cuervo Express).

Actualmente Guadalajara forma parte de la red de distribución, carga, mantenimiento, por su base de abastecimiento y administración permanente.

Guadalajara cuenta con dos terminales para autobuses una de ellas es de las más grandes dentro de la República Mexicana. Se trata de la Nueva Central para Autobuses. Ahí entran y salen autobuses de Turistar, Transportes Chihuahuenses, Futura, Línea Azul, Autobuses Americanos, Estrella Blanca, ETN, y de otras líneas más.

Prestan servicio 135 autobuses en 252 estaciones (más ocho que se utilizan los domingos en una vía equipados alterna, ya que un tramo por donde circula se cierra en ese día), su horario es de a 11:50p. m. Cuenta con 5 rutas (1, 1B, 2, 3 y 4)

El sistema de pago de peaje es mediante la Tarjeta Inteligente que los usuarios pueden adquirir en las instalaciones del Tren Ligero así como en diversos comercios y servicios ubicados a lo largo del corredor Vallarta. También se puede pagar con efectivo, depositando el importe exacto en una ranura que tiene implementada el pre-tren al ingreso de este, la máquina no da cambio. La tarifa oficial autorizada es de $9.50 pudiendo los usuarios realizar trasbordos con el Tren Ligero, y viceversa, pagando solo la mitad de dicha tarifa ($4.75), obteniendo un ahorro considerable para quienes utilizan el automóvil o el servicio de transporte público actual.

También cuenta con un sistema de monitoreo (Sistema BEA) el cual implementa tecnología de punta que permite realizar un monitoreo detallado de la calidad del servicio que se presta a los usuarios en cada uno de los viajes; a través del registro de la telemetría de la operación del autobús, como contador de pasajeros, grados de aceleración, frenado, tiempo de recorrido, salidas y llegadas puntuales a las terminales.


El sistema de BRT (del inglés "Bus Rapid Transit"), comúnmente llamado "Macrobús", se encuentra en operación desde el 10 de marzo del 2009 (se tomó como base el SIT de León, Guanajuato). Actualmente cuenta con una única ruta que circula desde la Barranca de Huentitán hasta la colonia Miravalle, dos puntos importantes de la ciudad, la mayoría de la ruta corre por la Calzada Independencia-Calzada Gobernador Curiel, cuenta con 27 estaciones al servicio del público en general y cuenta con 2 servicios: Servicio Parador, se detiene en las 27 estaciones de la línea y el Servicio Express que solamente se detiene en 12 de las 27 estaciones.

Las Rutas Alimentadoras (Macrobús) son líneas de autobuses convencionales que llevan desde estaciones del Macrobús a puntos importantes, Actualmente cuenta 15 rutas alimentadoras y cuenta con una flota de 103 minibuses alimentadores, del sistema de cobro electrónico así como de la publicidad y limpieza. Ofrece los servicios de transportación a la sociedad en general, a un precio económico. Se encuentran ubicados en las siete estaciones del Macrobús.

El trolebús de Guadalajara es un sistema de autobuses eléctricos que comunica una parte de la ciudad de Guadalajara, Jalisco (México).

La línea 3 es la única línea en operación, corre del Monumento de Los Arcos al Mercado Felipe Ángeles. Cuenta con una extensión de 34km y a lo largo de su trayecto se ubican 54 paradas. Su recorrido es el siguiente:


La línea cuenta demás con conexión al sistema Macrobús en la parada Independencia, así como con el Tren ligero en la de la estación Juárez, desde donde comparte una parte su trayecto con la Línea 1 de SITREN, hasta la terminal de Los Arcos.

Guadalajara cuenta con el Aeropuerto Internacional de Guadalajara “Miguel Hidalgo”, ubicado en el municipio de Tlajomulco de Zúñiga, a 15km de la ciudad. Esta terminal aérea, cuenta con una superficie de 1481 hectáreas, la cual cuenta también con dos pistas, la primera con número 10-28, tiene 4000 metros de largo por 60 de ancho, y la segunda, cuyo número es de 220, tiene 1770 de largo por 35 de ancho. Mantiene una amplia red de comunicación Nacional e Internacional desde distintos lugares de México, Estados Unidos y Sudamérica.

Construcciones históricas y ampliamente significativas son el distintivo de Guadalajara. Sus obras de arte, costumbres, tradiciones y leyendas representan para sus habitantes y turistas un foco de atracción de gran interés.

En lo que a Guadalajara corresponde, se tiene infraestructura para el turismo nacional, internacional, y local.
Se cuenta con hoteles, moteles, casas de huéspedes, suites, apartamentos, campamentos, aparcamiento para caravanas, repartidos por todo el municipio de Guadalajara.

Según cifras de la Secretaría de Turismo de Jalisco, en el 2006 había 180 hoteles en la ciudad que sumaban en total 12.248 cuartos. Así mismo, dentro de la Zona Metropolitana la cantidad de hoteles y habitaciones ascendía a 261 y 18.113.

Guadalajara cuenta con numerosas estaciones de radio y televisión, así como varios periódicos impresos. El periódico de mayor antigüedad en la ciudad es "El Informador", fundado en 1917 por la familia Álvarez del Castillo. Otros periódicos que circulan en la ciudad son "Mural", "Milenio Jalisco", "El Occidental" y "NTR Jalisco".

Más de 90% de los hogares de Guadalajara cuentan con televisión. Según Conaculta, el medio más consumido en la Zona Metropolitana de Guadalajara es la radio. La misma fuente indica que 50.7%% de los habitantes de Guadalajara leen el periódico.

Guadalajara cuenta con cobertura del correo postal mexicano y de telégrafos mexicanos en 11 oficinas para el envío y recepción de telégrafos, distribuidos en toda Guadalajara. Los servicios ofrecidos son giros postales, reembolsos, seguros postales, estampillas y envíos nacionales e internacionales.

La educación en esta ciudad es un factor principal para el desarrollo, ya que al contar con distintas universidades con prestigio internacional la educación se ha convertido en uno de los sectores económicos más activos al atraer inversión así como generar desarrollo económico y profesionales competitivos para desarrollarse en campos demandados tanto en la urbe como en la misma nación.

Guadalajara cuenta con la tercera universidad más antigua en México, la que cuenta con el mayor número de población estudiantil en el país (detrás de la Universidad Nacional Autónoma de México), la Universidad de Guadalajara, así como con la primera universidad privada de México; la Universidad Autónoma de Guadalajara (UAG). Cuenta con una de las primeras y más prestigiadas universidades jesuitas de la nación: el ITESO, y es la segunda sede de la Universidad Panamericana en México. Guadalajara también es la sede del Centro de Enseñanza Técnica Industrial (CETI) institución de educación superior que fue fundada con el apoyo de UNESCO en 1968 como parte del proyecto MEX-20, actualmente la única institución que permanece del sistema CENETI-CERETI a nivel nacional. La ciudad alberga universidades y preparatorias maristas como la Universidad Marista de Guadalajara (antes Universidad La Salle - Guadalajara), y universidades privadas en periodismo y comunicación como la universidad UNIVA. Alberga uno de los campus del ITESM y tres campus de la UVM distribuidos en la ZMG.

También cuenta con universidades que tienen convenios con empresas privadas en las cuales cursan sus últimos semestres como la Universidad Tecnológica de Jalisco (UTJ) quienes tienen un convenio con la empresa Tracsa y Tec Milenio la universidad del ITESM quienes tienen un convenio con la empresa Baratz.

La ciudad es reconocida por el prestigio académico internacional en medicina, derecho, administración de empresas, biología, arte, arquitectura y diseño, también es la única ciudad en contar con licenciaturas como diseño Urbano (urbanística), Orientación en Ciencias Computacionales y Didáctica de lenguas.

Las cifras de educación en el municipio se han mantenido en ascenso desde 1980, en el año 2000 en el municipio los alfabetas representaban el 96,83% de la población, que son 1.110.372 habitantes y los analfabetas eran el 3,08% de la población, 35.306 habitantes. En el desarrollo de la ciudad de 1950 al año 2000 la población alfabeta aumentó un 16,7% y la población analfabeta bajó un 16,7% de forma sincronizada con la población alfabeta.

En el municipio, en el año 2000, 217.008 habitantes tenían la primaria terminada que representaba el 19,54% de la población alfabeta.
Existen varias universidades y centros de investigación, tanto privados como públicos, que hacen de Guadalajara una ciudad que atraiga estudiantes del interior del país y del exterior del país. Se consideran Universidades dentro de Guadalajara, no tanto porque se sitúen en el municipio de Guadalajara, si no por estar distribuidas dentro de la Todo Zona Metropolitana de Guadalajara. Esto con la finalidad de distribuir la apertura a los habitantes de dicha zona.

La Universidad de Guadalajara es la máxima casa de estudios de Jalisco. Así también es la mejor universidad pública estatal del país y una de las 6 mejores universidades de México.

Existen actualmente 21 universidades de educación superior, de las cuales todas están certificadas por la Secretaría de Educación Pública (SEP), y la Universidad de Guadalajara, que esta última es la encargada de dar acreditación y validación a los sistemas educativos de dichas universidades, a excepción de las universidades autónomas que manejan su propio programa educativo, pero muestran validez a los programas educativos federales para la educación superior. Guadalajara es la ciudad mexicana que cuenta con más universidades afiliadas a la Red Iberoamericana de Universidades, la cual respalda el prestigio de las universidades más importantes de habla española.

La oferta cultural que se vive actualmente en Guadalajara es uno de los más amplios en el hemisferio. La ciudad cuenta con una de las agendas culturales más vastas en el continente, a lo cual se suma el interés del gobierno, de la Universidad de Guadalajara y de instituciones privadas, para subrayar los atributos culturales de la ciudad y el Estado de Jalisco. En la ciudad se exhiben obras de artistas internacionales y es un escaparate obligado para los eventos culturales internacionales cuyo radio de influencia alcanza la mayoría de los países de Hispanoamérica, incluido el suroeste de los Estados Unidos.

En la ciudad se dan grandes festivales de diversos temas con reconocimiento y alcance internacional, por mencionar los más conocidos:

Su centro histórico alberga edificios coloniales de carácter religioso y civil, los cuales constituyen una mezcla de estilos cuya raíz se halla en aportaciones culturales indígenas (principalmente de origen ute), incorporadas a lo mozárabe y lo castizo, y posteriormente en influencias europeas modernas (principalmente francesas e italianas). Asimismo, el centro histórico posee una excelente infraestructura de museos, teatros, galerías, bibliotecas, auditorios y salas de conciertos. Algunos de estos edificios datan del siglo XVI y XVII, como la Catedral de la arquidiócesis de Guadalajara, entre otros.

En cuanto a medios de comunicación, la ciudad tiene varias emisoras de radio enfocadas a la cultura, siendo Red Radio Universidad de Guadalajara una de las más importantes y la que se transmite al resto del estado y estados colindantes e internacionalmente a través de internet; también es la primera emisora vía Pod Cast en el país, la ciudad produce de igual forma un canal totalmente cultural; C7 dedicado al apoyo, difusión, y entretenimiento cultural y transmitido en televisión abierta, siendo Guadalajara la única ciudad en producir un canal de corte cultural en el país además de la Ciudad de México.

Guadalajara ha sido la primera ciudad mexicana en ser aceptada como miembro de la Asociación Internacional de Ciudades Educadoras debido al fuerte carácter e identidad que posee, potenciales para el desarrollo económico a través de la cultura.

También, es la ciudad que comenzó el movimiento de Bookcrossing en Latinoamérica, así mismo fue nombrada Capital Americana de la Cultura en 2005.

Las formas arquitectónicas de la colonia son producto de corrientes arquitectónicas francesas y españolas que durante la fundación de Guadalajara resplandecían en el continente europeo y que de manera paralela dejaron influencia en esta ciudad.
En el centro histórico se puede apreciar el más puro ejemplo de arquitectura neoclásica comenzando con la catedral metropolitana, el teatro degollado y edificios a los alrededores, en la colonia Lafayette se encuentra este estilo arquitectónico en casonas residenciales algunas cuantas convertidas en boutiques y restaurantes.
El centro histórico alberga edificios coloniales de carácter religioso y civil, los cuales destacan por su trascendencia arquitectónica e histórica, y constituyen una rica mezcla de estilos cuya raíz se halla en aportaciones culturales indígenas (principalmente de origen ute), incorporadas a lo mozárabe y lo castizo, y posteriormente en influencias europeas modernas (principalmente francesas e italianas) y americanas (concretamente, las procedentes de Estados Unidos).

Asimismo, el Centro Histórico de Guadalajara posee una excelente infraestructura de museos, teatros, galerías, bibliotecas, auditorios y salas de conciertos, Mención particular se puede hacer del Hospicio Cabañas (que data del siglo XVIII), el Teatro Degollado (considerado el "Opera House" más antiguo de México), el Teatro Galerías y el Teatro Diana. Actualmente existen proyectos en construcción que comprenden espacios culturales como el museo Guggenheim y el Centro Cultural Universitario. El Hospicio Cabañas, en el cual se alberga parte de la obra pictórica (murales y de caballete) de José Clemente Orozco, fue declarado Patrimonio de la Humanidad por la Unesco, en 1997. Dentro de las muchas estructuras de modernidad se encuentra el Templo Sede Internacional de la Iglesia La Luz del Mundo en la Colonia Hermosa Provincia que es el más grande en América Latina. 

Durante el Porfiriato el estilo francés invadió la ciudad debido a la pasión del entonces presidente Porfirio Díaz por las corrientes de estilo francés, también arquitectos italianos fueron los encargados de dar forma a las estructuras góticas que se levantan en la ciudad. El paso del tiempo dejó plasmadas diversas corrientes que pasan desde el barroco, al churrigueresco, neogótico y el neoclásico más puro. Hasta las líneas arquitectónicas propias de las décadas de los '40, '50 y '60 el art déco y las líneas audaces de los arquitectos posmodernistas de entonces. Algunos estilos arquitectónicos que se encuentran en la ciudad son:


La arquitectura moderna de Guadalajara tiene números figuras de una producción arquitectónica diversa desde el neo regionalismo hasta el brutalismo de los años setentas. Unos de estos arquitectos son: Rafael Urzua, Luis Barragán, Ignacio Díaz Morales, Pedro Castellano, Eric Coufal, Julio de la Peña, Eduardo Ibáñez Valencia.

También llamada Catedral de la Asunción de María Santísima, es parroquia sede de la Arquidiócesis de Guadalajara y uno de los edificios más representativos de la ciudad, no solo por sus torres con agujas neogóticas, sino porque tiene una gran historia. Cabe mencionar que la catedral auspicia al 9.º coro más antiguo del mundo: el “Coro del colegio de infantes de la Catedral metropolitana de Guadalajara”, el cual tiene más de 450 años en funcionamiento y actualmente es dirigido por el Mtro. Aurelio Martínez Corona.

Se terminó de construir el primero de agosto de 1992, y se abrió al culto el día 9 de agosto del mismo año, habiéndose iniciado su construcción el día 3 de julio de 1983.

La construcción tiene aproximadamente 15,500 metros cuadrados. Se levanta sobre un terreno de forma elíptica de 100 metros de longitud por 80 metros de anchura, ubicado en la Glorieta Central de la colonia Hermosa Provincia. El eje principal del templo esta orientado con dirección oriente-poniente, ubicándose el ingreso principal hacia el poniente.

Es un templo católico de estilo churrigueresco, dedicado (como su nombre lo indica) a la advocación mariana de Nuestra Señora de Guadalupe. La primera piedra se colocó el 7 de enero de 1777 y tras 4 años de obras, la iglesia se inauguró en 1781. La construcción fue promovida y costeada por el filántropo español y entonces Obispo de la diócesis, Fray Antonio Alcalde.

Es un templo católico dedicado al Santísimo Sacramento. Es de estilo neogótico y considerado la máxima obra en su estilo en México. Su construcción comenzó el 15 de agosto de 1897 y terminó 75 años después en 1972.

Es uno de los templos construidos y operados por La Iglesia de Jesucristo de los Santos de los Últimos Días, el número 105 construido por la Iglesia y el número 11 de México, ubicado en el municipio de Zapopan al noroeste de la Zona Metropolitana de Guadalajara.

Guadalajara cuenta con 19 edificaciones consideradas atractivos turísticos de carácter civil. Siendo aún utilizados la mayoría para asuntos gubernamentales, museos, plazas o panteones. Entre los cuales se puede notar la diversidad de estilos arquitectónicos según su fecha de construcción.

Los museos en Guadalajara son una extensión de la infraestructura cultural de esta ciudad, muchos de ellos destacan por su trascendencia arquitectónica e histórica, existen más de 189 foros de exposición artística entre centros culturales, museos, galerías privadas y espacios culturales del ayuntamiento, varios de ellos con siglos de existencia y algunos otros en proceso de construcción. Los museos en Guadalajara pertenecen al marco cultural de la ciudad, entre los cuales se encuentran en todos sus géneros exhibiendo Historia, Paleontología, Arqueología, Etnografía, Pinturas, Artesanías, Plástica, Fotografía, Escultura, obras de circuitos internacionales de arte, etc.

Guadalajara y la zona metropolitana que lo rodea cuentan con una variada gama de bibliotecas públicas, privadas y digitales para la búsqueda y consulta de información. El fomento de la cultura y el enriquecimiento de la lectura han facilitado al ciudadano a requerir de varias instalaciones en la ciudad. Algunas de las bibliotecas cuentan además de un recinto físico – entre ellos la histórica Biblioteca Iberoamericana Octavio Paz de la Universidad de Guadalajara y la Biblioteca Pública del Estado de Jalisco ubicado en la ciudad contigua de Zapopan – con opciones de consulta de información digital por Internet.

La Rotonda de los Jaliscienses Ilustres es un monumento de la ciudad de Guadalajara, Jalisco, México, ubicado en la cuadra flanqueada por las avenidas Fray Antonio Alcalde, Miguel Hidalgo y las calles Liceo e Independencia, en el corazón de la capital del estado de Jalisco conocido como centro histórico, a un costado de la Catedral de Guadalajara. Rinde homenaje a la memoria de los jaliscienses que han trascendido a través de la historia de Guadalajara.

Los Arcos es un monumento consistente en dos arcos muy representativo de Guadalajara (México), ubicado en la Avenida Vallarta, la principal vialidad de la ciudad, en su cruce por la calle Arcos, a una cuadra de la también emblemática Glorieta de La Minerva. Ambos arcos solían ser la entrada a la ciudad de Guadalajara. En medio del arco se lee: "Guadalajara capital del Reino de Nueva Galicia fundada en este lugar el día 14 de febrero de 1542." Aunque en la actualidad los arcos están muy lejos de ser la entrada a la ciudad, por el enorme crecimiento que esta ha tenido, son un monumento de gran importancia en Guadalajara, y muchas veces esta ciudad es representada con la imagen de este monumento.

Es un monumento representativo de la Ciudad de Guadalajara, México y la fuente más grande de dicha ciudad. La fuente está adornada con una estatua de la diosa romana Minerva (Atenea en la cultura Helénica) obra del escultor Joaquín Arias. La obra se realizó durante el periodo del gobernador Agustín Yáñez, quien encargo el proyecto al Arq. Julio de la Peña.

Más comúnmente conocido como “Arcos del Milenio” son una obra arquitectónica ubicada en la colonia Jardines del Bosque en la ciudad de Guadalajara, Con una altura de 52 metros, constará de seis monumentales arcos amarillos de metal, uno más grande que el anterior, ubicados entre las avenidas Lázaro Cárdenas y Mariano Otero. Su peso será de más de 1500 toneladas de acero, con 17 mil metros cuadrados de superficie. Enrique Carbajal González “Sebastián” es el autor del proyecto escultórico.

Monumental escultura formada por 5 piezas forjadas en bronce y labradas a mano. La figura central mide 25 metros de alto y las alegorías 6 metros cada una con un peso de 23 toneladas. Obra del jalisciense Víctor Manuel Contreras, es considerada una de las más altas del Mundo. Representa la inmolación de Quetzalcóatl elevándose de la tierra hacia el infinito para encender el sol y darnos nueva luz. Las cuatro esculturas que rodean la flama son los cuatro cielos de los cuatro puntos cardinales, la rosa de cemento que sirve de base y sostén de este conjunto escultórico es una hermosa fuente como espejo cristalino que equilibra y armoniza esta escultura.


Es un cementerio antiguo localizado en la ciudad de Guadalajara, México. Dicho cementerio fue anteriormente una huerta del hospital civil, pero fue convertido en panteón en 1848. Fue proyectada por el arquitecto Manuel Gómez Ibarra a solicitud del Obispo don Diego de Aranda y Carpinteiro. Su funcionamiento duró poco menos de 50 años, pues fue cerrado el 1 de noviembre de 1896. La decisión fue tomada por el Consejo Superior de Salubridad de esa época. Actualmente funciona como un museo que representa parte de la historia de Guadalajara, el cual alberga 900 nichos de cantera rosa. Anteriormente en el centro se encontraba la capilla de los hombres ilustres que hoy se encuentran en la "Rotonda de los Hombres Ilustres".

Aunque el panteón está clausurado para eventos funerarios, sigue abriendo sus puertas al público solo con el fin de que aprecie su interior. Para ello existen recorridos turísticos en los que se muestran las tumbas y se cuentan leyendas.

La cultura teatral en Guadalajara tiene gran importancia, con varios recintos donde artistas pueden exponer sus obras.


Guadalajara cuenta decenas de edificaciones de carácter religioso, teniendo 24 construcciones de mayor importancia en el municipio, entre las cuales se puede considerar como de mayor importancia a la Catedral de Guadalajara. Construida entre los años 1561 a 1618, es de estilo neoclásico. Sus características torres neogóticas datan de 1824 y sustituyen a las que derribara el temblor de 1813.

La construcción religiosa de mayor altura es la de Hermosa Provincia, de la Iglesia la Luz del Mundo, considerado también el templo religioso más grande de Latinoamérica. La construcción religiosa de mayor antigüedad en Guadalajara es la iglesia de San Francisco de Asís, que se edificó entre 1554 y 1746. De estilo barroco, esta obra se debe a la orden de los franciscanos.

Dentro de los estilos de estas construcciones religiosas se pueden encontrar el: neoclásico, barroco, neogótico, presentando así una exquisitez en los detalles de sus acabados. Los templos barrocos más bellos de la ciudad son el de San Felipe Neri y el de Santa Mónica. El primero cuenta con la que ha sido catalogada por los expertos como la torre más bella del occidente del país. Otros templos de la época colonial son: Santa Teresa, Jesús María, La Merced, Aránzazu, San José de Gracia, El Carmen, San Sebastián de Analco, San Agustín, Santa María de Gracia y San Juan de Dios.

La ciudad cuenta con dos ejemplos del arte neogótico del siglo XX en México: el Templo Expiatorio, diseñado por el arquitecto italiano Adamo Boari, autor del proyecto original del Palacio de Bellas Artes de Ciudad de México, y el templo de Nuestra Señora del Sagrado Rosario, conocido popularmente como Templo del Padre Galván, cuya edificación se inició en 1930 según el proyecto del arquitecto Pedro Castellanos Lambley.

Guadalajara cuenta con una gran variedad de platillos típicos, como pozole, tamales, tostadas, sopes, enchiladas, tacos, menudo, frijoles charros. Pero algo que lo distingue totalmente de todo el país son las “tortas ahogadas”, que es de birote salado (pan típico tapatío) untado con frijoles refritos, con carne frita de cerdo cortada en trozos – conocida también como “carnitas” – todo en salsa de tomate condimentada con especias; adicionalmente se come acompañado con cebollas desflemadas en limón y salsa picante; para beber, se puede acompañar, ya sea con el famoso tejuino – que se elaborar con una base de masa de maíz fermentada, acompañada con helado de limón – o con el tepache (hecho a base de la corteza de la piña fermentada).

Otra de las comidas típicas de Guadalajara y todo el estado de Jalisco es la “birria”, la cual normalmente es hecha con carne de cabra, res o borrego. La birria artesanal se hace en un horno especial, que puede estar bajo tierra y cubierto con hojas de maguey; la carne se puede mezclar con un caldo de tomate y especias, o consumida por separado. El postre que se considera como típico tapatío es la jericalla.

Otro de plato típico de la cocina tapatía es la carne en su jugo. Este platillo consiste en un caldo de carne de res con frijoles de la olla y va acompañado de tocino, cilantro, cebolla y rábano (en rodajas o entero).

Guadalajara además cuenta con numerosos restaurantes para degustar la gastronomía internacional; cuenta con lugares reconocidos y con algunos de los mejores restaurantes del país con especialidades en comida española, francesa, brasileña, japonesa, italiana, hindú, árabe, libanesa, griega, china, argentina, tailandesa, costarricense y varios restaurantes especializados en comida vegetariana y “orgánica”.

La ciudad cuenta con dos equipos en primera división en fútbol, el Club Deportivo Guadalajara, conocido popularmente como Chivas o el Rebaño Sagrado y el Club Atlas conocido con los apodos de Rojinegros del Atlas o Zorros. En la Segunda División de México el Club Deportivo Oro y el Club Leones Negros de la Universidad de Guadalajara; en tercera división está el legendario Club Deportivo Nacional, y en tiempos pasados contó con otros equipos como lo fueron el Club Social y Deportivo Jalisco, este último ya ha desaparecido.

En béisbol, los Charros de Jalisco disputaron la Liga Mexicana de Béisbol desde 1949 hasta 1952, la Liga de la Costa del Pacífico desde 1952 hasta 1955, nuevamente la Liga Mexicana desde 1964 hasta 1975, y actualmente la Liga Mexicana del Pacífico desde 2014.

En Básquetbol, los Astros de Jalisco participan en la Liga Nacional de Baloncesto Profesional desde 2019.

El estado de Jalisco es líder nacional en el ámbito deportivo olímpico, pues ha ganado 18 veces consecutivas la olimpiada nacional, también ha ganando en 9 ocasiones el mayor número de medallas en todas las disciplinas de las olimpiadas nacionales. La ciudad de Gudalajara ha sido relevante para estos triunfos pues el punto de entrenamiento para la mayoría de sus atletas son las instalaciones del CODE en esta ciudad, siendo dichas instalaciones uno de los mayores recintos mundiales dedicados a la formación deportiva.

En el mundo del golf, destaca la tapatía Lorena Ochoa quien ha sido una de las golfistas más jóvenes que ha ganado numerosos campeonatos internacionales en la historia del golf nacional, alcanzando el primer lugar en el ranking del LPGA en abril de 2007, siendo así, la mejor golfista del mundo en el máximo circuito del golf.

Los Juegos Panamericanos de 2011 se llevaron a cabo en la ciudad de Guadalajara, siendo esta la tercera vez que México organizó los Juegos Panamericanos y la primera fuera de Ciudad de México.

Guadalajara cuenta con varios recintos deportivos. Que se alterna su función primordial para eventos musicales, culturales.

Se encuentra ubicado dentro de las instalaciones de la Universidad Autónoma de Guadalajara. La capacidad oficial es para 25 000 aficionados. Es sede del equipo de fútbol, Estudiantes Tecos.

Un gimnasio para 1500 personas con una cancha de duela para basquetbol, 2 secciones una para butacas y otra de graderías, 1000 personas en butacas y 3000 en las graderías, pista de tartán y un foro para 8000 asistentes. Cuenta con alberca Olímpica techada con butacas para 800 espectadores, tiene un sistema para calentamiento del agua, fosa para clavados con trampolín de 10, 3 y 1 metros.

Recordemos que el Estadio Tecnológico fue demolido por el entonces Rector de la U. de G. Raúl López Padilla, con la promesa de construir uno más moderno y a la fecha ya han pasado 3 administraciones y sigue el terreno sin estadio, que se alquila a circos, evitando así que las nuevas promociones de estudiantes, practiquen en instalaciones propias, deportes de alto rendimiento en el área del atletismo.
Fue subsede de los Juegos Olímpicos de 1968, Mundial México 1970, Mundial Femenil 1971, Mundial Juvenil 1983, Mundial de Fútbol 1986. La capacidad oficial es para 65 000 aficionados, dicho estadio es el más grande en el país fuera de la capital, y es el 3º más grande de México. Actualmente es casa de los equipo Atlas y del club U. de G. (Leones Negros).
Es la actual casa del Club Deportivo Guadalajara. Este recinto es considerado el estadio más moderno en Latinoamérica. Cuenta con la capacidad de 45 000 aficionados y fue sede de los XVI Juegos Panamericanos y subsede de la Copa Mundial de Fútbol Sub-17 de 2011. Este estadio está ubicado en el municipio de Zapopan, Jalisco.

Plaza de toros, ubicada en el centro del municipio de Tlaquepaque, con un ruedo de 33 metros. Cuenta con la capacidad para aproximadamente 5000 aficionados a la fiesta brava y fue remozada en tu totalidad en el año de 1995 siendo eliminado el callejón con que contaba anteriormente.

Tiene una capacidad para 16 561 espectadores, tiene una altura de 25 metros con un ruedo de 46 metros entre barrera y barrera y un callejón de 2.5 metros de ancho.

Tri-Óvalo de 1380 metros de longitud para competencias de autos tipo NASCAR. Este inmueble es sede cada año de una fecha del serial NASCAR Corona Series. Tiene una capacidad aproximada para 15 000 espectadores.

Ubicado en la Unidad Deportiva Revolución, av. Pablo Neruda 3232. Este inmueble se utilizará para la justa deportiva de los Juegos panamericanos del 2011.

Ubicado en la Unidad López Mateos, será utilizado para Guadalajara 2011.

Ubicado en la Unidad deportiva Ávila Camacho.



La Vía RecreActiva en la ciudad de Guadalajara, es un programa social en el que se habilitan espacios viales para su empleo masivo con fines recreativos y de esparcimiento por personas de todas las edades; se pretende que el programa opere durante varias horas en días domingo. Tal acción implica el restringir temporalmente la circulación vehicular-motorizada a lo largo de uno o más cuerpos viales sobre vías primarias seleccionadas, tan solo permitiendo el desplazamiento a través de medios no-motorizados (a pie, bicicleta, patines, etc.) dentro de este espacio reservado.

Actualmente existen tres rutas:

Guadalajara mantiene una amplia relación con países, y ciudades hermanas alrededor del mundo. Con sede de Relaciones Exteriores en Ave. Juárez n.º 20, Cuauhtémoc. Los Cónsules Honorarios y Cónsules Generales que actualmente residen en Guadalajara son:
La ciudad de Guadalajara cuenta con varios hermanamientos alrededor del mundo.

Guadalajara cuenta con convenios de cooperación específica, cuyo objetivo es establecer actividades, con la finalidad de facilitar la ejecución del convenio. Estos convenios se celebran porque las partes signatarias focalizan la cooperación específicamente para fortalecer áreas complementarias como turismo, gobierno, seguridad, etc. Los convenios que tiene la ciudad, son con las siguientes ciudades alrededor del mundo:



</doc>
<doc id="37867" url="https://es.wikipedia.org/wiki?curid=37867" title="Sicilia">
Sicilia

Sicilia es una de las veinte regiones que conforman la República Italiana. Su capital es Palermo. Está ubicada en Italia insular, limitando al norte con el mar Tirreno, al este con el estrecho de Mesina que la separa de Calabria, al sureste con el mar Jónico y al sur y oeste con el mar Mediterráneo. Con en 2013 es la cuarta región más poblada del país —tras Lombardía, Lacio y Campania— y con , la más extensa. Es además la isla más poblada del Mediterráneo. Es una de las cinco regiones con estatuto especial.

Es la séptima mayor isla europea por dimensiones. Se trata de la principal isla italiana y la mayor del mar Mediterráneo. Pero además, se encuentran varias islas más pequeñas: los archipiélagos de las islas Eolias al nordeste, las islas Egadas al oeste, las islas Pelagias al suroeste, y las islas de Pantelaria al sur y Ustica al noroeste. 

Sicilia es la única región italiana que cuenta con dos ciudades entre las : Palermo, quinta, y Catania, décima, respectivamente quinta y séptima ciudad metropolitana del país. La isla es considerada un importante destino turístico de Europa, cuyos orígenes pueden retrotraerse al , cuando la popularización que le supuso el "Viaje a Italia" de Goethe la convirtió en una etapa obligada del Grand Tour, el viaje educativo y de ocio que los jóvenes aristócratas europeos, en buena parte británicos, realizaban como culminación de sus estudios antes de ingresar en la edad adulta. La Universidad más antigua de la isla es la de Catania (1434).

En latín, esta isla de forma triangular ubicada al sur de Italia se llamaba "Trinacria" (triangular). Sin embargo, el nombre se debe a que anteriormente los griegos la llamaban "Sikelia", debido a que la tribu nativa eran los sículos (Σικελοί /sikeloi/ en griego)".

La isla de Sicilia ha estado habitada desde la Prehistoria. De la época del Paleolítico quedan restos en el litoral septentrional (Trápani), y del neolítico en lugares como Termini Imerese. En el III milenio a. C. los sicanos, de cultura neolítica, habitaron la isla. Los sículos, que dominaban ya el cobre, se asentaron a mediados del II milenio a. C.

En la época de las colonizaciones antiguas, Sicilia había sido ocupada por los fenicios hacia el a. C.. En el siglo siguiente, fue colonizada por los griegos, que fundaron varias ciudades de importancia. La principal fue Siracusa (733 a. C.), y también Catania ( a. C.). En el a. C. empezó la penetración cartaginesa en la isla. Los tiranos de Siracusa, como Dionisio el Viejo, Dionisio el Joven o Timoleón, convirtieron a Sicilia en un pequeño imperio propio. Durante la primera guerra púnica, los romanos conquistaron toda Sicilia, salvo Siracusa, que consiguió mantenerse independiente (241 a. C.). Sin embargo, durante la segunda guerra púnica, Sicilia se alió con Cartago, por lo que los romanos la conquistaron militarmente en 212 a. C., pese a los intentos del famoso inventor Arquímedes por defenderla, que murió en la toma de la ciudad. Después de la conquista, Sicilia fue reducida a provincia romana.

Con la caída del Imperio romano, una serie de pueblos germánicos se sucedieron en la isla: primero la ocuparon y saquearon los vándalos (439-468), luego los hérulos (476) y finalmente los ostrogodos (491). En 535, desembarcó en ella el general Belisario, comandante de las tropas bizantinas, quien la incorporó al Imperio bizantino. Sicilia fue bizantina durante cuatro siglos. A partir del , los sarracenos del norte de África iniciaron ataques cada vez más fuertes sobre Sicilia, conquistando y fundando en la isla el emirato de Sicilia entre los años 827 y 902.

En la segunda mitad del , concretamente a partir de 1061, Sicilia fue conquistada por los normandos dirigidos por Roberto Guiscardo y su hermano Roger. Roberto Guiscardo expulsó a los bizantinos del sur de Italia, mientras su hermano Roger recibió del papa el título de conde de Sicilia. Más adelante, en 1130, Roger II consiguió unificar el sur de Italia y la isla de Sicilia obteniendo del Papa el título de rey, base de lo que después se conocerá como el Reino de las Dos Sicilias. En 1194, el emperador Enrique VI, tras un primer intento fallido en 1191, logró el trono de Sicilia tras conquistarla, reclamando la corona como herencia de su esposa, Constanza I de Sicilia. El reino formó parte de los dominios de los Hohenstaufen hasta que Manfredo fue derrotado en 1266 en la batalla de Benevento, con lo que todo el reino de Sicilia pasó a manos de los angevinos, en la persona de Carlos, conde de Anjou.

Las Vísperas sicilianas en 1282 provocaron la división del reino y acabaron con el reinado de Carlos de Anjou, sustituido por la influencia de la Corona de Aragón. La "Sicilia insular" quedó bajo dominio de Pedro el Grande, rey de Aragón, y la "Sicilia continental" formará el Reino de Nápoles bajo dominio angevino. A la muerte de Pedro, Sicilia pasó a su hijo Jaime. Cuando Jaime fue llamado al trono aragonés, a la muerte de su hermano Alfonso III, la isla quedó nuevamente unida a Aragón. No obstante, por el tratado de Anagni se devolvía Sicilia a los angevinos a cambio de quedar con Cerdeña. Los sicilianos, descontentos con esta perspectiva, eligieron rey a Federico, hijo menor del rey Pedro y que era lugarteniente general del reino. En agosto de 1302 se firmó el Tratado de Caltabellotta, por el que Federico fue reconocido como rey de "Trinacria" (en esa época no se utilizaba el nombre de Sicilia ). Así la isla quedaba en poder de una rama secundaria de la familia real aragonesa. Federico III, rey de Sicilia, casó a su hija María con Martín el Joven, hijo del aragonés Martín I. Como consecuencia, a la muerte sin sucesión legítima de Martín el Joven, en 1409 Martín el Humano se coronó rey de Sicilia , reuniéndose de nuevo Sicilia y la Corona de Aragón. A ello se añadió, en 1442, la conquista de Nápoles por parte del rey Alfonso V el Magnánimo. No obstante, al morir Alfonso V (1458) Nápoles se independizó y Sicilia siguió unida a Aragón. Fernando II el Católico recuperó Nápoles en 1504, pero mantuvo dos virreinatos diferentes, uno para Sicilia y otro para Nápoles.

La situación de Sicilia como un virreinato más del Imperio español continuó hasta 1713. Con ocasión de la guerra de Sucesión española, se entregó la isla al duque de Saboya, Víctor Amadeo II, a título de rey. No obstante, en 1720 cambió Sicilia por Cerdeña, y la isla quedó en manos del emperador austriaco, Carlos VI. Sin embargo, por el tratado de Viena Sicilia y Nápoles fueron entregadas a Carlos de Borbón, hijo de Felipe V de España, introduciendo así la dinastía Borbón en la isla. Cuando Carlos asumió el título de rey de España, dejó ambos reinos a su hijo Fernando, que asumió el trono de las Dos Sicilias.

En 1860, como parte del "Risorgimento," la expedición de los Mil liderada por Giuseppe Garibaldi desembarcó en Sicilia y en el collado de Pianto romano, cerca de Calatafimi, derrotó el 15 de mayo a los borbones. La marcha de Garibaldi fue finalmente completada con el asedio de Gaeta, donde se expulsó a la última resistencia borbón y Garibaldi anunció su dictadura en nombre de Víctor Manuel II. Sicilia se convirtió entonces en parte del reino de Italia (1861).

Desde entonces, la historia siciliana ha estado estrechamente vinculada a la de Italia. Ocupada por los nazis durante la Segunda Guerra Mundial, fue elegida por los Aliados para atacar Europa, en la operación generalmente conocida como desembarco de Sicilia, en 1943. Italia se convirtió en República en 1946 y como parte de la Constitución de Italia, Sicilia se constituyó como una de las cinco regiones con estatuto especial.

Sicilia es una isla perteneciente a Italia. Los del estrecho de Mesina la separan de la península itálica. Queda a de Túnez, en el norte de África, del que la separa el canal de Sicilia. El mar Tirreno baña el litoral norte. Geológicamente pertenece a la misma placa tectónica que la península italiana, y orográficamente es una región de los Apeninos como muchas otras italianas.

La región homónima comprende también diversas islas menores, como el archipiélago de las siete Eolias o Lípari y Ustica al norte, el de las tres Egadas al oeste y, al sur, las islas de Pantelaria, Lampedusa, Linosa y otras menores. Geográficamente, el archipiélago de Malta es parte integrante de Sicilia. Malta ha estado unida a Sicilia, incluso políticamente, hasta el año 1798, en que fue ocupada durante casi dos años por Napoleón. Las islas Pelagias, integradas en la provincia de Agrigento están, en cambio, geográficamente unidas a Túnez.

En términos generales, el relieve de la isla es montañoso. En particular en el norte, con una prolongación de los Apeninos que forma los llamados Apeninos sículos: montes de Palermo, montes de Trápani, Nebrodi, Madonia y Peloritani. En el sudeste se encuentran los montes Ibleos. El centro y el sur están compuestos por colinas. Existen algunas llanuras, como la de Catania y la llamada Conca d'Oro en la que se encuentra Palermo. En el centro están los montes Erei, sobre los que se alza, a 948 m sobre el nivel del mar, la ciudad de Enna. Al oeste se alzan otros montes de altura variable, como los Sicanos, que culminan en Rocca Busambra (1613 m) y Monte Cammarata (1578 m) .

Ubicada en el encuentro entre la placa euroasiática y la placa africana, la isla es célebre por el volcán activo Etna, el punto más alto de la isla con sus 3323 msnm. Al noreste, dentro de las islas Eolias existen otros volcanes: el Estrómboli y el Vulcano, que se encuentran en activo. Una característica geológica peculiar en la isla es la actividad volcánica sedimentaria que se observa en varios de los sitios de Agrigento o Caltanissetta. Sicilia está igualmente expuesta a terremotos, como el de Mesina del año 1908 o el del valle del río Belice en 1968.

La red hidrográfica está constituida por ríos numerosos pero cortos y de poco caudal. La mayor parte fluye desde el centro hacia el sur. El río Salso () recorre Enna y Caltanissetta antes de desembocar en el mar Mediterráneo por el puerto de Licata. Al este se encuentran el Alcantara () en la provincia de Mesina, que desemboca por Giardini Naxos; y el Simeto () que desemboca en el mar Jónico al sur de Catania. Otros ríos importantes en la isla están en el sudoeste: Belice () y Platani ().

La isla se beneficia de un clima mediterráneo, con inviernos suaves y húmedos y veranos muy cálidos y áridos. En la primavera el paisaje es verde y florido, los veranos son amarillentos y sin flores. La aridez es marcada en el sur. La afecta directamente el siroco, que llega de África y genera repentinos cambios de temperatura. Las precipitaciones son escasas.

Sicilia sufre de un déficit de agua crónico, ocasionando regularmente cortes de suministro. Esto no impide que la agricultura sea uno de sus principales recursos económicos. La nieve cae abundante por encima de , pero si hay ola de frío puede nevar también en las colinas e incluso por las ciudades costeras (este último fenómeno es más común en Mesina). Las montañas del interior, especialmente las de Nebrodi, Madonie y el Etna, tienen un clima propio de los Apeninos, con nevadas durante el invierno.

Sicilia concentra una población de aproximadamente cinco millones de personas (5 044 900 habitantes a fecha 31 de mayo de 2010). Hay otros diez millones de descendientes de sicilianos alrededor del mundo, principalmente en Norteamérica, Argentina, Australia, Venezuela, Colombia y otros países europeos e iberoamericanos.

Como el resto de la Italia meridional, la inmigración es muy baja en comparación con otras regiones de Italia, debido a que los trabajadores tienden a dirigirse al norte de Italia, por las mejores oportunidades de empleo. Las cifras ISTAT más recientes muestran alrededor de cien mil inmigrantes, lo que casi es el 2% de la población. Los rumanos, con más de diecisiete mil, son el grupo mayoritario, seguido por los tunecinos, los marroquíes, tamiles de Sri Lanka, albaneses y otros, principalmente de Europa Oriental.

El área de mayor densidad demográfica dentro de Sicilia es la costa oriental de la isla, sobre todo las laderas del Etna que incluyen Catania, que tiene la mayor y más densa comarca de la isla, y las zonas de Palermo y Mesina. Dieciséis municipios superan los 50 000 habitantes:

En Sicilia han existido tradicionalmente numerosas comunidades tradicionales lingüísticamente diversas. Además de italiano estándar, se ha hablado idioma siciliano, y en unos pocos municipios además han existido comunidades que hablaban arbëreshë (albanés), galoitaliano de Sicilia y grecobovesiano de Sicilia.

Hasta el 2015, Sicilia se dividía administrativamente en nueve provincias, cada una con una capital homónima a su provincia. Con la reforma del 2015, la Región siciliana suprimió sus nueve provincias, constituyendo nueve libres consorcios municipales ("Liberi consorzi comunali" en italiano). De estos, tres son ciudades metropolitanas (Palermo, Catania y Mesina).

Las pequeñas islas que rodean a Sicilia también forman parte de algunas de las provincias sicilianas: las islas Eolias de Mesina, la isla de Ustica (Palermo), las islas Egadas (Trápani), isla de Pantelaria (Trápani) y las islas Pelagias (Agrigento).

Sicilia es una región con estatuto especial desde 1946, tras la instauración del presidencialismo en Italia. Los electores sicilianos eligen a los 90 diputados regionales miembros de la Asamblea Regional Siciliana (ARS), órgano legislativo regional, y (desde el 2001) al presidente de la Junta Regional (también llamado presidente de la Región o gobernador)

Es conocida la cultura del cultivo de naranjas, limones, mandarinas, hortalizas, legumbres y frutos secos, como base de los productos de esta tierra. El cultivo de la vid y la producción de vinos son mayormente reconocidos en el extranjero. Entre los más conocidos están: el Vino de Marsala, de la provincia de Trápani, el Moscato de Pantelaria, Malvasía de Lipari, Nero de Avola; y el más importante vino siciliano de hoy día, Cerasuolo de Vittoria.

Ovinos, caprinos y equinos son abundantes, mientras los bovinos están presentes en número limitado. La pesca constituye un recurso valioso para la economía de Sicilia, haciendo de esta isla la principal región italiana en cuanto a cantidad de producto capturado, la consistencia de la flota pesquera y el número de pescadores empleados. Además de pez espada, se pesca atún, sardina y caballa, además del pescado azul típico del Mediterráneo, que provee a la industria conservera de la materia prima necesaria para el pescado enlatado y el ahumado. En Mazara del Vallo, en la provincia de Trápani, se obtienen buenas lubinas, y camarones en Ganzirri; y en al zona norte de Mesina, ostras. También son conocidas las salinas de Trápani, donde desde la antigüedad se extrae una finísima sal marina. 

La industria del turismo es una actividad en crecimiento, favorecida por la presencia de numerosos sitios arqueológicos (Morgantina, Valle de los Templos, Selinunte) y las bellezas naturales. En los últimos años se ha invertido en la capacidad e los albergues, favoreciendo el incremento de su presencia en la isla. Entre otros destinos turísticos de renombre están localidades como Taormina, Agrigento, Siracusa, Caltagirone, Cefalù y Piazza Armerina (Villa del Casale). El interior de la isla es rico en historia, tradiciones y sobre todo en arte y cultura, fortalezas, iglesias, bosques y espacios naturales de importancia que dan valor a las áreas internas de las provincias de Enna, Catania, Caltanissetta y Palermo. Sus volcanes, montañas, colinas y, especialmente, el mar, forman parte del gran sector turístico de la economía siciliana.

La opera dei pupi (teatro de títeres en italiano) es una representación teatral en la que sus protagonistas son caballeros de Carlomagno. Estas marionetas son una de las tradiciones más características de Sicilia. Palermo tiene los teatros de títeres más importantes. El teatro siciliano de marionetas Opera dei Pupi fue proclamado en 2001 e inscrito en 2008 en las lista del Patrimonio cultural inmaterial de la Humanidad de la UNESCO.

La lengua oficial hablada en Sicilia es el italiano. Gran parte de la población local habla también el siciliano. Aunque algunos lo consideran un dialecto, muchos consideran que se trata de una lengua distinta, con una rica historia y un extenso vocabulario (más de ), debido a la influencia de los distintos dominadores de la isla. Es una lengua romance que desciende del latín vulgar, con influencias del griego, el árabe, francés, provenzal, alemán, catalán y del español.

El siciliano también se habla en el centro y en el sur de Calabria y en la parte sur de Apulia, llamada Salento. El siciliano también tuvo una influencia significativa en la formación de la lengua maltesa, sobre todo hasta finales del . En siciliano las palabras masculinas acaban con "u" mientras que las femeninas acaban con "a". El plural es normalmente "i" tanto para las palabras masculinas como femeninas. El siciliano sustituye la "LL" italiana por "DD", de forma que por ejemplo "bello" (bonito) sería "beddu" en siciliano.

Tres equipos de fútbol de Sicilia jugaron en la Serie A de Italia, aunque ninguno actualmente. Palermo fue quinto tres veces, y también alcanzó octavos de final de la Copa UEFA. Catania fue octavo cuatro veces y llegó segundo en la Copa de los Alpes en 1964 (Mesina obtuvo un séptimo puesto como mejor resultado en cinco temporadas. La Targa Florio fue una prestigiosa carrera de automovilismo en ruta del Campeonato Mundial de Resistencia. En 1978 se la reemplazó por el Rally Targa Florio, que ha sido puntuable para el Campeonato Europeo de Rally. Por otra parte, el Autódromo de Pergusa albergó el Gran Premio del Mediterráneo, prueba no puntuable de Fórmula 1 en la década de 1960, y luego participó del circuito de la Fórmula 2 Europea y la Fórmula 3000. Luego albergó carreras del Campeonato Mundial de Superbikes, Campeonato Mundial de Resistencia, Campeonato FIA GT y Superstars Series. El Torneo de Palermo de tenis formó parte del ATP Tour desde 1979 hasta 2006 y del WTA Tour desde 1988 hasta 2013. La Universiada de 1997 se celebró en diversas ciudades de Sicilia.


<noinclude>


</doc>
<doc id="37869" url="https://es.wikipedia.org/wiki?curid=37869" title="Variable estadística">
Variable estadística

Una variable estadística es una característica que puede fluctuar y cuya variación es susceptible a adoptar diferentes valores, los cuales pueden medirse u observarse.
Las variables adquieren valor cuando se relacionan con otras variables, es decir, si forman parte de una hipótesis o de una teoría. En este caso se las denomina constructos o construcciones hipotéticas.

A partir de este concepto se puede mencionar que una variable es la que permite relacionarla con algún problema o fenómeno, el cual vamos a investigar y buscar posible soluciones.

Mediante este concepto se puede mencionar que las variables tienen una clasificación:


Las variables categóricas se dividen de la siguiente forma:


Y las variables numéricas se dividen de la siguiente manera:


En las ciencias de la salud es bastante frecuente encontrarse con variables categóricas, como el sexo, la raza, lugar de procedencia, categoría laboral, etc., son ejemplos de este tipo de variables con las que nos podemos encontrar en nuestro diario vivir.

Estas son variables sobre las que únicamente es posible obtener una medida de tipo nominal u ordinal (con muy pocos valores) los valores que presentan corresponden a categorías discretas. Estas categorías no pueden ser ordenadas y representan grupos diferentes como ya lo mencionamos.

También se toma en cuenta que una variable es medida utilizando una escala de medición, la elección de las escalas de medición a utilizarse depende del tipo de variable en estudio y el manejo estadístico al que será sometido la información es decir existe una correspondencia directa entre tipo de variable y escala de medición.

Las variables categóricas pueden clasificarse en:

Es aquella variable cualitativa cuya categoría no sigue ningún orden, se agrupa sin ninguna jerarquía entre sí.

Ejemplos:


No admiten puntuaciones numéricas ordenandos significativamente sin embargo a veces en las computadoras se muestran la categoría de estas variables mediante ciertos códigos computacionales por ejemplo a la variable género se le asigna los siguientes códigos: hombre-0, mujer-1.

Son aquella variables categóricas con orden secuencial o progresión natural esperable o jerarquía.

Ejemplos:


Es aquella variable categórica, la cual puede adoptar solamente dos valores.

Ejemplos:


También llamadas variables cuantitativas. Describen una característica en términos de un valor numérico o cantidad.

Son aquellas características que son medidas dentro de un rango continuo infinito de valores numéricos y se registran con números reales. Pueden presentar cualquier valor dentro de cierto intervalo.

Ejemplos: 


Son también llamadas discontinuas, y están asociadas a conteos o enumeraciones, razón por la cual, sólo permiten ser registradas con números enteros (0,1,2,3, etc.)

Ejemplos:



Según el nivel de medición o también según el criterio metodológico, pueden ser:

Son el tipo de variables que como su nombre lo indica expresan distintas cualidades, características o modalidad. Cada modalidad que se presenta se denomina atributo o categoría, y la medición consiste en una clasificación de dichos atributos. Las variables cualitativas pueden ser dicotómicas cuando sólo pueden tomar dos valores posibles, como "sí y no", "hombre y mujer" o ser politómicas cuando pueden adquirir tres o más valores. Dentro de ellas podemos distinguir: 



Son las variables que toman como argumento cantidades numéricas, son variables matemáticas. Las variables cuantitativas además pueden ser:



Según la influencia que le asignemos a unas variables sobre otras, estas podrán ser:

Una variable independiente es aquella cuyo valor no depende de otra variable. Es aquella característica o propiedad que se supone es la causa del fenómeno estudiado. En investigación experimental se llama así a la variable que el investigador manipula.

Las variables independientes es en la que el investigador escoge para establecer agrupaciones en el estudio, clasificando intrínsecamente a los casos del mismo. Un tipo especial son las variables de control, que modifican al resto de las variables independientes y que de no tenerse en cuenta adecuadamente pueden alterar los resultados por medio de un sesgo. 

La variable independiente se suele representar en el eje de abscisas.
La variable independiente es la que se le asignan valores arbitrarios

Una variable dependiente es aquella cuyos valores dependen de los que tomen otra variable. La variable dependiente es una función que se suele representar por la y. La variable dependiente se representa en el eje ordenadas.
Son las variables de respuesta que se observan en el estudio, y que podrían estar influidas por los valores de las variables independientes.

Hayman (1974 : 69) la define como propiedad o característica que se trata de cambiar mediante la manipulación de la variable independiente.

La variable dependiente es el factor que es observado y medido para determinar el efecto de la variable independiente.

Son aquellas características o propiedades que, de una manera u otra, afectan el resultado que se espera y están vinculadas con las variables independientes y dependientes. Y es muy similar a la variable moderadora aunque no son iguales solo son muy similares a la forma de relacionarlas.

Según Tuckman: Representan un tipo especial de variable independiente, que es secundaria, y se selecciona con la finalidad de determinar si afecta la relación entre la variable independiente primaria y las variables dependientes.

Es la variable que los valores que presenta son determinados por el propio investigador. Se la usa en investigaciones de experimentación.

Es la variable que no está determinada por el investigador, ya que no depende del mismo.

Denominadas también como constructos, estas variables no son observables, si no que se infieren a través de sus defectos o conductas.

Denominadas también experimentales, estas pueden ser objetos de observación y de medición directa.

Son aquellas que no se pueden, sino solo ser observadas o representar características humanas.

Son aquellas que son susceptibles a experimentación y manipulación.

Es aquella que cualquier condición o manipulación, introducida por el ambiente y provoque una respuesta en el organismo.

Es la respuesta del organismo hacia la variable estímulo.

Los diferentes tipos de variables antes mencionados tienen relación con los diferentes áreas de la Estadística pero de igual forma con el área de Investigación Formativa, el conocer y saber que variable utilizar depende de la relación que exista entre las variables.



</doc>
<doc id="37870" url="https://es.wikipedia.org/wiki?curid=37870" title="Bert y Ernie">
Bert y Ernie

Bert y Ernie (Epi y Blas en España, Beto y Enrique en Hispanoamérica) son dos marionetas con aparición regular en el programa televisivo "Sesame Street" ("Barrio Sésamo" en España y "Plaza Sésamo" en Hispanoamérica). En España sus nombres fueron traducidos tomando la primera letra de los nombres en la versión en inglés Bert y Ernie. Aparecen en muchos números cómicos que se convirtieron en una de las principales atracciones del programa. En Alemania en la cadena RTL ponían un corto de humor de Bert y Ernie con solo un ojo llamándose "Bernie und Ert".

Construidos por Don Sahlin con espuma sintética y felpa a partir de un esbozo simple de Jim Henson, aparecieron por primera vez en el programa en Estados Unidos en 1969 (siendo los dos únicos Muppets que aparecieron en el episodio piloto de "Sesame Street"), entre otros personajes como Kermit the Frog, como parte de un programa educativo para niños junto con el resto del programa, que acabaría teniendo un éxito internacional.

Según el guionista Jon Stone, la relación entre Bert y Ernie es un reflejo de la amistad entre Jim Henson y Frank Oz. Los nombres de Bert y Ernie coinciden con los nombres de dos personajes de "Qué bello es vivir", pero según Jon Stone, la coincidencia es casual.

Según Frank Oz, la personalidad básica de los personajes fue desarrollada por el propio Sahlin basándose en el contraste de su aspecto (Ernie/Epi/Enrique es bajo y de rostro horizontal mientras que Bert/Blas/Beto es alto y vertical), puesto que lo que se buscaba para "Sesame Street" era la interacción entre ambos. El personaje de Ernie interpreta un papel infantil y travieso y el de Bert el de un adulto responsable aunque gruñón y aparentemente aburrido. Son comunes las escenas en las que a Ernie se le ocurre alguna idea disparatada y Bert intenta convencerlo de que la abandone, normalmente con poco éxito, por lo que acaba perdiendo los estribos o llega incluso hasta a desmayarse, mientras que Ernie permanece ajeno a lo que está sucediendo.

Desde sus primeras apariciones, Bert y Ernie han sufrido el rumor de ser una pareja homosexual debido a ser dos hombres sin parentesco que viven en la misma casa. Los creadores han llegado a declarar en muchas ocasiones que no representan ningún papel sexual, sino la capacidad de dos personas con personalidades distintas para convivir pese a los problemas.
Aun así, el dúo ha sido víctima de parodias en programas como "Saturday Night Live", "Padre de familia" o "The Cleveland Show".

Jim Henson se dedicó desde el principio y durante años a dar vida (movimiento) y voz (en inglés) a Ernie, mientras que el papel de Bert quedó a cargo de Frank Oz.

La voz de Epi en España fue doblada por Pepe Martínez Blanco en su primera etapa (1976-80), y Juan Miguel Cuesta a partir de 1980.

En 2012 los responsables del telescopio de neutrinos "IceCube" anunciaron la detección de dos neutrinos superenergéticos, de más de 1.000 teraelectronvoltios. Sus descubridores los denominaron Bert y Ernie en honor a estos personajes.



</doc>
<doc id="37874" url="https://es.wikipedia.org/wiki?curid=37874" title="Heterosexualidad">
Heterosexualidad

La heterosexualidad es la atracción romántica, atracción sexual o comportamiento sexual entre personas de distinto sexo. Como orientación sexual, la heterosexualidad es: "Un patrón duradero de atracciones emocionales, románticas o sexuales hacia personas de distinto sexo"; también "se refiere al sentido de identidad de una persona basado en esas atracciones, comportamientos relacionados y pertenencia a una comunidad de personas que comparten esas atracciones". Junto con la bisexualidad y la homosexualidad, la heterosexualidad es una de las tres categorías principales de la orientación sexual dentro del continuo heterosexual-homosexual. El término "heterosexual" o "heterosexualidad" se suele aplicar a los seres humanos, pero la conducta heterosexual se observa en casi todos los mamíferos y en otros animales.

"Héteros" proviene de la palabra griega "ἕτερος" [jéteros], que significa "otro", utilizada en la ciencia como prefijo con el significado de "diferente"; y de la palabra latina "sexualis," que significa sexo (es decir, típico el sexo o la diferenciación sexual).

El término «heterosexualidad» fue utilizado por el médico norteamericano James G. Kiernan en mayo de 1892 en un artículo publicado en la revista "Chicago Medical Recorder" sobre la «perversión sexual» en el que la heterosexualidad era definida como un «apetito anormal» hacia las personas del sexo opuesto, de la misma forma que la «homosexualidad» sería el «apetito anormal» hacia las personas del propio sexo. El término «heterosexual» ya había sido utilizado por otros médicos cuando se referían a los posibles tratamientos que permitieran al «homosexual y al heterosexual convertirse en seres humanos con inclinaciones eróticas naturales y con pulsiones normales». Así la edición de 1901 del "Dorland Medical Dictionary" definía la heterosexualidad como el «apetito sexual anormal o pervertido por el otro sexo». Todavía en 1923 el "New International Dictionary" decía: «pasión sexual mórbida por una persona del sexo opuesto». Se definían, pues, tres categorías de individuos respecto a su actividad sexual —y así lo especificaba el alemán Richard von Krafft-Ebing en "Pychopatia Sexualis", obra publicada en 1886—: los homosexuales, los heterosexuales y los «normales», que son aquellos que no hacen de la sexualidad una práctica autónoma o exaltada —siempre respecto del sexo opuesto—, que se casan y tienen hijos. En conclusión, la heterosexualidad era definida, al igual que la homosexualidad, como una enfermedad que había que curar. Por apego a prácticas sexuales humanas repetitivas y extremas evidentes.

Esta concepción de la sexualidad fue cuestionada, entre otros, por Freud que en 1905 publicó la obra "Tres ensayos sobre la teoría sexual" en la que puso en duda que la atracción hacia el otro sexo fuera una cosa «natural». «El interés exclusivo del hombre por la mujer es asimismo un problema que requiere una explicación y no algo que viene dado», escribió. Según Freud no se nacería "heterosexual" —dándole al concepto el significado actual— sino que la atracción sexual hacia las personas del sexo opuesto sería el resultado de un aprendizaje iniciado en la más tierna infancia. André Gide desarrolló esta idea al proponer en 1911 el concepto de «heterosexismo» entendido como la imposición social de la atracción hacia el sexo opuesto: «Pensemos que en nuestras sociedades, en nuestras costumbres, todo predestina un sexo al otro; todo enseña la heterosexualidad, todo invita a ella, todo la provoca, teatro, libro, periódico...» Sin embargo, el punto de vista de Freud, de Gide y de otros autores no tuvo demasiados seguidores, ni siquiera entre los psicoanalistas, y la idea del carácter «innato» o «natural» de la atracción hacia el sexo opuesto se impuso.

Fue así como el término "heterosexual" dejó de referirse a una patología o a una enfermedad y pasó a definir la sexualidad «normal». Así por ejemplo, el diccionario francés "Le Petit Robert" definió al heterosexual como aquella persona «que experimenta una apetencia sexual normal por los individuos del sexo opuesto». En ediciones posteriores introdujo una corrección importante: el heterosexual es aquel «que experimenta una atracción sexual (considerada como normal) por los individuos del sexo opuesto». Y finalmente en los años 2000 eliminó el paréntesis: el heterosexual es aquel «que experimenta una atracción sexual por los individuos del sexo opuesto».

Cuando la heterosexualidad define un sistema social se estudia bajo el nombre de heteronormatividad y según los principios de Yogyakarta es una violación a los derechos sexuales y a los derechos humanos considerado como un tabú.

El uso actual del término heterosexual tiene sus raíces en la tradición del siglo XIX más amplio de la taxonomía de la personalidad. Se sigue influyendo en el desarrollo del concepto moderno de la orientación sexual, y se puede utilizar para describir la orientación sexual de los individuos, historia sexual, o la auto-identificación. Algunos rechazan el término “heterosexual” como la palabra se refiere solo a uno de los comportamientos sexuales y no se refiere a los sentimientos románticos no sexuales. El término “heterosexual” se sugiere que han entrado en uso como un neologismo después, y frente a la palabra “homosexual” por Karl Maria Kertbeny en 1868. En el argot de LGBT, el término “obtentor” se ha utilizado como una frase denigrante burlarse de los heterosexuales. Hipónimos de heterosexual incluye heteroflexible.

En el registro coloquial se usa con frecuencia el acortamiento «hétero» o «hetero», dependiendo de la zona geográfica. El término “derecho” se originó como un término del argot gay en la primera mitad del siglo XX para los heterosexuales, en última instancia, viene de la frase “ir directo” (como en “recta y estrecho”), o dejar de tener relaciones sexuales homosexuales. Uno de los primeros usos de la palabra de esta manera fue en 1941 por el autor G. W. Henry. El libro de Henry conversaciones preocupados con los varones homosexuales y se utiliza este término en relación con la referencia a la ex-gays. En la actualidad, simplemente es un término coloquial para “heterosexual“ que tiene, al igual que muchas palabras, cambios en su significado primario con el tiempo. Algunos se oponen al uso del término “derecho” porque implica que los no heterosexuales son torcidos.

El simbolismo heterosexual se remonta a los primeros artefactos de la humanidad, con rituales de fertilidad y el arte primitivo. Esto se expresó más tarde en el simbolismo de los ritos de fertilidad y el culto politeísta, que a menudo incluyen imágenes de los órganos reproductivos humanos, como Lingam en el Hinduismo. Los símbolos modernos de la heterosexualidad en las sociedades derivadas de las tradiciones europeas todavía hacen referencia a los utilizados en estas antiguas creencias. Uno de estos es la imagen es una combinación del símbolo de Marte, el dios romano de la guerra, como el símbolo de la masculinidad definitiva, y el símbolo de Venus, la diosa romana del amor, como el símbolo de la feminidad definitiva. El carácter Unicode para este símbolo combinado es ⚤ (U + 26A4).

La tradición judeocristiana tiene varias escrituras relacionadas con la heterosexualidad. En Génesis 2:24, hay un mandamiento que dice “Por tanto, dejará el hombre a su padre y a su madre, y se unirá a su mujer, y serán una sola carne” (Gen 2:24). En 1º Corintios, se aconseja a los cristianos :Ahora, para los asuntos de matrimonio escribió : Es bueno para el hombre no tocar a una mujer, por lo tanto, digo a los solteros y a las viudas que es bueno para ellos soportarlo así como yo. Pero si no pueden contenerse, que se casen, porque es mejor casarse que arder. Pero ya que hay tanta inmoralidad, cada uno tenga su propia mujer, y cada una tenga su propio marido. El hombre debe cumplir su deber conyugal con su esposa, e igualmente la mujer a su marido. El cuerpo de la mujer no pertenece sólo a ella sino también a su marido. De la misma manera, el cuerpo del marido no pertenece sólo a él sino también a su esposa. No se nieguen el uno al otro, excepto de común acuerdo y por un tiempo, para que puedan dedicarse a la oración. A continuación, se unen de nuevo para que Satanás no os tiente por causa de su falta de autocontrol. Lo digo como una concesión, no como un comando. (NVI)En su mayor parte, las tradiciones religiosas en el matrimonio se reservan mundialmente a las uniones heterosexuales, pero hay excepciones que incluyen cierta tradiciones budistas e hindúes, Unitarismo Universalista, Iglesia de la Comunidad Metropolitana y algunas diócesis anglicanas y algunos Quaker, Iglesia Unida de Canadá y congregaciones de judaísmo Reformado y Conservador.

Casi todas las religiones creen que se permite el sexo legal entre un hombre y una mujer, pero hay algunos que creen que es un pecado, como Los Shakers, Harmony Society, y Ephrata Cloister. Estas religiones tienden a ver todas las relaciones sexuales como algo pecaminoso, y promueven el celibato. Otras religiones ven las relaciones heterosexuales como inferior al celibato. Algunas religiones requieren el celibato para ciertas funciones, tales como sacerdotes católicos; Sin embargo, la Iglesia Católica también considera que el matrimonio heterosexual es sagrado y necesario.

La demografía de la orientación sexual es difíciles de establecer debido a la falta de datos fiables. Sin embargo, la historia de la sexualidad humana muestra que las actitudes y comportamiento han variado a través de las sociedades. De acuerdo con los principales estudios, el 89% y el 98% de personas han tenido contacto heterosexual durante su vida;, pero este porcentaje se reduce al 79-84% cuando uno o ambos sientes atracción del mismo sexo y da cuenta de la conducta. En un estudio realizado en 2006, el 80% de los encuestados informaron de forma anónima que se sentían atraídos hacia los heterosexuales, aunque el 97-98% se identificaron como heterosexuales. Un estudio de 1992 informó de que el 93,9% de los hombres en Gran Bretaña han tenido experiencias heterosexuales, mientras que en Francia se informó el número al 95,9%.

En los Estados Unidos, según un informe del "Instituto Williams" en abril de 2011, el 96% o aproximadamente 250 millones de la población adulta son heterosexuales.

De acuerdo con una encuesta del 2008, el 85% de los británicos tienen contacto sexual solo con su sexo opuesto mientras que solo el 94% de los británicos se identifican como heterosexuales. Del mismo modo, una encuesta realizada por la Oficina Nacional de Estadísticas del Reino Unido (ONS) en 2010 encontró que el 95 % de los británicos se sientes identificados como heterosexuales, 1,5% de los británicos se identificaron como homosexuales o bisexuales, y el último 3,5% dio respuestas más vagos como “no sé”, “otro”, o no respondió a la pregunta.

Datos proporcionados por la Encuesta Gallup en octubre de 2012 obtuvo información demográfica sin precedentes sobre los que se identifican como heterosexuales, llegando a la conclusión de que el 96,6% de todos los adultos estadounidenses se identifican como heterosexuales, con un margen de error de ±1%.
En una encuesta realizada en 2015 por Yougov se demostró que de 1.632 adultos del Reino Unido, el 88,7% se identificaron como heterosexuales, 5,5% como homosexuales y el 2,1% como bisexuales. Cuando se le preguntó en que lugar se colocarían en la escala de Kinsey, el 72% de todos los adultos y el 46% de los adultos de 18-24 años, tomó una puntuación de cero, significa que se identifican como totalmente heterosexuales. Sin embargo el 4% de la muestra total y el 6% de los adultos jóvenes, dijeron que se situaban en el número seis, es decir, una identidad totalmente homosexual.

En otra encuesta de Yougov de 1.000 adultos de los Estados Unidos, el 89% de la muestra se identificada como heterosexual, 4% como homosexuales (entre 2% como masculina homosexual y 2% como hembra homosexual) y 4% como bisexuales (de cualquier sexo).

La relación entre la biología y la orientación sexual es un tema de investigación. No hay un determinante simple y singular para la orientación sexual se ha demostrado de manera concluyente; aunque diversos estudios apuntan a varias posiciones incluso, contradictorias, pero los científicos plantean la hipótesis de que una combinación de factores genéticos, hormonales y sociales pueden determinan la orientación sexual. Las teorías biológicas para explicar las causas de la orientación sexual son las más populares, y los factores biológicos pueden implicar una compleja interacción de factores genéticos y el ambiente uterino temprano, o factores biológicos y sociales. Estos factores, que pueden estar relacionados con el desarrollo de la orientación heterosexual u otra , incluir genes, hormonas prenatales, y la estructura del cerebro y su interacción con el medio ambiente.




</doc>
<doc id="37876" url="https://es.wikipedia.org/wiki?curid=37876" title="Orientación sexual">
Orientación sexual

La orientación sexual es la atracción afectiva, romántica, sexual y psicológica que la persona siente de modo sostenido en el tiempo. Según la Asociación Estadounidense de Psicología, la orientación sexual deriva entre un continuo marcado por dos extremos, la atracción exclusiva por el sexo contrario, y la atracción exclusiva hacia individuos del mismo sexo. Por ello, para su estudio, se consideran tres categorías: la heterosexualidad –atracción hacia personas del sexo opuesto–, la homosexualidad –atracción hacia personas del mismo sexo– y la bisexualidad –atracción hacia ambos sexos–. El comportamiento sexual humano, la identidad de género y la identidad sexual son términos relacionados con la orientación sexual, ya que psicológicamente conforman la percepción sexual en una persona. La preferencia sexual sugiere un grado de elección, que determina la vida sexual de una persona al establecer un sexo como objeto de deseo, por lo que es incorrecto utilizar el término para referirse a la orientación sexual de una persona. La concordancia o discordancia se refiere a la relación o similitud entre preferencia sexual y la orientación sexual. Se aplica el término concordancia a una persona cuyas preferencias sexuales coinciden con su orientación sexual (por ejemplo: un varón declarado homosexual que sostiene relaciones sexuales con personas del sexo masculino), mientras que discordancia se refiere a la diferencia entre la preferencia sexual y la orientación sexual (por ejemplo: una mujer declarada heterosexual que siente atracción sexual a personas del sexo femenino, que sostiene relaciones sexuales con personas del sexo masculino y que prefiere ser llamada heterosexual).

Define un patrón de comportamiento dentro de las identidades de la orientación sexual que se refleja como la atracción emocional o sexual hacia un determinado género o identidad de género. Dentro de este espectro suele clasificarse la heterosexualidad y la homosexualidad por representar únicamente la atracción hacia un solo grupo específico de personas. La monosexualidad puede derivar en la atracción hacia personas que pertenecen a un grupo específico de personas, bajo las características de su género biológico o su identidad de género, incluyendo la posibilidad de un espectro extraordinario a la clasificación convencional de los géneros binarios.

En un concepto más alejado de la clasificación de la orientación sexual, suele decirse que la monosexualidad define un patrón de comportamiento sexual en el que se prefiere la interacción sexual con sólo una persona; haciendo referencia a la fidelidad de pareja, la dependencia, la demisexualidad o la atracción específica por una sola persona.

Dentro del estudio del comportamiento sexual humano y los roles de género, refiere al proceso mediante el cual una persona de identidad polisexual se establece en un solo patrón de comportamiento sexual en el que solo interactúa con un solo grupo específico de personas que tienen determinado sexo biológico o identidad de género (por ejemplo: un varón bisexual de preferencias sexuales androfílicas que ha decidido establecerse sexualmente dentro de relaciones con otros varones).

El término monosexualización no debe ser confundido con el término utilizado en diferentes campos de la biología y la sociología, ya que tienen diferentes significados y diferente aplicación. Dentro del campo de la biología, el término monosexualización refiere al proceso biológico en el que un feto adquiere mediante distintos procesos hormonales las características propias de un sexo. Dentro del campo de la sociología, el término monosexualización refiere al establecimiento del poder social dentro de un grupo de personas selecto por su género.

Polisexualidad refiere a un patrón de comportamiento en el que se siente atracción sexual o emocional hacia varios grupos de personas de determinado sexo biológico o identidad de género, de manera opuesta a la monosexualidad. El término no debe ser confundido con los términos pansexualidad y poliamor, ya que hacen referencia a distintos patrones dentro del comportamiento sexual.

La polisexualidad se distingue de la pansexualidad por representar únicamente la atracción hacia algunos grupos de personas de determinado sexo o identidad de género, mientras que la pansexualidad refiere a la atracción plural por distintos grupos específicos de personas en los que se encuentran los géneros binarios y las distintas identidades de género. Dentro de la polisexualidad puede incluirse la atracción hacia ambos géneros binarios o hacia identidades de género que no se encuentran dentro del espectro de los género binarios. Diferenciado del concepto de poliamor que refiere a la flexibilidad poligámica de la aceptación de más de una pareja sexual.

En los estudios referentes a la clasificación de la orientación sexual, suele hablarse de patrones de comportamiento que no se definen como heterosexualidad u homosexualidad, propiamente. En este sentido suelen aplicarse variedad de términos que hacen referencia a los patrones de comportamiento sexual de un individuo, y no al tipo de orientación sexual con la que el individuo se identifica o se reconoce. El término "tendencia" suele ser utilizado para describir la presencia de patrones de comportamiento sexual (heterosexual u homosexual) regulares en una persona que se reconoce dentro de determinada orientación sexual, cuyos comportamientos, en ocasiones, se alejan de lo que se establece clínicamente como propio de esa orientación sexual. El término tendencia suele ser adherido a un calificativo de la orientación sexual para denotar la presencia de patrones sexuales impropios de la orientación sexual de la que se habla, sin llegar a la necesidad de clasificarlo como bisexualidad o pansexualidad. Existen dos dimensiones dentro de esta clasificación "heterosexualidad con tendencias homosexuales", donde una persona que se identifica como heterosexual presenta algún patrón de conducta homosexual, y "homosexualidad con tendencias heterosexuales", donde una persona que se identifica como homosexual presenta algún patrón de conducta heterosexual.

Androfilia es un término que hace referencia a un patrón en el comportamiento sexual en el que se siente atracción sexual por individuos del género masculino, sin importar el tipo de orientación sexual que esté presente. La androfilia puede aparecer como un patrón sexual regular dentro de la clasificación de las orientaciones sexuales en los caso de: un femenino heterosexual, un masculino homosexual, un bisexual y un pansexual. El término suele utilizarse como un concepto de menor extensión que refiere únicamente a los patrones cronofílicos de la atracción por personas adultas del género masculino o a la atracción por pubescentes o post-pubescentes de sexo masculino. Andromimetofilia es un término similar a androfilia que refiere a la atracción por individuos femeninos que presentan rasgos culturales o sociales que convencionalmente se asignan a las personas del género masculino en la sociedad tradicional, pudiendo también ser aplicado en algunos casos a femeninos transgénero (transexuales, transvestistas, andróginos y algunos casos de genderqueer). Suele clasificarse a la andromimetofilia como un patrón de comportamiento sexual anormal o parafilia como en el fetichismo transvestista.

Ginefilia o ginecofilia es un término que hace referencia a un patrón en el comportamiento sexual en el que se siente atracción sexual por individuos del género femenino, sin importar el tipo de orientación sexual que esté presente. La ginefilia o ginecofilia puede aparecer como un patrón sexual regular dentro de la clasificación de las orientaciones sexuales en los caso de: un masculino heterosexual, un femenino homosexual, un bisexual y un pansexual. El término suele utilizarse como un concepto de menor extensión que refiere únicamente a los patrones cronofílicos de la atracción por personas adultas del género femenino o a la atracción por pubescentes o post-pubescentes de sexo femenino. Ginemimetofilia o ginecomimetofilia es un término similar a ginecofilia que refiere a la atracción por individuos masculinos que presentan rasgos culturales o sociales que convencionalmente se asignan a las personas del género femenino en la sociedad tradicional, pudiendo también ser aplicado en algunos casos a masculinos transgénero (transexuales, transvestistas, andróginos y algunos casos de genderqueer). Suele clasificarse a la ginemimetofilia como un patrón de comportamiento sexual anormal o parafilia como en el fetichismo transvestista.

Los términos androfilia y ginecofilia pueden ser utilizados para referirse a la preferencia sexual de una persona, sin la consideración sobre su verdadera orientación sexual. En ocasiones funcionan como un sustituto ético que pretende eliminar la clasificación convencional de la orientación sexual como heterosexual, bisexual u homosexual; aplicando los términos androfilia y ginefilia para eliminar las asociaciones culturales de la homosexualidad con la feminidad y la masculinidad, independientemente del sexo o identidad de género del indivduo.

Los criterios de la orientación sexual pueden ser clasificados dentro de los parámetros de la monosexualidad y la polisexualidad; siendo la heterosexualidad y la homosexualidad categorías de la monosexualidad; mientras que la bisexualidad y la pansexualidad caen bajo los parámetros de la polisexualidad. Notablemente se consideran tres orientaciones sexuales principales, la heterosexualidad, la homosexualidad y la bisexualidad. Otras orientaciones pueden ser consideradas como orientaciones sexuales específicas o pueden ser catalogadas como una variante de alguna de las orientaciones sexuales primarias, como la pansexualidad que es frecuentemente catalogada como una variante de la bisexualidad. Otra categoría incluye las orientaciones sexuales no aceptadas, como la demisexualidad. La asexualidad es una falta de atracción sexual.

Las causas por las que una persona experimenta una u otra orientación sexual son objeto de numerosos estudios. Se han investigado posibles influencias genéticas, hormonales, culturales, sociales y del desarrollo, pero no hay consenso en la comunidad científica para determinar con exactitud qué es lo que determina la orientación sexual.

Según John Money, la orientación sexual sucede por una variación del hipotálamo del cerebro. En el caso de la mujer, si la célula monosexual es masculinizante, ella será homosexual; mientras que si es feminizante, será heterosexual. En el hombre sucede al revés; es la monosexualidad masculinizante la que lo hace heterosexual y la feminizante homosexual.

La heterosexualidad (del griego "ἕτερος" [jéteros] 'otro, diferente' y el sufijo "sexualidad") es un término social utilizado para referirse a la atracción sexual o emocional hacia personas del sexo opuesto, siendo marcada por la complementación de géneros binarios (emparejamiento de masculino y femenino). La heterosexualidad es un comportamiento sexual apreciable en diversas especies animales, el cual constituye una mayoría sexual en el promedio poblacional de determinadas especies, es decir, es un patrón de comportamiento muy frecuente entre los individuos de una misma especie. La heterosexualidad es un modelo evolutivo que define el comportamiento sexual reproductivo de las especies, manifestándose con la adaptabilidad, complementación de las gónadas y la compatibilidad de las células reproductivas entre dos individuos de la misma especie.

La heterosexualidad se establece como un modelo social percibido en las nociones tradicionales de la familia, la sexualidad y el matrimonio. Este modelo social tradicional frecuentemente se ajusta a la heteronormatividad y el heterosexismo para establecer distintos patrones de comportamiento y distintos aspectos culturales que, prácticamente, suprimen la posibilidad de otras variantes de la orientación sexual. La heterosexualidad, debido a su mayoría poblacional, se ha colocado socialmente como un patrón de los aspectos sociales dentro de la religión y la política. Los modelos políticos y religiosos de la heterosexualidad en la sociedad frecuentemente estipulan la heteronormatividad social, no como una norma, sino como una estipulación que supone a la heterosexualidad como una mayoría poblacional naturalmente designada o una forma de vida en la mayoría poblacional, recayendo en los aspectos del heterosexismo.

La homosexualidad (del griego "ὁμός", ""homo"" que significa "igual" y el sufijo "sexualidad") es el término social utilizado para referirse a la atracción sexual o emocional hacia personas del mismo sexo. La homosexualidad no tiene un marco basado en la complementación binaria de los género para la reproducción, sino que establece dos posibles variantes: la homosexualidad que concentra el emparejamiento de un masculino con otro masculino y relaciones que empareja a un femenino con otro femenino. Popularmente se le llama lesbiana a una mujer homosexual; a un varón homosexual se le conoce popularmente como gay, aunque el adjetivo también aplica a mujeres homosexuales. La homosexualidad es un tipo de conducta natural manifestada entre los patrones de comportamiento sexual de diferentes especies animales.

La bisexualidad es una orientación sexual que involucra atracción física y/o sentimental hacia más de un género. Términos similares como la heteroflexibilidad, la bicuriosidad, la pansexualidad y la polisexualidad suelen ser catalogados bajo los criterios de la bisexualidad, pero los términos difieren de éste. Al igual que la homosexualidad, la bisexualidad se manifiesta como un comportamiento sexual natural en diversas especies animales.

La heteroflexibilidad y la bicuriosidad son fijadas principalmente como preferencias sexuales en las que se tiene contacto sexual o relaciones emocionales ocasionales con personas del mismo sexo cuando una persona se identifica como heterosexual. La polisexualidad refiere a una variante en la clasificación de las orientaciones sexuales en la que se catalogan las orientaciones sexuales que tienen como objeto de atracción sexual más de un sexo o identidad de género, entonces, asumiendo a la bisexualidad y a la pansexualidad como subcategorías de la polisexualidad. La pansexualidad refiere a una variante de la bisexualidad en la que se siente atracción por personas independientemente del género de estas.

La bisexualidad es una fijación transitoria entre la heterosexualidad y la homosexualidad según su ubicación en el continuo homosexual-heterosexual propuesto por Alfred Kinsey, es decir, es una combinación de la conducta heterosexual y la conducta homosexual. Según Sigmund Freud, el ser humano nace con orientación sexual establecida como bisexual y conforme se desarrolla, adquiere y unifica su sexualidad hacia una sola orientación, ya sea heterosexual u homosexual.

La asexualidad es definida como la falta de algún tipo de orientación sexual, por lo que frecuentemente no es catalogada bajo los parámetros de la orientación sexual. Dentro de la conducta asexual, el individuo no manifiesta atracción sexual hacia cualquier individuo de cualquier sexo o identidad de género. El término asexualidad también suele ser utilizado dentro de otros contextos para definir la falta de interés en la práctica de relaciones sexuales con cualquier tipo de persona.

La asexualidad no debe ser confundida con la abstinencia sexual o el celibato, que son sólo la supresión de las relaciones sexuales por motivos religiosos o personales, no involucrando la supresión del deseo sexual. Los asexuales pueden experimentar una atracción emocional o deseos de intimacía con otras personas. La asexualidad es aceptada como orientación sexual por algunos especialistas, pero algunos otros difieren al asegurar que la asexualidad no es una orientación sexual. La asexualidad es de descubrimiento y estudio relativamente recientes; algunas organizaciones como "Asexual Visibility and Education Network" buscan el reconocimiento de la asexualidad como una orientación sexual que se catalogue bajo los parámetros psicológico-sociales de la clasificación de la heterosexualidad, la homosexualidad y la bisexualidad.

La pansexualidad es una orientación sexual humana caracterizada por la atracción por otras personas independientemente de su sexo y su género. Por eso, pueden sentirse atraídas por varones, por mujeres y también por aquellas personas que no se sienten identificadas con la dicotomía varón/mujer o con la de masculino/femenino, incluidas las personas no-binarias y las personas intersexuales. Las personas pansexuales afirman que para ellos el sexo y el género son conceptos vacíos de significado o que no tienen importancia a la hora de sentir atracción.

Demisexualidad es un término acuñado por "Asexual Visibility and Education Network" que refiere a la atracción sexual exclusivamente hacia personas con las que previamente se han desarrollado lazos emocionales estables y de cierta duración. Antes de ello, el demisexual se comporta como un asexual e incluso puede llegar identificarse como tal. La demisexualidad puede ser de fijaciones hetero- y homosexuales que se desarrollan en una atracción sexual secundaria hacia personas con las que se han desarrollado estrechos vínculos amorosos, mientras que como atracción sexual primaria se manifiesta una aparente orientación asexual. Normalmente, las relaciones comienzan por la etapa primaria (atracción física) y en algunos casos se llega a la secundaria (sentimientos profundos de amor, cariño, lealtad y compromiso), siendo muy común que sin un interés físico no se cultiva la relación de pareja. En otros casos se decide no pasar de ahí (relaciones muy esporádicas o únicamente sexuales, etc). En la demisexualidad ocurre al contrario; para alcanzar la etapa primaria hay que alcanzar antes la secundaria.

Existen distintas hipótesis que relacionan el origen de la orientación sexual como el producto de una serie de determinaciones biológicas y sociales a las que se somete el individuo durante su etapa pre-natal o post-natal. Las teorías "queer" modernas que refieren al origen de la orientación sexual suelen favorecer la hipótesis de que la orientación sexual se debe completamente a eventos biológicos que ocurren en la etapa pre-natal del individuo, rechazando los componentes sociales y su influencia en la vida del individuo debido a una nula relación entre las relaciones familiares y la orientación sexual de una persona. Los estudios de la orientación sexual suelen estar principalmente enfocados al origen de la homosexualidad y la bisexualidad en el comportamiento sexual humano, ya que se presupone a la heterosexualidad como un modelo natural propio de la reproducción sexual, aunque también la heterosexualidad sea objeto de estudio.

El origen biológico de la orientación sexual suele ser explicado en diversas hipótesis. En diversos estudios sobre el origen biológico de la orientación sexual suelen identificarse tres dimensiones biológicas principales que podrían ser determinantes de la orientación sexual y diversos factores del comportamiento sexual humano; en estas dimensiones se clasifican las hipótesis por su origen genético, por su origen hormonal y por su origen estructural.

Dentro de las hipótesis de origen estructural, meramente biológicas, se encuentran las hipótesis de la naturaleza sexual animal y la homosexualidad en animales, además la fisiología humana que comprende el sistema nervioso en relación con la orientación sexual y la hipótesis rechazada de que las orientaciones "queer" se deban a los efectos patológicos de un virus. Las hipótesis que corresponden a la explicación de la orientación sexual como origen genético normalmente basan sus hipótesis en la posibilidad de existencia de un gen específico o modelo de determinación evoluctiva que explique las distintas variantes de la orientación sexual. Las hipótesis que corresponden a la explicación de la orientación sexual como origen hormonal suelen hacer hincapié en la determinación bioquímica de la orientación sexual, producto de la alteración de la acción del hipotálamo debido a su exposición a agentes hormonales sexuales en la etapa pre-natal.

La heterosexualidad es una conducta cuya finalidad principal es que se produzca el apareamiento y la subsiguiente reproducción, como mecanismo involucrado en la conservación de las especies. Los sexos se identifican, salvo anomalías, en base al dimorfismo sexual que a su vez influye en las gónadas y éstas en las hormonas que gobiernan las conductas reproductivas; es decir, en las características fenotípicas. La atracción intersexual es el mecanismo biológico que facilita el proceso hasta alcanzar la combinación del material genético de ambos progenitores y eventualmente la crianza de la descendencia. Las características evolutivas de la selección natural permiten el desarrollo de adecuaciones corporales que permitan la fecundación y la combinación genética de dos individuos de la misma especie en un producto generacional. Naturalmente, las especies que rigen su ciclo reproductivo de acuerdo a la reproducción sexual, presentan órganos especializados que facilitan la fecundación en el acto de la cópula o apareamiento.

La homosexualidad y la bisexualidad son patrones de comportamiento sexual que pueden ser apreciados en alrededor de 1500 especies animales que, pueden o no, tener una relación evolutiva con la familia "Hominidae" a la que pertenece el "Homo sapiens". La homosexualidad y la bisexualidad son recurrencias poblacionales en distintas especies animales que se manifiestan como una variante biológica del comportamiento sexual animal. En las especies animales que manifiestan conducta homosexual suelen observarse patrones de comportamiento comunes que no necesariamente involucran la cópula homosexual, sino que, en algunas ocasiones, puede manifestarse simplemente como la necesidad de un individuo de establecer vínculos biosociales con individuos de su mismo sexo.

La observación del comportamiento homosexual animal en especies es originado de un estudio contemporáneo de los modelos queer que intentan fundamentar el comportamiento homosexual humano como una variante biológica común en distintas especies. En el estudio de los patrones de homosexualidad animal se reconoce a Joan Roughgarden, Bruce Bagemihl y Paul Vasey como los introductores globales de las investigaciones sobre la naturalidad homosexual en especies animales ajenas a la humana. Los patrones de comportamiento sexual animal relacionados con la homosexualidad y la bisexualidad se manifiestan en una menor recurrencia poblacional que la heterosexualidad. La poca recurrencia de orientaciones distintas a la mayoría sexual heterosexual se sustenta bajo el modelo tradicional de la demografía de la orientación sexual, lo que podría suponer una variante biológica poco frecuente en el comportamiento sexual de diversas especies animales.

La asexualidad puede manifestarse como un tipo de comportamiento sexual animal con poca recurrencia poblacional, ya que su origen biológico se atribuye a la determinación fisiológica y la capacidad hormonal de un individuo. La hipótesis sugiere la determinación de la asexualidad como la baja o nula respuesta sensitiva de la testosterona en función de la excitación sexual o un tipo de determinación "in utero". El modelo de la asexualidad animal basa su especificación en el modelo de la homosexualidad animal, debido a la existencia poblacional de individuos pertenecientes a especies animales que no han sido castrados, los cuales no experimentan ningún tipo de comportamiento sexual heterosexual u homosexual.

El gay virus o gay germ es una hipótesis rechazada sobre la causa de la homosexualidad, cuyos principios sugieren la existencia de un virus humano que provoca el comportamiento homosexual. Fue propuesta por Gregory Cochran y Paul W. Ewald como un intento de encontrar una causa microbiana a lo que entonces se consideraba una enfermedad. La hipótesis partía del supuesto de que la homosexualidad era una conducta patológica provocada por la actividad de un virus, el cual desencadenaba respuestas patológicas que causaban una alteración de la estructura cerebral del supuesto infectado. Cochran y Ewald concluyeron que el supuesto virus se concentraba en zonas urbanas, cuyos habitantes eran más propensos a contraer el virus ya que existía una mayor cantidad de personas homosexuales en zonas urbanas..

La hipótesis de la estructura cerebral en relación a la orientación sexual se basa en el análisis estructural y fisiológico de la conformación cerebral y la manifestación de diferencias físicas entre personas heterosexuales y personas homosexuales (diferencias obtenidas del análisis del hipotálamo, sección encefálica determinante en los procesos naturales del comportamiento sexual humano). La hipótesis de la estructura cerebral sugiere que la determinación de la orientación sexual de un individuo depende del estructura y desarrollo del hipotálamo en la conformación fisiológica del sujeto, desarrollo que puede presentarse en distintos procesos bioquímicos que se relacionan con la hipótesis de determinación "in utero".

La diferencia en la estructura cerebral corresponde a cada uno de los género binarios y las distintas orientaciones sexuales, ya que se presentan diferencias apreciables en la proporción del hipotálamo de las muestras según su género u orientación sexual. En el año de 1990, Dick F. Swaab y Michel A. Hofman, determinaron características del dimorfismo sexual cerebral que se manifestaba en una distinta proporción de tamaño en el núcleo supraquiasmático entre varones heterosexuales y varones homosexuales (estudio que estableció la base para determinar las componentes biológicas de la determinación de la transexualidad). El experimento de Swaab y Hofman consistió en el sometimiento de ejemplares de ratas al tratamiento con ATD ("1,4,6-Androstatriene-3,17-dione"), un componente químico Inhibidor que evita la transformación de la testosterona en estradiol, capaz de alterar el comportamiento social y sexual animal. El experimento concluyó en la presentación de resultados en tres distintas poblaciones de ratas. Las ratas que fueron tratadas pre-natalmente y post-natalmente con ATD presentaron una mayor cantidad de neuronas concentradas en el núcleo supraquiasmático, además de que una muestra de dicha población presentó comportamiento bisexual. La tercera población de ratas no se sometió a ningún tratamiento, por lo que no manifestó comportamientos extraordinarios. Los resultados obtenidos de la observación del grupo de ratas sirvieron de base para la hipótesis que sugiere que un número elevado de neuronas vasopresoras en el núcleo supraquiasmático de un masculino adulto homosexual puede reflejarse en las diferencias en la interacción de las hormonas sexuales y el desarrollo cerebral pre-natal y post-natal. El estradiol es frecuentemente relacionado con una importancia estructural que lo liga a la diferenciación de la estructura cerebral de ambos géneros binarios, pudiendo tener resultados como un determinante hormonal pre-natal del comportamiento sexual humano.

En el año de 1992, Laura Allen y Roger Gorski establecieron la proporción de la comisura anterior en tres poblaciones distintas que contenían 30 varones homosexuales, 30 varones heterosexuales y 30 mujeres heterosexuales. Los resultados arrojaron una diferencia en la proporción de la comisura anterior de las muestras, mostrando que los varones homosexuales poseían comisuras anteriores de mayor tamaño, seguidas en proporción por las muestras de mujeres heterosexuales y varones heterosexuales, quienes registraron un menor tamaño en las comisuras anteriores. El estudio fue retomado por William Byne y Bruce Parsons en el año de 1993, quienes registraron distintos márgenes en la proporción de la comisura anterior de varones heterosexuales y varones homosexuales, concluyendo en que el tamaño de las comisuras anteriores era variable y no dependía de género, ni orientación sexual.

William Byne y Bruce Parsons sugieren una hipótesis que relaciona el transgénero con la homosexualidad, estableciendo una hipótesis que sugiere que los rasgos conductuales biológicamente establecidos que se relacionan con las diferencias de género facilitan el desarrollo de la homosexualidad en masculinos, experimento que se corrobora con el fenómeno de la homosexualidad precursada por la inconformidad de género infantil. Byne y Parsons sugieren que dichas conductas son producidas por efectos hormonales en la corteza cerebral, y no en el hipotálamo.

Simon LeVay estudió cuatro tipos de neuronas del hipotálamo, denominadas INAH1, INAH2, INAH3 y INAH4. Las neuronas INAH resultan de gran importancia en el estudio de LeVay debido a su función en el comportamiento sexual humano y a la distinta proporción de tamaño que presentaron en muestras masculinas y femeninas pertenecientes a estudios de años anteriores. El experimento de LeVay consistió en el análisis de 41 cerebros de cadáveres identificados con distintos géneros y orientaciones sexuales: 19 varones homosexuales fallecidos por SIDA, 16 varones heterosexuales (6 de los cuales fallecieron de SIDA) y 6 mujeres heterosexuales (una de ellas muerta de SIDA). LeVay no encontró pruebas de diferencias de tamaño de las neuronas INAH1, INAH2 o INAH4. Aunque el grupo INAH3 parecía ser el doble de tamaño en masculinos heterosexuales que en el de los masculinos homosexuales que sería de un tamaño similar al INAH3 del cerebro de los femeninos heterosexuales. El estudio no es completamente aceptado por su insignificancia estadística y la posibilidad que los resultados fuesen alterados por la presencia de muerte de SIDA en el análisis.

El experimento de LeVay fue retomado por William Byne. Byne analizó las neuronas INAH de distintas muestras humanas de 14 masculinos homosexuales fallecidos de SIDA, 14 masculinos heterosexuales (10 fallecidos de SIDA) y 34 femeninos heterosexuales (9 fallecidos de SIDA). Encontraron diferencias de tamaño de las INAH3 entre varones y mujeres, de manera similar al estudio de LeVay. El tamaño de las INAH3 de los varones homosexuales parecía ligeramente menor que la de los heterosexuales pero mayor que la de las mujeres heterosexuales, aunque ninguna de ambas diferencias tenía significancia estadística.

En 2010, Dick F. Swaab y Alicia Garcia-Falgueras determinaron que la orientación sexual se debe completamente a la reacción hormonal del feto con la testosterona liberada en la gestación materna. Dicha reacción es determinante en la diferenciación dimórfica del producto como un macho biológico o una hembra biológica, reacción que también tiene efectos sobre las células nerviosas del individuo. Esta alteración en las células nerviosas refuerza la hipótesis de la determinación "in utero", atribuyendo la orientación sexual a eventos totalmente biológicos y no sociales, debido a que la homoparentalidad y la monoparentalidad no tienen efectos en la orientación sexual de los infantes.

En diversos estudios en gemelos, dedicados a analizar las probabilidades estadísticas de la homosexualidad y las componentes genéticas de la misma, se ha tratado de aislar los factores genéticos que producen el comportamiento homosexual en humanos, basados en la coincidencia estadística de las homosexualidad en gemelos de monocigoto (fenómeno popularmente denominado en el habla inglesa como "gay twins").

El estudio original es establecido por Michael Bailey y Richard Pillard de la Universidad de Boston en el año 1991. El estudio se condujo mediante el análisis de gemelos de monocigoto y gemelos de dicigoto en una encuesta poblacional. En el análisis de gemelos de monocigoto, también llamados gemelos idénticos, se determinó que un porcentaje de 52% en la población analizada presentaba orientación sexual homosexual cuando el otro individuo presentaba orientación sexual homosexual. En el análisis de gemelos de dicigoto, también llamados gemelos fraternales, se determinó que el 22% de la población analizada presentaba orientación sexual homosexual cuando el otro individuo presentaba orientación sexual homosexual. Los resultados de la "homosexualidad compartida" entre gemelos monocigóticos estableció la hipótesis de la existencia de un componente genético que determina la orientación sexual de un individuo mediante un grupo de variantes genéticas del polimorfismo genético y la herencia genética. El estudio de Bailey y Pillard fue corroborado por Scott L. Hershberger en el año 2001, en donde demostró la existencia clara de un componente genético en la determinación de la orientación sexual, debido a la concordacia en orientación sexual heterosexual y homosexual que presentaron individuos de monocigoto que analizó en la reproducción del análisis original. Michael Bailey, con el apoyo de Michael P. Dunne y Nicholas G. Martin, realizaron un censo poblacional sobre la orientación sexual de gemelos de monocigoto y dicigoto que vivían en Australia en el año 2000. Como resultado del censo, se determinó que un 30% de los 4900 gemelos monocigóticos encuestados, resultaron ser ambos homosexuales.

Dean Hamer encabezó una encuesta poblacional en el año 1993 sobre el análisis genético de 76 hermanos masculinos gay y el análisis de sus respectivos antecedentes genealógicos. La encuesta arrojó la coincidencia poblacional que establecía que los varones homosexuales presentaban una mayor cantidad de familiares homosexuales en su rama materna que en su rama paterna. Estos resultados promovieron el estudio de los antecedentes familiares de masculinos homosexuales, y el análisis y rastreo genético de los ligamientos del cromosoma X en su mapa genético. Hamer estableció la hipótesis de la existencia de un gen particular que determina la homosexualidad (hipótesis conocida como "gay gene" en el habla inglesa) que parecía ser transmitido maternalmetne y que parecía ubicarse en la ranura de "Xq28". Hamer eligió 40 pares de hermanos que presentaban orientación sexual homosexual y encontró que 33 de ellos compartían un conjunto de cinco marcadores genéticos idénticos en el cromosoma X. Hamer reportó en julio 19 de 1993 para la revista "Science", la posible existencia de un gen o varios genes que determinan el comportamiento sexual de una persona heterosexual u homosexual.

El experimento de Hamer fue reproducido posteriormente por Stella Hu en el año 1995, quien analizó una muestra poblacional de varones homosexuales y determinó que el 64% del material genético compartía un mismo marcador ubicado en la ranura Xq28 del cromosoma X. Estudios posteriormente realizados sobre el análisis de varones homosexuales (Estudio de Bailey del año 1999; y el estudio de Macknight y Malcolm del año 2000) y su genética establecen distintas discrepancias científicas, ya que no lograron replicar los resultados obtenidos por Hamer y Hu. Brian S. Mustanski realizó un análisis en el año 2005 sobre el genoma completo de los individuos y los familiares de éstos en los experimentos de Hamer y Hu. Mustanski encontró distintos locus hipotéticos para el gen que determina la orientación sexual en 7q36, 8p12 y 10q26, el último mostrando una mayor relación con la genética maternal (lo que respaldaría la hipótesis de Hamer).

En el año 2008 se produjo una leve respuesta social sobre la interpretación, según organizaciones políticas de bases religiosas y conservadoras en Estados Unidos, sobre el artículo "Answers to Your Questions For a Better Understanding of Sexual Orientation & Homosexuality" (2008) de la American Psychological Association; este artículo establece la posibilidad de que la orientación sexual puede ser determinada de acuerdo a la interacción de factores biológicos y sociales, asegurando que es imposible determinar las causas exactas de la homosexualidad hasta el momento debido a la inexistencia de estudios científicos verificables. Ante esta publicación se afirmó que la American Psychological Association había rechazado la hipótesis genética de la orientación sexual, lo que se interpretó públicamente como que esta organización había afirmado que la homosexualidad era adquirida, voluntaria, un padecimiento psiquiátrico o una condición psicológicamente ocasionada. Se desconoce una publicación oficial verificable de la American Psychological Association que sustente lo expresado por dichas organizaciones.

A pesar de que se desconoce la existencia y "locus" preciso del "gen de la homosexualidad", se señala que podría ser determinada por la herencia de un gen polimórfico complejo, es decir, un gen raro que comprende varias características, pero que sólo puede manifestar una única característica en la construcción del fenotipo. Actualmente se establece un nuevo modelo de la "genética homosexual" que se determina por la presencia de un gen polimórfico que determina la feminización y la masculinización biológica de un individuo. Dicho modelo sugiere la presencia de la homosexualidad como una posible variante recesiva de la orientación sexual heterosexual.

El modelo natural de la heterosexualidad presupone un adecuamiento de los organismos animales a los procesos de la reproducción sexual. La determinación evolutiva corresponde al modelo de la selección natural como determinante de la orientación sexual, relacionándose con la hipótesis de la naturaleza animal. La heterosexualidad basa su modelo en la complementación natural de géneros para los procesos de reproducción. En el caso humano, la orientación sexual heterosexual es un modelo que asegura la reproducción debido a que se ajusta fisiológicamente a la complementación de gametos y gónadas para producir la replicación de la especie. Dentro de la evolución del sexo, los animales han evolucionado para adquirir un determinado aparato reproductor y aditamentos de cortejo dimórficos, propios de hembras biológicas o machos biológicos.

Una hipótesis que concierne al origen genético de la homosexualidad y la bisexualidad, estipula la posible existencia de un gen en la construcción cromosomática que determine la conducta sexual homosexual en el humano. Existen especies animales en que los patrones sexuales e intersexualidad son ocasionados por procesos evolutivos detonados por cambios genéticos, produciendo ejemplares "metamasculinos" y "metafemeninos", tal es el caso de la especie "Drosophila melanogaster" (especie que comparte varias similitudes genéticas con la especie humana). En la misma hipótesis del origen genético de la homosexualidad se establece que la homosexualidad puede ser transmitida a la descendencia por la figura materna. También se sostiene que la identidad materna de la descendencia homosexual tiene una mayor capacidad de fecundidad que la identidad materna de la descendencia heterosexual, sugiriendo a la homosexualidad como un modelo evolutivo de selección natural diseñado para reducir el nivel de fecundidad de una generación de una especie (descendencia homosexual), ya que la anterior generación de la especie (identidad materna) poseía la fecundidad suficiente para producir más ejemplares de la especie.

Se sugiere que la orientación sexual es definida desde el nacimiento y que depende de la capacidad del feto a reaccionar a diferentes agentes hormonales que lo someten a procesos químicos de la monosexualización "in utero". El feto es sometido a la acción de la testosterona liberada por el organismo materno, desencadenando la sexualización del feto cuando la testosterona entre en contacto con el feto, o no exista contacto alguno; el contacto con la testosterona permitirá la evolución del feto como un macho biológico y su nulo contacto producirá la evolución del feto como una hembra biológica, aunque pueden ocurrir otras variantes como la intersexualidad y la androginia física cuando existen niveles irregulares en el contacto de la testosterona debido a una acción de los antígenos maternos. Esta hipótesis relaciona a la identidad materna con el origen de la homosexualidad.

En el año de 1990, Dick F. Swaab y Michel A. Hofman, determinaron las diferencias cerebrales del núcleo supraquiasmático entre varones heterosexuales y varones homosexuales . El experimento de Swaab y Hofman consistió en el sometimiento de grupos de ratas al tratamiento con ATD, un componente químico Inhibidor que evita la transformación de la testosterona en estradiol, capaz de alterar el comportamiento social y sexual animal. El experimento concluyó en la presentación de resultados en tres distintas poblaciones de ratas. Las ratas que fueron tratadas pre-natalmente y post-natalmente con ATD presentaron una mayor cantidad de neuronas concentradas en el núcleo supraquiasmático, además de que una muestra de dicha población presentó comportamiento bisexual. La tercera población de ratas no se sometió a ningún tratamiento, por lo que no manifestó comportamientos extraordinarios. Los resultados obtenidos de la observación del grupo de ratas sirvieron de base para la hipótesis que sugiere que un número elevado de neuronas vasopresoras en el núcleo supraquiasmático de un masculino adulto homosexual puede reflejarse en las diferencias en la interacción de las hormonas sexuales y el desarrollo cerebral pre-natal y post-natal. El estradiol es frecuentemente relacionado con una importancia estructural que lo liga a la diferenciación de la estructura cerebral de ambos géneros binarios, pudiendo tener resultados como un determinante hormonal pre-natal del comportamiento sexual humano. William Byne y Bruce Parsons sugieren que tanto el transgénero y la homosexualidad se deben a efectos hormonales ocurridos en la corteza cerebral, experimento que es respaldado con el fenómeno de la homosexualidad precursada por la inconformidad de género infantil.

Dick F. Swaab y Alicia Garcia-Falgueras determinaron en un estudio de 2010 que el cerebro humano manifiesta una diferenciación dimórfica a medida que el feto es sometido a agentes hormonales naturales en el desarrollo gestacional dentro del cuerpo materno, estabeciendo el origen de la homosexualidad como un evento totalmente biológico ocasionado por eventos bioquímicos "in utero". El cerebro del producto se desarrolla como consecuencia de la asimilación de las hormonas maternas que promueven la sexualización (hormonas liberadas por el organismo materno que promueven la diferenciación del feto, es decir, incitan al desarrollo corporal del feto para que se diferencíe como masculino o como femenino). La sexualización de un feto depende de la asimilación del feto y su reacción catalizante o nula reacción con la testosterona; en el caso de reaccionar con la testosterona liberada por el organismo materno, el feto asimilará la forma masculina, en el caso de no reaccionar con la testosterona, tomará la forma femenina. La testosterona actuará en diferentes magnitudes dentro del organismo, lo que produciría distintos efectos bioquímicos sobre las células nerviosas del feto, manifestándose en varias posibilidades sexuales que afecten la identidad de género y la orientación sexual del sujeto. Esta hipótesis refuerza la hipótesis de la orientación sexual como producto de la acción hormonal "in utero". La hipótesis sirve como base para los estudios "queer" modernos que rechazan todo tipo de componente social o psicológico dentro de la determinación de la orientación sexual (principalmente la homosexualidad y la bisexualidad), ya que se ha demostrado que el ambiente familiar monoparental u homoparental no tiene ningún tipo efecto en la identidad de género o la orientación sexual del infante sometido a la interacción con dicho ambiente.

Como variantes de la hipótesis de la orientación sexual como un proceso hormonal "in utero" se establecen hipótesis adicionales como la hipótesis del efecto del orden de nacimiento de los hermanos y la hipótesis de la inconformidad de género infantil como un proceso enteramente biológico. La hipótesis del efecto del orden de nacimiento de los hermanos sugiere que existe una mayor probabilidad de homosexualidad en individuos masculinos que descienden de una cadena de nacimientos de otros individuos masculinos, es decir, la homosexualidad en una persona es más probable si esta es la menor de una serie de nacimientos masculinos anteriores (hermanos mayores). Este proceso es determinado por una acción hormonal materna que es establecida por una acumulación de estrógenos en los posibles nacimientos femeninos que se convirtieron en nacimientos masculinos. Esta hipótesis refuerza la hipótesis de que la orientación sexual es determinada por procesos hormonales. La incorformidad de género infantil es un fenómeno psicológico en el que un individuo prepubescente no se identifica con los patrones sociológico y psicológicos de su género, en cambio se identifica con los del género opuesto. De esta manera, se considera a este fenómeno un precursor de la homosexualidad, al alterar la identidad de género del individuo.

El bisfenol A (BPA) es un componente químico presente en varios artículos plásticos (refractarios, alimentos enlatados y empaques plásticos), el cual suele ser reconocido por sus efectos hormonales en el cuerpo humano y ser registrado como causante de diversos padecimientos como: cáncer, pubertad precoz, obesidad, impotencia sexual, alteraciones en el desarrollo mamario, infertilidad, deseo sexual reducido, etcétera. El bisfenol A suele ser identificado como un posible componente de riesgo que afecta el comportamiento sexual animal, debido a que el BPA puede comportarse como estrógeno y alterar las funciones hormonales naturales del cuerpo humano. En el año 2010 se hizo un estudio científico en el que se aplicaban dosis de BPA a varios especímenes de ratones con el fin de observar las consecuencias de este químico en el desarrollo hormonal; los resultados arrojaron que la presencia de BPA implica periodos más cortos de reproductividad entre los especímenes femeninos, además de una concurrencia ocasional de cáncer endócrino.

La relación del BPA con el desarrollo hormonal de la homosexualidad surge con la investigación fisiológica del BPA como feminizador de la química hormonal masculina, resultando en alteraciones endócrinas que propician el desarrollo de la feminización física. En el año 2009 se publicó un experimento con macacos ("Macaca fascicularis") en el que se observó una aparente feminización conductual en los infantes que fueron sometidos a dosis de BPA durante la gestación; los especímenes masculinos manifestaron una respuesta conductual, social y sensorial hacia sus madres, similar a la respuestas de los especímenes de género femenino, marcándose aparentemente con una conducta de identificación transgénero. Este estudio alzó una respuesta social que afirmó que el incremento poblacional de la homosexualidad masculina podría ser causado por la interacción del BPA (utilizado comercialmente desde el año 1957) en el desarrollo "in utero; "aunque de esto se entiende que, según la demografía de la orientación sexual, la homosexualidad es más probable en la población y estadística actual debido a la simple duplicación poblacional que existe desde los años 1950, marcado con el periodo de la revolución sexual que permitió la libre expresión de la homosexualidad abierta. En el año 2011 se realizó otro experimento en el que sometió a la dosis de BPA a ratas adultas, ratas que manifestaron una feminización en sus respuestas sensoriales, ya que sus sentidos de ubicación espacial se asemejaron con los sentidos habituales de las ratas de género femenino, además se observó que las ratas macho sometidas al BPA tendían a ser menos atractivas para las hembras. Se desconoce un estudio oficial y verificable que posicione la homosexualidad como consecuencia de la interacción (adulta, infantil o" in utero") con el BPA.

La hipótesis del efecto del orden de nacimiento de los hermanos es una hipótesis propuesta por el sexólogo Ray Blanchard, la cual establece el origen de la orientación sexual homosexual en masculinos debido a la intervención de diversos procesos hormonales desencadenados por la entidad materna en la etapa de gestación. La hipótesis establece que de una serie de nacimientos masculinos anteriores, aumenta la posibilidad de homosexualidad en nacimientos masculinos posteriores, es decir, a medida que la taza de orden de nacimiento de varones en nacimientos exitosos aumenta, la probabilidad de que un feto masculino posterior presente homosexualidad aumenta. La hipótesis es simplemente identificada como el "efecto del orden de nacimiento de los hermanos" debido a la relación entre los hermanos mayores masculinos heterosexuales y su relación con la determinación biológica del hermano masculino menor homosexual.

Blanchard estableció en 2004 que este efecto de homosexualidad predeterminada por el nacimiento de hermanos mayores se debía a la "hipótesis de la inmudad materna", la cual sugiere que a medida que la identidad materna consigue embarazos masculinos exitosos, experimentará una inmunización progresiva hacia los antígenos (testosterona) que desencadenarán la monosexualización del feto. Esta inmunización lograda después de cada embarazo masculino desarrollará en el sistema inmunitario materno, anticuerpos "anti-masculinos" que retendrán un porcentaje de la testosterona que necesita el feto para desarrollar la monosexualización y la diferenciación cerebral dimórfica, por lo que en caso de ser un nuevo masculino, experimentará una "feminización cerebral" "in utero" que se manifestará en la orientación sexual homosexual.

Blanchard sugirió en el año 2008 que cada embarazo masculino exitoso produciría un 33% de mayores probabilidades de que el último nacimiento masculino sea homosexual, sin embargo, la existencia de embarazos femeninos exitosos en el historial reproductivo de la figura materna reduce dicha probabilidad. La hermandad atribuida a diferentes identidades maternas no tiene efectos en la orientación sexual, es decir, la existencia de hermanos de distintas madres en un mismo entorno familiar no es influyente en la determinación de la homosexualidad.

La inconformidad de género infantil es un fenómeno social que se caracteriza por la identificación social de un infante con los atributos sociales tradicionales del género opuesto a su género biológico. El infante presentará una inconformidad con su género biológico que frecuentemente es alimentada por los deseos de pertenecer y ser reconocido como una persona del género opuesto a su género biológico. Suele identificarse con la autoidentificación del infante como una persona del género opuesto y/o la adopción de ciertos comportamientos que se relacionan tradicionalmente con los del género opuesto (manifestándose en comportamientos transvestistas y preferencia por juguetes diseñados tradicionalmente para infantes del género opuesto). Este fenómeno es estudiado prinicipalmente en la población masculina y es psicológica, biológica y socialmente relacionado con la orientación sexual homosexual.

El sexólogo Richard Green analizó los patrones de la inconformidad de género infantil como precursora de la homosexualidad en su libro "The "Sissy Boy Syndrome" and the Development of Homosexuality" (1987). Green asegura que la inconformidad de género infantil, fenómeno poblacionalmente mayor en masculinos, puede derivar en orientación sexual homosexual en el posterior desarrollo del individuo. William Byne y Bruce Parsons sugieren que la homosexualidad y el trasgénero en expresión de transexualidad se relacionan debido a un desarrollo "in utero" de la estructura cerebral, estableciendo una hipótesis que sugiere que los rasgos conductuales biológicamente establecidos que se relacionan con las diferencias de género facilitan el desarrollo de la homosexualidad en masculinos debido a una predisposición natural ocasionada por la acción de agentes hormonales que produjeron una feminización cerebral dimórfica en un individuo masculino en etapa prenatal. Byne y Parsons aseguran que su experimento se respalda con el fenómeno de la homosexualidad precursada por la inconformidad de género infantil. Byne y Parsons sugieren que dichas conductas son producidas por efectos hormonales en la corteza cerebral, y no en el hipotálamo como lo aseguran los estudios de Dick F. Swaab y Michel A. Hofman (1990) y los estudios de Laura Allen y Roger Gorski (1992).

El origen ambiental es una hipótesis que atribuye el establecimiento de la orientación sexual de un individuo en los elementos socio-culturales que conforman su entorno. En dichas hipótesis se incluyen diversas hipótesis de origen psicológico que ubican a la orientación sexual como producto de eventos conscientes o inconscientes dentro del desarrollo social y biológico de un individuo. Halperin creía la homosexualidad podía ser ocasionada por problemas familiares no solucionados, concluyó diciendo que un padre débil y una madre fuerte pueden influir en el varón para acabar siendo homosexual.

Las hipótesis freudianas de la sexualidad suelen resultar relevantes en la explicación tradicional, no médica, del comportamiento sexual humano. Sigmund Freud creía que toda acción humana era inconscientemente motivada por un impulso sexual al que denominaba libido, evento producido por una transformación analógica de energía del impulso sexual. En la hipótesis del psicoanálisis de establece que dichos impulsos son reprimidos, de tal manera que encuentran una manera inconsciente de manifestarse según la cultura que rodea al individuo. Las posturas de Freud representaron una revolución sexual a la moral victoriana que ocultaba la sexualidad humana, contradiciéndola con las posturas de que todo comportamiento lleva un componente sexual en la hipótesis de la libido y que los infantes son completamente sexuales, proponiendo además, un modelo que sugería la conformación del deseo sexual infantil en la hipótesis del desarrollo psicosexual.

Freud consiguió diversas hipótesis del comportamiento sexual ajeno al heterosexual debido a los estudios de sexología anteriores de Richard von Krafft-Ebing y Magnus Hirschfeld. En una crítica al trabajo del endocrinólogo Eugen Steinach, el cual consistió en el trasplante de los testículos de un varón homosexual en un varón heterosexual en un intento por convertir al varón heterosexual en homosexual, Freud aseguró que el trabajo de Steinach fue relevante en las determinaciones orgánicas del homoerotismo. En este sentido, Freud consideraba que el experimento de Steinach era prematuro y que hubiera sido efectivo en alterar la orientación sexual de una persona sólo en casos en que la homosexualidad estuviera fuertemente asociada con características físicas típicas del sexo contrario. El experimento de Steinach era improbable debido a la respuesta inmune de ambos pacientes y a la imposibilidad de la homosexualidad como producto orgánico de las gónadas.

Entre los artículos de Freud sobre el tema están "Tres Ensayos Sobre la hipótesis de la Sexualidad" (1905) y "Algunos Mecanismos Neuróticos sobre Celos, Paranoia y Homosexualidad" (1922). Freud consideraba que los humanos nacían con una predisposición natural a la bisexualidad, la cual se convertía en heterosexualidad u homosexualidad de acuerdo a las asimilaciones psicológicas del objeto sexual en la novela familiar del sujeto. La creencia de Freud sobre la bisexualidad inicial se debe a la noción clínica victoriana de que los infantes integraban los dos géneros en su cuerpo debido a la nula manifestación del dimorfismo sexual hasta la etapa de la pubertad. Freud menciona que ciertos casos de homosexualidad se deben a la disforia en la experiencia heterosexual, provocando una inversión de la libido en su objeto sexual.

La homosexualidad femenina según la perspectiva de Freud fue analizada en el ensayo "La Psicogénesis de un Caso de Homosexualidad en una Mujer" (1920), en donde describe el caso de una mujer joven que es sometida al psicoanálisis por sus padres, en espera de que alguno de estos métodos la "curara" del lesbianismo. En el ensayo, Freud plasma sus consideraciones de que la homosexualidad no era una enfermedad o un problema neurótico y que la terapia no presentaba ningún tipo de resultado exitoso, decía que intentar transformar un homosexual en heterosexual, era tan imposible como intentar transformar un heterosexual en homosexual.

En la tradición psicoanalítica de la escuela freudiana (seguidores académicos de las hipótesis propuestas por Freud) se establece la homosexualidad como un producto de diversos eventos que sufre un individuo en las etapas del desarrollo psicosexual, en donde no ocurre una "madurez heterosexual" debido a una repercusión en alguna de estas etapas. La escuela freudiana rastrea un origen de la homosexualidad en la etapa fálica y las relaciones edípicas con los progenitores, en el que el individuo se identificará con la identidad parental del género opuesto y adquirirá la preferencia sexual de dicha identidad (Ejemplo: un varón que se identifica con su madre, desarrollará la preferencia androfílica que éste observa en su madre), no logrando así, la "madurez heterosexual".

En las nociones no contemporáneas de la orientación sexual se presupone, según la noción popular, que la heterosexualidad es una orientación sexual predominante que es producto de la correcta crianza del infante, la cual puede ser alterada por distintos factores. Suelen indentificarse distintas creencias populares que referían al origen de la homosexualidad, entre las que se ubica la homosexualidad como producto de un contagio social, como producto de una familia no convencional o como producto de traumas psicológicos. La mayoría de estas hipótesis suelen ser rechazadas ya que son irrelevantes en la determinación de la orientación sexual.

En la determinación de la orientación sexual según el ambiente familiar, se establecen distintas hipótesis. La hipótesis de la heterosexualidad como producto de un ambiente familiar sano es rechazada, debido a la probabilidad poblacional de otro tipo de orientaciones sexuales se presenten en ambientes familiares sanos y ambientes familiares poco sanos. Este modelo se preasume equivocadamente de acuerdo a que el ambiente familiar sano (formado, según la noción tradicional, por la familia nuclear que se compone de una pareja heterosexual) es propicio para el desarrollo de infantes sanos, ya que este modelo considera a otras orientaciones sexuales, ajenas a la heterosexualidad, como desórdenes mentales ocasionados por la interacción familiar particular. Del modelo anterior se desprende la hipótesis de la monoparentalidad y la homoparentalidad como determinantes en la conformación de la orientación sexual de un infante. Tanto la monoparentalidad, como la homoparentalidad, no presentan ningún efecto en la orientación sexual del infante, por lo que no son determinantes en el estudio de la orientación sexual. En la visión ética contemporánea sobre la adopción por parte de personas LGBT, se asegura que las estructuras familiar conformadas por LGBT son igualmente funcionales que la familia tradicional heterosexual, por lo que se asegura una neutralidad en el tema, de acuerdo a organismo oficiales como la American Psychological Association y la American Psychiatric Association.

La hipótesis del abuso sexual como determinante en la homosexualidad se entiende con una visión abstracta de la violación sexual masculino-masculino. El que perpetra la violación puede negar su homosexualidad en un forma confusa de agresividad que se cristaliza en acciones maquilladas de comportamientos hipermasculinos, proyectando posiblemente, deseos homosexuales en su vícitma. En una recapitulación de la violación, la víctima puede experimentar pérdida temporal de control, acompañada de miedo constante y frecuentes cuestionamientos sobre su verdadera identidad sexual u orientación sexual. A pesar del modelo anterior, la orientación de los participantes se encuentra establecida desde un principio, por lo que la violación sexual no es relevante en la determinación de la orientación sexual en ambos involucrados.

A la hipótesis de que la orientación sexual es determinada por los efectos del ambiente, se incluyen otros conceptos como la cultura y la región en la que se habita, basados en la noción tradicional no contemporánea de la diversidad de la orientación sexual. Ambos conceptos no suelen relacionarse mucho, ya que no son un factor determinante en el establecimiento de la orientación sexual, sino que sólo suprimen socialmente la expresión de ésta. Por diversos motivos como las legislaciones, los estereotipos, las creencias religiosas y el convencionalismo de la sociedad, un individuo será incapaz de reconocer su verdadera orientación sexual si la sociedad se lo impide. En sociedades donde existen pensamientos liberales que permiten el LGBT, será frecuente encontrar personas abiertamente reconocidas como LGBT. Debido a que este tipo de pensamiento liberal se concentra principalmente en zonas urbanas poco sujetas a las creencias religiosas y con legislaciones más modernas, se considera a la región en que se habita como un precursor de la orientación sexual; en cambio, si orientaciones sexuales como la homosexualidad son castigadas socialmente, se encontrarán pocas personas abiertamente homosexuales. En diversas visiones de la cultura occidental, la homosexualidad se ha convertido en una normalidad poblacional, dejando atrás las creencias anteriores al siglo XIX que aseguraban que la homosexualidad era una contradicción innatural que practicaban las personas que vivían en pecado, además de las creencias menos discriminatorias del siglo XIX y el siglo XX de que la homosexualidad era una perversión. En otro tipo de nociones tradicionales no contemporáneas, se relaciona el ambiente homosocial y el ambiente heterosocial como determinantes en la construcción de patrones conductuales de la orientación sexual heterosexual u homosexual. La homosocialidad y la heterosocialidad no tienen efectos en la conformación de la orientación sexual del individuo.

La hipótesis de la orientación sexual como producto de una elección voluntaria o como una transgresión social es completamente rechazada por los estudios científicos debido a que se entiende ampliamente que la orientación sexual no puede ser elegida, ni cambiada. Este modelo únicamente ampara la orientación sexual, y no la preferencia sexual, la cual puede tener un grado de elección voluntaria y manifestarse en comportamientos relacionados con la androfilia y la ginecofila. Comportamientos adicionales como la heteroflexibilidad y la bicuriosidad no involucran una orientación sexual específica, ya regularmente representan una curiosidad personal, ajena a la verdadera orientación sexual del individuo. Una persona puede negar su verdadera orientación sexual al grado de llevar a cabo actos sexuales impropios de la naturaleza de su orientación sexual real; este hecho no altera la verdadera orientación del individuo.

La terapia de reorientación sexual es un método terapéutico de intervenciones sociales en el que se pretende alterar la orientación sexual de un individuo, regularmente aquellas minorías sexuales para que empaten con la orientación sexual heterosexual. Las terapias suelen registrar una aparente efectividad que, en realidad, no altera la orientación sexual del individuo, sino que sólo la suprime bajo la motivación personal alimentada con el deseo de la integración social o la identificación religiosa. La terapia de reorientación sexual es desaprobada por organizaciones como: American Psychological Association, American Psychiatric Association, American Psychoanalitical Association, American Academy of Pediatrics, American Counseling Association, National Association of Social Workers, American School Counselor Association y Canadian Psychological Association. Estas organizaciones aseguran que la terapia de reorientación sexual no es efectiva o puede ser psicológicamente impactante o disfórica para el individuo, además de que puede atentar contra los derechos y libertades de la expresión sexual.

La determinación exacta de la demografía mundial de la orientación sexual suele resultar difícil, y en algunos sentidos, controversial debido al empate o nulo empate de la población con las expectativas demográficas mundiales de la orientación sexual. Se suele utilizar el término "minoría sexual" o "queer" para agrupar a las distintas expresiones sexuales LGBT, nomalmente reflejando un menor índice estadístico que se compara con los índices estadísitcos de la heterosexualidad y el cisgénero.

En el estudio demográfico de la orientación sexual suelen establecerse distintos criterios según las particulareidades del estudio, enfocándose a la categorización estadística de la autoidentificación de la población como heterosexual, homosexual o bisexual; además de estudios estadísticos que únicamente se enfocan a la revelación de las experiencias sexuales de la población, donde se encontraría el índice poblacional que reporta algún tipo de comportamiento sexual con persona del mismo sexo. Gran parte de estos resultados estadísticos varían en el índice de homosexualidad global entre 1% hasta 12%.

Los estudios demográficos que conciernen a la determinación estadística de la población homosexual y bisexual, suelen seguir el criterio en el que se agrupan en esta categoría, únicamente a las personas que se reconocen abierta y voluntariamente como homosexuales y bisexuales, ignorando otro tipo de criterios como las experiencias sexuales y la atracción hacia personas del mismo sexo. En diversos estudios demográficos de la orientación sexual se reporta un mayor porcentaje de masculinos que se reconocen como homosexuales y un mayor porcentaje de femeninos que se reconocen como bisexuales. En el Informe Kinsey sobre la concurrencia poblacional de la homosexualidad refleja, a partir de los registros de "Sexual Behaviour in the Human Male" (1948) y "Sexual Behaviour in the Human Female" (1953), que un estimado de 10% de masculinos manifestaba comportamiento exclusivamente homosexual y que un estimado de 2%-6% de femeninos manifestaba comportamiento exclusivamente homosexual.

Los estudios demográficos sobre el porcentaje poblacional que reporta algún tipo conductas sexuales (atracción sexual, deseos sexuales o relaciones sexuales) hacia personas del mismo sexo no tienden a considerar la identificación de la persona como heterosexual, homosexual o bisexual. En el Informe Kinsey se registran distintos porcentaje que refieren únicamente a las experiencias sexuales de tipo homosexual que registró una población selecta, donde: 37% de los varones entrevistados experimentaron alguna vez un orgasmo homosexual a partir de la adolescencia, 13% de los varones sintieron deseos homosexuales, 25% de ellos tuvieron experiencias homosexuales no incidentales entre las edades de 16 a 55 años y 18% mantuvieron igual número de relaciones heterosexuales que homosexuales; mientras que en un informe Kinsey sobre la homosexualidad femenina se registró que 13% de mujeres habían experimentado algún orgasmo homosexual a partir de la adolescencia.

En el estudio "Homosexuality/Heterosexuality: Concepts of Sexual Orientation" (1990) de David P. McWhirter, Stephanie A. Sanders y June Machover Reinisch se registró que un porcentaje de 13.95% de masculinos y 4.25% de femeninos habían sostenido experiencias homosexuales regulares de una manera no incidental. En "The Janus Report on Sexual Behavior" (1993) por Sam Janus y Cynthia L. Janus se registró que 9% de los varones y 5% de las mujeres se encontraban envueltos en recurrentes experiencias homosexuales. En "The Social Organization of Sexuality: Sexual Practices in the United States" (1994) de Edward O. Laumann, John H. Gagnon, Robert T. Michael y Stuart Michaels, se reportó que 7.7% de los varones y 7.5% de las mujeres presentaban constantes deseos homosexuales. En "The Prevalence of Homosexual Behavior and Attraction in the United States, the United Kingdom and France: Results of National Population-based Samples" (1995) por Randall Sell, James A. Wells y David Wypij, se reportó que 6%-10% de los varones y 2%-4% de las mujeres reportaban haber tenido experiencias homosexuales.

En el año 1983, Paula Nurius publica un estudio estadístico que retrata el comportamiento sexual y los principios médicos y psicológicos de la orientación sexual. El estudio incluyó a 689 estudiantes universitarios a quienes se les preguntó la frecuencia con la que practicaban relaciones sexuales y lo que involucraban en ellas. Los resultados serían clasificados en un graduado de 0-100 en heteroerotismo y homoerotismo; los resultados que dieran una puntuación de 10, o menor, en ambos graduados serían catalogados como "asexual". Los resultados del estudio arrojaron un porcentaje total de 5% de masculinos y 10% de femeninos clasificados en la categoría "asexual".

En consideraciones generales de la demografía mundial de la orientación sexual, se considera que aproximadamente el 1% de la población mundial es asexual. Este estudio, realizado en Reino Unido a 18,884 habitantes en el año 1994, es uno de los trabajos más citados actualmente para establecer una visión estadística de la población mundial que es asexual; este estudio fue realizado originalmente para recolectar información sobre los hábitos y comportamientos sexuales de las habitantes de Reino Unido como parte de una investigación fundamentada en la salud sexual y el SIDA.

A finales de los años 1940 y los primeros años de 1950, Alfred C. Kinsey publica distintos ensayos basados en recopilaciones y encuestas sobre el comportamiento sexual humano en un contexto moderno, distinto de las nociones tradicionales que soportaban la moral victoriana conservadora. Los estudios de Kinsey fueron revolucionarios por reconsiderar la frecuencia de los componentes sexuales de la naturaleza humana. En 1948 publica el libro "Sexual Behaviour in the Human Male" ("Comportamiento Sexual en el Humano Masculino"), seguido cinco años más tarde por una contraparte llamada "Sexual Behaviour in the Human Female" ("Comportamiento Sexual en el Humano Femenino"), todo como una respuesta a las cuestionantes modernas del comportamiento sexual humano en estudios de la Universidad de Indiana. 

El resultado de la investigación de ambos libros es conocido como Informe Kinsey, en él se plasman distintas estadísticas que refieren al comportamiento sexual de una muestra poblacional masculina y una muestra poblacional femenina. La moral condenó dichos resultados por incluir estadísticas que no solían ser consideradas en informes sobre la sexualidad humana, el informe incluía frecuentes reseñas sobre la homosexualidad y la sexualidad adolescente. Los hallazgos referían principalmente a la orientación sexual y los componentes de su determinación psicológica, demostrando la frecuencia de la homosexualidad y la bisexualidad en la sociedad y los deseos o experiencias sexuales homosexuales en la población heterosexual (tomando en cuenta varias muestras de distintos estratos socio-económicos).

Kinsey reportó en "Sexual Behaviour in the Human Male" (1948) lo siguiente:


Kinsey reportó en "Sexual Behaviour in the Human Female" (1953) lo siguiente:






</doc>
<doc id="37884" url="https://es.wikipedia.org/wiki?curid=37884" title="Convergència i Unió">
Convergència i Unió

Convergència i Unió (CiU; ) fue una federación de dos partidos políticos españoles de ideología nacionalista catalana, creada en 1978 como coalición y disuelta en 2015. Estaba integrada por Convergencia Democrática de Cataluña, de centroderecha, y Unión Democrática de Cataluña, de ideología democristiana. Estuvo a la cabeza del Gobierno de Cataluña entre 1980 y 2003, y desde 2010 hasta 2015.

El pacto entre CDC y UDC fue firmado oficialmente el 19 de septiembre de 1978. Convergència i Unió gobernó Cataluña ininterrumpidamente desde 1980 hasta 2003, bajo la presidencia de Jordi Pujol; obteniendo tres mayorías absolutas en los años 1984, 1988 y 1992, y cuatro mayorías simples en 1980, 1995, 1999, 2003. Sin embargo, en las elecciones de 1999 y 2003 el Partido de los Socialistas de Cataluña (PSC) obtuvo la mayoría de votos aunque no de diputados. 

Durante estos años CiU se caracterizó por colaborar a la gobernabilidad de España, apoyando a Adolfo Suárez durante la Transición (Pujol fue una pieza clave de la construcción de la España de las Autonomías) luego al PSOE de Felipe González en 1993 y más tarde a José María Aznar (ver "Pacto del Majestic") tras la obtención de la mayoría simple del Partido Popular en las elecciones generales de 1996 (a cambio, el PP de Cataluña apoyó a CiU en el Parlamento de Cataluña). La estrategia política del partido se correspondía con la visión de Pujol de la autonomía:

Desde enero de 2001, cuando Artur Mas —elegido como "sucesor" por el propio Pujol— asumió el cargo de "conseller en cap" de la Generalidad, se fue produciendo un relevo paulatino en el liderazgo de la federación. Esta designación abrió una crisis en la coalición de CiU, ya que otras personalidades como Duran i Lleida, líder de "Unió", también aspiraban a ese puesto. El 2 de diciembre de ese año CiU fue constituida oficialmente como una federación de partidos.

Tras 23 años en el Gobierno, los malos resultados obtenidos en las elecciones de 2003 permitieron la coalición entre el PSC, Esquerra Republicana de Catalunya e Iniciativa per Catalunya Verds - Esquerra Alternativa, que conformaron el "Tripartito catalán", que se reeditó en las elecciones de 2006.

En la primera legislatura del tripartito afloró el "problema del tres por ciento", cuando el entonces Presidente Pasqual Maragall afirmó en un debate en el Parlamento que «hemos llegado al meollo de la cuestión: ustedes tienen un problema y este problema se llama tres por ciento». La declaración provocó un gran revuelo, aunque finalmente Maragall se vio obligado a retirar la acusación con el fin de no perder el apoyo de CiU al nuevo Estatuto, aprobado finalmente en 2006.

El 28 de noviembre de 2010, CiU obtuvo una amplia victoria en las elecciones al Parlamento de Cataluña, con el 46 % de los diputados de la cámara (62 escaños) lo que le hizo recuperar la Presidencia de la Generalitat.
En plena crisis económica y de relación con el gobierno central, Mas y el resto de dirigentes de CDC dieron su apoyo a la autodeterminación de Cataluña. Algunas fuentes señalan que una razón fundamental del giro político fue la voluntad de distraer la atención de los numerosos escándalos de corrupción y financiación ilegal en los que CiU estaba implicado.

Según el Tribunal de Cuentas, en 2011 CiU tenía un patrimonio neto negativo, es decir, en quiebra técnica, de 10.184.954 euros, y Unió Democràtica de Catalunya un patrimonio neto negativo de 11.288.910 euros.

El 25 de septiembre de 2012 posteriormente a una multitudinaria participación en la histórica Diada del 11 de septiembre por el volumen de ciudadanos participantes, Artur Mas convoca elecciones anticipadas previstas para el 25 de noviembre, dos años antes de lo previsto, tras reconocer el fracaso del pacto fiscal para Cataluña (al no conseguir acuerdos en su reunión con el presidente del Gobierno español Mariano Rajoy) y articulando su programa electoral entorno al soberanismo. Finalmente, la formación obtuvo 50 escaños, 12 menos que en las anteriores elecciones, lo que se interpretó como un fracaso de la apuesta de CiU para ser el guía único del proceso secesionista en Cataluña.

Debido a las diferencias entre la posición de la dirección de UDC y la del líder de CDC Artur Mas con respecto al proceso soberanista, se realizó el 14 de junio de 2015 una consulta a la militancia de UDC, en la que preguntaban si UDC debería comprometerse a seguir con el proceso pero con unas determinadas condiciones, entre ellas no vulnerar la legalidad con declaraciones unilaterales de independencia o iniciando procesos constituyentes al margen de la legalidad, lo que sería contrario a lo suscrito en la hoja de ruta firmada por CDC, ERC y entidades soberanistas. La militancia de UDC decidió apoyar a la dirección con un ajustado 50,9 % a favor. Tras esto, se realizaron reuniones entre dirigentes de UDC y CDC que desembocaron en un ultimátum de CDC a UDC para que esta última decidiera en un plazo de "dos o tres días" si se sumaba o no a la hoja de ruta independentista.

El miércoles 17 tras una reunión del Comité de Gobierno de UDC, se anunció la salida de los tres consejeros de UDC del gobierno de la Generalitat de Cataluña, aunque se comprometían a mantener la estabilidad parlamentaria hasta el final de la legislatura. Ese mismo día por la noche, la Comisión Ejecutiva Nacional de CDC se reunió y en una rueda de prensa al día siguiente confirmó que UDC y CDC no se presentarían juntos a las elecciones autonómicas y que el proyecto político de la federación de CiU se había acabado.

En enero de 2018 CiU fue condenada por financiación ilegal. La sentencia del conocido como 'caso del Palau de la Música' condenó a CiU por lucrarse con 6,6 millones de euros a través del cobro de comisiones irregulares a cambio de adjudicar obra pública a la constructora Ferrovial. Esta trama permitió, entre 1999 y 2009, el expolio continuado de 23,7 millones de la arcas del Palacio de la Música Catalana. Parte del dinero -14 millones- fue utilizado para lucro personal de los ex directivos de la institución cultural mientras que CDC (integrante de CiU) se financió de forma irregular -6,6 millones- a través de pagos ficticios de Ferrovial al Palau.

En las tablas siguientes se muestran los resultados obtenidos por la federación en cada una de las elecciones al Parlamento de Cataluña y a las elecciones generales españolas, así como el candidato para presidente en cada caso.






</doc>
<doc id="37887" url="https://es.wikipedia.org/wiki?curid=37887" title="Nacionalismo catalán">
Nacionalismo catalán

El nacionalismo catalán () o catalanismo político es una corriente de pensamiento político que está articulado sobre la tesis de que Cataluña es una nación (no necesariamente independiente), con base en los derechos históricos de Cataluña, en su historia, en su lengua, y en el derecho civil catalán, considerando que las instituciones del Principado de Cataluña fueron sustituidas por nuevas instituciones de inspiración castellana, con el Decreto de Nueva Planta de Cataluña, promulgado por Felipe V de España el 16 de enero de 1716, implicando la derogación de las constituciones catalanas y la extinción de dicho principado como un estado dentro del «Estado compuesto» de la monarquía hispánica (de conformidad con el , que establece la división provincial de España de Javier de Burgos, el único principado sobreviviente es el Principado de Asturias). Entre algunos sectores el concepto de nación catalana es extendido a todos los territorios de habla catalana, los llamados Países Catalanes.

Esta corriente política se consolidó ideológicamente en la primera década del siglo XX, como una variante del catalanismo, surgido como movimiento cultural en la década de los años 1830, y articulado como movimiento político en las últimas décadas del siglo XIX, siguiendo los parámetros del nacionalismo. Según John H. Elliot el término «catalanismo», «hasta entonces reducido al movimiento cultural, comenzó a adquirir un serio significado político en el curso del llamado sexenio revolucionario, de 1868 a 1874». Más concretamente el adjetivo «catalanista» empezó a ser usado entre 1870 y 1871 para referirse a sí mismos por los miembros de la Jove Catalunya y de la revista La Renaixensa. Como movimiento político nace a finales de la década de 1880.

Es una corriente de pensamiento transversal que aglutina tanto a partidos políticos y ciudadanos de izquierda como de centro y de derecha. Tras la muerte de Franco, encontró una prolongada vehiculización hegemónica en el pujolismo, movimiento político en torno a la ideología nacionalista e identitaria de Jordi Pujol, presidente de la Generalidad de Cataluña.

Pueden distinguirse básicamente dos corrientes en el nacionalismo catalán, en función de la relación que los nacionalistas catalanes creen que debe tener Cataluña con el resto de España, y en función del fin último de sus objetivos políticos.

Corriente liderada principalmente por "Convergència Democràtica de Catalunya", ERC y CUP que defienden que Cataluña sea reconocida como nación, y la Generalidad de Cataluña obtenga mayores cuotas de autogobierno y sea reconocido el que sólo los catalanes puedan decidir si Cataluña debe permanecer integrada en España, entendida como un Estado "plurinacional" y confederal, o adoptar la independencia unilateral. La opción hacia la independencia ha crecido notablemente en los últimos años.

Defendido principalmente por el PSC e ICV, así como por miembros de Unió Democrática de Catalunya (antes integrada en CiU) que propugnan un federalismo asimétrico, en el que se ponga fin al "café para todos". El PSC está a favor de celebrar un referéndum con opción a la independencia, siempre y cuando se permita legalmente y deje abierta la opción federal, la cual pediría votar.

El nacionalismo e independentismo catalán plantea que la cultura catalana es diferente a la española, y defiende la tesis de que Cataluña es una nación oprimida por España desde su ocupación por las tropas borbónicas en 1714, y la posterior supresión de las instituciones catalanas y la prohibición de su lengua en la administración mediante los Decretos de Nueva Planta promulgados por Felipe V. Desde un punto de vista cultural, el nacionalismo catalán promueve el uso de la lengua catalana en todos los ámbitos de la vida social de Cataluña, a un nivel superior a la lengua castellana, entendiendo que el catalán es la lengua propia de Cataluña. Además, defiende el derecho a utilizar la lengua catalana tanto en las instituciones españolas como europeas, con base en su cantidad de hablantes y a su tradición literaria e histórica.

Los nacionalistas e independentistas catalanes denuncian que Cataluña está sometida a un agravio económico por parte del Estado, debido al déficit de la balanza fiscal para Cataluña, entendiendo que Cataluña recibe mucho menos de lo que aporta en concepto de impuestos. Por esas razones, se argumenta que Cataluña viene reclamando históricamente un mayor nivel de autogobierno con respecto a España, tanto desde el punto de vista legislativo como ejecutivo, judicial, cultural y económico.

Desde un punto de vista simbólico, el nacionalismo catalán defiende la idea de que Cataluña, aunque forme parte de España, pueda tener selecciones deportivas propias, diferenciadas de las selecciones españolas, que puedan participar de forma oficial en los acontecimientos de mayor nivel internacional, siguiendo el ejemplo de otros territorios sin estado propio como Escocia, País de Gales o Macao, que sí tienen selecciones deportivas propias reconocidas por algunos organismos deportivos internacionales.

Debe diferenciarse el nacionalismo catalán del catalanismo, que si bien ensalza los símbolos y tradiciones catalanas, defiende la preservación de la cultura y la lengua catalanas, y defiende la obtención de mayores cuotas de autonomía, no articula sus planteamientos políticos bajo los parámetros del nacionalismo. Pese a que, según diversas encuestas, la mayoría de catalanistas consideran que Cataluña es una nación, no hacen de ello el motor de su acción política y defienden la plena integración de Cataluña en España, descartando la opción del independentismo catalán. A este ámbito se adscriben partidos como el Partido de los socialistas de Cataluña ("Partit dels Socialistes de Catalunya") o Iniciativa por Cataluña-Verdes ("Iniciativa per Catalunya-Verds"), que no se reconocen como "nacionalistas" sino como "catalanistas", y aunque defienden pública y formalmente la idea de que Cataluña es una nación, defienden su pertenencia a España, ya sea bajo el marco actual del estado de las autonomías, o con la fórmula de un estado federal.

Una característica destacada del nacionalismo catalán es un discurso crítico hacia el centralismo de Madrid y el nacionalismo español. Marginalmente también se ha criticado el papel histórico de Francia. Con frecuencia en el discurso de líderes nacionalistas catalanes, tanto de la derecha como de la izquierda, han aparecido declaraciones adscribibles al antiandalucismo, esto ha sido fuente de polémicas durante diversas campañas electorales. Tras la muerte de Franco, una parte del nacionalismo catalán —al igual que otros nacionalismos periféricos de España— ha revestido con frecuencia a su "ethos" del argumento de «autoridad moral» de ser víctimas del fascismo. De acuerdo a Enric Ucelay-Da Cal, el mensaje del catalanismo, negador de España, abusa con frecuencia de circunloquios como «Estado español», para referirse a España.

Tras el fracaso del Sexenio, un sector del republicanismo federal encabezado por Valentí Almirall, dio un "giro catalanista" y rompió con el grueso del Partido Federal, que dirigía Pi y Margall. En 1879 Almirall fundó el "Diari Català", que aunque tuvo una breve vida —cerró en 1881— fue el primer diario escrito íntegramente en catalán. Al año siguiente convocaba el Primer Congreso Catalanista del que surgiría en 1882 el Centre Català, la primera entidad catalanista claramente reivindicativa, aunque no se planteó como partido político sino como una organización de difusión del catalanismo y de presión sobre el gobierno. En 1885 se presentó al rey Alfonso XII un Memorial de agravios, en el que se denunciaban los tratados comerciales que se iban a firmar y las propuestas unificadoras del Código Civil; en 1886 se organizó una campaña contra el convenio comercial que se iba a firmar con Gran Bretaña —y que culminó en el mitin del teatro Novedades de Barcelona que reunió a más de cuatro mil asistentes—; y en 1888 otra en defensa del derecho civil catalán, campaña que alcanzó su objetivo —«la primera victoria del catalanismo», la llamó un cronista—.

En 1886, Almirall publicó su obra fundamental "Lo catalanisme", en el que defendía el «particularismo» catalán y la necesidad de reconocer «las personalidades de las diferentes regiones en que la historia, la geografía y el carácter de los habitantes han dividido la península». Este libro constituyó la primera formulación coherente y amplia del «regionalismo» catalán y tuvo un notable impacto —décadas después Almirall fue considerado como el fundador del catalanismo político—. Según Almirall, «el Estado lo integraban dos comunidades básicas: la catalana (positivista, analítica, igualitaria y democrática) y la castellana (idealista, abstracta, generalizadora y dominadora), por lo que «la única posibilidad de democratizar y modernizar España era ceder la división política del centro anquilosado a la periferia más desarrollada para vertebrar "una confederación o estado compuesto", o una estructura dual similar a la del Austria-Hungría».

Durante esos mismos años ochenta fue cuando comenzó la difusión de los símbolos del catalanismo, la mayoría de los cuales no tuvieron que ser inventados, sino que ya existían previamente a su "nacionalización": la bandera —"les quatre barres de sang", 1880—, el himno —"Els Segadors", 1882—, el día de la patria "l'11 de setembre", 1886—, la danza "nacional" —la sardana, 1892—, los dos patronos de Cataluña —Sant Jordi, 1885, y la Virgen de Montserrat, 1881—.

En 1887 el Centre Català vivió una crisis producto de la ruptura entre las dos corrientes que lo integraban, una más izquierdista y federalista encabezada por Almirall, y otra más catalanista y conservadora aglutinada en torno al diario La Renaixença, fundado en 1881. Los integrantes de esta segunda corriente abandonaron el Centre Catalá en noviembre para fundar la Lliga de Catalunya, a la que se unió el Centre Escolar Catalanista, una asociación de estudiantes universitarios de la que formaban parte los futuros dirigentes del nacionalismo catalán: Enric Prat de la Riba, Francesc Cambó y Josep Puig i Cadafalch. A partir de ese momento la hegemonía catalanista pasó del Centre Català a la Lliga que en el transcurso de los "Jocs Florals" de 1888 presentó un segundo memorial de agravios a la reina regente en la que en otras cosas le pedían «que vuelva a poseer la nación catalana sus Cortes generales libres e independientes», el servicio militar voluntario, «la lengua catalana oficial en Cataluña», enseñanza en catalán, tribunal supremo catalán y que el rey jurara «en Cataluña sus constituciones fundamentales». 

En 1891 la Lliga de Catalunya propuso la formación de la Unió Catalanista que enseguida obtuvo el apoyo de entidades y periódicos catalanistas, y también de particulares —a diferencia de lo que había ocurrido cuatro antes con el fracasado Gran Consell Regional Català propuesto por Bernat Torroja, presidente de la Associació Catalanista de Reus, y que pretendía reunir a los presidentes de las entidades catalanistas y los directores de los periódicos afines—. La Unió celebró en marzo de 1892 su primera asamblea en Manresa, a la que asistieron 250 delegados en representación de unas 160 localidades, donde se aprobaron las "Bases per a la Constitució Regional Catalana", más conocidas como las Bases de Manresa, que se suelen considerar como el «acta de nacimiento del catalanismo político», al menos el de raíz conservadora.

«Las Bases son un proyecto autonomista, en absoluto independentista, de talante tradicional y corporativista. Estructuradas en diecisiete artículos propugnan la posibilidad de modernizar el Derecho civil, la oficialidad exclusiva del catalán, la reserva para los naturales de los cargos públicos incluidos los militares, la comarca como entidad administrativa básica, la soberanía interior exclusiva, unas cortes de elección corporativa, un tribunal superior en última instancia, la ampliación de los poderes municipales, el servicio militar voluntario, un cuerpo de orden público y moneda propios y una enseñanza sensible a la especificidad catalana».

Fue desde principios del siglo XX cuando el nacionalismo catalán empezó a tener importancia política con la victoria electoral en 1901 de la "Lliga Regionalista", un partido nacionalista conservador. Dicho partido estuvo presidido por Bartolomé Robert desde su fundación hasta su fallecimiento en 1902. En 1906 el ejército asaltó la redacción de un periódico de tendencia catalanista, lo que desató las iras de todos los nacionalistas. Eso se tradujo en la formación política "Solidaridad Catalana", fruto de la coalición de las dos partes del movimiento. En las elecciones de 1907 obtuvo 41 de los 44 escaños del congreso catalán. La Semana Trágica de Barcelona ocasionó la disolución de Solidaridad.

El gobierno conservador de Eduardo Dato aprobó, en 1913, la creación de la Mancomunidad de Cataluña, una especie de gobierno autónomo que englobaba las 4 diputaciones provinciales y que estaba dirigido por líderes de la Lliga. A partir de 1918 fue el partido más importante de Cataluña, aunque nunca consiguió la mayoría de los escaños catalanes en las cortes generales españolas. Su naturaleza conservadora hizo que participase en los últimos gobiernos de la Restauración, y que en 1923 no se opusiera a la dictadura de Primo de Rivera, que sin embargo disolvió la Mancomunidad. Por su parte, la mayoría del proletariado apoyaba el anarquismo, representado por la CNT.

Es poco antes de la dictadura de Primo de Rivera cuando, liderado por Francesc Macià aparece el primer partido independentista catalán, Estat Català. Macià, que fundaría después los "escamots", una organización paramilitar, y que en 1926 instituiría para estos rituales militares y "la Creu des Ardits", una medalla condecorativa, perpetró ese mismo año el complot de Prats de Molló, un intento de invasión militar desde Francia.

Pasada la dictadura, Estat Català se unió a partidos y organizaciones de izquierdas para constituir Esquerra Republicana de Catalunya, que se convirtió en hegemónico en Cataluña durante la Segunda República. Es este periodo, y tras la proclamación unilateral de una efímera República Catalana, el nacionalismo catalán consigue un Estatuto de autonomía de Cataluña (1932) (que restauró la Generalidad de Cataluña).

En 1934 el presidente de la Generalidad de Catalunya, Luis Companys, en el marco insurreccional de octubre, proclamó el Estado Catalán dentro de la República Federal Española; el fracaso de la revuelta mitificaría dentro del nacionalismo catalán como mártires a figuras como las de Jaume Compte y Companys y como indigno el papel de Josep Dencàs.

La victoria franquista en la guerra civil inició una época de represión en contra de cualquier nacionalismo considerado traidor a España.

A pesar de la falta de libertad empezaron a organizarse huelgas obreras, como las de 1951, 1956, 1971 o 1974, a partir de las cuales la acción fue mayor.

Poco después, el 20 de noviembre de 1975, murió Francisco Franco, y con su muerte se inició la Transición a la democracia.

En 1977, a principios de la Transición Española, se reinstituyó la Generalidad de Cataluña con Josep Tarradellas a la cabeza. Tras la redacción de la Constitución Española de 1978, que reconocía a España como un estado con diversas nacionalidades y regiones, y el establecimiento de un gobierno, el 11 de agosto de 1980 Cataluña se convirtió en una autonomía dentro de España. Ese mismo año, las elecciones en Cataluña dieron el poder a Convergència i Unió, liderada por Jordi Pujol, que se mantuvo en el poder hasta el año 2003.

CiU siguió en el poder hasta su derrota electoral el 16 de noviembre de 2003. La nueva Generalitat pasó a estar formada por la terna política de PSC – ERC – ICV-EUiA con Pasqual Maragall como presidente de la Generalitat.

A nivel de diputados CiU consigue el mayor número, seguido del PSC, ERC, PP e ICV. El pacto de gobierno, primero de izquierdas desde hacia más de 20 años fue promovido por ERC que gracias a un aumento considerable en sus votos se erige como una de las principales fuerzas políticas de Cataluña, recuperando poco a poco la posición que tenía años atrás.

El día 30 de septiembre de 2005, después de más de dos años de negociaciones, el parlamento catalán aprueba un proyecto de nuevo estatuto de autonomía con una amplia mayoría parlamentaria, logrando así uno de los principales objetivos marcados en el programa del gobierno. Aprobado por el Congreso de los Diputados y el Senado, entró en vigor tras ser refrendado por los catalanes mediante consulta popular. El Partido Popular recurrió ante el Tribunal Constitucional español 187 artículos de dicho Estatuto. Tras cuatro años de espera, el Tribunal emitió sentencia el 28 de junio de 2010 declarando 14 artículos inconstitucionales, desatando variable rechazo en todos los partidos catalanes con representación parlamentaria a excepción de Ciutadans y el Partido Popular, que se mostró satisfecho con la resolución. A consecuencia de la sentencia, se convocó una manifestación el 10 de julio de 2010 dónde quedó patente un auge del nacionalismo catalán, aun así en las últimas elecciones autonómicas y municipales ERC perdió una considerable cifra de votos que quedaron repartidos entre otras alternativas al nacionalismo como pueden ser CiU, CUP o SI.

El Parlamento de Cataluña aprobó el 27 de septiembre de 2012 una resolución pidiendo celebrar el referéndum de autodeterminación de Cataluña durante la décima legislatura de la Cataluña autonómica «prioritariamente», con posterioridad a las elecciones al Parlamento de Cataluña de 2012:

La resolución fue votada después del debate de política general con el resultado de 84 votos a favor (CiU, ICV-EUiA, ERC, SI, más otros dos diputados), 21 en contra (PPC y C's) y 25 abstenciones (PSC). El Presidente de la Generalidad de Cataluña, Artur Mas, declaró en el discurso ante el Parlamento que había llegado la hora de que el pueblo de Cataluña ejerciera el derecho de autodeterminación.

Después de la negativa del gobierno español a negociar sobre el «pacto fiscal», CiU decidió que no tenía sentido seguir con la legislatura, ya que esa era su propuesta política más importante. Esta circunstancia, unida a la gran participación en la manifestación de la Diada de 2012, empujó a CiU a convocar elecciones anticipadas y presentarse esta vez con una consulta de autodeterminación en el programa electoral. Las elecciones se celebraron el 25 de noviembre de 2012. CiU volvió a ser la fuerza política más votada, si bien bajó en número de escaños. El conjunto de fuerzas políticas partidarias de la realización de la consulta aumentó ligeramente su representación en el Parlamento de Cataluña, pasando de los 86 diputados de la IX legislatura (CiU, ICV-EUiA, ERC y SI) a los 87 de la X legislatura (CiU, ERC, ICV-EUiA y CUP).
Tras las elecciones, CiU negoció con ERC su apoyo a la investidura de Artur Mas como presidente de la Generalidad de Cataluña. El 19 de diciembre de 2012 firmaron un pacto de gobernabilidad que incluyó entre sus puntos la denominada «Consulta sobre el futuro político de Cataluña». El acuerdo estipuló que la fecha de la consulta debería ser pactada entre ambas formaciones, las cuales se comprometieron a llevarlo a cabo en 2014 «salvo que el contexto socioeconómico y político requieran una prórroga». El acuerdo permitió que Artur Mas fuera investido Presidente de la Generalidad de Cataluña por segunda vez.
El 23 de enero de 2013 el Parlamento de Cataluña aprobó con 85 votos a favor, 41 en contra, 2 abstenciones y 5 diputados que se negaron a votar la "Declaración de Soberanía y del derecho a decidir del Pueblo de Cataluña", manifestando que:

Los principios recogidos en el texto son los de legitimidad democrática, transparencia, diálogo, cohesión social, europeismo, legalidad, papel principal del Parlamento y participación, todos ellos precedidos y legitimados por el de soberanía que se reafirma diciendo que «el pueblo de Cataluña tiene, por razones de legitimidad democrática, carácter de sujeto político y jurídico soberano».

CiU (50 diputados), ERC (21) y ICV-EUiA (13) apoyaron la declaración de soberanía. El PPC (19) y C's (9) se opusieron a la propuesta. De los diputados del PSC, 15 votaron en contra y 5 no votaron pese a estar en el hemiciclo, desobedeciendo así las órdenes de la dirección del partido de votar en contra de la propuesta. La CUP dio un «sí crítico» con 1 voto a favor y 2 abstenciones.

El 8 de mayo de 2013 el Tribunal Constitucional suspendió esta declaración cautelarmente al admitir a trámite la impugnación presentada por la Abogacía del Estado, que la consideró «un acto de poder constituyente» y «un desafío abierto contra la Constitución». El 25 de marzo de 2014 el Tribunal Constitucional dictaminó finalmente que esta declaración de soberanía era "inconstitucional y nula", y por tanto no amparaba la celebración de un referéndum de autodeterminación en Cataluña. No obstante el gobierno de la Generalidad minimizó el impacto de la sentencia.

El nacionalismo catalán responde a una tradición mayoritariamente antimilitarista; no obstante existió hasta la guerra civil una cierta continuidad en un activismo miliciano y paramilitar —minoritario frente al "pactismo"— dentro del catalanismo, vinculado a un tipo de nacionalismo radical propio principalmente de varones jóvenes, nutrido también por el desarrollo de los deportes de masas y las asociaciones de excursionismo y por la influencia del CADCI.
En la década de 1920 Acció Catalana articuló su propia respuesta armada clandestina, mediante la creación de la "Societat d'Estudis Militars" o "Servei d'Entrenament Militar" (S.E.M.), sección paramilitar liderada por Luis Nicolau d'Olwer, desarticulada por la policía en 1926. Estat Català también contó con unas juventudes radicalizadas: los "escamots".

Terra Lliure («Tierra Libre» en español) es el nombre en catalán de una organización terrorista armada independentista catalana de extrema izquierda fundada en 1978 y autodisuelta en 1991. Cometió más de 200 atentados, cuyo balance asciende a cinco víctimas mortales (cuatro de ellas miembros de la organización) y varias decenas de heridos. Durante su existencia, las Fuerzas de Seguridad del Estado llegaron a detener a 300 personas vinculadas a la organización. Terra Lliure se autodisolvió en 1991, abandonando la lucha armada. Algunos de sus dirigentes y militantes ingresaron posteriormente en Esquerra Republicana de Catalunya, que les exigió la renuncia explícita a la violencia como condición sine qua non para dicho ingreso. Los presos de la organización fueron saliendo de la cárcel tras ser indultados o cumplir condena. En 1996 ya no quedaba ningún miembro de Terra Lliure en prisión.

Se dio a conocer oficialmente en un partido de fútbol en el Camp Nou en Barcelona el 23 de junio de 1981, en el marco de la campaña "Som una Nació" (Somos una Nación). La primera asamblea de la organización se celebró en el sur de Francia. La mayoría de sus miembros provenían de Exèrcit Popular Català (EPOCA), o de organizaciones como el Front d'Alliberament Català (FAC) y el Partit Socialista d'Alliberament Nacional (PSAN).

En su primer documento público, cuyo título es ""Crida de Terra Lliure"" (Llamada de Tierra Libre), se define a sí misma como "organización revolucionaria que lucha por la independencia total de los Países Catalanes" y hace un llamamiento a la "lucha contra el proceso de destrucción sistemática a que está sometida nuestra Nación", destrucción que concreta en varios puntos, el primero de los cuales se refiere a la "destrucción política que supone la separación de los Países Catalanes en tres regiones autónomas con lenguas y símbolos diferentes, instituciones diferentes...". El documento, fechado en los "Països Catalans" el 24 de junio de 1981, termina con un llamamiento: "Visca la Terra! Independència o mort! Visca la lluita armada! Una sola nació, Països Catalans!" (¡Viva la Tierra! ¡Independencia o muerte! ¡Viva la lucha armada! Una sola nación, ¡Países Catalanes!).

Sus primeras fases fueron de organización y de consolidación. El 26 de enero de 1979 falleció al tratar de huir de la Policía Nacional un miembro de la organización, Martí Marcó, de 19 años de edad, y poco después muere también Fèlix Goñi "Bruc" al estallarle la bomba que estaba manipulando.

La bandera independentista catalana es principalmente la estelada, a pesar de que también se utiliza la bandera de Cataluña





</doc>
<doc id="37888" url="https://es.wikipedia.org/wiki?curid=37888" title="Catalán">
Catalán

Catalán puede hacer referencia a:

Asimismo, puede referirse a los siguientes topónimos:

También, en astronomía, puede hacer referencia a:

Asimismo, en historia, puede referirse a:

También, como apellido, puede hacer referencia a las siguientes personas:


Además, como Catalan (sin tilde), puede referirse a:


</doc>
<doc id="37896" url="https://es.wikipedia.org/wiki?curid=37896" title="Setenta y nueve">
Setenta y nueve

El setenta y nueve (79) es el número natural que sigue al setenta y ocho y precede al ochenta.




</doc>
<doc id="37898" url="https://es.wikipedia.org/wiki?curid=37898" title="García Sánchez I de Pamplona">
García Sánchez I de Pamplona

García Sánchez I (c. 919-22 de febrero de 970) fue el primer rey de Nájera y por herencia paterna rey de Pamplona que gobernó el reino desde 925 hasta su muerte. Era hijo del rey de Pamplona Sancho Garcés I y de la reina Toda Aznárez, nieta del rey Fortún Garcés.

En el año 923 su padre el rey pamplonés Sancho Garcés I, en colaboración con Ordoño II de León, recupera Nájera y La Rioja Media y Alta, estas nuevas tierras se las deja bajo dominio a García Sánchez I con la denominación de Reino de Nájera, convirtiéndose así en el primer monarca del dicho reino.

A la muerte de Sancho Garcés I, el 10 de diciembre de 925, le heredó su hijo García Sánchez, de seis años de edad, bajo la tutela de su tío Jimeno Garcés, cuya muerte en 931 provocó una crisis por el control de la tutoría. Gracias a la intervención de su madre la reina Toda y a la mediación de Abderramán III, sobrino carnal de esta, la situación quedó controlada: la reina visitó a su sobrino el califa en su campamento de Calahorra en 934 y le pidió que reconociera a su hijo García como nuevo rey de Pamplona. A cambio, Abderramán obtuvo de ella el compromiso de romper relaciones con la monarquía asturleonesa. No obstante, esta sumisión al califato de Córdoba no duró mucho. 

Después de ser García Sánchez mayor de edad en 933, asumió la tutela de Andregoto Galíndez, su prometida, y por consiguiente el gobierno del condado de Aragón, hasta que el matrimonio fue anulado antes de 943, año en que aparece por primera vez con su segunda esposa, Teresa Ramírez.

La intervención de la reina madre en los asuntos de gobierno influyó para que el reino de Pamplona alcanzase una posición clave entre los reinos cristianos en los años venideros. Tres hermanas de García Sánchez estuvieron casadas con reyes leoneses: Urraca con Ramiro II, Sancha con Ordoño II y Onneca con Alfonso IV. Por esta causa, los navarros intervinieron en las guerras civiles del reino de León. Al morir Ramiro II, los navarros mediaron, ayudando la reina Toda a su nieto Sancho a ocupar el trono. Después, cuando Sancho fue expulsado por su otro nieto Ordoño el Malo, la reina madre intercedió para que Abderramán ayudase a su nieto Sancho a recuperar el trono. 

En 939 participó en la coalición formada por Ramiro II de León, Fernán González y tropas asturianas y gallegas, que obtuvieron una gran victoria sobre las de Abderramán III en la batalla de Simancas. 

En 953 dona, junto a su madre Toda, al monasterio de San Martín de Albelda (La Rioja), la villa Bagibel, situada en los montes de Cameros, primera vez que aparece escrita esta denominación geográfica. 

En 961 tomó parte en las disputas entre el condado de Castilla y el reino de León y apresó a Fernán González, conde de Castilla, pero se negó a entregarlo a los musulmanes. En 963 formó una alianza cristiana contra Alhakén II y fue derrotado por los musulmanes. 

Murió el 22 de febrero de 970. Fue sepultado en el pórtico de la pequeña iglesia de San Esteban, en el castillo de Monjardín.

Contrajo un primer matrimonio con Andregoto Galíndez, hija del conde de Aragón Galindo Aznárez II y Sancha Garcés. Galindo Aznárez no había tenido hijos varones legítimos, por lo cual el condado de Aragón lo heredó su hija Andregoto y después su hijo Sancho. Andregoto fue repudiada por su marido antes de 943 y se retiró a sus tierras de Aybar donde falleció después de 971. Esta unión matrimonial daría lugar a la posterior unión de Aragón y Pamplona. De este primer matrimonio nacieron:


García Sánchez se casó por segunda vez antes de 943 con Teresa, posiblemente hija del rey Ramiro II de León y de Adosinda Gutiérrez, naciendo de este matrimonio:


García Sánchez I tuvo por ancestros a:




</doc>
<doc id="37906" url="https://es.wikipedia.org/wiki?curid=37906" title="Estearato de litio">
Estearato de litio

El estearato de litio es una sal del ácido esteárico con el catión litio que tiene la función de espesar aceites, con el fin de elaborar grasas de alta temperatura, es usado como lubricante, hidrofugante, antiapelmazante y desmoldeante.
Aspecto:
Polvo blanco

Contenido metálico: 
(% m/m) 2,0-2,3 

Punto de Fusión: 
(°C) 205-220 

Ácido libre: 
(% m/m) <1 

Humedad: 
(% m/m) <3 

Densidad aparente: 
(g/cc) 0,25-0,30

Granulometría 100 Mesh: 
(% m/m) <8


</doc>
<doc id="37919" url="https://es.wikipedia.org/wiki?curid=37919" title="Glaciar">
Glaciar

Un glaciar es una gruesa masa de hielo y nieve originada en la superficie terrestre por acumulación, compactación y recristalización de la nieve, mostrando evidencias de flujo en el pasado o en la actualidad. Su existencia es posible cuando la precipitación anual de nieve supera la fusionada en verano, por lo cual la mayoría se encuentra en zonas cercanas a los polos, aunque existen en otras zonas, en montañas. El proceso del crecimiento y establecimiento del glaciar se llama glaciación. Los glaciares del mundo son variados y se pueden clasificar según su forma ya sea de valle, de nicho, campo de hielo o por régimen climático como el tropical, temperado o polar, o condiciones térmicas de base fría, base caliente o politermal.

Un 10 % de la Tierra está cubierta de glaciares, y en tiempos geológicos recientes ese porcentaje llegó al 30 %. Los glaciares del mundo acumulan más del 75 % del agua dulce del mundo. En la actualidad 91 % del volumen y 84 % del área total de glaciares está en la Antártida, 8 % del volumen y 14 % del área en Groenlandia sumando el resto de los glaciares 4 % del área y menos del 1 % del volumen.

Los casquetes polares, glaciares continentales o inlandsis, son los glaciares más importantes que existen actualmente sobre la Tierra. Ocupan en total 15 millones de km, lo que significa el 90% las áreas cubiertas por el hielo. El casquete de la Antártida es el más extenso.

Los glaciares son producto del clima y están permanentemente intercambiando masa con otras partes del sistema hidrológico. Los glaciares crecen con la adición de nieve y otros tipos de hielo y pierden masa por fusión de hielo en agua, evaporación (sublimación) y el desmembramiento de témpanos de hielo. La diferencia entre ganancias y pérdidas de masa de un glaciar se llama balance glaciar. Cuando el balance de masa da negativo el glaciar pierde masa y cuando es positivo gana masa creciendo. A la adición de masa de un glaciar se le llama "acumulación" y a la pérdida "ablación".

Las principales formas de acumulación son la precipitación directa de nieve, la escarcha, el congelamiento de agua líquida, nieve transportada por vientos, nieve y hielo traídos por avalanchas, cencelladas y el congelamiento de agua en las capas basales. En los glaciares se suele trazar una línea imaginaria llamada "línea de equilibrio" la cual divide al glaciar en cuestión en dos zonas, una de acumulación y una de ablación en términos netos.

En los lugares de un glaciar donde la acumulación de nieve es mayor a la ablación se va acumulando nieve de año a año y las capas más profundas de la nieve se van transformando en hielo glaciar. La transformación en hielo glaciar se debe a dos procesos, uno de compactación y otro de metamorfismo. La velocidad de la transformación depende de la humedad y la temperatura. Los cristales de nieve que precipitan sobre un glaciar tienen formas que van desde hexágonos y agujas a otras más complicadas, pero estas formas son inestables al acumularse ya sea en un glaciar o en otra parte y se evaporan en áreas de alta exposición y reciben condensación en lugares más protegidos, lo que termina por darles un aspecto más redondo. Antes de convertirse en hielo glaciar la nieve se torna en neviza, que esencialmente es nieve que ha sobrevivido un año por lo menos.

En los glaciares, donde la fusión se da en la zona de acumulación de nieve, la nieve puede convertirse en hielo a través de la fusión y el recongelamiento (en períodos de varios años). En la Antártida, donde la fusión es muy lenta o no existe (incluso en verano), la compactación que convierte la nieve en hielo puede tardar miles de años. La enorme presión sobre los cristales de hielo hace que éstos tengan una deformación plástica, cuyo comportamiento hace que los glaciares se muevan lentamente bajo la fuerza de la gravedad como si se tratase de un enorme flujo de tierra. A este casquete de hielo en la Antártida se lo clasifica como polar o frío por estar totalmente refrigerado, por estar ubicado en el polo y por no pasar por una estación estival de fusión ya que el verano es casi inexistente.
El tamaño de los glaciares depende del clima de la región en que se encuentren. El balance entre la diferencia de lo que se acumula en la parte superior con respecto a lo que se derrite en la parte inferior recibe el nombre de balance glaciar. En los glaciares de montaña, el hielo se va compactando en los circos, que vendrían a ser la zona de acumulación equivalente a lo que sería la cuenca de recepción de los torrentes. En el caso de los glaciares continentales, la acumulación sucede también en la parte superior del glaciar pero es un resultado más de la formación de escarcha, es decir, del paso directo del vapor de agua del aire al estado sólido por las bajas temperaturas de los glaciares, que por las precipitaciones de nieve. El hielo acumulado se comprime y ejerce una presión considerable sobre el hielo más profundo. A su vez, el peso del glaciar ejerce una presión centrífuga que provoca el empuje del hielo hacia el borde exterior del mismo donde se derrite; a esta parte se la conoce como zona de ablación. Cuando llegan al mar, forman los icebergs al fragmentarse sobre el agua oceánica, como puede verse en una imagen de satélite de Google maps correspondiente a la Bahía de Melville, al noroeste de Groenlandia. En los glaciares de valle, la línea que separa estas dos zonas (la de acumulación y la de ablación) se llama línea de nieve o línea de equilibrio. La elevación de esta línea varía de acuerdo con las temperaturas y la cantidad de nieve caída y es mucho mayor en las vertientes o laderas de solana que en las de umbría. También es mucho mayor en las de sotavento que en las de barlovento.

Los glaciares de Groenlandia y de la Antártida resultan mucho más difíciles de medir, ya que los avances y retrocesos del frente pueden estar compensados por una mayor o menor acumulación de hielo en la parte superior, presentándose una especie de ciclos de avance y retroceso que se retroalimentan mutuamente dando origen a una compensación dinámica en las dimensiones del glaciar. En otras palabras: un descenso de la altura del glaciar de la Antártida, por ejemplo, podría generar un mayor empuje hacia afuera, y al mismo tiempo, un mayor margen para que se acumule de nuevo una cantidad de hielo similar a la que existía previamente: recordemos que esta altura (unos 3 km) está determinada por el balance glaciar, que tiene una especie de techo determinado sobre el cual no se puede acumular más hielo por la escasa cantidad de vapor de agua que tiene el aire a gran altura (por lo general, a más de 3000 m).

Existen varias formas de clasificar a los glaciares. Respecto a los glaciares de roca existe una disputa en si deben ser considerados glaciares o no.

El hielo de los glaciares suele ser distinguido en "hielo temperado" que esta a la temperatura de fusión y "hielo frío" que está por debajo de esta temperatura. Esta clasificación se ha extrapolado a glaciares enteros con las siguientes categorías como resultado: 

En realidad, esta clasificación de los glaciares no resulta muy funcional porque se trata de un fenómeno más complejo de lo que parece. Por ejemplo, los glaciares polares no son los únicos en los que la temperatura de la masa glaciar está por debajo del punto de fusión. También en los glaciares de montaña, incluso de la zona intertropical, se encuentran en esa situación, aunque su masa, espesor y presión que soportan sea mucho menor que en los glaciares polares. Todo ello hay que tenerlo muy en cuenta, porque en este último caso han dado origen a aludes o avalanchas de enormes proporciones y de trágicas consecuencias, como sucedió en los aluviones de Ranrahirca y Yungay en Perú y de Armero en Colombia. En el caso del Perú, las catástrofes de Ranrahírca (10 de enero de 1962) y de Yungay (31 de mayo de 1970) tuvieron motivos sísmicos, ya que los terremotos fueron los causantes del desprendimiento violento de grandes masas del glaciar del Huascarán. Y en el caso de Armero, el motivo fue una erupción volcánica del Nevado del Ruiz el 13 de noviembre de 1985. Durante cierto tiempo antes de la Tragedia de Armero, el volcán estuvo en erupción, lo que motivo el progresivo calentamiento de la base del glaciar, que conservó su estado sólido (por el motivo ya señalado de que el hielo es mal conductor del calor) pero que, al derretirse dicha base actuó como un lubricante y se produjo el catastrófico aluvión o desprendimiento de hielo, agua, tierra, rocas y árboles (lahar) que arrasaron a la ciudad de Armero causando más de 20 000 muertos.

Una forma es clasificar a los glaciares por su morfología aunque es preciso tener en cuenta que existe un continuo entre las diversas morfologías y que cada glaciar es único. Basándose en clasificaciones morfológicas anteriores los glaciólogos Douglas Benn y David Evans han clasificado a los glaciares en: 

Dado que los glaciares están compuestos por agua forman parte del ciclo hidrológico. Los glaciares actúan como reservas de agua que retienen parte de las precipitaciones. Los glaciares del mundo albergan 68,7 % del agua dulce de la Tierra. El agua líquida de los glaciares puede provenir de dos fuentes: de la fusión de nieve o hielo o directamente de lluvia. El sistema hidrológico interno de un glaciar es complejo variando de lugares de percolación a sistemas de túneles, grietas y cuevas.

Existen varias formas en las que agua líquida puede ser almacenada dentro de un glaciar como en nieve y firn, en crevasses, en lagunas supraglaciales, cavidades englaciales y subglaciales aparte del sistema de drenaje subglacial y englacial así como también en los sedimentos subglaciales. 
Los glaciares afectan la hidrología de las hoyas hidrográficas aún en cuencas donde la superficie glaciarizada es reducida. La descarga de agua de un glaciar suele ser estacional siendo más alta en verano.

En el caso de glaciares temperados estos están en la primavera tardía cubiertos por nieve a la temperatura de fusión. En los glaciares temperados el agua de fusión percola a través del firn hasta llegar a un nivel donde el firn se encuentra saturado de agua líquida. Esta agua se encuentra impedida de seguir percolando por el hielo que hay debajo del firn en los glaciares el cual es prácticamente impermeable. Esto termina por forma un acuífero abierto en el firn. El grosor del acuífero va a depender de la eficiencia del drenaje englacial y también de la gradiente hidráulica. En los estos del firn aproximadamente 40 % del espacio de los poros puede ser ocupado por agua siendo el 60 % restante ocupado por aire atrapado.

El hielo se comporta como un sólido quebradizo hasta que su acumulación alcanza los 50 metros de espesor. Una vez sobrepasado este límite, el hielo se comporta como un material plástico y empieza a fluir. El hielo glaciar consiste en capas de moléculas empaquetadas unas sobre otras. Las uniones entre las capas son más débiles que las existentes dentro de cada capa, por lo que cuando el esfuerzo sobrepasa las fuerzas de los enlaces que mantienen a las capas unidas, éstas se desplazan unas sobre otras.

Otro tipo de movimiento es el deslizamiento basal. Este se produce cuando el glaciar entero se desplaza sobre el terreno en el que se encuentra. En este proceso, el agua de fusión contribuye al desplazamiento del hielo mediante la lubricación. El agua líquida se origina como consecuencia de que el punto de fusión disminuye a medida que aumenta la presión. Otras fuentes para el origen del agua de fusión pueden ser la fricción del hielo contra la roca, lo que aumenta la temperatura y por último, el calor proveniente de la Tierra.

El desplazamiento de un glaciar no es uniforme ya que está condicionado por la fricción y la fuerza de gravedad. Debido a la fricción, el hielo glaciar inferior se mueve más lento que las partes superiores. A diferencia de las zonas inferiores, el hielo ubicado en los 50 metros superiores, no están sujetos a la fricción y por lo tanto son más rígidos. A esta sección se la conoce como zona de fractura. El hielo de la zona de fractura viaja encima del hielo inferior y cuando este pasa a través de terrenos irregulares, la zona de fractura crea grietas que pueden tener hasta 50 metros de profundidad, donde el flujo plástico las sella. La rimaya es un tipo especial de grieta que suele formarse en los circos glaciares y tiene una dirección transversal al movimiento por gravedad del glaciar. Podría decirse que es una grieta que se forma en los puntos donde se separa la nieve del fondo del circo del hielo que todavía está bien adherido en la parte superior.

La velocidad de desplazamiento de los glaciares está determinada por la fricción y la pendiente. Como se sabe, la fricción hace que el hielo de fondo se desplace a una velocidad menor que las partes superiores. En el caso de los glaciares alpinos, esto también se aplica para la fricción de las paredes de los valles, por lo que las regiones centrales son las que presentan un mayor desplazamiento. Esto fue confirmado en experimentos realizados en el siglo XIX en los que se utilizaron estacas alineadas en glaciares alpinos y se analizó su evolución. Posteriormente se confirmó que las regiones centrales se habían desplazado mayores distancias, además de tener mayor profundidad. Sucede exactamente lo mismo, aunque a menor velocidad, que el agua de los ríos moviéndose en sus cauces. Es el caso del Glaciar de Malaespina, donde vemos las capas centrales que indican un mayor espesor, crecimiento y velocidad en esa parte central, desarrollando así, una forma típicamente redondeada como sucede en los deltas o conos de deyección fluviales. Sin embargo, esta idea dista de ser algo absoluto, ya que existen glaciares que al llegar al nivel del mar presentan dos tipos de comportamientos: en el primer caso, el glaciar que termina sobre una morrena terminal lo cual le confiere una forma redondeada (glaciar pata de elefante) y dicha zona que descansa sobre la propia morrena se mantiene aislada sobre las aguas marinas, por lo que conserva la masa terminal de hielo. El segundo caso se presenta cuando el frente del glaciar termina en un mar donde las corrientes marinas y las propias mareas mantienen todo el año unas aguas relativamente cálidas (superior a los 0º y hasta los 10º) con lo que el frente del glaciar hace sucumbir rápidamente al avance del hielo, lo que le da una forma particular cóncava hacia adelante porque el agua oceánica, que tiene mayor temperatura y velocidad funde de manera efectiva el frente del glaciar a pesar de ser la parte de mayor espesor y de mayor rapidez de movimiento, como sucede en el glaciar Columbia de Alaska. Como resulta obvio, el agua oceánica durante el pleamar tiene mayor velocidad cuando choca con el glaciar, al que derrite en el frente de choque que se produce donde la marea alcanza su nivel máximo de penetración en el valle glaciar.

Las velocidades medias varían. Algunos presentan velocidades tan lentas que los árboles pueden establecerse entre los derrubios depositados. En otros casos, sin embargo, se desplazan varios metros por día. Tal es el caso del glaciar Byrd, un glaciar de desbordamiento en la Antártida que, de acuerdo a estudios satelitales, se desplazaba de 750 a 800 metros por año (unos dos metros por día).

El avance de muchos glaciares puede estar caracterizado por períodos de avance extremadamente rápidos llamados oleadas. Los glaciares que exhiben oleadas, se comportan de una manera normal hasta que repentinamente aceleran su movimiento para después volver a su estado anterior. Durante las oleadas, la velocidad de desplazamiento es hasta 100 veces mayor que bajo condiciones normales.

En realidad, el avance o retroceso de un glaciar tanto si es de valle como continental depende del balance glaciar. Ello significa que algo que no se suele citar es que en la ecuación de la velocidad y volumen del hielo que se funde hay que tomar en cuenta, no solo la pérdida del hielo que se funde sino la alimentación, que resulta mucho más difícil de medir y abarca períodos mucho más largos, difíciles de cuantificar, no solo en términos meteorológicos sino también en términos climáticos.

Las rocas y los sedimentos son incorporados al glaciar por varios procesos. Los glaciares erosionan el terreno principalmente de dos maneras: la abrasión y arranque.

A medida que el glaciar fluye sobre la superficie fracturada del lecho de roca, ablanda y levanta bloques de roca que incorpora al hielo. Este proceso conocido como "arranque glaciar", se produce cuando el agua de deshielo penetra en las grietas y las diaclasas del lecho de roca y del fondo del glaciar y se hiela recristalizándose. Conforme el agua se expande al congelarse, actúa como una palanca que suelta la roca levantándola. De esta manera, sedimentos de todos los tamaños entran a formar parte de la carga del glaciar.

La "abrasión" ocurre cuando el hielo y la carga de fragmentos rocosos se deslizan sobre el lecho de roca y funcionan como un papel de lija que alisa y pule la superficie situada debajo. La roca pulverizada por la abrasión recibe el nombre de harina de roca. Esta harina está formada por granos de roca de un tamaño del orden de los 0,002 a 0,00625 mm. A veces, la cantidad de harina de roca producida es tan elevada que las corrientes de agua de fusión adquieren un color grisáceo.

Una de las características visibles de la erosión y abrasión glaciar son las estrías glaciares producidas sobre las superficies rocosas del lecho; fragmentos de roca con afilados bordes contenidos en el hielo marcan surcos a modo de arañazos finos. Cartografiando la dirección de las estrías se puede determinar el desplazamiento del flujo glaciar, lo cual es una información de interés en el caso de antiguos glaciares.

La velocidad de erosión de un glaciar es muy variable. Esta erosión diferencial llevada a cabo por el hielo está controlada por cuatro factores importantes:

En ambientes de alta montaña, los glaciares pueden presentar una cobertura detrítica superficial continua, conocida con el nombre de "debris covered glacier". Esta capa produce, tanto en la zona de acumulación, como en la zona de ablación, un proceso progresivo de adelgazamiento de masa que genera una importante acumulación de detritos en ambientes supraglaciales. Este tipo de glaciares recubiertos representan la fase intermedia dentro del continuum de los sistemas glaciales (dependientes del flujo de detritos y del hielo dentro del sistema), desde glaciares descubiertos a glaciares rocosos.

El origen de los detritos supraglaciales se asocia a la existencia de una secuencia: cara libre, talud en laderas con escarpes rocosos, que presentan alta sensibilidad a la meteorización y descargan detritos en forma directa sobre la superficie glacial. La acumulación de detritos supraglaciales influye directamente sobre los procesos de ablación y de flujo de hielo, debido a alteraciones en el albedo y en la conductividad térmica del glaciar. En este sentido, Strem (1959), Naakawo & Yonng (1981, 1982) (en Ferrando, 2003) y Benn & Evans (1998) definen un umbral inferior a 1 cm en la capa de detritos como el espesor que favorece la fusión del hielo y una capa de detritos de 1 cm o más como aislante del hielo subyacente. Los procesos de fusión del hielo pueden favorecer el aumento en la capa detrítica supraglacial, debido a la incorporación de material intraglaciar al manto del "debris covered glacier" o cobertura detrítica glaciar. Esta situación, puede generar fenómenos de ablación diferencial, generando procesos de inversión del relieve, caracterizados por la fusión «in situ» del hielo intersticial de la cobertura detrítica en las zonas recubiertas del glaciar; este proceso es conocido con el nombre de "Karst glacial" o "Criokarst".

El incremento de detritos sobre la superficie glacial, puede provocar en casos extremos, procesos de ablación con tasas que tienden a cero, generando, en consecuencia, una ineficiente evacuación de los detritos y un proceso cada vez mayor de control topográfico en la dinámica del sistema, además de un mayor desarrollo de morrenas medianas y centrales.

Una vez que el material es incorporado al glaciar, puede ser transportado varios kilómetros antes de ser depositado en la zona de ablación. Todos los depósitos dejados por los glaciares reciben el nombre de derrubios glaciares. Los derrubios glaciares se dividen por los geólogos en dos tipos distintos:

Los grandes bloques que se encuentran en el till o libres sobre la superficie se denominan erráticos glaciares si son diferentes al lecho de roca en el que se encuentran (esto es, su litología no es la misma que la roca encajada subyacente). Los bloques erráticos de un glaciar son rocas acarreadas y luego abandonadas por la corriente de hielo. Su estudio litológico permite averiguar la trayectoria del glaciar que los depositó.

Morrena es el nombre más común para los sedimentos descabalados de los glaciares. El término tiene origen francés y fue acuñado por los campesinos para referirse a los rebordes y terraplenes de derrubios encontrados cerca de los márgenes de glaciares en los Alpes franceses. Actualmente, el término es más amplio, porque se aplica a una serie de formas, todas ellas compuestas por till. En muchos glaciares de valle se pueden distinguir los siguientes tipos de morrenas: 

Sin el efecto de las glaciaciones los valles de montaña tienen una característica forma de V, producida por la erosión del agua en la vertical. Sin embargo, durante las glaciaciones esos valles se ensancharon y ahondaron, lo que dio lugar a la creación de un valle glaciar en forma de U. Además de su profundización y ensanchamiento, el glaciar también alisa los valles gracias a la erosión. De esta manera va eliminando los espolones de tierra que se extienden en el valle. Como resultado de esta interacción se crean acantilados triangulares llamados espolones truncados, debido a que muchos glaciares profundizan sus valles más de lo que hacen sus afluentes pequeños.

Por consiguiente, cuando los glaciares acaban retrocediendo, los valles de los glaciares afluentes quedan por encima de la depresión glaciar principal debido a su menor poder erosivo, y se los denomina valles suspendidos. Las partes del suelo que fueron afectadas por el arranque y la abrasión, pueden ser rellenadas por los denominados lagos paternoster, nombre del latín ("Padre nuestro") que hace referencia a una estación de las cuentas del rosario.

En la cabecera de un glaciar hay una estructura muy importante, se llama circo glaciar y tiene una forma de cubeta con paredes escarpadas en tres lados, pero con un lado a veces semiabierto que desciende hacia el valle. En los circos se da la acumulación del hielo. Estos empiezan como irregularidades en el lado de la montaña que luego van aumentando de tamaño por el acuñamiento del hielo. Después de que el glaciar se derrite, estos circos suelen ser ocupados por un pequeño lago de montaña denominado tarn. Los lagos formados en un antiguo glaciar de montaña pueden deberse a dos motivos: cuando son represados por las morrenas laterales y la morrena terminal, la cual termina siendo abierta por la erosión del río que emana del lago glaciar, como sucede en la laguna de Mucubají en Venezuela y los que se deben a la sobreexcavación del glaciar al encontrar atravesada en el valle una roca muy dura (gneiss y granito en los Andes venezolanos, como puede verse en la Laguna Negra). En este caso, lo mismo que sucede con los ríos de lava, el hielo puede acumularse en el fondo del valle y ascender cuando encuentra una roca muy dura, desparramándose valle abajo después de salvar el obstáculo.

A veces cuando hay dos glaciares separados por una divisoria, y ésta, ubicada entre los circos, es erosionada se crea una garganta o paso. A esta estructura se le denomina collado, paso, abra o brecha, como sucede en la Brecha de Rolando en los Pirineos, entre el circo de Gavarnie en Francia y el de Ordesa en España.

Los glaciares también son responsables de la creación de fiordos, ensenadas profundas y escarpadas que se encuentran en las altas latitudes. Con profundidades que pueden superar el kilómetro, son provocados por la elevación postglacial del nivel del mar y, por lo tanto, a medida que este aumentaba, las aguas marinas iban penetrando hacia el interior del valle glaciar. El fiordo escandinavo más largo es el de Sogne, con más de 200 km tierra adentro.

En latitudes más bajas, el aumento postglacial del nivel del mar produjo también un fenómeno similar que se denomina ría: un valle, en este caso fluvial, ocupado por las aguas marinas después del último período glacial del Pleistoceno, por el propio aumento del nivel del mar al haberse derretido los grandes glaciares continentales de Eurasia y América del Norte.

Además de las características que los glaciares crean en un terreno montañoso, también es probable encontrar crestas sinuosas de bordes agudos que reciben el nombre de aristas y picos piramidales y agudos llamados horns.

Ambos rasgos pueden tener el mismo proceso desencadenante: el aumento de tamaño de los circos producidos por arranque y por la acción del hielo. En el caso de los horns, el motivo de su formación son los circos que rodean a una sola montaña.

Las aristas surgen de manera similar; la única diferencia se encuentra que en los circos no están ubicados en círculo, sino más bien en lados opuestos a lo largo de una divisoria. Las aristas también pueden producirse con el encuentro de dos glaciares paralelos. En este caso, las lenguas glaciares van estrechando las divisorias a medida que se erosionan y pulen los valles adyacentes.

Son formadas por el paso del glaciar cuando esculpe pequeñas colinas a partir de protuberancias del lecho de rocas. Una protuberancia de roca de este tipo recibe el nombre de roca aborregada. Las rocas aborregadas son formadas cuando la abrasión glaciar alisa la suave pendiente que está en el frente del hielo glaciar que se aproxima y el arranque aumenta la inclinación del lado opuesto a medida que el hielo pasa por encima de la protuberancia. Estas rocas indican la dirección del flujo del glaciar.

Las morrenas no son las únicas formas depositadas por los glaciares. En determinadas áreas que en alguna ocasión estuvieron cubiertas por glaciares de casquete continentales existe una variedad especial de paisaje glaciar caracterizado por colinas lisas, alargadas y paralelas llamadas colinas asimétricas.

Las colinas asimétricas son de perfil aerodinámico compuestas principalmente por till. Su altura oscila entre 15 a 50 metros y pueden llegar a medir hasta 1 km de longitud. El lado empinado de la colina mira la dirección desde la cual avanzó el hielo, mientras que la pendiente más larga sigue la dirección de desplazamiento del hielo.

Las colinas asimétricas no aparecen en forma aislada, por el contrario, se encuentran agrupados en lo que se denomina "campos de colinas". Uno de ellos se encuentra en Rochester, Nueva York, y se calcula que contiene unas 10 000 colinas.

Aunque no se sabe con certeza cómo se forman, si se observa el aspecto aerodinámico, se puede inferir que fueron moldeadas en la zona de flujo plástico de un glaciar antiguo. Se cree que muchas colinas se originan cuando los glaciares avanzan sobre derrubios glaciares previamente depositados, remodelando el material.

El agua que surge de la zona de ablación se aleja del glaciar en una capa plana que transporta fino sedimento; a medida que disminuye la velocidad, los sedimentos contenidos empiezan a depositarse y entonces el agua de fusión empieza a desarrollar canales anastomosados. Cuando esta estructura se forma en asociación de un glaciar de casquete, recibe el nombre de llanura aluvial y cuando está fundamentalmente confinada en un valle de montaña, se la suele denominar tren de valle.

Las llanuras de aluvión y los trenes de valle suelen estar acompañados de pequeñas depresiones conocidas como "kettles" o marmitas de gigante, como se les denominan en español (término adoptado del francés), aunque es una forma menor del relieve que se forma en las corrientes fluviales, por lo que no debería considerarse en sentido estricto como un término relacionado con los glaciares, aunque son muy frecuentes en terrenos fluvioglaciares. Sin embargo, hay que tener en cuenta que un molino glaciar puede producir marmitas de gigante en el fondo de los glaciares y quedar al descubierto tras el retroceso de los mismos. Las depresiones de glaciar se producen también en depósitos de till. Las depresiones mayores se producen cuando enormes bloques de hielo quedan estancados en el derrubio glaciar y después de derretirse dejan huecos en el sedimento, dando origen, casi siempre, a un sistema formado por numerosos lagos interconectados entre sí con formas alargadas y paralelas entre sí, con una dirección más o menos coincidente con la dirección del avance del hielo durante los períodos glaciales que se sucedieron en el Pleistoceno. Es una morfología glaciar muy frecuente en Finlandia (que suele denominarse «el país de los 10 000 lagos»), en Canadá y en algunos de los estados de Estados Unidos como Alaska, Wisconsin y Minnesota. La amplitud de estas depresiones, por lo general, no supera los 2 km, salvo en Minnesota y otras partes, aunque en algunos casos llegan a alcanzar los 50 km de diámetro. Las profundidades oscilan entre los 10 y los 50 metros.

Cuando un glaciar disminuye su tamaño hasta un punto crítico, el flujo se detiene y el hielo se estanca. Mientras tanto, las aguas de fusión que corren por encima, en el interior y por debajo del hielo dejan depósitos de derrubios estratificados. Por ello, a medida que el hielo va derritiéndose, va dejando depósitos estratificados en forma de colinas, terrazas y cúmulos. A este tipo de depósitos se los conoce como depósitos en contacto con el hielo.

Cuando estos depósitos tienen la forma de colinas de laderas empinadas o montículos se los llama kames. Algunos kames se forman cuando el agua de fusión deposita sedimentos a través de aberturas en el interior del hielo. En otros casos, solo son el resultado de abanicos o deltas hacia el exterior del hielo, producidos por el agua de fusión.

Cuando el hielo glaciar ocupa un valle pueden formarse terrazas de kame a lo largo de los lados del valle.

Un tercer tipo de depósito en contacto con el hielo está caracterizado por sinuosas crestas largas y estrechas compuestas fundamentalmente de arena y grava. Algunas de estas crestas tienen alturas que superan los 100 metros y sus longitudes sobrepasan los 100 km. Se trata de los eskers, crestas depositadas por los ríos de aguas de fusión que fluyen por debajo de una masa de hielo glaciar que avanza lentamente. Estos ríos sirven de aliviadero al agua de fusión que forma el glaciar en contacto con el suelo y ocupan una especie de cuevas muy alargadas bajo el glaciar. El origen de estas colinas alargadas se encuentra en la distinta capacidad de arrastre de sedimentos entre el hielo (que es mucho mayor) y el agua: en el cauce de estos ríos subterráneos se van acumulando materiales arrastrados por el glaciar que el agua no puede seguir transportando. De aquí que los eskers sean colinas alargadas por donde pasaron los ríos internos de un glaciar. Son muy frecuentes en Finlandia y suelen presentar una dirección en el mismo sentido de desplazamiento del glaciar.

En 1821, un ingeniero suizo, Ignaz Venetz, presentó un artículo en el que sugería la presencia de rasgos de paisaje glaciar a distancias considerables de los Alpes. Esta idea fue negada por otro científico suizo, Louis Agassiz, pero cuando se encaminó a demostrar su invalidez, en realidad terminó acreditando las presunciones de este colega y otros que le siguieron, como De Saussure, Esmark y Charpentier. En efecto, un año más tarde de su excursión con Charpentier (1836), Agassiz planteó la hipótesis de una gran época glacial que habría tenido efectos generales y de largo alcance. Su contribución a la llamada Teoría Glacial consolidó su prestigio como naturalista.

Con el tiempo, y gracias al refinamiento del conocimiento geológico, se comprobó que hubo varios períodos de avance y retroceso de los glaciares y que las temperaturas reinantes en la Tierra eran muy diferentes de las actuales.

Se ha establecido una división cuádruple de la glaciación cuaternaria para Norteamérica y Europa. Estas divisiones se basaron principalmente en el estudio de los depósitos glaciares. En América del Norte, cada una de estas cuatro etapas fue denominada por el estado en el que se encontraban depósitos de esa etapa eran patentes. En orden de aparición esos períodos glaciales («glaciaciones») de la glaciación cuaternaria son los siguientes: Günz (Nebrasquiense en Norteamérica), Mindel (Kansaniense en Norteamérica), Riss (Illinoisiense en Norteamérica), y Würm (Wisconsinense en Norteamérica). Esta clasificación fue refinada gracias al estudio detallado de los sedimentos del fondo oceánico. Gracias a que los sedimentos del fondo oceánico, a diferencia de los continentales, no están afectados por discontinuidades estratigráficas, sino que resultan de un proceso continuo, son especialmente útiles para determinar los ciclos climáticos del planeta.

De esta manera, las divisiones identificadas han pasado a ser unas veinte y la duración de cada una de éstas es de aproximadamente 100 000 años. Todos estos ciclos están ubicados en lo que se conoce como la glaciación cuaternaria.

Durante su auge, el hielo dejó su marca en casi el 30 % de la superficie continental cubriendo por completo unos 10 millones de kilómetros cuadrados de América del Norte, 5 millones de km² de Europa y 4 millones de km² de Siberia. La cantidad de hielo glaciar en el hemisferio norte fue el doble que en el sur. Esto se justifica porque en el hemisferio sur, el hielo no encontró para cubrirlo más territorio que el continente antártico.

En la actualidad se considera que la glaciación empezó entre hace 2 y 3 millones de años, definiendo lo que se conoce como Pleistoceno.

Los glaciares del Pleistoceno, así como su influencia sobre la aparición y expansión territorial de los seres humanos se explica en el libro de Gwen Schultz "Glaciers and the ice age. Earth and its inhabitants during the Pleistocene" 

Los efectos de la glaciación cuaternaria todavía se evidencian. Se sabe que especies de animales y plantas se vieron obligadas a emigrar mientras que otras no pudieron adaptarse. No obstante, la evidencia más importante es el actual levantamiento que experimentan Escandinavia y Norteamérica. Por ejemplo, se sabe que la bahía de Hudson en los últimos miles de años se elevó unos 300 metros. El motivo de este ascenso de la corteza se debe a un equilibrio isostático. Esta teoría sostiene que cuando una masa, como un glaciar, pandea la corteza terrestre, esta última se hunde por la presión, pero una vez que el glaciar se derrite, la corteza empieza a elevarse hasta su posición original, es decir, a su nivel de equilibrio, al liberarse del peso del propio glaciar. A esta especie de rebote también se le denomina movimiento eustático.

A pesar del conocimiento adquirido durante los últimos años, poco se sabe acerca de las causas de las glaciaciones.

Las glaciaciones generalizadas han sido raras en la historia de la Tierra. Sin embargo, la Edad de Hielo en el pleistoceno no fue el único evento de glaciación ya que se han identificado depósitos denominados tilitas, una roca sedimentaria formada cuando se litifica el till glacial.

Estos depósitos encontrados en estratos de edades diferentes presentan características similares como fragmentos de roca estriada, algunas superpuestas a superficies de lecho de roca pulida y acanalada o asociadas con areniscas y conglomerados que muestran rasgos de depósitos de llanura aluvial.

Se han identificado dos episodios glaciares Precámbricos, el primero hace aproximadamente 2000 millones de años y el segundo hace unos 600 millones de años. Además, en rocas del Paleozoico Superior, de una antigüedad de unos 250 millones de años se encontró un registro bien documentado de una época glacial anterior.

Aunque existen diferentes ideas científicas acerca de los factores determinantes de las glaciaciones las hipótesis más importantes son dos: la tectónica de placas y las variaciones de la órbita terrestre.

Debido a que los glaciares se pueden formar solo sobre tierra firme, la idea de la tectónica de placas sugiere que la evidencia de glaciaciones anteriores se encuentra presente en latitudes tropicales debido a que las placas tectónicas a la deriva han transportado a los continentes desde latitudes tropicales hasta regiones cercanas a los polos. La evidencia de estructuras glaciares en Sudamérica, África, Australia y la India avalan esta idea, debido a que se sabe que experimentaron un período glacial cerca del final del Paleozoico, hace unos 250 millones de años.

La idea de que las evidencias de glaciaciones encontradas en las latitudes medias está estrechamente relacionada al desplazamiento de las placas tectónicas y fue confirmada con la ausencia de rasgos glaciares en el mismo período para las latitudes más altas de Norteamérica y Eurasia, lo que indica, como es obvio, que sus ubicaciones eran muy diferentes de las actuales. En otro orden de ideas, el que actualmente se exploten minas de carbón en el archipiélago de Svalbard también sirve para corroborar la idea del desplazamiento de las placas tectónicas, ya que no existe actualmente en dicho archipiélago una vegetación suficiente como para explicar estos yacimientos de carbón mineral.

Los cambios climáticos también están relacionados con las posiciones de los continentes, por lo que han variado en conjunto con el desplazamiento de placas que, además, afectó los patrones de corrientes oceánicas lo que a su vez llevó a cambios en la transmisión del calor y la humedad. Debido a que los continentes se desplazan muy despacio (cerca de 2 centímetros al año), semejantes cambios probablemente ocurren en períodos de millones de años.

Debido a que el desplazamiento de las placas tectónicas es muy lento, esta explicación no puede utilizarse para explicar la alternancia entre climas glacial e interglacial que se produjo durante el Pleistoceno. Por tal motivo, los científicos creen que tales oscilaciones climáticas del Pleistoceno deben estar ligadas a variaciones de la órbita terrestre. Esta hipótesis fue formulada por el yugoslavo Milutin Milankovitch y se basa en la premisa de que las variaciones de la radiación solar entrante constituyen un factor fundamental en el control del clima terrestre.

El modelo está basado en tres elementos:

A pesar de que las condiciones de Milankovitch no parecen justificar grandes cambios en la radiación incidente, el cambio se hace sentir porque cambia el grado de contraste de las estaciones.

Un estudio de sedimentos marinos que contenían ciertos microorganismos climáticamente sensibles hasta hace cerca de medio millón de años atrás fueron comparados con estudios de la geometría de la órbita terrestre, el resultado fue contundente: los cambios climáticos están estrechamente relacionados con los períodos de oblicuidad, precesión y excentricidad de la órbita de la Tierra.

En general, con los datos recogidos se puede afirmar que la tectónica de placas es solo aplicable para períodos muy largos, mientras que la propuesta de Milankovitch, apoyada por otros trabajos, se ajusta a las alternancias periódicas de los episodios glaciales e interglaciales del Pleistoceno. Debe tenerse en cuenta que estas proposiciones, están sujetas a críticas. Todavía no se sabe con certeza si hay otros factores involucrados.





</doc>
<doc id="37920" url="https://es.wikipedia.org/wiki?curid=37920" title="Thriller">
Thriller

El término thriller puede hacer referencia, en esta enciclopedia:


</doc>
<doc id="37923" url="https://es.wikipedia.org/wiki?curid=37923" title="Valencia (química)">
Valencia (química)

La valencia es el número de electrones que le faltan o debe ceder un elemento químico para completar su último nivel de energía. Estos electrones son los que pone en juego durante una reacción química o para establecer un enlace químico con otro elemento. Hay elementos con más de una valencia, por ello fue reemplazado este concepto con el de números de oxidación que finalmente representa lo mismo. A través del siglo XX, el concepto de valencia ha evolucionado en una amplia gama de aproximaciones para describir el enlace químico, incluyendo la estructura de Lewis (1916), la teoría del enlace de valencia (1927), la teoría de los orbitales moleculares (1928), la teoría de repulsión de pares electrónicos de la capa de valencia (1958) y todos los métodos avanzados de química cuántica.

La etimología de la palabra «valencia» proviene de 1543, significando "molde", del latín "valentía" "poder, capacidad", y el significado químico refiriéndose al «poder combinante de un elemento» está registrado desde 1884, del alemán "Valenz".

En 1890, William Higgins publicó bocetos sobre lo que él llamó combinaciones de partículas "últimas", que esbozaban el concepto de enlaces de valencia. Si, por ejemplo, de acuerdo a Higgins, la fuerza entre la partícula última de oxígeno y la partícula última de nitrógeno era 6, luego la fuerza del enlace debería ser dividida acordemente, y de modo similar para las otras combinaciones de partículas últimas: estas son las de la tabla periódica. 

Sin embargo, el origen no exacto de la teoría de las valencias químicas puede ser rastreado a una publicación de Edward Frankland, en la que combinó las viejas teorías de los radicales libres y «teoría de tipos» con conceptos sobre afinidad química para mostrar que ciertos elementos tienen la tendencia a combinarse con otros elementos para formar compuestos conteniendo tres equivalentes del átomo unido, por ejemplo, en los grupos de tres átomos (vg. "NO, NH, NI," etc.) o cinco, por ejemplo en los grupos de cinco átomos (vg. "NO, NHO, PO," etc.) Es en este modo, según Franklin, que sus afinidades están mejor satisfechas. Siguiendo estos ejemplos y postulados, Franklin declaró cuán obvio esto es que: 



El concepto fue desarrollado a mediados del siglo XIX, en un intento por racionalizar la fórmula química de compuestos químicos diferentes. En 1919, Irving Langmuir, tomó prestado el término para explicar el modelo del átomo cúbico de Gilbert N. Lewis al enunciar que "el número de pares de electrones que cualquier átomo dado comparte con el átomo adyacente es denominado la "covalencia" del átomo." El prefijo "co-" significa «junto», así que un enlace covalente significa que los átomos comparten valencia. De ahí, si un átomo, por ejemplo, tiene una valencia +1, significa que perdió un electrón, y otro con una valencia de -1, significa que tiene un electrón adicional. Luego, un enlace entre estos dos átomos resultaría porque se complementarían o compartirían sus tendencias en el balance de la valencia. Subsecuentemente, actualmente es más común hablar de enlace covalente en vez de valencia, que ha caído en desuso del nivel más alto de trabajo, con los avances en la teoría del enlace químico, pero aún es usado ampliamente en estudios elementales donde provee una introducción heurística a la materia.

Se creía originalmente que el número de enlaces formados por un elemento dado era una propiedad química fija y, en efecto, en muchos casos, es una buena aproximación. Por ejemplo, en muchos de sus compuestos, el carbono forma cuatro enlaces, el oxígeno dos y el hidrógeno uno. Sin embargo, pronto se hizo evidente que, para muchos elementos, la valencia podría variar entre compuestos diferentes. Uno de los primeros ejemplos en ser identificado era el fósforo, que algunas veces se comporta como si tuviera una valencia de tres, y otras como si tuviera una valencia de cinco. Un método para resolver este problema consiste en especificar la valencia para cada compuesto individual: aunque elimina mucho de la generalidad del concepto, esto ha dado origen a la idea de número de oxidación (usado en la nomenclatura Stock y a la notación lambda en la nomenclatura IUPAC de química inorgánica).

La Unión Internacional de Química Pura y Aplicada (IUPAC) ha hecho algunos intentos de llegar a una definición desambigua de valencia. La versión actual, adoptada en 1994, es la siguiente:

Esta definición reimpone una valencia única para cada elemento a expensas de despreciar, en muchos casos, una gran parte de su química.La mención del hidrógeno y el cloro es por razones históricas, aunque ambos en la práctica forman compuestos principalmente en los que sus átomos forman un enlace simple. Las excepciones en el caso del hidrógeno incluyen el ion bifluoruro, [HF], y los diversos hidruros de boro tales como el diborano: estos son ejemplos de enlace de tres centros. El cloro forma un número de fluoruro—ClF, ClF y ClF—y su valencia, de acuerdo a la definición de la IUPAC, es cinco. El flúor es el elemento para el que el mayor número de átomos se combinan con átomos de otros elementos: es univalente en todos sus compuestos, excepto en el ion [HF]. En efecto, la definición IUPAC sólo puede ser resuelta al fijar las valencias del hidrógeno y el flúor como uno, convención que ha sido seguida acá.

Las valencias de la mayoría de los elementos se basan en el fluoruro más alto conocido.




</doc>
<doc id="37925" url="https://es.wikipedia.org/wiki?curid=37925" title="Ortografía del español">
Ortografía del español

La ortografía del español utiliza una variante del alfabeto latino, que consta de 27 letras:"a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "ñ", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y" y "z". Asimismo, se emplean también cinco dígrafos para representar otros tantos fonemas: «ch», «ll», «rr», «gu» y «qu», considerados estos dos últimos como variantes posicionales para la representación de los fonemas /g/ y /k/.

Los dígrafos "ch" y "ll" tienen valores fonéticos específicos, por lo que en la "Ortografía de la lengua española" de 1754 comenzó a considerárseles como letras del alfabeto español y a partir de la publicación de la cuarta edición del "Diccionario de la lengua española" en 1803 se ordenaron separadamente de "c" y "l." En el X Congreso de la Asociación de Academias de la Lengua Española celebrado en Madrid en 1994, y por recomendación de varios organismos, se acordó reordenar los dígrafos "ch" y "ll" en el lugar que el alfabeto latino universal les asigna, aunque todavía seguían formando parte del abecedario. Con la publicación de la "Ortografía de la lengua española" en 2010, ambas dejaron de considerarse letras del abecedario. Las vocales "(a, e, i, o, u)" aceptan, además, el acento agudo o tilde (´, como en "á, é, í, ó, ú") para indicar la sílaba acentuada; y la vocal "u" acepta la diéresis o crema (¨), que la modifica en las sílabas "güe", "güi" para indicar su sonoridad.

Desarrollada en varias etapas a partir del período alfonsino, la ortografía se estandarizó definitivamente bajo la guía de la Real Academia Española, y ha sufrido escasas modificaciones desde la publicación de la "Ortografía de la lengua española" de 1854. Las sucesivas decisiones han aplicado criterios a veces fonológicos y a veces etimológicos, dando lugar a un sistema híbrido y fuertemente convencional. Si bien la correspondencia entre grafía y lenguaje hablado es predecible a partir de la escritura (es decir, un hablante competente es capaz de determinar inequívocamente la pronunciación estimada correcta para casi cualquier texto), no sucede así a la inversa, existiendo numerosas letras que representan gráficamente fonemas idénticos (el número de fonemas del español típicamente oscila entre 22 y 24, según el dialecto ). Los proyectos de reforma de la grafía en búsqueda de una correspondencia biunívoca (los primeros datan del siglo XVII) han sido invariablemente rechazados. La divergencia de la fonología de la lengua entre sus diversos dialectos hace hoy imposible la elaboración de una grafía puramente fonética que refleje adecuadamente la variedad de la lengua; la mayoría de las propuestas actuales se limitan a la simplificación de los símbolos homófonos, que se conservan por razones etimológicas.

El alfabeto español consta de 27 letras:

En 1754, con la publicación de la "Ortografía de la lengua española" de ese mismo año, los dígrafos "ch" y "ll" comenzaron a ser considerados como letras del alfabeto español, y entre 1803 y 1994 recibían encabezados separados en los diccionarios y a la hora de ordenar alfabéticamente las palabras. Nunca, sin embargo, se los consideró unidades estrictas; cuando la ortografía exige inicial mayúscula, en las palabras que comienzan con uno de estos dígrafos, se escribe en mayúscula solo el primero de los grafemas que lo componen. A partir del año 2010, con la publicación de la nueva "Ortografía de la lengua española", los dígrafos "ch" y "ll" dejan de ser considerados letras del abecedario español, pero seguirán utilizándose como hasta ahora en la escritura de las palabras españolas.
El dígrafo "rr", llamado "erre doble" o "doble erre", nunca se consideró por separado como una letra del alfabeto español, probablemente por no aparecer nunca en posición inicial de las palabras.

La "w" y la "k" aparecen solo raramente en palabras españolas e indican invariablemente términos adoptados por préstamo o cultismo en el curso de los últimos dos siglos.

Varios de los grafemas reciben más de un nombre. La "b" se conoce como "be" a secas, "be alta" (en Cataluña), "be grande" (en México, Colombia, Venezuela y Perú) o "be larga" (en Argentina, Chile, Colombia, Paraguay, República Dominicana y Uruguay); por haber representado en latín el fonema consonante bilabial sonoro (que tiene como alófonos la consonante oclusiva bilabial sonora y la fricativa bilabial sonora ), se la llama a veces "b labial" en Colombia, aunque la pronunciación de la "v" es en la inmensa mayoría de los dialectos también labial e idéntica. A su vez, esta última se conoce como "uve" (en España y Puerto Rico), "ve", "ve baja", "ve chica" (en Perú), "ve pequeña" (en Colombia y Venezuela) o "ve corta" (en Argentina, Chile, Colombia, República Dominicana y Uruguay); por el mismo afán de precisión histórica en algunos manuales se designa como "v dental", aunque la pronunciación dental desapareció hace siglos del sistema de la lengua.

La letra "w" es llamada "uve doble" en España, "doble u" en México, Colombia, Costa Rica y República Dominicana; "doble ve" en otros países de Centroamérica y algunos países de Sudamérica, como Argentina, Chile, Ecuador o Venezuela, y "ve doble" en otros como Perú.

"Ll" y "rr" se designan indistintamente como "elle" y "doble erre" o como "doble ele" y "erre doble". Son de las pocas consonantes que se presentan duplicadas en la grafía actual, junto con la "c" y excepcionalmente la "n", y las únicas con pronunciaciones distintivas.

La "i" se llama a veces "i latina" para distinguirla de la "y", "y griega" (o "i griega"). En algunos lugares, se prefiere el nombre de "ye" para esta última.

Antiguamente se empleaba a veces "zeda" como nombre para "z", una práctica hoy en desuso y desechada por la RAE.

La Ortografía de 2010 propone unificar los nombres de las letras: elige "be" para "b", "i" (no "i latina"), "uve" para "v", "uve doble" para "w", "ye" para "y" y "zeta" para "z", y desecha definitivamente los nombres arcaicos "ere" para la "r", y "ceta", "ceda" y "zeda" para la "z".

La afirmación de que la ortografía del castellano es principalmente fonográfica (o fonética) es extendida pero errónea. Una ortografía fonográfica tiende a respetar el principio fonémico según el cual el conjunto de fonemas de una lengua y el conjunto de letras con las que esta se escribe deben corresponderse biunívocamente, es decir, para cada letra debe haber un solo fonema y para cada fonema debe haber una sola letra.
Si bien, efectivamente, en comparación con otras lenguas europeas, casi se respeta la regularidad del principio fonético, existe una serie de desviaciones de la misma que rompen notablemente con dicha regularidad. Destacan entre estas los fenómenos de la poligrafía (distintas representaciones gráficas para un mismo fonema) y la polifonía (distintos fonemas representados por una misma letra).


A la existencia de letras que no tienen correspondencia con fonema alguno ("h", "u" mudas), también se pueden añadir otras divergencias antifonográficas entre las que cabe mencionar la heterografía (escritura diferente) de morfemas uniformes ("nazco", "naces"), la composición de fonogramas ("ch" para o bien "ll" para ).

Las polifonías tienen su origen en consideraciones etimológicas que pertenecen a la historia de la lengua, dando así lugar a sistemáticas dificultades para determinar el uso correcto de b/v, h/g en posición inicial, c/s/z, g/j y ll/y y haciendo que numerosas articulaciones, alófonas o no, no se distingan en la grafía. La "h" que en la actualidad no representa ningún sonido, pero que reviste sonoridad al anteponerle una "c"- (ch), también debe entenderse como una anomalía de origen etimológico que dificulta la escritura del español según las normas ortográficas vigentes.

Los cambios ortográficos son aportados principalmente por las clases alfabetizadas, aunque frecuentemente sean modificaciones de los hablantes. Las instituciones y organismos competentes sancionan los respectivos cambios ortográficos.

Las reformas son cambios realizados sobre un sistema de normas ortográficas preexistentes. Frecuentemente son pequeñas intervenciones: incorporación de nuevos signos, eliminación de grafías obsoletas, adición de nuevas reglas para el uso de los diferentes signos ortográficos…; lo que favorece su aceptación. El fin de tales cambios es mejorar la coherencia interna del sistema para cumplir eficazmente su fin.

Las ventajas lingüísticas de una reforma profunda son:


Las ventajas didácticas y sociales son:


Las dificultades de una reforma profunda son:


Las desventajas de una reforma profunda son:


Durante los primeros siglos de desarrollo del español, la rareza de la lengua escrita y la aún imprecisa catadura de la misma hicieron innecesaria una codificación de su grafía. El primer intento de dotar de un código gráfico sistemático data del reinado de Alfonso X, que intentó ajustar las diversas soluciones adoptadas por sus predecesores a un criterio fundamentalmente fonográfico.

Alfonso X reunió en su corte un gran número de estudiosos, que se abocaron a elaborar una compilación enciclopédica del saber de la época, continuando y ampliando la obra de la escuela de traductores de Toledo. El romance se utilizó como lengua intermedia en las traducciones del árabe o el griego al latín. La profusión de copias realizadas en el "scriptorium" real y el impacto de las traducciones sobre el "corpus" de la lengua romance difundió y dio fuerza a las convenciones fijadas por el rey.

Muchas de las que aparecen retrospectivamente como irregularidades o imprecisiones en la grafía alfonsí se deben, en realidad, a la notable diferencia que el sistema fonológico de la época tenía respecto del actual. El sistema de consonantes coronales, por ejemplo, incluía cuatro fricativas y dos africadas, frente a las tres fricativas/africadas como máximo que tienen los dialectos contemporáneos: /d/ se escribía "z" como en "dezir", /t/ se escribía "ç" como en "março", /s/ se escribía "s" o "ss" entre dos vocales como en "saber, missa", /z/ se escribía "s" como en "osar", /ʃ/ se escribía "x" como en "dixe" y /ʒ/ se escribía "j" o "i" como en "aguiiar/aguijar". La ortografía real intentó reflejar con notable fidelidad las propiedades fonológicas del habla de la época. Invención suya fue la duplicación de N y más tarde L para indicar la palatalización en /ɲ/ y /ʎ/, la primera de las cuales los copistas transformaban en la abreviatura que daría con el tiempo la eñe.

Otras características de la grafía alfonsí son la variación en la grafía de las vocales átonas, probablemente reflejando un valor fonético aún irregular (por ejemplo, en el manuscrito del "Mío Cid" aparecen tanto «veluntad» como «voluntad»), la inconsistencia en la supresión de la "e" final ("noche" ~ "noch") y la total ausencia de acentos o tildes, sea con valor diacrítico o fonético.

La aparición de la imprenta y el consecuente incremento del ritmo y volumen de aparición de las obras escritas acabó por deshacer el sistema alfonsino, fijado únicamente a través de la convención y no codificado en una obra sistemática. Los constantes añadidos léxicos, algunos producidos por la influencia de las lenguas vecinas y otros muchos por el aluvión de cultismos pergeñados por traductores, literatos y juristas, que cada vez con más frecuencia empleaban la lengua vernácula en sus escritos, suscitaron cuestiones de grafía que respondían muchas veces a criterios etimológicos e históricos antes que a la correspondencia estrecha entre fonema y grafema propuesta por la obra alfonsina.

Por otra parte, las modificaciones en la fonología de la lengua habían afectado esta correspondencia, y buena parte de las decisiones alfonsinas resultaban ya arbitrarias para los lectores de la época. Sumado a ello el purismo y el gusto tradicionalista de los autores del "Siglo de Oro", tuvo lugar una importante y extendida controversia, que duró siglos, acerca de cuáles deberían ser los principios rectores para establecer los criterios gráficos.

Elio Antonio de Nebrija, autor de la primera "Gramática" de la lengua castellana, fue también el primero en publicar unas "Reglas de orthographia". Estas codificaron por primera vez los principios de la línea que basa en la pronunciación su criterio ordenador, aunque no le faltaron en ocasiones razonamientos etimológicos en casos difíciles. En todo caso, la idea de Nebrija de que la lengua era "instrumento del Imperio" se extendía también a lo oral y buscaba unificar la pronunciación en todo el territorio de la Corona de Castilla de acuerdo con la prestigiosa forma vallisoletana, abandonando definitivamente el romance burgalés que había dado lugar a los primeros escritos prealfonsinos.

En 1531 Alejo Venegas dio a la imprenta su "Tractado de orthographia y accentos", que contiene significativas diferencias con el de Nebrija; sostiene, por ejemplo, la oposición fonológica entre "b" y "v" y la existencia de la vocal cerrada anterior redondeada , la vieja "ypsilon" griega. En 1609 se imprimió en México una "Ortographia castellana", obra del sevillano Mateo Alemán, aún más radical que los anteriores con respecto a la necesidad de prescindir de los signos convencionales y fijar la ortografía con base en la fonética; eliminaba por ejemplo la "ph", que aún Nebrija había mantenido, y proponía grafías diferentes para y . Similarmente atrevido era el "Arte de la lengua española castellana" de Bartolomé Jiménez Patón, aparecido en 1614.

El punto culminante del movimiento fonetista estuvo dado por la aparición en 1627 del "Arte de la lengua española kastellana" de Gonzalo Correas, que tuvo una versión ampliada y corregida en 1630, bajo el título de "Ortografía kastellana nueva i perfeta". Como la grafía elegida para su título por Correas evidencia, el movimiento por la transcripción exacta de la fonología se deshacía en Correas de cualquier prurito histórico; propuso distinguir por completo y , como había hecho Alemán, prescindir de las confusas "c" y "q", utilizar "gh" para el fonema , eliminar los elementos mudos en todos los grupos consonánticos y llevó a cabo sin residuos su propósito de desarrollar exactamente la simetría entre fonemas y grafemas. El rigor de su doctrina le granjeó el aprecio de algunos de sus sucesores, como Gregorio Mayans, y de los reformadores americanos, aunque hizo de su obra una curiosidad para eruditos, pues rompía de manera radical con los usos.

Tras la Guerra de Sucesión, el acceso de Felipe de Anjou al trono con el nombre de Felipe V dio lugar a un marcado afrancesamiento de las instituciones culturales. Entre ellas se contó la Real Academia Española, fundada en 1713 con la idea de fijar, de acuerdo con el ideal sistemático de la época, la pureza de la lengua.

La concepción de la RAE se hizo evidente en su "Diccionario de autoridades", publicado a lo largo de la década de 1720, en que el "buen decir" se recaba de la obra de un canon bien seleccionado de autores y eruditos. Bajo la influencia de Adrián Conink, el "Diccionario de autoridades" rompió de cuajo con la tendencia fonetista y recuperó los principios que la Académie française había empleado para fijar la lengua francesa: la etimología y la pronunciación histórica. De ese modo, restauró la diferencia entre "b" y "v" a pesar de que fonológicamente había desaparecido, impuso grafías latinizantes para los vocablos de origen griego ―"th" para las "θ" etimológicas, "rh" para las ρ, "ps" para las "ψ", "ph" para las "φ"―, recuperó las "h" mudas y fijó la grafía de los grupos consonánticos en atención a su origen.

Para la primera edición de la "Orthographia española" (1741), los criterios resultaban ya menos claros. En esa ocasión, la Academia optó por conservar el grupo PH, pero simplificó los restantes helenismos a sus formas fonéticas; además, eliminó las iniciales procedentes del latín, o las suplió con una "e" epentética, sin observar mayor regularidad.

Las decisiones de la Academia provocaron el rechazo generalizado de los estudiosos, que la consideraron incoherente y anacrónica. Así, Mayans escribía en 1745:
La edición de 1754 avanzó en ese sentido, eliminando las P de origen helénico en algunos grupos consonánticos, suprimiendo la PH e introduciendo reglas de acentuación. La influencia de Correas y otros autores se hizo sentir en esa etapa, aunque los elementos etimologizantes, como la reduplicación de S, se conservaban, así como la extraordinaria, al sentir de sus contemporáneos, afirmación de que la sílaba española varía en cantidad al igual que la latina, o la doctrina de que la H representaba un sonido "aspirado" (presumiblemente ) y que la pronunciación que no lo incluyese debía considerarse defectuosa. Pese al apoyo real, decisiones en este sentido hicieron que no faltasen quienes desconocieran la pretensión de la RAE de servir de árbitro último acerca de cuestiones lingüísticas; Mayans y Antonio Bordazar publicaron sendas ortografías, y reeditaron las "Reglas…" de Nebrija, considerando simplemente que las prescripciones académicas eran equivalentes a la doctrina de cualquier otro erudito, y no se privaron de criticarlas pública y privadamente.

La tendencia a simplificar continuó, quizás por mor de esta oposición; en 1763 se eliminó la "s" duplicada y se prescribió el uso de los acentos, incluyendo el circunflejo en las sílabas que la Academia sostenía largas. En 1803 incluyó en el alfabeto la "ch" y la "ll" con valor propio y eliminó el uso etimológico de la primera, a la vez que permitió la elisión de las consonantes líquidas en algunos grupos triples heredados del latín; la "k" se excluyó del alfabeto en esta ocasión. En 1815, se ordenó definitivamente el uso de la "q", permitiéndola sólo ante "e" e "i", se eliminó la "x" como fricativa salvo en posición final, y se limitó el uso de Y a su valor de consonante, salvo a final de palabra.

La segunda mitad del siglo vio rendirse progresivamente a los objetores y aceptarse las reglas académicas en España. La oposición no tardó en reeditarse, pero esta vez desde la otra orilla del océano.

Como en las restantes instituciones de la Corona, la Academia no incluía en su número a americanos ni tomaba en consideración los procesos que la lengua experimentaba en contacto con la diversidad lingüística de las tierras conquistadas. De ese modo, los estudiosos americanos de la lengua debieron llevar a cabo su tarea fuera de ella y, a veces, en franca oposición.

En 1823 vio la luz un escrito del venezolano Andrés Bello y el colombiano Juan García del Río, titulado "Indicaciones sobre la conveniencia de simplificar la ortografía en América", publicado en Londres. A pesar de que Bello reconocía el buen trabajo de la Academia al ordenar y simplificar la grafía de la lengua, consideraba que las limitaciones etimológicas que ella misma se imponía provocaban efectos desastrosos en la enseñanza en ambas orillas del Atlántico. La tesis de Bello se apoyaba en que el empleo de la etimología como criterio lingüístico era ocioso, pues en nada se vinculan la lectura y en general el uso de la lengua con su conocimiento histórico, y, en vista de los problemas que producía, contrario al uso racional.

Bello promovía una simplificación en dos etapas, para evitar los problemas de choque con los que se habían enfrentado Bartolomé Jiménez Patón y Gonzalo Correas, y una redistribución del silabario en atención a la realidad del uso lingüístico. Propuso eliminar la ambigua "c" y la "h" muda, asignar a "g" e "y" solo uno de sus valores, escribir siempre "rr" para representar la consonante vibrante y dedicar un cuerpo de estudiosos a resolver sobre el terreno la diferencia entre "b" y "v" (betacismo).

Veinte años más tarde, durante su exilio en Chile, Domingo Faustino Sarmiento formuló una propuesta no muy distinta. A diferencia de Bello, Sarmiento prefería conservar la "c" en lugar de la "k" y prescindir de la "v", la "x" y la "z".

Aunque las propuestas de Bello y Sarmiento no se plasmaron totalmente, aspectos de ambas se adoptaron en una propuesta hecha por la Facultad de Humanidades de la Universidad de Chile al gobierno de este país, que finalmente se adoptó allí, en Argentina, Colombia, Ecuador, Nicaragua y Venezuela. Asimismo, la influencia de Bello se había visto en la propuesta de la Academia Literaria y Científica de Profesores de Instrucción Primaria de Madrid, que el año anterior había adoptado muchos de sus principios. Sin embargo, Isabel II puso fin a este proyecto el 25 de abril de 1844 al imponer, por decreto real, el acatamiento a la Academia a través del "Prontuario de ortografía de la lengua castellana, dispuesto por real orden para el uso de las escuelas públicas, por la real Academia española, con arreglo al sistema adoptado en la novena edición de su Diccionario" <nowiki>[</nowiki>sic<nowiki>]</nowiki>.

La diferencia en usos duró hasta 1927, cuando Chile, el último país en sostener la ortografía de Bello, vigente por más de ochenta años allí, promulgó el 6 de agosto de ese año, la restitución de las normas académicas de la RAE en la enseñanza y documentos oficiales a partir del 12 de octubre de 1927.

El resultado de la larga divergencia y de la oposición planteada en otros marcos a la RAE ha sido una flexibilización de los criterios de esta; las ediciones del "Diccionario" y la "Ortografía" de la década de 1990 han reconocido finalmente que ciertas pronunciaciones varían entre España y América, han aseverado el estatus predominante del seseo y el yeísmo, y admitido el reconocimiento gráfico de las variaciones en la formación de diptongos. Otras acciones han seguido opuesto curso, al recomendar la grafía del grupo consonántico completo en los cultismos, tras siglos de supresión. De la misma fecha data la omisión de "ch" y "ll" del orden alfabético.

En el Primer Congreso Internacional de la Lengua Española, llevado a cabo en Zacatecas (México) en 1997, Gabriel García Márquez reeditó la propuesta de Bello, Correas y otros precursores, defendiendo la supresión de las grafías arbitrarias y abogando por la «jubilación de la ortografía». La polémica provocada fue difundida ampliamente por la prensa con propuestas a favor y en contra, aunque la discusión rara vez adoptó criterios propiamente lingüísticos.

El grafema A representa un fonema cuya realización general es la vocal abierta anterior no redondeada, . El español estándar no hace distinción fonológica con otras vocales abiertas, de modo que en dialectos influidos por otras lenguas puede pronunciarse también como una schwa, (Cataluña) o una (Asturias), u otra vocal similar. El dígrafo "-an" a final de palabra puede realizarse nasalizando la vocal en en muchas variantes dialectales.

El grafema B tiene en todos los dialectos del español al menos dos realizaciones alófonas (tres en algunos dialectos). En todos los dialectos representa el fonema labial sonoro no nasal /"b"/ que tiene diversas relaciones fonéticas según su posición dentro de una palabra. En posición inicial absoluta (después de pausa) o tras nasal, corresponde siempre a la oclusiva bilabial sonora, ; en posición medial, la explosión no se produce ―los labios no llegan a tocarse― y la articulación se corresponde en realidad una aproximante bilabial .

La lenición en posición medial es un fenómeno común a todas los fonemas sonoros del español (que en posiciones que no favorecen la lenición tienen alófonos de plosivos sonoros); fenómenos similares tienen lugar en < "d" > y < "g" >; sin embargo, son más pronunciados en algunos dialectos. Los hablantes de dialectos que prefieren una realización fricativa o aproximante encuentran que en aquellos la distinción de las plosivas sonoras entre sí y con la consonante aproximante labiovelar sonora, ―el sonido de < "hu-" > ante vocal― se desvanece. La grafía poética suele representarlo reemplazando < "b" >, < "d" > o < "hu-" > por < "g[ü]" >, como en el poema:
En el grupo <"obs-">, <"abs-">, presente en cultismos de origen latino, la <"b"> normalmente no se pronuncia (en muchas partes de España), aunque en algunas variantes se mantiene: en México o en Cataluña se pronuncia como . La grafía alternativa sin <"b"> se admite frecuentemente en el caso del grupo <"-bs-">, dando origen a dobletes como "oscuro"/"obscuro". Aunque en el grupo <"abs-"> la elisión de la sigue las mismas reglas, la Academia no acepta la omisión de la B en estos casos.

El grafema V tiene exactamente el mismo valor fonético que B. Se conserva la distinción puramente por razones etimológicas. No obstante, algunos hablantes cometen la hipercorrección de pronunciar ciertas palabras con una labiodental en el habla formal o enfática.

El grafema C se corresponde con dos fonemas diferentes, el llamado "duro" o velar () y el "blando" o dento-alveolar ( o ). El primer valor corresponde a su pronunciación frente a las vocales <"a, o, u"> y todas las consonantes; es idéntico al representado por las grafías K y Q.

El segundo valor corresponde a una de las articulaciones más variables del idioma español. En español medieval este fonema fue una africada que evolucionó de manera diferente en diversas regiones. En todo el centro y norte de la península ibérica <"c"> ante <"e, i"> evolucionó a fricativa interdental sorda, ; sin embargo, en la mayoría de los dialectos del español este fonema no existe, dando lugar al fenómeno llamado seseo. La asimilación de esta a la consonante fricativa alveolar sorda, , se ha perdido hace siglos y el sonido se ha asimilado al de la grafía S. A su vez, la pronunciación de ésta presenta algunas diferencias entre regiones, con realizaciones variadas: ápico-alveolar, lámino-alveolar, ápico-dental, etc.

El dígrafo "Ch" ya no se considera más una letra ni forma parte del abecedario español. Representa a la consonante africada postalveolar sorda, ; la representación digráfica se debe a la evolución del fonema a partir de la plosiva velar sonora, /k/, por palatalización y asimilación. En algunos dialectos andaluces, mexicanos norteños o chilenos (en este último caso, reprobado socialmente) pierde por completo la plosión y se realiza como la consonante fricativa postalveolar sorda, .

Muy antiguamente el dígrafo se empleó con valor de /k/ en palabras de etimología griega, como "chimera" (hoy "quimera") o "chloro" (hoy "cloro"), pero este uso se abandonó definitivamente en el siglo XVIII. De hecho el término "archivo" originalmente se pronunció como "arquibo", pero debido al mantenimiento de la ortografía varió su pronunciación.

El grafema D representa el fonema /d/ que tiene en español estándar dos alófonos: y . En posición inicial absoluta (después de pausa) o tras nasal o lateral, corresponde siempre a la consonante plosiva alveolar sonora, ; en posición medial, la plosión no se produce ―la lengua no llega a ocluir el flujo interdental― y la articulación se corresponde en realidad con una aproximante, . Esta última a veces se transcribe simplemente como (aunque no representa el mismo sonido "débil" que el dígrafo TH en inglés en "they" 'ellos').

Algunos dialectos tienden a retener en final de palabra, aunque es muy frecuente su lenición a una auténtica fricativa (centro de España, México) y en otros dialectos incluso se da la elisión completa, aunque en ocasiones esta última pronunciación se considera poco culta y por tanto depende mucho del registro lingüístico.

El grafema E representa normalmente a la vocal media anterior no redondeada, . En muchos dialectos americanos se realiza como la vocal semiabierta anterior no redondeada, . En algunas variedades dialectales en Andalucía existe oposición fonémica entre y ("él no viene" / "tú no vienes").

La Academia sostiene tradicionalmente que E no tiene nunca valor breve en español y que, por lo tanto, forma diptongo solo con I y U. Esto no es cierto para todos los dialectos del español; "peor", por ejemplo, suele pronunciarse como monosílabo. En otros las realizaciones en diptongo se cierran, asimilándose a la I.

El grafema F representa invariablemente a la consonante fricativa labiodental sorda, . El uso arcaico de PH para este fonema en palabras de origen griego se abandonó a partir de la "Ortografía" de 1754 de la RAE.

En algunos dialectos rurales de Argentina, Costa Rica, México y el sureste de España, la F inicial o medial tiende a realizarse como una palatal , asimilándose a "J":

El grafema G comparte con C la dualidad de valores según el grafema siguiente sea A, O, U o bien E, I. El llamado "duro" es la consonante fricativa velar sorda, , el mismo sonido de J; en los dialectos que suavizan esta última en una consonante fricativa glotal sorda, , la G se suaviza también. Corresponde a su pronunciación frente a las vocales E e I.

El llamado "blando" es la consonante plosiva velar sonora, ; en posición media, en todos los dialectos del español experimenta lenición y se transforma en una consonante aproximante velar, (en la sección correspondiente a la B se explican las confusiones a las que esta lenición puede dar lugar en algunos casos). Corresponde a su pronunciación frente a las vocales A, O y U y las consonantes.

Para representar las secuencias , , y se recurre a la inserción de una U muda entre la G y la vocal correspondiente. De ese modo, "guerra" corresponde a la pronunciación , y "seguido" a .

A su vez, para las secuencias , , y , se recurre a una marca diacrítica, la diéresis o crema, colocada sobre la U; es el caso, por ejemplo, de "pingüino", que representa . Muchos dialectos eliden la o de estas secuencias. A nivel gráfico, la omisión de la diéresis es una de las faltas gráficas más frecuentes entre los hispanohablantes. En español el sonido de es difiere de que es más abierto aún. La segunda articulación se da en español ibérico en formas como "juego, fuego, luego" que son pronunciadas como: .

En algunos préstamos del inglés, la secuencia "-NG" en posición final ―que no aparece en otros términos en español― se realiza como .

El grafema H se sigue usando en español estándar puramente por razones etimológicas o históricas, puesto que en español estándar no tiene valor fónico (es mudo). Sin embargo, dialectalmente en áreas de Andalucía y Extremadura sigue representando el sonido del español medieval. La mayoría de las H del español proceden las más de las veces el lugar donde existía una F en latín (como en "hijo", del latín "filius") o una ḥāʼ (ح) arábiga (como en "alcohol"), que en español medieval siguió siendo articulado como . Raramente en palabras de origen árabe se realiza como una consonante plosiva glotal, deteniendo momentáneamente la fonación. En algunos préstamos modernos, sobre todo del inglés, adquiere el valor de una consonante fricativa glotal sorda, que tiene en la fonética del idioma de origen, o se asimila a la fricativa velar sorda representada por G o J; así, se realiza como o , no .

Además de su uso etimológico, la H se emplea sistemáticamente prefijando las grafías "IE", "UE" en posición inicial de palabra; en este caso, las vocales breves representadas normalmente por I y U se transforman casi sin excepción en sus equivalentes consonánticos, la aproximante palatal, , y la aproximante labiovelar, . En los dialectos en que las oclusivas sonoras se reemplazan en posición medial por las aproximantes correspondientes, esta última pronunciación es virtualmente idéntica a la de G; véase la explicación en la sección correspondiente a la B. Otros dialectos no admiten en posición inicial, y añaden una epentética o refuerzo velar.

También se empleó en los comienzos del idioma escrito para diferenciar la U de la V, de grafía similar, a comienzos de palabra. Así, "hueso" y otras palabras que transformaron la O larga inicial de latín en el diptongo UE se escriben con H, a efectos de distinguirlas de otros términos en VE.

El grafema I representa a la vocal cerrada anterior no redondeada, , o a su alófono en posición inicial, la semivocal, . Su valor vocálico es idéntico al que tiene la Y frente a consonante o en posición final en todos los dialectos del español; la diferencia de uso no es etimológica, sino sistemática. Se fijó la Y como forma estándar en posición final y la I para las restantes en la edición de 1815 de la "Ortografía" de la RAE; con anterioridad a esta, las vacilaciones fueron numerosas. Las grafías "rei" o "i", por ejemplo, fueron frecuentes en lugar de las modernas grafías "rey" e "y".

En algunos dialectos, como el del castellano septentrional, la <"i"> en posición inicial en palabras como hierba, hiena o hierro también se pronuncia como aproximante palatal.

El grafema J representa siempre una consonante fricativa articulada en la región posterior del aparato fonador, pero su articulación precisa varía enormemente entre dialectos. La pronunciación consagrada como estándar tradicionalmente corresponde a la consonante fricativa velar sorda, , pero esta es rarísima fuera de España; en los dialectos americanos se realiza como una palatal, , o, menos frecuentemente, como una glotal, .

La homofonía entre "GE", "GI" y "JE", "JI" es causa frecuente de errores ortográficos y ha llevado a la reiterada propuesta de supresión de la primera grafía; los sistemas de Andrés Bello, de Domingo F. Sarmiento y de la Academia Literaria i Científica de Profesores de Instrucción Primaria de Madrid eliminaban la primera en favor de la segunda. Lo mismo hizo Juan Ramón Jiménez en las ediciones de sus obras. La distinción se preserva con criterio etimológico y ha dado lugar a múltiples inconsistencias históricas; hasta época reciente, la Academia recomendaba la grafía "muger".

En algunos, pocos, casos, la J alterna con la X considerada homófona, estimándose correctas ambas formas. Es el caso de "México (Méjico)", "Texas (Tejas)" o Don "Quixote (Quijote)" de la Mancha. La pronunciación corresponde a la fricativa que se emplee para J, no a la normal para X.

En algunos préstamos del inglés y el francés, la J se utiliza con su valor de origen, normalmente la consonante fricativa postalveolar sonora, ; el ejemplo más frecuente es "jazz".

El grafema K corresponde a la consonante oclusiva velar sorda, , el mismo sonido representado por la C ante A, O, U o consonante, y por el grupo "QU". No se empleaba en las "Reglas de Ortografía" de Antonio de Nebrija y las vacilaciones respecto a su uso fueron numerosas en la etapa de la primera fijación de la grafía española. El inédito "Abecé Español" de Gregorio Mayans la calificaba de "letra peregrina y superflua", aunque defendía su uso para nombres extranjeros. La RAE suprimió el uso de esta letra en 1815, aunque la admitió nuevamente en 1869. En la gran mayoría de los vocablos que la emplean existe una grafía alternativa aceptada con "QU".

Por la mayor regularidad de su uso ―al no variar su pronunciación de acuerdo a la vocal subsiguiente, y no requerir de una "U" muda―, numerosos de los proyectos de simplificación la promovían como única expresión gráfica del sonido . Hoy es un rasgo distintivo de la grafía apocopada empleada en la comunicación electrónica y de la jerga "okupa" y de sectores jóvenes del anarquismo.

El grafema L corresponde prototípicamente a la consonante aproximante alveolar lateral, , aunque existen alófonos dentales o postalveolares.

El dígrafo "Ll" ya no se considera más una letra ni forma parte del abecedario español. Representa, en la articulación considerada estándar por la Academia, a la consonante aproximante lateral palatal, . Sin embargo, es extendido el fenómeno del yeísmo, que afecta a la mayoría de dialectos hablados, hace que su articulación se haya fusionado con la del fonema aproximante palatal . Este sonido a su vez presenta bastante variación entre los dialectos del español. De origen no aclarado, el yeísmo es hoy la tendencia dominante en la pronunciación del español y, de hecho, se conserva principalmente donde la coexistencia con otro sistema fonológico ―como el del catalán o el quechua y el aimara― preserva la conciencia de la oposición. Curiosamente, en parte de Galicia (principalmente en las provincias de A Coruña y Pontevedra), donde hay coexistencia con el sistema fonológico del gallego, que tradicionalmente presenta pero carece de , se da una curiosa forma de yeísmo en la que ambos fonemas son sustituidos por el fonema oclusivo palatal sonoro (); por alguna razón tal sustitución, a pesar de ser habitual por parte de los hablantes en ambas leguas, solo es reconocida de forma oficial por algunos lingüistas y exclusivamente en el ámbito del gallego, siendo un hecho desconocido para la mayoría de la población.

En el español rioplatense se ha desplazado a una pronunciación postalveolar. En general la pronunciación es sonora (llamada "zheísmo" o "rehilamiento"), [] o [], similar a la representada por la grafía "j" en francés o portugués; en algunos sociolectos (muy marcadamente en Buenos Aires) se prefiere la sorda [] (llamada "sheísmo"), similar a la representada por la grafía "sh" en inglés, un fenómeno único en el uso del español.

En algunos préstamos del inglés, como "hall", donde el grafema representa un alófono velarizado de , tiene el valor de aquel; la pronunciación yeísta es considerada inapropiada en estos casos.

El grafema M representa la consonante nasal bilabial, ; tiene un alófono labiodental () frente a . Independientemente de la forma estricta de su realización, la grafía impone su uso frente a B, mientras que frente a la homófona V se utiliza N; la distinción se remonta al período alfonsino, en que la oposición entre bilabial y labiodental aún existía. También se usa la M siempre antes de P.

La M en español no se duplica salvo en los nombres Emma y Emmanuel y algunas palabras de origen extranjero como emmental o gamma.

El grafema N representa la consonante nasal alveolar, , o su alófono velar, , cuando precede a una consonante de articulación posterior. No sigue esta regla la grafía "NV", que corresponde en realidad a ; su distinción con "MB" se remonta al período alfonsino, en que la oposición entre bilabial y labiodental aún existía.

En posición final, la secuencia de "vocal + N" se realiza en algunos dialectos nasalizando la vocal precedente.

El grafema Ñ (que también se usa en gallego, filipino, wólof, bretón y varias lenguas amerindias), representa la consonante nasal palatal, . Hallado sobre todo como resultado de la evolución de la "NN" latina (como en "año", "caña", "leño"), su forma gráfica deriva de la grafía abreviada de los copistas, que representaban las dos enes superpuestas. En español existe solo en posición inicial o medial; las escasas palabras que por razones etimológicas deberían llevarla a fin de palabra ―el caso de "desdén", de "desdeñar"― la reemplazan fonética y gráficamente por "N".

En algunos dialectos americanos, en especial en Ciudad de México y el Río de la Plata, se realiza como una consonante nasal alveolar palatalizada ; la diferencia articulatoria concierne a la posición del ápice de la lengua, que en no cumple función articulatoria, mientras que en hace contacto con el alveolo a la vez que el domo se eleva hacia el paladar.

El grafema O representa la vocal media posterior redondeada, . Es más abierta que la hallada en la mayoría de las lenguas indoeuropeas, pero a la vez fonéticamente distintiva respecto a la vocal semiabierta posterior redondeada, , que no aparece en la mayoría de los dialectos del español.

La Academia tradicionalmente no reconoce la posibilidad de que la O sea breve en castellano y, por lo tanto, no considera que "OE", "EO", "OA" y "AO" puedan constituir diptongos. En varios dialectos y, sobre todo, en la pronunciación más formal, el hiato se rompe insertando una consonante plosiva glotal sorda, , que no existe como fonema en español; en otros se transforma en una vocal cerrada posterior redondeada, .

El grafema P representa la consonante plosiva bilabial sorda, . En muchos dialectos sufre lenición o asimilación frente a otra consonante y existe aún la posibilidad de su supresión, aunque esa realización se considera a veces poco culta. En posición inicial es normalmente muda en los cultismos de origen griego, como "pneuma" o "psicología"; de hecho, la "Ortografía" de 1741 la eliminó de los grupos "PT" y "PS", conservados hasta entonces con intención etimológica. Sin embargo, y en contra de la pronunciación más extendida, se han restituido a la grafía, admitiéndose ambas alternativas; la "Ortografía" de 1999 recomienda el uso de las formas con P.

El grafema Q aparece en español únicamente en la secuencia "QU", con el valor de la consonante plosiva velar sorda, , y sólo ante E e I. Se utiliza como sustituto de la C frente a estas letras, debido a la pronunciación variable de aquella. Hasta finales del siglo XVIII se utilizó con criterio etimológico para las palabras que la emplearan en latín, como "quando" o "quasi"; de ellas se conserva algún cultismo, como "quórum", pero ha desaparecido en la mayoría, y de hecho es normativo reemplazar por la letra hispana correspondiente (Por ejemplo, "cuórum" en lugar de "quórum").

Algún vocablo de origen extranjero, en particular semita, la adopta para transcribir la consonante plosiva uvular sorda, , representada en árabe como ﻕ; sin embargo, la Academia desaconseja por foráneas estas grafías, como "Iraq" o "burqa", prefiriendo el uso de la igualmente extranjera K para dar "Irak" o "burka".

El grafema R tiene en español dos valores netamente distintos. Entre vocales, y en algunas otras posiciones, representa la vibrante simple alveolar, ; a comienzo de palabra y siguiendo a una consonante nasal, a la consonante vibrante alveolar, . En muchos dialectos, en posición final cobra también este último valor.

Las reglas para determinar el valor exacto no son simples y combinan criterios sistemáticos y etimológicos. La pronunciación corresponde sistemáticamente a R en posición inicial ("rama", "Roque") o postnasal ("Enrique", "inri") o a "RR" en cualquier posición ("perro", "guitarra"); por razones etimológicas, se emplea la grafía R también siguiendo a un prefijo de origen latino, como en "alrededor" o "subrayar". En estos casos, existe una pronunciación alternativa con , relativamente infrecuente.

El dígrafo "RH" se conservó con valor etimológico para vocablos de origen griego hasta el siglo XVIII, pero se abandonó al tiempo que "PH". El "Abecé" de Mayans le daba a este el valor de una vibrante aspirada , pero la mayoría de las fuentes no recogen esta pronunciación en ninguna etapa del español.

El dígrafo rr representa una vibrante alveolar múltiple en las ortografías de español, catalán y albanés. Su nombre es erre doble o doble erre, a fin de diferenciarla de la letra R ("erre"), que suele representar la vibrante alveolar simple, aunque también puede representar la consonante múltiple al inicio de una palabra. Nunca ha sido tratada como una letra del alfabeto español, probablemente porque no aparece escrita en posición inicial, aunque sí tiene consideración de letra en albanés.

El grafema S representa arquetípicamente la consonante fricativa alveolar sorda, , aunque existen diversas realizaciones distintas para el fonema; en la mayor parte de los dialectos americanos es lamino-alveolar o dental (), mientras que en España es normalmente apico-alveolar (), un sonido que hablantes de otros dialectos suelen confundir con .

En casi toda América Central, la mayor parte de Sudamérica y la mitad sur de España, la S en posición final de sílaba se elide o pronuncia de manera aspirada como una consonante fricativa glotal sorda ; esta pronunciación se considera en algunas zonas poco culta o descuidada y las formas acrolectales destacan las sibilantes, pero es habitual en el habla cotidiana. En Andalucía Oriental y la Región de Murcia la elisión de la "S" se compensa abriendo (relajando) la vocal nuclear de la sílaba.

Debido al seseo, en la mayor parte de América el dígrafo "SC" delante de "E" o "I" representa simplemente una .

El dígrafo "SH" existe en algunos préstamos, como "flash" o "geisha", o regionalismos como "cafishio". La Academia admite el uso de tales extranjerismos destacándolos como tales con resalte tipográfico, pero recomendando adaptar los términos tanto en grafía como en pronunciación con una "S" ("flas", "gueisa"). Normalmente se realiza como , con una variante en España para la posición final.

El grafema T representa la consonante plosiva alveolar sorda, ; su realización es a veces dental, .

El dígrafo "TH", utilizado para representar la heredada de la theta griega, se abandonó en el siglo XVIII y hoy sobrevive solo en poquísimos cultismos.

La doble T puede escribirse con un grafema por cada letra o uno solo para las dos letras, ambas maneras son aceptables.

El grafema U representa la vocal cerrada posterior redondeada, ; tiene un alófono aproximante labiovelar ante otras vocales. Es muda después de Q, con alguna excepción como "quórum", y después de G y antes de E o I; se emplea la diéresis si se desea hacerla sonora frente a G. En ciertos diptongos iniciales como "hueco" o "huevo" tiene reforzamiento velar: y . Además, es la vocal menos usada de todas.

El grafema V tiene las mismas realizaciones alófonas que B que tiene exactamente el mismo valor fonético que este. En algunas regiones de México y de Sudamérica donde en esta última región el castellano convive con el guaraní, por influencia de este (donde la pronunciación labiodental es predominante) se observa la hipercorrección de pronunciar como /v/. Se conserva generalmente la distinción puramente por razones etimológicas. Se utiliza siempre V después de N.

El grafema W no es propio del español, y se emplea solo en un puñado de préstamos y nombres extranjeros. Según el origen del término, la pronunciación utilizada es (si la palabra es de origen alemán [como en "wolframio"]) o (si la palabra es de origen inglés [como en "whisky"]).

La mayor parte de los vocablos con W cuentan también con formas hispanizadas; así, la Academia admite "volframio" y "güisqui". Algunos nombres propios procedentes del alemán como "Wagner" (/bágner/) o "Volkswagen" (/bolksbágen/) ―donde "w" representa en alemán el sonido ― han sido adaptados al español con o .

El grafema X representa normalmente la secuencia consonántica ; ante consonante, la mayoría de dialectos tienden a suprimir la plosión y reducirla a o aspiración, aunque en diversos países de lengua española esta pronunciación se considera con frecuencia inculta.

Hasta mediados del siglo XIX se usó con el valor de (sonido de la "j" del castellano actual estándar), remedando la χ griega; la conveniencia de esta práctica fue causa de arduos debates entre los gramáticos anteriores, y la Academia la conservó hasta 1815. Desaconsejada a partir de esa fecha, se conservó sin embargo en unos pocos términos ―"box" (/bój/), "carcax" (/karkáj/)―, hasta su desaparición en 1844. Hoy se utiliza sólo en topónimos y antropónimos de origen náhuatl, como "México" (/méjiko/) u "Oaxaca" (/oajáka/), alternando con una grafía con J considerada equivalente. En español de México X puede representar cuatro realizaciones (en los nahuatlismos más antiguos e integrados), (en algunos topónimos nahuas), además de las realizaciones y del español estándar.

El seseo hace que el grupo "XC" tenga valores diferentes en América, Canarias y parte de Andalucía, donde es en lenguaje formal, y el norte de España, donde es casi siempre . También existen las pronunciaciones y .

En ciertos préstamos de origen portugués, gallego o catalán, así como en la toponimia y las voces de origen mesoamericano, la equis tiene valor de .

El grafema Y tiene valor de consonante entre vocales, en inicio de palabra o tras nasal y de semivocal tras vocal.

La realización del primero varía según los dialectos. En buena parte de América y España se emplea la consonante fricativa palatal sonora (), mientras que en rioplatense se desplaza a posición postalveolar () y en ocasiones se ensordece en . Tiene alófonos: la africado palatal, , o la postalveolar, , tras consonante nasal o en posición inicial. En ciertas zonas de Galicia, tanto la Y como la LL adquieren una pronunciación oclusiva palatal sonora ().

En posición final tiene el valor de una consonante aproximante palatal y en la conjunción "y" puede sonar del mismo modo o como la vocal [i]. Se fijó su uso de manera sistemática en 1815; en la ortografía preacadémica se utilizaba libremente como sustituto de "I". Varios dobletes toponímicos y antroponímicos restan de este uso, como "Ybarra"/"Ibarra".

En español actual, esta es la única consonante que se puede acentuar, siempre y cuando le corresponda el fonema vocálico [i], y se dé a su vez un hiato. Solo se da este fenómeno en grafías arcaicas como "Aýna" (municipio de España), y en nombres propios y poco comunes como "Laýna", e "Ýscar". Estos ejemplos son excepciones en cuanto a la acentuación de la "y", los cuales se conservan así por razones puramente etimológicas, ya que son grafías arcaicas que a pesar de las reformas que la RAE ha hecho a la ortografía del español, estas han sobrevivido con el paso del tiempo.

El grafema Z tiene el mismo valor que la C suave en el dialecto correspondiente, es decir, la consonante fricativa dental sorda, , en el norte y centro de España y la consonante fricativa alveolar sorda, , en el resto de los dialectos.

La frecuencia de uso de las letras en el castellano es la siguiente:

Las normas en cuanto al uso de mayúsculas en español han sufrido notables variaciones a lo largo de los años. Aunque hoy se reserva por lo general para los nombres propios, existen numerosas excepciones y los manuales de estilo de los distintos medios de comunicación son contradictorios entre sí y con las prescripciones de la Academia. Sigue siendo de uso habitual la mayúscula para las disciplinas académicas y para los títulos nobiliarios u honoríficos cuando están usados de manera pronominal; en la mayoría de los casos restantes se tiende a su supresión.

Tras el abandono del acento circunflejo ( ^ ) en el siglo XIX, el español emplea como diacrítico exclusivamente el acento agudo ( ´ ), que se coloca sobre la vocal central de una sílaba para indicar que esta es tónica en algunos casos.

Los criterios empleados parten de un conocimiento de la pronunciación usual correcta, sin el cual las normas de ortografía carecerían de sentido ya que su aplicación resultaría imposible y la escritura debe adaptarse a la pronunciación y no al revés como suele creerse. Así se distingue para el uso de los acentos varios tipos de palabras.

Las palabras oxítonas (tradicionalmente denominadas «agudas») se acentúan gráficamente si terminan en vocal, en "N" o en "S" recayendo así la pronunciación sobre la última sílaba; a la inversa, se acentúan las paroxítonas (tradicionalmente «graves» o «llanas») si terminan en consonante, excepto cuando terminan en "N" o "S" (salvo que la "S" vaya precedida de consonante, verbigracia: bíceps). Todas las proparoxítonas («esdrújulas» y «sobresdrújulas») llevan acento gráfico, con excepción de los adverbios en "-mente" derivados de un adjetivo que no lo lleve en su forma base.

El acento se utiliza también sobre la vocal débil ("I" o "U") de un diptongo para señalar la ruptura del mismo ("país"), el uso que tradicionalmente se reservaba a la diéresis y con que aún se la emplea en la grafía poética. La excepción es el diptongo "UI", que no se considera hiato aun si se acentúa de acuerdo con las reglas precedentes.

Un buen número de monosílabos, en especial adverbios y conjunciones, llevan acento puramente diacrítico para distinguirlos de sus homógrafos; así, "tu" es el pronombre posesivo de segunda persona, mientras que "tú" es el pronombre personal. En varios casos el acento diacrítico se ha suprimido en las últimas ediciones de la "Ortografía". Se emplea también para distinguir entre interrogativos y relativos ("donde" y "dónde").

Es un error frecuente considerar que las letras mayúsculas no llevan tilde. Un ejemplo es el diario argentino "La Nación", en el que falta esta en el título en la primera página.

Sobre este y otros casos, la RAE publicó en 1999 lo siguiente:

En este sentido, el periódico español "El País" modificó su cabecera en el año 2007 para que «EL PAÍS» pasara a escribirse con tilde.

La diéresis o crema (¨) se emplea para indicar que la "U" escrita tras una "G" se pronuncia. En poesía se emplea a veces para forzar un hiato en la escansión de una sílaba que normalmente conforma un diptongo, en tal caso puede recaer tanto sobre la "I" como sobre la "U", verbigracia:

Esta sección contiene una serie de patrones recurrentes, que aunque pudieran tener excepciones marginales constituyen una guía razonable de la ortografía.

El español es excepcional en indicar el comienzo de una frase interrogativa o exclamativa con una variante invertida del signo empleado en posición final (¿, ¡), un uso que se extendió a partir del siglo XVIII.

Es una peculiaridad lógica, ya que muchas frases interrogativas y exclamativas, a diferencia de otros idiomas, son –en su forma escrita– idénticas a las afirmativas. Con ello se facilita la lectura. Por otra parte, los signos de interrogación y admiración permiten señalar el comienzo y el final de una expresión interrogativa o exclamativa dentro de una oración, p. ej.: "Salimos a las cinco, ¿verdad? Ya estoy harto, ¡demonios! ¿¡cómo!?" Si –como en los ejemplos anteriores– el signo de interrogación o admiración aparece al final de la oración, ya no se escribe punto después de ellos porque estos signos ya funcionan como punto de la oración.

La "Ortografía de la lengua española" (obra muy a menudo mencionada mediante la sigla "OLE") corresponde a la recopilación normativa de la ortografía de la lengua española. Está editada y elaborada por la Real Academia Española (RAE) desde su primera edición en 1741, cuando se publicó como "Orthographía española", y a partir de 1999 realizada en conjunto con la Asociación de Academias de la Lengua Española. La edición más reciente es la del año 2010.

Con la publicación del "Prontuario de ortografía de la lengua castellana, dispuesto de real orden para el uso de las escuelas públicas, por la real Academia española, con arreglo al sistema adoptado en la novena edición de su Diccionario" ["sic"] (1844), los acuerdos de la RAE con respecto a la ortografía alcanzaron el nivel de normativa, desplazando otros posibles manuales de ortografía, principalmente el de Andrés Bello (ver ortografía de Bello).

El director de la Academia Mexicana de la Lengua, José G. Moreno de Alba, anunció el domingo 28 de noviembre de 2010 en Guadalajara (México) el acuerdo de las veintidós Academias de la lengua española sobre la nueva edición de la "Ortografía", que se publicó en diciembre de 2010.

Con respecto a los cambios introducidos por la RAE en diciembre de 2010, puede consultarse el artículo Innovaciones en la ortografía española (2010).





</doc>
<doc id="37929" url="https://es.wikipedia.org/wiki?curid=37929" title="Cumbre de las Américas">
Cumbre de las Américas

La Cumbre de las Américas es una reunión de jefes de estado y de gobierno de los países de América para tratar temas diplomáticos y/o comerciales de importancia a nivel continental. En un principio se formó para tratar la implementación del ALCA.

Participan en el evento los 35 Estados independientes americanos. Asisten sus mandatarios.

Cuba ha expresado que, en caso de ser invitada a esta Cumbre, asistiría desde su tradicional política exterior de principios y que lo haría con respeto, sin embargo en la cumbre de 2015 realizada en Panamá fue invitada y participante de la misma.

Aunque el proceso de participación es esencialmente protocolar y se abordan múltiples temas, en años recientes las discusiones se han concentrado en la formación de un Área de Libre Comercio de las Américas (ALCA) que debió haber entrado en vigor en el mes de enero de 2005, pero no consiguió el consenso necesario.

En 2018, la canciller peruana, Cayetana Aljovín, anunció el 13 de febrero de 2018 que el gobierno del Perú retiraba la invitación al presidente venezolano Nicolás Maduro, una decisión respaldada por los 12 países del Grupo de Lima, Estados Unidos y el presidente de la Organización de Estados Americanos (OEA), Luis Almagro. Por lo que Venezuela no tuvo representación oficial del Gobierno de Venezuela, pero sí de dirigentes opositores como Luis Florido, María Corina Machado, Julio Borges, Lilian Tintori y asociaciones civiles de venezolanos a los cuales se les permitió debatir en representación de Venezuela. Bolivia, Cuba y Ecuador habían mostrado su preferencia a que Venezuela participara en la Cumbre. Algunos medios de comunicación reportaron que, la ausencia por primera vez del Presidente de Estados Unidos, Donald Trump, causó desagrado en la región.





</doc>
<doc id="37931" url="https://es.wikipedia.org/wiki?curid=37931" title="Georg Friedrich Händel">
Georg Friedrich Händel

Georg Friedrich Händel (); en inglés George Frideric (o Frederick) Handel (Halle, Brandeburgo-Prusia; - Londres; 14 de abril de 1759) fue un compositor alemán, posteriormente nacionalizado inglés, considerado una de las figuras cumbre de la historia de la música, especialmente la barroca, y uno de los más influyentes compositores de la música occidental y universal. En la historia de la música, es el primer compositor moderno en haber adaptado y enfocado su música para satisfacer los gustos y necesidades del público, en vez de los de la nobleza y de los mecenas, como era habitual.

Considerado el sucesor y continuador de Henry Purcell, marcó toda una era en la música inglesa. Para varios expertos es el primer gran maestro de la música basada en la técnica de la homofonía y el más grande dentro del ámbito de los géneros de la ópera seria italiana y para algunos hasta en el oratorio, por delante de Johann Sebastian Bach.

Su legado musical, síntesis de los estilos alemán, italiano, francés e inglés de la primera mitad del , incluye obras en prácticamente todos los géneros de su época, donde , (entre ellos "El Mesías") y un legado coral son lo más sobresaliente e importante de su producción musical.

Georg Friedrich Händel nació el en la ciudad de Halle, ubicada en el Ducado de Magdeburgo del Sacro Imperio Romano Germánico (actual Alemania), hijo de Georg Händel y Dorothea Taust. Su padre, que tenía 63 años cuando nació Georg Friedrich, era un cirujano-barbero de prestigio que sirvió en la corte del Ducado de Sajonia-Weissenfels y el Margraviato de Brandeburgo. De acuerdo con el primer biógrafo de Händel, John Mainwaring, «había descubierto tan fuerte propensión por la música que su padre, que siempre había deseado que se dedicara al estudio del Derecho Civil, tuvo razones para alarmarse. Le prohibió estrictamente que jugueteara con cualquier instrumento musical, pero encontró medios para conseguir un pequeño clavicordio que transportó en privado a una habitación en la parte superior de la casa. Se escapaba a esta sala constantemente cuando la familia estaba durmiendo». A una edad temprana, se convirtió en un hábil intérprete de órgano y clave. 
Händel y su padre viajaron a Weissenfels para visitar al hermanastro de Händel, Carl, o sobrino, Georg Christian, que servía como ayudante de cámara para el duque Juan Adolfo I. En este viaje, el joven Händel se sentó en el taburete del órgano de la iglesia del palacio, donde sorprendió a todos con su interpretación del instrumento. Esta actuación lo ayudó a él y al duque a convencer a su padre para que le permitiera tomar lecciones de composición musical y técnica del teclado con Friedrich Wilhelm Zachow, el organista de la "Marienkirche" de Halle. Zachow componía la música para los cultos dominicales en la iglesia luterana y de él aprendió sobre armonía y contrapunto, la copia y el análisis de partituras y aprendió a tocar el oboe, violín, clave y órgano. En 1698, Händel interpretó para Federico I de Prusia y se reunió con Giovanni Bononcini en Berlín.

En 1702, cuando tenía 17 años y por seguir los deseos de su padre, Händel comenzó a estudiar Derecho con Christian Thomasius en la Universidad de Halle. Le nombraron organista de la catedral calvinista de Halle por un año y parece que estuvo satisfecho con ello. Un año después, en 1703, viajó a Hamburgo, donde fue admitido como violinista y clavecinista en la orquesta de la Oper am Gänsemarkt de Hamburgo. Allí conoció a los compositores Johann Mattheson, Christoph Graupner y Reinhard Keiser y compuso sus dos primeras óperas, "Almira" y "Nero", en 1705. Escribió otras dos óperas, "Daphne" y "Florindo", en 1708 y no está claro si dirigió esas representaciones.
De acuerdo con Mainwaring, en 1706 Händel viajó a Italia por invitación de Fernando de Médici. Otras fuentes afirman que lo invitó Juan Gastón de Médici, a quien había conocido en 1703-1704 en Hamburgo. De Médici, que tenía un gran interés en la ópera, estaba tratando de hacer que Florencia fuera la capital musical de Italia mediante la atracción de destacados talentos de su época. En Italia, Händel conoció al libretista Antonio Salvi, con quien más tarde colaboró. Fue a Roma y, como la ópera fue (temporalmente) prohibida en los Estados Pontificios, compuso música sacra para el clero romano. Disfrutó del mecenazgo tanto de la nobleza como del clero. Su famoso "Dixit Dominus" (1707) es de esta época. También compuso cantatas en estilo pastoral para reuniones musicales en los palacios de los cardenales Pietro Ottoboni, Benedetto Pamphili y Carlo Colonna. En 1707, compuso su primer oratorio "El triunfo del tiempo y del desengaño" y un año después escribió "La resurrezione", basado en textos bíblicos. Ambos se representaron en un ambiente privado para las familias Ruspoli y Ottoboni en 1709 y 1710, respectivamente. 

La primera de todas sus óperas italianas, "Rodrigo", fue producida en el Teatro Cocomero de Florencia en 1707. "Agripina" se representó por primera vez en 1709 en el Teatro San Giovanni Grisostomo, propiedad de la familia Grimani. Contaba con libreto del cardenal Vincenzo Grimani y se escenificó durante 27 noches consecutivas. Tal fue el reconocimiento que se le dio a Händel en este país, que recibió el sobrenombre de «"Il caro Sassone"» («el querido sajón», en referencia a los orígenes alemanes del compositor), como nombre afectivo.

En 1710, regresó de Italia a Alemania y se convirtió en el maestro de capilla del príncipe elector de Hannover Jorge, que en 1714 se convertiría en Jorge I de Gran Bretaña. Visitó a Ana María Luisa de Médici y a su marido en Düsseldorf en su camino hacia Londres en 1710. Obtuvo un gran éxito con su ópera "Rinaldo", basada en el poema épico "Jerusalén liberada" del poeta italiano Torquato Tasso, a pesar de que la había compuesto rápidamente, con muchos préstamos de sus más tempranas obras italianas. Esta obra contiene el aria "Cara sposa, amante cara" y la famosa "Lascia ch'io pianga".

En vista del éxito cosechado, en 1712 decidió establecerse en Inglaterra. En el verano de 1713, vivió en la casa de Mathew Andrews Barn Elms Surrey. Recibió un salario anual de 200 libras de la reina Ana, después de que hubiera compuesto para ella "Utrecht Te Deum and Jubilate", interpretado por primera vez en 1713. El biógrafo de Händel Jonathan Keates sugirió que pudo haber viajado a Londres en 1710 y establecido allí en 1712 como espía para el futuro sucesor de la reina Ana, su anterior empleador el príncipe Jorge de Hannover.

Uno de sus mecenas más importantes fue Richard Boyle, un joven y acaudalado aristócrata de familia anglo-irlandesa. Para el joven Lord Burlington, Händel compuso en 1715 "Amadigi di Gaula", una ópera mágica sobre una damisela en apuros, basada en la novela de caballerías de Antoine Houdar de La Motte.

No compuso óperas durante cinco años porque le resultaba difícil imaginar la concepción de una obra de tales características como una estructura coherente. En julio de 1717, se representó la "Música acuática" más de tres veces en el Támesis para el Rey y sus invitados. 

En 1717, Händel se convirtió en el compositor de Cannons en Middlesex, donde puso la primera piedra para sus composiciones corales futuras en los "Chandos Anthems". Romain Rolland afirma que estos himnos fueron tan importantes para sus oratorios como las cantatas lo fueron para sus óperas. Otra obra, que escribió para el Primer Duque de Chandos, el dueño de Cannons, fue "Acis y Galatea", que fue la más importante obra representada durante la vida del compositor. Winton Dean escribió, «la música corta la respiración y perturba la memoria».

En 1719, el duque de Chandos se convirtió en uno de los más importantes clientes del compositor y estaba entre los principales suscriptores de su nueva compañía de ópera, la Royal Academy of Music, pero su mecenazgo declinó después de que Chandos perdiera dinero en la burbuja de la Compañía del Mar del Sur, que explotó en 1720 en una de las mayores crisis financieras de la historia. El propio compositor había invertido en valores de la Compañía en 1716, cuando los precios eran bajos y los había vendido antes de 1720.

En mayo de 1719, Thomas Pelham-Holles, Lord Chambelán, ordenó a Händel buscar nuevos cantantes para la Royal Academy of Music, fundada por un grupo de aristócratas para asegurarse un suministro constante de óperas barroca u óperas serias. El compositor viajó a Dresde para asistir a la ópera recién construida. Vio "Teofane" de Antonio Lotti, y contrató a los miembros del reparto para la compañía. Händel pudo haber invitado a John Smith, su compañero de estudios en Halle, y su hijo John Christopher Smith, para convertirse en su secretario y amanuense. Para 1723, se había instalado en una casa de estilo georgiano en el número 25 de Brook Street, que alquiló para el resto de su vida. Esta casa, donde ensayaba, copiaba música y vendía entradas, es ahora la Handel House Museum. Durante doce meses, entre 1724 y 1725, Händel escribió tres óperas destacadas y exitosas, "Julio César en Egipto", "Tamerlán" y "Rodelinda". Las óperas están llenas de arias da capo, como "Svegliatevi nel core". Después de componer "Silete venti", se concentró en la ópera y dejó de escribir cantatas. "Scipione", del que se deriva la marcha lenta del regimiento de la Grenadier Guards británica, se representó como un recurso provisional, a la espera de la llegada de la soprano Faustina Bordoni. Además, tuvo el privilegio de contar con los servicios de varios de los principales virtuosos vocales de primera línea de la ópera italiana: el castrato contralto Senesino, la soprano Francesca Cuzzoni y el bajo Antonio Montagnana, entre otros.

Allí escribió 14 óperas para esa institución entre 1720 y 1728, que le hicieron famoso en toda Europa. La estabilidad económica de la empresa, la disposición de los prestigiosos solistas y de una excelente orquesta, así como el gran entusiasmo del público, le permitieron llevar a la época de gloria de la Royal Academy of Music, que incluyó varias de las piezas cumbre de la ópera seria.

El 11 de junio de 1727 falleció Jorge I, pero antes de morir había firmado el «Acta de Naturalización» de Händel. El compositor era ya súbdito británico. En ese momento cambió su nombre a «George Frideric Handel». A Jorge I le sucedió Jorge II y para su coronación se encargó la música al compositor. Así nacieron los "Coronation Anthems" «Zadok the Priest», que ha sido interpretado en cada coronación desde entonces; «My Heart is Inditing», «Let Thy Hand be Strengthened» y «The King shall Rejoice». Las dimensiones de la orquesta y músicos requeridos eran extraordinarias (se pudo leer en un periódico «hay 40 voces y unos 160 violines, trompetas, oboes, timbales y bajos, proporcionalmente, además de un órgano, que fue instalado detrás del altar»). Después de nueve años de existencia, la Royal Academy of Music cesó su función, pero Händel pronto inició una nueva compañía.

El King's Theatre de Haymarket, fundado en 1705 por el arquitecto y dramaturgo John Vanbrugh, rápidamente se convirtió en un teatro de ópera. Entre 1711 y 1739, se estrenaron allí más de 25 óperas de Händel. En 1729, se convirtió en gerente adjunto del teatro con John James Heidegger.

Viajó a Italia para contratar nuevos cantantes y también compuso siete óperas más, entre las que se encuentran "Parténope" y "Orlando". Con "Athalia", su primer oratorio inglés, "Deborah" y "Esther", Händel sentó las bases del uso tradicional del coro que marcó sus oratorios posteriores. Se mostró más seguro de sí mismo, abierto en sus presentaciones y más diverso en su composición. Pudo invertir de nuevo en la Compañía del Mar del Sur gracias al éxito comercial de "Esther" y "Deborah". Reelaboró su "Acis y Galatea", que entonces se convirtió en su mayor éxito obtenido. No pudo competir con la Ópera de la Nobleza, que contrató a músicos como Johann Adolph Hasse, Nicola Porpora y el más famoso castrato, Farinelli. El gran apoyo a esta compañía de Federico, príncipe de Gales, causó conflictos en la familia real. En marzo de 1734, Händel compuso un himno nupcial «This is the day which the Lord hath made» y la serenata "Parnasso in festa" para Ana de Hannover.

En 1734, el consejo de inversores principales esperaba que Händel se retirara cuando expirara su contrato en el King's Theatre, pues ya no se tenía interés en que continuara como compositor. Se instaló allí la Ópera de la Nobleza, la cual contaba con el compositor italiano Nicola Porpora. Por tanto, inmediatamente buscó otro teatro. Le ofrecieron trabajar en el recién construido Covent Garden Theatre y el compositor aceptó.

En cooperación con John Rich, empresario de este nuevo teatro y conocido por sus espectaculares producciones, Händel acordó producir cinco óperas para la temporada 1734-1735, dos de ellas fueron reposiciones de "Il pastor fido" y "Arianna" y tres estrenos: "Oreste" (un pastiche más que ópera), "Alcina" y "Ariodante". Sugirió usar su pequeño coro e introducir la danza de Marie Sallé, para la que compuso "Terpsícore". En 1735, introdujo conciertos para órgano entre los actos. Por primera vez dejó que Gizziello fuera sustituto en las arias, quien no tuvo tiempo para aprender su parte. Económicamente, "Ariodante" fue un fracaso, a pesar de que presentaba suites de ballet al final de cada acto. "Alcina", su última ópera con un contenido mágico, y "Alexander's Feast" fueron protagonizadas por Anna Maria Strada y John Beard. Este último se convirtió en el tenor solista permanente de Händel durante el resto de su vida. La pieza fue un gran éxito y le permitió al compositor hacer la transición entre la escritura de óperas italianas a las obras corales inglesas.

En abril de 1737, cuando tenía 52 años, sufrió un derrame cerebral que le inhabilitó el uso de cuatro dedos de su mano derecha, lo que le impidió tocar. Incluso parecía que el trastorno a veces afectaba a su entendimiento. Nadie esperaba que fuera a ser capaz alguna vez de volver a tocar, pero se recuperó con notable rapidez del problema. Viajó a un balneario a Aquisgrán para recuperarse. Durante seis semanas tomó largos baños calientes y terminó tocando el órgano para un público sorprendido. Incluso pudo escribir una de sus óperas más populares, "Serse" (que incluye la famosa aria "Ombra mai fù" que escribió para el castrato Caffarelli), justo un año después de su aflicción.

En 1741, se representó tres veces su última ópera, "Deidamía", una coproducción con el conde de Holderness. Händel renunció a la composición de óperas y se dedicó a la de oratorios, que tenían más éxito. Durante el verano de ese año, William Cavendish lo invitó a Dublín, capital del Reino de Irlanda, para dar conciertos en beneficio de hospitales locales. Allí tuvo lugar el estreno de "El Mesías," en el New Music Hall en Fishamble Street, el 13 de abril de 1742, con un coro de 26 niños y cinco hombres que provenían de los coros de las catedrales de San Patricio y de la Santísima Trinidad.

El estreno de "Sansón", el 18 de febrero de 1743 en el Covent Garden Theatre, y el uso de solistas en inglés fue un gran éxito. La obra es muy teatral y el papel del coro se hizo cada vez más importante en sus oratorios posteriores. En 1747, escribió el oratorio "Alexander Balus". Esta obra se escenificó en el Covent Garden Theatre el 23 de marzo de 1748 y para el aria "Hark! hark! He strikes the golden lyre," compuso el acompañamiento para mandolina, arpa, violín, viola y violonchelo.

En 1749, compuso la "Música para los reales fuegos de artificio" y a su primera representación asistieron 12000 personas. Un año después se organizó una interpretación del "Mesías" a beneficio del Hospital Foundling, que tuvo un gran éxito y estuvo seguido de conciertos anuales durante el resto de su vida. En reconocimiento a su patrocinio, lo nombraron gobernador del hospital el día después de su concierto inicial. Legó una copia del "Mesías" a la institución después de su muerte. Su implicación con el Hospital es conmemorada hoy con una exposición permanente en el Museo Foundling de Londres, que también alberga la "Gerald Coke Handel Collection". También realizó obras de beneficencia junto a organizaciones que asistían a músicos pobres y a sus familias.

En agosto de 1750, mientras viajaba en carruaje a Londres de regreso desde Alemania, Händel sufrió un accidente entre La Haya y Haarlem y resultó herido. Un año después, comenzó a tener problemas de visión en un ojo debido a una catarata, que fue operada por John Taylor. La operación no mejoró su vista, sino que posiblemente la empeoró. En esa época estaba componiendo el oratorio "Jephtha". Al año siguiente, en 1752, se quedó completamente ciego. 

A comienzos de abril de 1759, se sintió mal mientras dirigía su oratorio "El Mesías". Terminado el concierto, se desmayó. Falleció el 14 de abril de 1759, en su casa en la calle Brook, 74. Fue enterrado, según su deseo plasmado en su testamento, en la abadía de Westminster, panteón de las personas más célebres del Reino Unido. Más de tres mil personas asistieron a su funeral, que fue celebrado con honores de Estado.

Händel poseía una colección de arte que subastaron un año después de su muerte, en 1760. El catálogo de la subasta contenía un listado de aproximadamente setenta pinturas y diez grabados (legó otras pinturas).

El estilo de Händel es una síntesis de los principales estilos nacionales musicales de su época, tomando los mejores elementos y características de cada uno de ellos y superándolos por separado, como sus compatriotas contemporáneos Johann Sebastian Bach y Georg Philipp Telemann, donde además se añade el estilo inglés de Henry Purcell, al que Händel le da un nuevo y vigoroso empuje, siendo el verdadero continuador de este compositor. Todo ello fruto de sus estancias en Inglaterra, Alemania e Italia, dando prueba de que era un auténtico cosmopolita de su tiempo.

Su estilo tiene la solidez y el contrapunto de la música alemana, la melodía y el enfoque vocal del "bel canto" de la italiana, la elegancia y solemnidad de la escuela francesa y la audacia, sencillez y fuerza de la inglesa. Händel es un fiel continuador de estos estilos y técnicas, en que no aporta ninguna novedad a todas estas corrientes musicales de la primera mitad del , aunque como Jean-Philippe Rameau, su música, especialmente en óperas y oratorios, adquiere un nuevo y especial sentido dramático y monumental, triunfante, poderoso y solemne que es único entre la música de su tiempo.

Generalmente, su producción tiene una estructura simple y sencilla, de lenguaje vocal en la línea del "bel canto" italiano, pero templado y conteniendo un pudor expresivo que recuerda a Purcell en vez de a los compositores italianos, cuyas cualidades cautivan rápidamente al auditorio, y la homofonía. Es, en esencia, de corte mayormente italiano, que es el estilo más presente en su música y el que más ha influenciado en todos los aspectos de su obra.

Händel introdujo en sus obras instrumentos musicales poco corrientes, como la viola de amor y la violetta marina ("Orlando"), el laúd ("Oda para el día de Santa Cecilia"), tres trombones ("Saul"), clarinetes o pequeñas cornetas ("Tamerlán"), tiorba, trompa ("Música acuática"), liricordio, contrafagot, viola da gamba, carillón, órgano positivo y arpa ("Julio César en Egipto" y "Alexander's Feast").

El catálogo musical de las obras de Händel, que abarca en total 612 registros más 25 suplementos y obras dudosas y perdidas,
fue elaborado y publicado en 1978 y 1986 en tres volúmenes. Se conoce con las siglas «HWV», que significan "Händel-Werke-Verzeichnis" (en alemán, «Catálogo de obras de Händel»). A diferencia de otros catálogos que están ordenados cronológicamente, el HWV está clasificado por tipo de obra de géneros y según su naturaleza vocal o instrumental.

Sus obras se dividen en siete grandes grupos, contenidos en dos grandes bloques: música vocal (dramática, oratorios, profana y religiosa) y música instrumental (orquestal, de cámara y para clave) donde abarca todos y cada uno de los géneros de su época.

En cuanto a música vocal, los géneros y obras que el compositor compuso, que suman 290 piezas en total, son 43 óperas en italiano, alemán e inglés —46 si se añaden "Sémele", "Acis y Galatea" y "Hércules"—, dos músicas incidentales para espectáculos en inglés, 26 oratorios en italiano, alemán e inglés, cuatro odas y serenatas en italiano e inglés, 100 cantatas en italiano y español, 21 dúos, dos tríos, 26 arias sueltas, 16 obras para conciertos espirituales, 41 himnos, cinco Te Deum, un "Jubilate" y tres himnos ingleses.

Las óperas componen el punto central de su obra junto con los oratorios y es uno de los más importantes compositores de dicho género, y el más destacado en el subgénero dramático de la ópera barroca, la ópera seria del . Sin embargo, dentro de la ópera seria no se puede comparar a Händel con sus antecesores, Alessandro Scarlatti, y sucesores, Johann Adolph Hasse, Christoph Willibald Gluck y Nicola Porpora, ya que fue diferente de ellos por sus formas poco italianas y la utilización de recursos de la ópera alemana y francesa.

Händel tendía cada vez más a sustituir solistas italianos por ingleses. La razón más importante de este cambio fue la disminución de los rendimientos financieros de . Así fue cómo se creó la tradición con los que habría de regir su representación en el futuro. Las obras se interpretaban sin trajes ni acción y los cantantes aparecían con su propia ropa. Su obra más famosa, el oratorio "El Mesías" con su coro «Aleluya», es una de las obras más populares de la música coral y se ha convertido en la pieza central de la temporada navideña.

En música instrumental, compuso 78 obras en el ámbito orquestal: 34 conciertos para solistas, 23 "concierti grossi", cuatro oberturas, siete suites, dos sinfonías, seis movimientos de danzas y conciertos sueltos y dos marchas. Escribió 68 en el ámbito de cámara: 22 sonatas para un instrumento solista y bajo continuo, 25 sonatas en trío y 19 movimientos sueltos de danzas, marchas y sonatas. Entre todas ellas destacan dos obras orquestales, "Música para los reales fuegos de artificio" y "Música acuática".

Entre 1703 y 1706, mientras residía en Hamburgo compuso la cuarta estructura de la Suite N°4, "Zarabande" para clavecín en D-menor, la obra fue publicada recién en 1733.

Sus obras para teclado, en especial las 186 destinadas al clavicémbalo —30 suites y oberturas y 156 movimientos de suite sueltas—, son una de las cimas de la música barroca, junto con Johann Sebastian Bach, Jean-Philippe Rameau, François Couperin y Domenico Scarlatti para estos instrumentos. Sus obras más importantes y conocidas son dos colecciones de suites (HWV 426-433 y HWV 434-438) —publicadas en Londres en 1720—, seis fugas (HWV 605-610) y doce conciertos para órgano (Op. 4 y Op. 7). En este ámbito, su maestro Friedrich Wilhelm Zachow le familiarizó con la escuela alemana del clave y órgano, donde recibió influencias de Johann Kuhnau, Johann Jakob Froberger, Johann Caspar Kerll y Dietrich Buxtehude. Esta música tiene un aspecto libre y espontáneo, al igual que el resto de su música instrumental. Este novedoso y poco común género lo interpretaba él mismo en los intermedios de . Händel, en estos conciertos, demostraba su talento como organista y su original sonoridad y con ellos se ganaba el favor del público. Sin embargo, se tiene una imagen incompleta de cómo debía de sonar realmente esa música, ya que en las partituras no aparecen las ornamentaciones ni las secciones reservadas a la improvisación. Händel, como Bach, fue un notable improvisador al teclado.

Sus suites HWV 426-433 tienen una originalidad y una variedad muy grande en varios aspectos, en referencia a la suite francesa para teclado, siguiendo la norma como el resto de su obra instrumental. Adoptó variados patrones y movimientos de diversos géneros: la sonata de iglesia, como en el caso de la "Suite 2", la estructura clásica de la suite, como en el caso de la "Suite 1", o una combinación de ambos géneros, como en la "Suite 7", y diversos estilos, como el concierto en la "Suite 4". Estas suites tienen una gran potencia y sentido dramático, un aire de grandeza que casi desborda el marco del clave y utilizan tonalidades poco usadas en la primera mitad del , como en el caso de la "Suite en fa sostenido menor", y, entre la producción de teclado, es en estas obras donde está más patente la originalidad de Händel.

Después de su muerte, sus óperas italianas cayeron en el olvido, a excepción de selecciones como el aria de "Serse", "Ombra mai fù". Se continuaron interpretando sus oratorios, pero no mucho después de la muerte del compositor se pensaba que necesitaban un poco de modernización y Wolfgang Amadeus Mozart orquestó una versión alemana del "Mesías" y otras obras. En el vigesimoquinto aniversario de su muerte, tuvo lugar un acto de conmemoración en la Abadía de Westminster. 

A lo largo del y la primera mitad del , en particular en los países de habla inglesa, su reputación se basaba principalmente en sus oratorios ingleses, que se representaban habitualmente por enormes coros de cantantes aficionados en ocasiones solemnes. El centenario de su muerte, en 1859, fue celebrado con una representación del "Mesías" en The Crystal Palace, que implicaba 2765 cantantes y 460 instrumentistas, que actuaron para un público de unas 10 000 personas.

En general, Händel ha gozado de una gran estima entre sus compañeros compositores, tanto en su propia época como desde entonces. Johann Sebastian Bach intentó, sin éxito, reunirse con Händel mientras estaba visitando Halle. A Mozart se le atribuye haber dicho de él «Händel entiende los afectos mejor que cualquiera de nosotros. Cuando quiere, golpea como un rayo». Para Ludwig van Beethoven era «el maestro de todos nosotros ... el compositor más grande que jamás haya existido. Me dejaría al descubierto la cabeza y me arrodillaría ante su tumba». Beethoven destacó, sobre todo, la sencillez y el atractivo popular de la música de Händel cuando dijo: «Recurre a él para aprender cómo lograr grandes efectos, por medios tan simples».

Georg Friedrich Händel es uno de los compositores más conocidos de la música clásica. Se ha utilizado su imagen en diversos formatos artísticos y de otra índole, como pósteres, caricaturas y postales. Se han emitido sellos postales y otros documentos filatélicos y numismáticos en numerosos países del mundo, en muchos casos para conmemorar los aniversarios de su nacimiento y muerte. También se han acuñado monedas, medallas y medallones conmemorativos. Su imagen u obras han sido utilizadas en diversos artículos de "merchandising", como relojes, vitolas de puro, naipes, muñecos de juguete, platos, tazas y camisetas. Además, se han erigido estatuas y placas conmemorativas en diferentes ciudades y existen numerosos bustos y retratos con su figura.

En 1942, se estrenó la película biográfica "The Great Mr. Handel", dirigida por Norman Walker y protagonizada por Wilfrid Lawson. Además, se ha utilizado su música en más de 400 películas y programas de televisión.

En su condición de autor de música religiosa, su nombre figura entre las celebraciones del Calendario de Santos Luterano y comparte fecha con Johann Sebastian Bach y Heinrich Schütz. Además, se le honra con un día festivo del Calendario de Santos de la iglesia episcopal. Se celebra el 28 de julio y lo comparte con Bach y Henry Purcell. Händel y Bach también se conmemoran en el calendario de los santos preparado por la Order of Saint Luke para uso de la Iglesia metodista unida.

El cráter de impacto en el planeta Mercurio denominado «cráter Handel» lleva su nombre.



</doc>
<doc id="37938" url="https://es.wikipedia.org/wiki?curid=37938" title="BCN">
BCN

BCN puede referirse a: 


</doc>
<doc id="37939" url="https://es.wikipedia.org/wiki?curid=37939" title="Idioma purépecha">
Idioma purépecha

El idioma purépecha, tarasco o michoacano (autoglotónimo: P'urhépecha, pronunciación: []), es un idioma hablado por los miembros del pueblo purépecha del occidente de México.

El purépecha presenta muchas características lingüísticas que lo hacen parecer un idioma singular, muy diferente de otras lenguas de Mesoamérica. De hecho, la lengua p'urhépecha ha sido clasificada como un idioma o lengua aislada, ya que hasta ahora no se ha podido establecer ninguna relación de origen común con alguna de las lenguas que se hablaron, o hablan, en México u otro país, aunque Morris Swadesh, sugirió alguna similitud remota de tipo léxico con otros idiomas de América. Esta propuesta no ha tenido demasiada aceptación entre los especialistas en estas otros idiomas.

El purépecha es una de las lenguas indígenas de México con mayor vitalidad. Se divide en tres variantes dialectales: la de la región lacustre, la central y la serrana (algunos incluyen una cuarta, la de la Ciénaga). De acuerdo con cifras del XII Censo General de Población y Vivienda de 2000, existen unos 121 409 hablantes asentados en 22 municipios y siendo un 25 % de los mismos monolingüe en purépecha y el resto bilingües en español. En la actualidad se hablan 56 idiomas a lo largo y ancho del territorio mexicano. Las estadísticas oficiales (INEGI 1996) indican que en el estado de Michoacán se hablan 38 lenguas indígenas con el siguiente número de hablantes:

Su auge repuntó partir de 1895, año desde el cual se dio inicio a un movimiento de apoyo a través de la Academia de la Lengua Purépecha ("P'urhe Uandakueri Juramukua") por el cual se ha conseguido el fortalecimiento y difusión de esta lengua. El purépecha es, actualmente, una lengua literaria debido a la gran difusión que han tenido los Concursos Regionales de Cuento en Lenguas Indígenas coordinados por la Dirección General de Culturas Populares e Indígenas y los estados de Hidalgo, Querétaro, Michoacán y México, los cuales han fortalecido también el carácter literario de las lenguas náhuatl, otomí y mazahua.

La clave ISO 639-2 para el idioma es nai.

El purépecha se considera habitualmente una lengua aislada de Mesoamérica. Conserva algunos rasgos tipológicos poco frecuentes en el área lingüística mesoamericana como la presencia de dos fonemas vibrantes: la vibrante simple // y la retrofleja //. De los cinco rasgos típicos del área lingüística mesoamericana, el purépecha sólo posee de manera inobjetable uno, el sistema vigesimal de numeración.

Sin embargo, Greenberg considera que esta lengua tiene cierto parentesco con el chibcha, aunque esta idea es rechazada por la mayoría de lingüistas especialistas en lenguas americanas, como Campbell, quien la considera una lengua aislada.

El purépecha se habla principalmente en la parte occidental y central del estado, principalmente en el área comprendida entre el lago de Pátzcuaro y la sierra al occidente de éste conocida como la Meseta Tarasca. El territorio abarca a 22 municipios del estado de Michoacán, que en conjunto ocupan una área de 8.370 km², lo cual representa el 14 % de la superficie del estado, de los cuales, 14 tienen una proporción significativa de hablantes de la lengua. Estos son: Chilchota, Charapan, Nahuatzen, Paracho, Tangamandapio, Cherán, Quiroga, Erongarícuaro, Coeneo, Los Reyes, Tzintzuntzan, Tingambato, Pátzcuaro y Uruapan.

En Morelia se habla únicamente por los migrantes de estas regiones, pues no es una lengua madre de dicha ciudad. De estos migrantes la mayoría son estudiantes de la Universidad Michoacana de San Nicolás de Hidalgo, la cual cuenta con esta lengua dentro de su departamento de idiomas. 

El centro del antiguo reino tarasco se encontraba alrededor del lago de Pátzcuaro, que aún es un centro importante de la comunidad purépecha. Este reino abarcaba casi todo el estado de Michoacán y partes considerables de Guanajuato y Guerrero, así como porciones de los estados de México, Querétaro y Jalisco.

Tradicionalmente se reconocen cuatro variedades geográficas o dialectos: la de la Meseta o Sierra, la de la zona lacustre, la de la Cañada y la de la Ciénaga. La región de la Meseta concentra aproximadamente el 62 % de los hablantes, y de hecho fue hasta los años 1980 la zona menos comunicada del territorio. La zona lacustre concentra el 17.8 % de los hablantes, la Cañada o Región de los Once Pueblos el 14.7 %, y la Ciénaga sólo el 5.2 %.

"Ethnologue" distingue dos variedades: el dialecto central hablado aproximadamente por 120 000 personas (1990) alrededor de Pátzcuaro y el dialecto occidental de las tierras altas hablado en las cercanías de Zamora, Los Reyes de Salgado, Paracho, y Pamatácuaro, todos ellos en las vecindad del volcán Paricutín.

Es una de las lenguas más ampliamente usadas por el grupo étnico que la sustenta como parte de su identidad y es una de las pocas lenguas indígenas que ha llegado a tener su propia academia de la lengua. Sin embargo, es una lengua que está experimentando una contracción paulatina y en algunos poblados se ha reducido al mínimo y en una o dos generaciones se perderá por completo si continúa el actual proceso de abandono de la lengua.

Esta lengua, junto con todas las lenguas indígenas de México y el español, fueron reconocidas como "lenguas nacionales" debido a la Ley General de Derechos Lingüísticos de los Pueblos Indígenas promulgada y publicada en el año 2003.

El purépecha se empezó a escribir desde el siglo XVI, cuando los frailes lenguatarios adaptaron las grafías latinas para ello. Los intentos de normalización moderna se remontan al Proyecto Tarasco (1939), cuando se empezaron a discutir algunas propuestas ortográficas. Actualmente no existe todavía consenso en esa cuestión, aunque tiene cierto peso el estándar propuesto por la Academia de la Lengua Purépecha y la propuesta de la Secretaría de Educación Pública (SEP). Esta última es la usada por la Dirección General de Educación Indígena para los libros escolares.

En los siguientes cuadros se muestran los fonemas del purépecha con sus principales alófonos. El inventario vocálico está formado por las siguientes unidades:

Las dos vocales medias /e, o/ son poco frecuentes, de hecho /o/ es bastante inusual. Por su parte el inventario consonántico está formado por:

Morfológicamente el purépecha hace extenso uso de la flexión y tiene una compleja morfología. Desde el punto de vista tipológico se trata de una lengua aglutinante en la que las palabras muestran un límite claro y son segmentables fácilmente en morfemas existiendo generalmente una correspondencia unívoca entre morfema y función que éste desempeña como en:

Uno de los subsistemas más productivos dentro de la lengua purépecha es el de los clasificadores o locativos corporales. Los locativos corporales son en algunos casos unidades léxicas y en otros casos partículas gramaticales. Su función es interesante, ya que delimitan y nombran objetos que no necesariamente tienen qué ver directamente con el cuerpo humano, sin embargo se los dota de brazos, piernas, boca, cabeza, etc. También hacen referencia a la parte del cuerpo que recibe o ejerce la acción de un verbo.

Algunos ejemplos de locativo corporal:

El verbo “cortarse la mano” existe como tal, donde se puede identificar la partícula "k’u", que se refiere a la palma de la mano. Vemos también que en este caso sólo hay yuxtaposición y se prescinde del instrumental "jimbo" (“con”), que tendría que ir justo después de "tsakapu" (“piedra”).

Aquí observamos que el verbo "uarhini" está adicionado con la partícula "me", referente no precisamente a una parte del cuerpo humano, sino a los líquidos, es decir; a cosas que se relacionan generalmente con el agua. Aún con ese locativo presente en el verbo, se reitera la idea del agua con la palabra "itsï", adicionada con el locativo "rhu", cuyo significado literal es ”en”.

El verbo “acariciar” es "putini", de manera que "putimukuni" es algo así como acariciar con la boca. Es precisamente a la boca a los que se refiere la partícula "mu", que está totalmente inserta en la morfología del verbo. El resto de la frase es algo de rutina, un sustantivo adicionado con caso aplicativo.

El purépecha es una lengua dotada de caso morfológico. Desde el siglo XVI, fecha en que se documentó por primera vez la lengua se detectan ciertos desarrollos históricos. Por ejemplo el marcaje de ciertos casos mediante clíticos o postposiciones está dando paso a genuinas marcas de caso, que se añaden a los casos ya presentes en el siglo XVI. El purépecha es una lengua exclusivamente sufijante, Swadesh identificó hasta 150 sufijos.

El alineamiento morfosintáctico es de tipo nominativo-acusativo con el sujeto sin marca explícita (morfo cero) y el objeto (indirecto o directo) marcado con "-ni":

El orden sintáctico es claramente uno, en el que el sujeto precede al verbo (hay cierta discusión adicional sobre si es más básico SOV o SVO), por ejemplo:

La flexión nominal en purépecha incluye número y caso (no existen diferencias de género gramatical). La categoría de número distingue entre plural y no-plural, el plural se marca con "-echa" o "-cha" (el primero en formas cuya forma singular acaba en /u/ o /a/ (esta se elide en el plural), mientras que la otra forma aparece tras formas acabadas en /i, ɨ, e/ en el singular), algunos ejemplos:
La flexión de caso distingue los siguientes casos:
Los casos comitativo e instrumental de hecho no son casos propiamente dichos ya que son postposiciones escritas tras el nombre no afijos.






</doc>
<doc id="37940" url="https://es.wikipedia.org/wiki?curid=37940" title="Río Gállego">
Río Gállego

El Gállego (en aragonés "río Galligo") es un río de la península ibérica que discurre por la comunidad autónoma española de Aragón. Se trata de uno de los principales afluentes del Ebro. Drena una cuenca de y tiene una longitud de . El curioso nombre de este río deriva del nombre en latín de su lugar de procedencia, la Galia: el "Gallicus", el Gállego.

El río Gállego nace en el pirenaico Col d'Aneu, a 2200 metros de altura, en las proximidades del collado del Portalet. Se abre paso por el Valle de Tena recorriendo los municipios de Sallent, Panticosa y Biescas, cortando las sierras prepirenaicas en la "Foz de Santa Elena". A partir de Sabiñánigo comienza a describir un amplio codo hasta Triste, desde donde prosigue nuevamente su primitiva dirección N-S para no dejarla ya hasta su incorporación al río Ebro, a la altura de Zaragoza.
En la cuenca alta es donde recibe los principales afluentes: río Aguas Limpias (regulado por el pantano de la Sarra), Caldarés, Escarra, Lana Mayor y Aurín, que son los que determinan en gran parte el caudal y las características del río Gállego. En la cuenca media y baja, los ríos Guarga, Seco, Asabón y Sotón aportan escaso caudal. Está regulado el curso del río por los embalses hidroeléctricos del Gállego, Lanuza, Búbal, Sabiñánigo y los de regadíos de La Peña y Ardisa, del que se deriva el agua al pantano de la Sotonera a través del canal del Gállego (90 m³/segundo). 

Aguas abajo del pantano de Ardisa, en el término municipal de Puendeluna, se deriva en un azud del que parte el canal que, pasando por las poblaciones de Puendeluna y Marracos, suministra de agua a la central eléctrica del Salto del Lobo para después volver a unir su caudal al río. Aguas abajo el cauce se encuentra con nuevos azudes, como el de Ontinar o el del Rabal, que alimentan los sistemas de riego de las acequias de Camarera, Urdán, o Rabal.

El caudal del Gállego en su desembocadura en Zaragoza es débil; la intensa regulación y las derivaciones caudal provocan que su caudal medio actual represente el 10% de su caudal natural.

En su curso alto, entre el embalse de Lanuza y el de Búbal, se encuentran las estrechas gargantas de Escarrilla y Costechal, separadas por la desembocadura del río Escarra, ambas propicias para el desarrollo del barranquismo.

En su tramo alto se practica piragüismo en aguas bravas al igual que en sus afluentes el Caldares y el Escarra. Sin embargo, esta actividad, junto con el rafting, alcanza su máximo desarrollo en el tramo de Murillo de Gállego, aguas abajo del embalse de la Peña, zona en la que supone un importante aporte a la economía local.

La superficie de la cuenca hidrográfica sería de unos alrededor de .

El río sufrió un importante impacto medioambiental debido a un importante vertido de lindano por parte de una empresa de pesticidas, contaminación que afectó a parte del municipio. Fuentes oficiales han calculado que entre 1975 y 1989 fueron arrojadas entre 115 000 y 160 000 toneladas de residuos tóxicos dos vertederos en Sabiñánigo, que de alguna forma se filtraron parcialmente al río.


</doc>
<doc id="37942" url="https://es.wikipedia.org/wiki?curid=37942" title="El Prat de Llobregat">
El Prat de Llobregat

El Prat de Llobregat es un municipio que se encuentra en la comarca del Bajo Llobregat en la provincia de Barcelona, comunidad autónoma de Cataluña, España, y forma parte del área metropolitana de Barcelona.

En su término municipal se encuentran infraestructuras de gran importancia para Barcelona como el Aeropuerto de Barcelona (IATA:BCN), y, una vez finalizadas las obras del desvío del río Llobregat, una parte de la zona portuaria del puerto de Barcelona.

La ciudad se encuentra a orillas del mar Mediterráneo y su término municipal, de una superficie de 31,40 km², linda con los de Barcelona, Hospitalet de Llobregat, Cornellá de Llobregat, San Baudilio de Llobregat y Viladecans. El terreno es prácticamente llano y su altitud máxima (5 metros) se encuentra en la Plaça de la Vila.

Las localidades hermanadas con El Prat son Garrovillas de Alconétar (Cáceres), Gibara (Cuba), Kukra Hill (Nicaragua) y el condado de Fingal (Irlanda).
El Prat de Llobregat es conocido también por su raza autóctona de gallos y gallinas, los pollos potablava. Se organiza anualmente la Feria Avícola del Prat de Llobregat para promocionarlos. 


Un dossier con datos demográficos más extensos se puede obtener en este enlace

La población del Prat es de 64.132 habitantes, según el padrón de habitantes el 1 de enero de 2018. Después de vivir fuertes aumentos demográficos a partir de mediados del siglo XX, la población del Prat se mantiene más o menos estable desde 1980. En el año 2010 es el cuarto municipio en población del Baix Llobregat, después de Cornellà, Santo Boi y Viladecans.

El Prat de Llobregat tiene un clima mediterráneo típico. De acuerdo a la clasificación climática de Köppen el clima de El Prat de Llobregat es de tipo Csa (mediterráneo).

La temperatura media anual se sitúa alrededor de los 16 °C, que van de los 9,2 °C en enero a los 24,4 °C en agosto, siendo así la amplitud térmica anual de 15,6 °C. En invierno las heladas son ocasionales, con una media de casi cuatro días al año, y las nevadas son excepcionales, con una media de menos de un día cada dos años. En verano las máximas no son demasiado altas, con 28,5 °C de media en agosto, pero las mínimas superan ligeramente los 20 °C en este mes. Por otra parte, la oscilación térmica diaria es reducida, situándose en 8,5 °C.

La precipitación anual se sitúa algo por debajo de los 600 mm. El mínimo de precipitación se da en la primera mitad del verano, en junio y julio con una media ligeramente superior a los 20 mm en este último, sin embargo, a pesar de tener un clima mediterráneo de tipo Csa caracterizado por un verano seco, el mes de agosto (el más cálido) resulta el tercer mes más lluvioso del año, con una media ligeramente superior a los 60 mm. El máximo de precipitaciones se da entre agosto y noviembre, con una media ligeramente por encima de los 90 mm en octubre, el mes más lluvioso. La humedad media del 69% es significativa debido a la condición marítima de la ciudad, variando poco a lo largo del año.

A continuación se muestra una tabla con los valores climatológicos en el periodo de referencia 1981-2010 del observatorio de la AEMET situado en el Aeropuerto de Barcelona-El Prat, situado a 4 msnm. Nótese que los valores extremos están también tomados en el periodo 1981-2010.

A continuación algunos récords climatológicos registrados en el observatorio de la AEMET situado en Alicante, considerados a partir del año 1924 para la temperatura y precipitación y a partir de 1961 para el viento. El récord de temperatura máxima absoluta es de 37,4 °C registrada el 27 de agosto de 2010, y la mínima de −8 °C registrada el 27 de diciembre de 1962. La precipitación máxima en un día es de 186,7 mm registrados el 25 de septiembre de 193, y la máxima racha de viento es de 139 km/h registrada el 15 de noviembre de 2001.

El municipio del Prat limita al norte de Hospitalet de Llobregat, al este con el mar Mediterráneo, al oeste con Cornellá de Llobregat y al sur de Viladecans.

El Prat es el único municipio creado cuyo territorio se encuentra íntegramente en el delta del Llobregat. Las tierras de El Prat comienzan a ser habitadas hacia el siglo X, mucho antes de la creación del núcleo urbano. A finales del siglo XVII El Prat todavía no formaba poblado pero poseía una vida legal independiente con un consejo y unas ordenaciones municipales desde 1689.

El siglo XVII es el del nacimiento de El Prat urbano. Así, entre 1720 y 1740 comienzan a construirse las primeras casas alrededor de los edificios de la plaza, hecho motivado por la autorización concedida a Bernat Gual, un granjero, para abrir una carnicería próxima a un cruce de caminos (lo que hoy es la Plaça de la Vila). Posteriormente, se concedía el derecho papal para tener una parroquia propia, y más tarde crecían a los alrededores el hostal (que hacía las funciones de taberna y panadería, aparte de las que su nombre indica). La apertura de la carnicería, su buena situación, la construcción de la parroquia y la del hostal, favorecieron que varios artesanos se instalaran en el lugar. A lo largo del siglo las casas se agruparán en dos hileras que se extenderán paralelamente hacia el norte resiguiendo los dos lados de uno de los caminos. Así se formará la primera calle del pueblo, la única que habrá durante mucho tiempo: la calle Mayor.

Poco después el pequeño núcleo se ve favorecido por la barca que el mismo Bernat Gual pone en servicio para pasar el río, ya que para ir a comerciar a Barcelona se requería remontar el río (que en aquel tiempo transcurría justo al este de la hilera de casas) hasta el primer puente, en Martorell, a 23 km de allí, lo que suponía un viaje de un día sólo para ir. Con la barca, el trayecto quedó reducido a ocho kilómetros, cosa que hizo que muchos más campesinos se interesaran en instalarse en El Prat.

Al empezar el siglo XIX, la economía de la población está basada todavía en una agricultura tradicional, fundamentada en el trabajo familiar y en la contratación temporal de jornaleros en los momentos de más trabajo. A lo largo del siglo, el crecimiento de la actividad agrícola, juntamente con el incremento demográfico, provocará un aumento del número de jornaleros. Éstos, junto a los artesanos, serán los artífices del crecimiento del núcleo urbano.

La construcción del puente de Ferran Puig para cruzar el río (1873), la llegada del ferrocarril (1881) y el descubrimiento del agua artesiana (1893) abrieron perspectivas de desarrollo a la población que se materializarán en el siglo XX.

El Prat afronta la entrada del nuevo siglo con un hecho fundamental en el campo: la consolidación de los cultivos de regadío, que han conseguido desbancar totalmente a los cereales de secano. Casi todas las tierras han sido adaptadas a los nuevos productos, mucho más rentables, y eso ha repercutido en la mejora de la situación general de la población agrícola y jornalera. El Prat vive unos años de expansión gracias a la comercialización de los excedentes agrícolas, y gozan de especial reconocimiento por su calidad la alcachofa, la lechuga y el melón.

La ruptura con las formas de vida tradicionales vendrá provocada, básicamente, por la llegada de la industria y la instalación de la aviación. El paso de mano de obra del campo a la fábrica, la llegada masiva de trabajadores de otros lugares y la consolidación de la semanada en substitución del inseguro jornal, contribuirán a alterar profundamente la configuración social y cultural de El Prat. En 1917, con la instalación de la Papelera Española, en 1923 con los tres aeródromos en funcionamiento (eran los campos de la Aeronáutica Naval, el de Josep Canudas y el de la compañía francesa Latecoère) y en 1926 con la puesta en funcionamiento de La Seda, serán años clave en el proceso de transición de la sociedad agraria a la industrial.

La consolidación del proceso industrializador comportará la llegada masiva de nuevos pobladores que se encontrarán con una ciudad que no está preparada para acoger este flujo demográfico. El Prat de 1950 tenía 10.038 habitantes y 25 años más tarde, en 1975, la población total era de 51.058 personas. Los principales déficit se sitúan en la falta de viviendas y de plazas escolares pero también se hacen evidentes en los servicios, especialmente en el agua y en el alcantarillado.

A lo largo de los años setenta del siglo XX El Prat vivió un importante crecimiento, no siempre equilibrado, para adaptarse a la nueva realidad social. En los años ochenta el crecimiento urbano continuó y se ampliaron los polígonos industriales con la llegada de nuevas empresas. Las preocupaciones urbanísticas van dirigidas a conseguir una mejora en los equipamientos, especialmente en los barrios más densificados y con más carencias, en un intento de racionalizar el urbanismo incontrolado de las décadas anteriores y equilibrar el crecimiento.

El escudo del Prat de Llobregat se define por el siguiente blasón:
Fue aprobado el 29 de marzo de 2001 y publicado en el DOGC el 23 de abril del mismo año con el número de documento 3373.


</doc>
<doc id="37944" url="https://es.wikipedia.org/wiki?curid=37944" title="Pollo de raza Prat">
Pollo de raza Prat

El pollo de raza Prat es una raza de pollo autóctona de El Prat de Llobregat (provincia de Barcelona), España, y criada tradicionalmente en su término municipal. Esta raza se la conoce popularmente como "pota blava", pata azul en catalán, cuya denominación también es utilizada para referirse a los habitantes de El Prat de Llobregat.

Se caracteriza por el color azulado de sus patas, por la que se la atribuye a menudo la denominación el colorido rubio oscuro de su plumaje, sin llegar a colores rojizos.

Es especialmente apreciado como plato durante las fiestas navideñas, especialmente como capón.

Para su fomento se celebra la feria avícola de la raza prat durante la segunda semana de diciembre.

El pollo de raza Prat es la única raza de pollo de España que ha recibido la "indicación geográfica protegida" (IGP) de la Unión Europea, y cada pieza se comercializa con una etiqueta numerada y acreditativa de esta condición.

Para recibir este sello el pollo debe haberse criado en los términos municipales de Casteldefels, Cornellá de Llobregat, Gavá, San Baudilio de Llobregat, San Feliú de Llobregat, San Clemente de Llobregat, Viladecans y Santa Coloma de Cervelló aparte, por supuesto, del de la ciudad que le da su nombre, El Prat de Llobregat.



</doc>
<doc id="37946" url="https://es.wikipedia.org/wiki?curid=37946" title="Primer mandato de George W. Bush como Presidente de los Estados Unidos">
Primer mandato de George W. Bush como Presidente de los Estados Unidos

El Primer mandato de George Walker Bush como Presidente de los Estados Unidos comenzó el 20 de enero de 2001, cuando tras vencer a Al Gore, tomó posesión como el 43º presidente de los Estados Unidos. Este primer mandato finalizó el 20 de enero de 2005, cuando fue reelegido para un segundo mandato derrotando a John Kerry. Este primer mandato estuvo marcado por el atentado del 11 de septiembre a las Torres Gemelas en 2001 y por el inicio de la guerra en Irak en 2003. Sin embargo, el primer hecho que marcó este mandato ocurrió durante las elecciones presidenciales, donde el Tribunal Supremo de Estados Unidos decretó que no había lugar a un nuevo recuento de votos en el estado de Florida, dando la victoria al Partido Republicano con un resultado ajustado. Este acontecimiento provocó el fin del recuento manual, para ser sustituido por el recuento electrónico. En su primer discurso como Presidente de los Estados Unidos, George W. Bush marcó los objetivos principales de su mandato: la reducción de los impuestos y una mayor defensa. Estos objetivos luego se tradujeron en la política denominada Bush Tax Cuts y la estrategia de seguridad nacional de los Estados Unidos de América, que se firmó tras el 11S. Del mismo modo, en sus primeros días aplazó las iniciativas del anterior presidente respecto a la asistencia sanitaria a los jubilados estadounidenses. 

El primer año del primer mandato de Bush estuvo marcado por la recesión económica de 2001. Esta recesión que comenzó cuando Bill Clinton aun era presidente de Estados Unidos, no comenzó a ser objeto de debate hasta que sucedió el atentado del 11 de septiembre, pues se esperaba poder alcanzar un "aterrizaje suave" en el aspecto económico. Con esta recesión, el objetivo principal de la política económica de Bush fue la reducción de los impuestos, lo que se denominó Bush Tax cuts.

El déficit público también marcó el primer mandato de George W. Bush como presidente de los Estados Unidos. En el comienzo de su legislatura, el presupuesto estadounidense presentaba un superávit de 236.400 millones de dólares. Sin embargo, durante su primer mandato se alcanzó el déficit presupuestario más alto de la historia de Estados Unidos, llegando en 2003 a ser de 450.000 millones de dólares. Este hecho se debió, según comunicó el Secretario del Tesoro John W. Snow, al aumento del gasto militar, a la recesión económica que atravesaba el país en aquellos años y a la quiebra y escándalos de grandes corporaciones empresariales. 

Uno de los objetivos de Bush durante su primer mandato como Presidente de los Estados Unidos fue la reducción de los impuestos. Esto se tradujo en dos leyes promulgadas en 2001 y 2003 que modificaban y reformaban la política fiscal estadounidense. Estas reformas fueron popularmente conocidas como Bush tax cuts y redujeron principalmente los impuestos sobre la renta, los impuestos sobre los dividendos y el impuesto sobre el patrimonio. La primera fue la Ley de reconciliación de alivio fiscal y crecimiento económico (EGTRRA) de 2001 y la Ley de reconciliación de alivio fiscal sobre crecimiento y trabajo (JGTRRA) de 2003. La reforma de 2001 tenía como objetivo aliviar la presión fiscal sobre las familias y paliar la recesión económica que atravesaba Estados Unidos. Respecto a la ley de 2003 el énfasis se puso en el crecimiento de las empresas, la creación de dos millones de puestos de trabajo y favorecer la confianza de los inversores en los mercados. George W. Bush defendió así su rebaja de impuestos:"Para estimular la economía, el Congreso no necesita gastar más dinero. Lo que debe hacer es recortar los impuestos"
La quiebra de la multinacional energética Eron fue el primer escándalo que salpicó a George W. Bush en su primer mandato como presidente. La empresa energética quebró de manera inesperada en noviembre de 2001 causando que los 20.000 empleados de la empresa perdieran su trabajo y 4.500 jubilados perdieran sus ahorros, pues estaban depositados en acciones de Eron cuyo valor cayó de 85 dólares a menos de 1 dólar de manera inesperada.Esta quiebra propició una investigación donde se descubrió que el presidente ejecutivo junto con la empresa auditora Arthur Andersen habían ocultado, falsificado y eliminado documentos que reflejaban la situación de bancarrota que atravesaba la multinacional. Esto permitió que los ejecutivos de la empresa vendieran sus acciones y obtuviesen beneficios millonarios antes de que se produjese la quiebra de la multinacional energética . 

Esta empresa se encontraba estrechamente ligada a la Casa Blanca, pues el presidente de Eron Kenneth Lay era íntimo amigo de George W. Bush. Además, la multinacional había donado más de medio millón de dólares a las campañas políticas de Bush y 100.000 dólares más cuando esté llegó a la Casa Blanca, siendo el mayor contribuyente de la carrera política de Bush. Estos hechos propiciaron que Bush y su gabinete se viesen envueltos en la quiebra de Eron. También influyeron en el escándalo de Eron las confesiones por parte de la Casa Blanca de que los ejecutivos de Eron les habían informado de la situación financiera de la empresa.No obstante, los miembros del gabinete afectados explicaron que no se había informado al Presidente de esta situación. 

Tras este escándalo, George W. Bush expresó su apoyo a la investigación que desde el Departamento de Justicia se llevó a cabo para esclarecer el escándalo de la quiebra repentina de la séptima empresa de Estados Unidos en negocios anuales. "El presidente piensa que es vital para el Departamento de Justicia continuar la investigación a dondequiera que se dirija, a quien sea que lleve por delante y hacer lo que sea para investigar cualquier mala actuación".

Las relaciones internacionales durante el primer mandato de George W. Bush versaron sobre sus relaciones con distintas organizaciones supranacionales, pues su Administración tenía como objetivo fijar a Estados Unidos como la Superpotencia mundial. En este sentido, en el año 2002, Estados Unidos dejó de colaborar con la Corte Penal Internacional al no firmar su ratificación. Del mismo modo, también estableció como condición obligatoria la no colaboración con este tribunal para aquellos países que se beneficiasen de los programas estadounidenses de ayudas. Sin embargo, el hito que marcó las relaciones internacionales de este periodo de tiempo fue el inicio de la guerra de Irak, que constituía una de las prioridades de Bush y del Partido Republicano. 

Tras los atentados del 11 de septiembre de 2001, conocidos como 11S o 9/11, George W. Bush emitió una serie de discursos para reconfortar al pueblo estadounidense y anunciar una guerra contra el terrorismo. El 21 de septiembre de 2001, 10 días después del atentado, Bush pronunció un discurso ante las dos cámaras legislativas donde anunció que la guerra contra el terrorismo no acabaría hasta que el último grupo terrorista no existiese, disponiendo para ello de todos los instrumentos diplomáticos e internacionales para acabar con el "terrorismo de alcance mundial." Y exigió la entrega inmediata de Osama bin Laden. "Y perseguiremos a las naciones que proporcionan ayuda o asilo al terrorismo. Todas las naciones de todas las regiones tienen que tomar ahora una decisión: o están con nosotros o están con el terrorismo [...] Desde hoy en adelante, cualquier nación que siga acogiendo o apoyando el terrorismo será considerada por Estados Unidos como un régimen hostil [...] Nuestra nación ha sido avisada, no somos inmunes a los ataques. Tomaremos medidas defensivas contra el terrorismo para proteger a los estadounidenses [...] Mientras Estados Unidos siga siendo fuerte y decidido, esta no será una era del terror. Será una era de libertad aquí y en todo el mundo".La guerra contra el terrorismo anunciada por Bush comenzó con la invasión de Afganistán en octubre de 2001. El 7 de octubre de 2001, Bush dio un ultimátum a Afganistán pidiendo que entregasen inmediatamente a Osama bin Laden, amenazando al régimen talibán, quien gobernaba el país asiático en aquella época, de emprender acciones militares. Ante la negativa del gobierno talibán, Bush ordenó la invasión de Afganistán que dio comienzo a la guerra. Del mismo modo, Bush aprobó el envío de ayuda humanitaria al país asiático. En septiembre de 2002, ya comenzada la guerra en Afganistán, George W. Bush aprobó la estrategia de seguridad nacional de los Estados Unidos de América donde se especificaba que Estados Unidos actuaría solo en derecho de su propia defensa y sus propios intereses, sin perjuicio de la posición al respecto de las organizaciones internacionales. También se incluía el derecho a emprender acciones preventivas contra los grupos terroristas. En el documento estratégico firmado por Bush se explicaba que Estados Unidos permanecería al lado de los países débiles que luchasen por su libertad. "Estados Unidos permanecerá junto a cualquier nación que esté determinada a construir un futuro mejor al tiempo que busca las recompensas de la libertad para su pueblo.[...] Mientras Estados Unidos de manera constante se esfuerza por conseguir el apoyo de la comunidad internacional, no dudaremos en actuar solos, si fuera necesario, ejerciendo nuestro derecho a la propia defensa y recurriendo a la acción preventiva contra los terroristas, impidiendo que hagan daño a nuestro pueblo y a nuestro país." 

Durante el año 2003, el presidente George W. Bush afirmó que el gobierno de Saddam Hussein poseía armas de destrucción masiva y que suponía una amenaza grave para el pueblo estadounidense. Bush aseguró en marzo de ese mismo año que estaban preparados, junto con sus aliados, para acabar con el padecimiento del pueblo iraquí y destruir las armas químicas y biológicas de Irak. Las armas de destrucción masiva fueron el desencadenante y justificación por parte del Presidente de los Estados Unidos de la invasión del país asiático y la consiguiente guerra. Del mismo modo, Bush también justificó el ataque preventivo a Irak por las sospechas de lazos entre Al Qaeda y el gobierno de Irak en el aquel momento. 

El 16 de marzo de 2003 en la cumbre de las Azores, George W. Bush junto con sus aliados Tony Blair y José María Aznar dieron un ultimátum al régimen de Saddam Hussein exigiéndole que se desarmase o abandonase el país asiático o por el contrario, el país seria intervenido militarmente para derrocar el gobierno de Saddam Hussein. Al día siguiente, George W. Bush dio un discurso donde le daba 48 horas al líder del régimen para cumplir el últimátum. "Todas las décadas de engaños y crueldades han llegado a su fin. Saddam Hussein y sus hijos deben abandonar Irak dentro de 48 horas. Su negativa a hacerlo tendrá como resultado un conflicto militar que comenzará cuando nosotros lo decidamos. [...] Es demasiado tarde para que Saddam Hussein permanezca en el poder. No es demasiado tarde para que los militares iraquíes actúen con honor y protejan a su país, permitiendo el ingreso en paz de las fuerzas de la coalición con el fin de eliminar las armas de destrucción masiva.[...] La amenaza terrorista a América y al mundo disminuirá en el momento en el que Saddam Hussein sea desarmado."Finalmente el 20 de marzo de 2003 tuvo lugar la invasión de Irak y la posterior guerra por parte del gobierno de los Estados Unidos bajo el amparo de la resolución 1441 de la ONU del año 2002 que establecía un vacío sobre si el uso de la fuerza militar estaba amparado o no.
Durante su primer mandato, George W. Bush inició una propuesta de modificación de la Ley migratoria de Estados Unidos. Con ella esperaba regularizar la situación de los inmigrantes irregulares que habitan en Estados Unidos. Se calcula que en este periodo eran entre 8 y 12 millones de inmigrantes, gran parte procedentes de México. La reforma propuesta por el presidente en el año 2004 tenía como objetivo regularizar la situación de los inmigrantes indocumentados de manera temporal ejerciendo trabajos en los cuales no hubiese nacionales disponibles. De esta manera, se regularizaría su situación de manera temporal y se les permitiría estar junto a su cónyuge e hijos, si los tuvieran. Del mismo modo, también se proponían requisitos a estos trabajadores temporales como el abono de una cuota de inscripción y la obligación de que el empleador fuese responsable de ellos. "Como nación que valora a los inmigrantes y depende de ellos, debemos tener unas leyes de inmigración que funcionen y que nos hagan sentir orgullosos". Esta propuesta obtuvo respuesta por parte del entonces Presidente de México Vicente Fox que aseguró que la iniciativa no cumplía los estándares y expectativas de su nación, aunque la consideraba un buen comienzo.
Durante el segundo mandato de George W. Bush, esta propuesta fue declinada por el Senado de los Estados Unidos lo que produjo que se retrasase su discusión hasta después del 2008, año en el que se celebraron las elecciones presidenciales y terminó la presidencia de George W. Bush.

Respecto a educación, la gran reforma llevada a cabo durante el primer mandato de George W. Bush fue la política "No Child Left Behind" (LCLB). Sin embargo, también llevó a cabo otra serie de iniciativas educativas como el impulso a la segregación entre sexos en las escuelas públicas, destinando para ello una parte de las partidas presupuestarias y otorgando ayudas a las escuelas que aplicasen esa iniciativa

Esta constituye la segunda gran política llevada a cabo en el primer mandato de George W. Bush. "Que ningún niño se quede atrás", en español fue una política educativa que se firmó el 8 de enero de 2002. Esta política reformó la Ley de Educación de Primaria y Secundaria (ESEA) y es la legislación que rige las directrices desde pre-escolar hasta la secundaria. El propio George W. Bush explicó que "se están quedando atrás demasiados de nuestros niños y jóvenes más necesitados". Con esta reforma comenzaron a exigirse exámenes anuales para medir los resultados de las escuelas de los 50 estados del país, sin embargo, esta reforma no constituía un cambio estructural ni una variación de las asignaturas o del plan de estudios, sino que supuso la fijación de un baremo de resultados para las escuelas. Los colegios que no alcanzasen ese mínimo de resultados en dos años recibirían una amonestación y sus alumnos tendrían la posibilidad de cambiarse de centro escolar. Si esta situación no cambiase en un periodo de 4 años se procedería al cierre de la escuela o a la renovación del profesorado. Esta reforma de la ESEA también introdujo la iniciativa Early Reading First para desarrollar la capacidad lectora de los estudiantes de pre-escolar estadounidenses, ampliando los recursos federales destinados a esta causa en un total de 300 millones de dólares por año fiscal. Con estas iniciativas aprobadas durante este primer mandato, el presidente George W. Bush pretendía mejorar el rendimiento de las escuelas públicas en 12 años y mejorar la calidad del profesorado mediante una designación de 2.8 millones de dólares. 

El programa federal Medicare fue objeto de reformas del primer mandato del 43º Presidente de los Estados Unidos. Aprobado en 1965, constituye el seguro de asistencia médica para aquellos estadounidenses que tienen 65 años o más. La reforma entró en vigor en el año 2006, pero fue aprobada durante el primer mandato del presidente George W. Bush, concretamente el 8 de diciembre de 2003, mediante la Ley de Prescripción, Mejoramiento y Modernización de medicamentos. Esta reforma permitió la inclusión y la rivalidad de aseguradoras privadas con el programa de asistencia a jubilados. Esta reforma fue objeto de numerosas críticas por parte del Partido Demócrata, quien afirmó que se trataba de "un proyecto derechista para privatizar el Medicare". Estas críticas fueron contestadas por el Partido Republicano quienes explicaron que la reforma "es el paso más grande para hacer del Medicare un sistema moderno". Esta reforma provocó que muchas empresas estadounidenses recortaran o suprimieran la asistencia sanitaria de los jubilados de su cobertura antes de la entrada en vigor de la reforma. En el año 2003, aproximadamente un tercio de las empresas que contaban con más de 500 empleados ofertaban el plan de asistencia médica para los jubilados que no gozaban del Medicare, en contraposición al año 1993 donde el porcentaje de empresas se encontraba en el 50.

Del mismo modo, mediante esta reforma también se introdujeron ayudas para sufragar el coste de los medicamentos recetados, en concreto, durante 2004, el gobierno puso en marcha ayudas anuales de 600 dólares por personas a aquellos beneficiarios con menos ingresos.

El 5 de noviembre de 2003, el presidente George W. Bush firmó una ley que castigaba y prohibía el aborto pasados los tres primeros meses de gestación. Esta práctica, denominada "de nacimiento parcial" (PBA) comenzó a estar penada con hasta dos años y medio de cárcel a los médicos que la practicasen. Con esta ley se limitaba la interrupción del embarazo en cualquier circunstancia, pues no se contemplaba la interrupción por peligro de la vida de la madre. El presidente de los Estados Unidos expresó en un discurso anterior que "combatiremos a cualquiera que trate de prohibirla [la ley] mediante los tribunales". 

En respuesta a la legislación firmada por el presidente, el 25 de abril de 2004 cientos de miles de personas se manifestaron en las calles de Washington para defender el derecho al aborto. Esta manifestación estuvo encabezada por la entonces senadora demócrata Hillary Clinton, quien durante la marcha pidió el voto para el candidato demócrata a la presidencia de Estados Unidos John Kerry y apuntó durante su discurso que: "Este Gobierno está lleno de gente que menosprecia las leyes de acoso sexual, que cree que la diferencia de salario entre hombres y mujeres no existe y que cree que la legalización del aborto es la peor abominación de nuestra historia constitucional". 

Finalmente, esta legislación que prohibía el método "de nacimiento parcial" fue declarada inconstitucional por un tribunal federal de los Estados Unidos. La jueza federal Phyllis J. Hamilton dictó en su fallo que esta ley no podría ser aplicada a la Federación de Paternidad Planeada, una organización no gubernamental que dirige las clínicas en las que se practican gran parte de los abortos en Estados Unidos.




</doc>
<doc id="37951" url="https://es.wikipedia.org/wiki?curid=37951" title="Acuariofilia">
Acuariofilia

La acuariofilia moderna es la afición a la cría de peces y otros organismos acuáticos en acuario, bajo condiciones controladas. Ha evolucionado tremendamente a lo largo de los siglos, desde el mantenimiento de carpas doradas con fines ornamentales en recipientes y estanques, desde hace 2000 años.

Existen referencias en la antigua China sobre la cría de peces dorados en depósitos cerámicos, a temperatura ambiente. El nivel de la acuariofilia era totalmente básico, sin sistemas de soporte de vida para los peces, que requerían especies resistentes y constantes cambios de agua. Este sistema arcaico se perpetuó hasta nuestros días a través de las peceras, y se superó con el desarrollo de los modernos acuarios.

Actualmente la acuariofilia es una afición que puede llegar a altos niveles de conocimiento y sofisticación, que traspasan la frontera de afición para convertirse en una verdadera ciencia, la acuariología.

El origen de la acuariofilia es muy antiguo, y va ligado al de la acuicultura. Los antecedentes de cultivo de peces, fundamentalmente carpas, se remontan a los sumerios, que ya utilizaban estanques para mantener peces vivos destinados a alimentación. Los romanos también criaban carpas destinadas al consumo.

En China, los bancales inundados para el cultivo de arroz eran utilizados para la cría de carpas, como fuente complementaria de proteína. De estos cultivos aparecieron formas coloreadas de carpines dorados y carpas koi que fueron seleccionadas por su belleza. Posteriormente fueron llevadas a Japón, donde se desarrollaron nuevas variedades.

Durante la Dinastía Song, gobernantes de China en el periodo que va de 960 a 1279, fue cuando se popularizó más el mantenimiento de peces dorados en recipientes de cerámica.

Sobre el año 1750 se introdujo en Europa los primeros peces de colores, pero hasta el siglo XX no se empezó a intentar la cría de especies tropicales, debido a la dificultad para obtener ejemplares vivos (tenía que hacer largos viajes), y la dificultad de mantenimiento (aparte de condiciones ambientales y alimentación, no existían sistema de calefacción adecuados).

Sobre los años 1930 el acuario se empezó a convertir en un objeto de decoración en algunos hogares y lugares públicos.

En los inicios de la acuariofilia, dos especies destacan por encima de todas: el carpín dorado y la carpa koi.

Los primeros peces que se mantuvieron con éxito en pequeñas vasijas cerámicas fueron variedades doradas de "Carassius auratus", un ciprínido de tamaño medio de gran resistencia y colorido, capaz de resistir las difíciles condiciones de mantenimiento de la época. Aunque también se utilizaba en estanques ornamentales, este se convirtió en el reino de las carpas koi, un ciprínido similar pero de mucho mayor tamaño, más adecuado para los grandes estanques.

Siglos de cría selectiva llevaron al desarrollo de variedades muy diferentes a las salvajes, en morfología y en colorido. Las carpas koi fueron por el camino del colorido (y en menor medida la forma), al ser peces de estanque visibles fundamentalmente por su parte dorsal. Se han desarrollado numerosas variedades, algunas enormemente cotizadas y de gran belleza.

Del carpín dorado inicialmente se desarrollaron variedades de aletas más largas y mayor colorido, y con el tiempo variantes morfológicas muy diferentes a la forma salvaje, con aletas duplicadas y extrañas formas corporales, en algunos casos bellas, en otros de discutible moralidad, como el caso de las variedades de ojos de burbuja.

Los peces dorados fueron los primeros peces en ser mantenidos por los acuariófilos, pero en el planeta existían otros cientos de especies de peces de atractivos colores e interesantes comportamientos adecuados para ser mantenidos en acuarios domésticos, pero que requerían unas condiciones mucho más exigentes. Además, la gran mayoría de ellos, al ser tropicales, requerían temperaturas estables y superiores a la mayoría de países en los que se practicaba la afición. Además, la gran mayoría de especies eran altamente delicadas, al ser capturadas en su medio natural.

Las primeras especies que se consiguieron reproducir con éxito fueron los peces paraíso y los luchadores de Siam, peces resistentes capaces de aguantar temperaturas medianamente bajas, y con un órgano de respiración auxiliar (el laberinto), que les permite respirar aire atmosférico, algo fundamental ante los inexistentes sistemas de aireación del momento.

Con el desarrollo de termocalentadores, bombas de aire y filtros, se fue ampliando el número de especies mantenidas con éxito. Y con el desarrollo del cultivo de peces ornamentales, se fueron obteniendo ejemplares mucho más resistentes a la cautividad, incluso en especies que eran consideradas de dificultad, como el Pez Disco. 
Los primeros acuarios marinos tropicales eran solo de peces, con peces payaso, damiselas y pomacántidos. Presentaban las mismas dificultades que los peces tropicales de agua dulce, y algunas más.

Los peces marinos son más sensibles a las variaciones: en general, el mar es un medio mucho más estable que cualquier río o lago. Además, existe el agravante del agua de mar, que hay que ir al mar a por ella, o prepararla a base de agua dulce y sales marinas (muy diferente a la sal común).

El avance en el conocimiento y el desarrollo de la cría de peces, la tecnología y las sales marinas hizo que se popularizasen los acuarios marinos a finales de los años 1980. En la actualidad es posible montar un acuario de arrecife doméstico, algo impensable antes de los años 1980. La tendencia actual en acuarios de arrecife es la de limitar el número de peces e incrementar el volumen de roca viva (sustrato rocoso poroso colonizado por infinidad de pequeños microorganismos) y de otros invertebrados, tales como cnidarios (corales, anémonas...) y moluscos (caracoles, bivalvos...). Algunos complementos tecnológicos importantes en un acuario marino, sobre todo si en él se trata de representar un ecosistema de arrecife, son la iluminación de alta intensidad y espectro continuo (conseguida generalmente con "lámparas de descarga" de halogenuros metálicos) y la espumación cíclica del agua, mediante un dispositivo denominado "skimmer" o "espumador" que elimina residuos orgánicos mediante separación fraccionada de la espuma producida en una columna, por la cual pasa continuamente el agua del acuario.

El principio básico de la acuariofilia moderna es la recreación de un ecosistema acuático artificial en el que puedan desarrollar un comportamiento natural todo tipo de especies acuáticas, y estabilizado a través de sistemas técnicos auxiliares. Ya no es una afición centrada en el mantenimiento exclusivo de peces, sino una afición basada en una ciencia, la acuariología. Existen muchos acuarios sin peces o casi sin peces, creeados específicamente para plantas acuáticas (disciplina denominada paisajismo acuático), invertebrados, anfibios y reptiles acuáticos.

El mercado mundial mueve cerca de mil millones de dólares por año, y la industria de equipamientos y accesorios (incluyendo libros y revistas sobre el tema) es de más de 15.000 millones de dólares.

En Estados Unidos la acuariofilia ocupa el tercer lugar en el orden de pasatiempos más practicados (el primero es la fotografía y el segundo la filatelia). En Japón existen cerca de 1,2 millones de acuaristas (los japoneses creen que los acuarios "traen suerte"). En Brasil existen más de 500.000 acuarios, para un total de 2 millones de acuarios en todo el mundo, aproximadamente.

La cría de especies ornamentales es una fuente de diseminación de peces no nativos (exóticos) en cuerpos de agua. Esto ocurre porque los peces que originalmente se mantuvieron en estanques suelen ser liberados en medios silvestres, causando así una de las principales causas de pérdida de biodiversidad del planeta.

Los peces de acuario no deberían ser liberados en un medio ambiente que no sea el que les corresponde. Las especies invasoras pueden afectar seriamente a sus nuevos hogares al aprovecharse o competir con las especies nativas. Si un acuarista desea deshacerse de su pez, debe donarlo, venderlo o sacrificarlo con anestésicos o por congelamiento. Algunos ejemplos destacados de invasiones por parte de peces exóticos son:

Algunas de las características que hacen que una especie exótica sea una potencial especie invasora, son:


Las principales familias de peces tropicales de agua dulce usadas en acuariofilia son las siguientes:



</doc>
<doc id="37956" url="https://es.wikipedia.org/wiki?curid=37956" title="Red de fibra óptica">
Red de fibra óptica

Las redes de fibra óptica se emplean en telecomunicación y redes de comunicaciones o redes de computadoras.

En las redes de comunicaciones por fibra óptica (FO) se emplean sistemas de emisión láser. Las ondas de luz tienen una frecuencia alta y la capacidad de una señal para transportar información aumenta con la frecuencia.

En los primeros tiempos de la FO se utilizaron también emisores LED, aunque desde el 2007 están prácticamente en desuso.

Las redes de área local (comúnmente abreviadas LAN, del idioma inglés "Local Area Network") de FO son ampliamente utilizadas para comunicación a larga distancia, proporcionando conexiones transcontinentales y transoceánicas, ya que una ventaja de los sistemas de fibra óptica es la gran distancia que puede recorrer una señal antes de necesitar un repetidor o regenerador para recuperar su intensidad. En la actualidad, los repetidores de los sistemas de transmisión por FO están separados entre sí unos 100 km, frente a aproximadamente 1,5 km en los sistemas eléctricos. Los amplificadores ópticos recientemente desarrollados pueden aumentar todavía más esta distancia.

Una aplicación cada vez más extendida de la FO son las LAN. Las LAN están formadas por un conjunto de computadoras que pueden compartir datos, aplicaciones y recursos, por ejemplo impresoras. Las computadoras de una LAN están separadas por distancias de hasta unos pocos kilómetros, y suelen usarse en oficinas o campus universitarios. Una LAN permite la transferencia rápida y eficaz de información entre un grupo de usuarios y reduce los costes de explotación. Este sistema aumenta el rendimiento de los equipos y permite fácilmente la incorporación de nuevos usuarios a la red. El desarrollo de nuevos componentes electroópticos y de óptica integrada aumentará aún más la capacidad de los sistemas de fibra.

Otros recursos informáticos conectados son las redes de área amplia (WAN) y las centralitas particulares (PBX). Las WAN son similares a las LAN, pero conectan entre sí computadoras separadas por distancias mayores, situadas en distintos lugares de datos de corta duración empleados por la mayoría de las aplicaciones informáticas.
Al momento de conectar las WAN se hace a través de sus intefaces seriales, para conectar "router" con pc a través de las interfaces ethernet.

Es un cable diseñado para ser utilizado en estructuras aéreas, comúnmente redes eléctricas o de distribución energética (postes o torres), posee características técnicas que permiten soportar condiciones ambientales extremas y la forma de instalación es a través de soportes y abrazaderas especiales.

Es un cable diseñado para permanecer sumergido en el agua. Estos cables logran alcanzar grandes distancias, por lo que son muy utilizados para conectar continentes. Adentro, en su composición, disponen de cables de energía para alimentar los amplificadores ópticos que normalmente forman parte de sistema de comunicaciones y, al encontrarse ubicados a grandes profundidades, se dificulta su mantenimiento, debiendo recurrirse a robots subacuáticos (ROVs) y buques/ tripulaciones especialmente dedicados a esas tareas como el C/ V "Île-de-Bréhat" o el C/ V "León Thevenin". 

Hubo algunos ataques de tiburones a cables submarinos que exigió mayor protección de los mismos.

El cable OPGW (Optical Ground Wire) es un cable que tiene fibras ópticas insertadas dentro de un tubo, en el núcleo central del cable de tierra de los circuitos eléctricos. Sus fibras ópticas están completamente protegidas y rodeadas por pesados cables a tierra. Es utilizado por las compañías eléctricas para suministrar comunicaciones a lo largo de las rutas de las líneas de alta tensión (generalmente montados en el hilo de guarda) y poseen gran disponibilidad en el servicio de transmisión de información.

Las redes por FO son un modelo de red que permite satisfacer las nuevas y crecientes necesidades de capacidad de transmisión y seguridad demandadas por las empresas operadoras de telecomunicación, todo ello además con la mayor economía posible.

Mediante las nuevas tecnologías, con elementos de red puramente ópticos, se consiguen los objetivos de aumento de capacidad de transmisión y seguridad.

Cuando las empresas encargadas de abastecer las necesidades de comunicación por medio de FO necesitaron mayor capacidad entre dos puntos, pero no disponían de las tecnologías necesarias o de unas FO que pudieran llevar mayor cantidad de datos, la única opción que les quedaba era instalar más FO entre estos puntos. Pero para llevar a cabo esta solución había que invertir mucho tiempo y dinero, o bien añadir un mayor número de señales multiplexadas por división en el tiempo en la misma FO, lo que también tiene un límite.

Es en este punto cuando la multiplexación por división de longitud de onda (WDM) proporcionó la obtención, a partir de una única fibra, de muchas fibras virtuales, transmitiendo cada señal sobre una portadora óptica con una longitud de onda diferente. De este modo se podían enviar muchas señales por la misma FO como si cada una de estas señales viajara en su propia fibra. Existen básicamente dos tipos de WDM, 


Los diseñadores de las redes utilizan muchos elementos de red para incrementar la capacidad de las fibras ya que un corte en la FO puede tener serias consecuencias.

En las arquitecturas eléctricas empleadas hasta ahora, cada elemento realiza su propia restauración de señal. Para un sistema tradicional de fibras óticas con muchos canales en una fibra, una rotura de la fibra podría acarrear el fallo de muchos sistemas independientes. Sin embargo, las redes ópticas pueden realizar la protección de una forma más rápida y más económica, realizando la restauración de señales en la capa óptica, mejor que en la capa eléctrica. Además, la capa óptica puede proporcionar capacidad de restauración de señales en las redes que actualmente no tienen un esquema de protección. Así, implementando redes ópticas, se puede añadir la capacidad de restauración a los sistemas asíncronos embebidos sin necesidad de mejorar los esquemas de protección eléctrica.

En los sistemas que utilizan únicamente multiplexación eléctrica, cada punto que demultiplexa señales necesitará un elemento de red eléctrica para cada uno de los canales, incluso si no están pasando datos en ese canal. En cambio, si lo que estamos utilizando es una red óptica, solo aquellas longitudes de onda que suban o bajen datos a un sitio necesitarán el correspondiente nodo eléctrico. Los otros canales pueden pasar simplemente de forma óptica, proporcionando así un gran ahorro de gastos en equipos y administración de red.

Otro de los grandes aspectos económicos de las redes ópticas es la capacidad para aprovechar el ancho de banda, algo que no sucedía con las fibras simples. Para maximizar la capacidad posible en una FO, las empresas de servicios pueden mejorar sus ingresos con la venta de longitudes de onda, independientemente de la tasa de datos ("bit rate") que se necesite. Para los clientes, este servicio proporciona el mismo ancho de banda que una fibra dedicada, entre otros.




</doc>
<doc id="37958" url="https://es.wikipedia.org/wiki?curid=37958" title="Enlace infrarrojo">
Enlace infrarrojo

Los enlaces infrarrojos se encuentran limitados por el espacio y los obstáculos. El hecho de que la longitud de onda de los rayos infrarrojos sea tan pequeña (850-900 nm), hace que no pueda propagarse de la misma forma en que lo hacen las señales de radio.

Es por este motivo que las redes infrarrojas suelen estar dirigidas a oficinas o plantas de oficinas de reducido tamaño. Algunas empresas, van un poco más allá, transmitiendo datos de un edificio a otro mediante la colocación de antenas en las ventanas de cada edificio.

Por otro lado, las transmisiones infrarrojas presentan la ventaja, frente a las de radio, de no transmitir a frecuencias bajas, donde el espectro está más limitado, no teniendo que restringir, por tanto, su ancho de banda a las frecuencias libres.

A la hora de transmitir, las estaciones infrarrojas pueden usar tres tipos de métodos para ello: punto a punto, casi-difuso y difuso. 

En el modo "punto a punto", el tipo de emisión por parte del transmisor se hace de forma direccional. Por ello, las estaciones deben verse directamente, para poder dirigir el haz de luz directamente de una hacia la otra. Por este motivo, este es el tipo de red inalámbrica más limitado, pues a todos los inconvenientes de las comunicaciones infrarrojas hay que unir el hecho de tener que colocar las estaciones enfrentadas. Este método se suele usar en redes inalámbricas Token Ring, donde el anillo está formado por una unión de enlaces punto a punto entre las distintas estaciones, conformando cada uno de los segmentos.

En el modo "casi-difuso", el tipo de emisión es radial; esto es, la emisión se produce en todas direcciones, al contrario que en el modo punto a punto. Para conseguir esto, lo que se hace es transmitir hacia distintas superficies reflectantes, las cuales redirigirán el haz de luz hacia la/s estación/es receptora/s. De esta forma, se rompe la limitación impuesta en el modo punto a punto de la direccionalidad del enlace. En función de cómo sea esta superficie reflectante, podemos distinguir dos tipos de reflexión: pasiva y activa. En la reflexión pasiva, la superficie reflectante simplemente refleja la señal, debido a las cualidades reflexivas del material. En la reflexión activa, por el contrario, el medio reflectante no sólo refleja la señal, sino que además la amplifica. En este caso, el medio reflectante se conoce como satélite. Destacar que, mientras la reflexión pasiva es más flexible y barata, requiere de una mayor potencia de emisión por parte de las estaciones, debido al hecho de no contar con etapa repetidora.

El modo de emisión "difuso", por otro lado, se diferencia del casi-difuso en que debe ser capaz de abarcar, mediante múltiples reflexiones, todo el recinto en el cual se encuentran las estaciones. Obviamente, esto requiere una potencia de emisión mayor que los dos modos anteriores, puesto que el número de rebotes incide directamente en el camino recorrido por la señal y las pérdidas aumentan.

Según el caso que comentábamos antes de las empresas que utilizaban enlaces de un edificio a otro mediante antenas en las ventanas, podemos observar que, obviamente, este enlace será punto a punto, mientras que en las redes interiores lo más lógico es realizar enlaces difusos.

Dependiendo de las necesidades de la red inalámbrica, esta puede adoptar dos configuraciones posibles:

1) Peer to Peer o Ad Hoc: Es el tipo de configuración más sencilla, en el que dos o más estaciones se conectan directamente, de forma visible, formando una especie de anillo.

2) Modo Infraestructura: En este tipo de configuración, se añade un elemento llamado "punto de acceso" (más conocido como AP (Access Point)). Dicho elemento, permite formar redes de menor tamaño que serán interconectadas a través de él. En ocasiones, dependiendo del tipo de punto de acceso, las redes pueden ser de tipos distintos, siendo este dispositivo el encargado de realizar la conversión entre señales.




</doc>
<doc id="37960" url="https://es.wikipedia.org/wiki?curid=37960" title="Cólera">
Cólera

El cólera es una enfermedad infecto-contagiosa intestinal aguda, provocada por los serotipos O1 y O139 de la bacteria "Vibrio cholerae", que produce una diarrea secretoria caracterizada por deposiciones acuosas abundantes, pálidas y lechosas, semejantes al agua del lavado de arroz, con un contenido elevado de sodio, bicarbonato y potasio, y una escasa cantidad de proteínas.

En su forma grave, se caracteriza por una diarrea acuosa de gran volumen que lleva rápidamente a la deshidratación del organismo.

La enfermedad ha recibido varios nombres a lo largo de la historia, tales como "enfermedad azul", "enfermedad negra", "fiebre álgida grave", "pasión colérica", "diarrea colérica", "cholera morbus", "cholera gravis" y, simplemente "cólera".

El origen del término es debatido. Puede provenir del griego χολή ("bilis o hiel") y ῥέω ("corriente"), es decir, corriente o flujo de bilis; o del griego χολέρα derivado de χολή, que significa "bilis".

Heinrich Häser y Aulo Cornelio Celso creyeron que el cólera se derivaba de la bilis (por eso se lo llamó "cholera morbus", "enfermedad de la bilis"), Alejandro de Tralles, que provenía de los intestinos, mientras que Rudolf Kraus y Alexis Littré estaban a favor de su transmisión por medio del agua de los arroyos.

Las primeras descripciones de la enfermedad se pueden ver en los escritos de Hipócrates (460-377 a. C.), Galeno (129-216) y Wang Shuhe (180-270). En la historia de la India antigua, existen escritos que describen la enfermedad en las poblaciones asentadas en la ribera del río Ganges. Sin embargo, no es demostrable que dichas descripciones sean producidas específicamente por el "V. cholerae", ni tampoco es claro que se haya presentado en la forma epidémica que actualmente se conoce de la enfermedad.

La primera referencia en la historia documentada occidental de la existencia del cólera en India, se encuentra poco después de la llegada de Vasco de Gama a Calicut el año 1498. Fue en el año 1503 cuando se describe una epidemia de cólera asiática en el ejército del soberano de Calicut; y posteriormente en el año 1543 en la población de la ciudad.

La primera referencia documentada de un brote de cólera fuera de la India es del año 1629, y ocurrió en Yakarta, de la isla de Java.

Desde esa época hasta 1817, hay sesenta y cuatro reportes de brotes relativamente aislados de cólera, primeramente en la región de Goa, el primer territorio conocido por los europeos en India; y posteriormente en otras localidades de la costa oeste de dicho país, avanzando progresivamente hacia el este y el norte. En la costa de Coromandel se describen epidemias de la enfermedad entre los años 1772 y 1782. En Ganjam el cólera era prevalente en el año 1781. En Uttar Pradesh se desató una epidemia en abril de 1783. Entre 1781 y 1782 la enfermedad se había extendido a Sri Lanka y Birmania. Otros brotes epidémicos en India ocurrieron durante 1787 y 1794 en Arcot y Vellore; en el año 1790 nuevamente en Ganjam; en el año 1814 en Bengala. Fuera de India, destacan brotes en Mauricio y Reunión en 1775, y en Sri Lanka el año 1804. Tras un período de receso de los brotes, se inicia la primera pandemia de cólera el año 1817.

En 1854 el médico italiano Filippo Pacini describió el bacilo "vibrio cholerae", que en el mismo año también fue descrito por el catalán Joaquín Balcells y Pascual y en 1856 probablemente por los dos portugueses António Augusto da Costa Simões y José Ferreira de Macedo Pinto. También en 1854, John Snow, médico británico, demostró que el cólera era causado por el consumo de aguas contaminadas con materias fecales, al comprobar que los casos de esta enfermedad se agrupaban en las zonas donde el agua consumida estaba contaminada con heces. En 1884, Robert Koch, desconocedor del trabajo de Filippo Pacini, aisló e identificó la bacteria vibrio que causaba el cólera. Dada su gran preeminencia, el descubrimiento fue ampliamente difundido.

Tras este descubrimiento, en 1885, la vacuna anticolérica fue preparada y administrada por primera vez a miles de personas gracias al doctor Jaime Ferrán y Clúa.

A lo largo del siglo XIX, el cólera se propagó por el mundo desde su reservorio original en el delta del Ganges, en la India. Seis pandemias en sucesión mataron a millones de personas en todos los continentes. La actual pandemia (la séptima) comenzó en el sur de Asia en 1961 y llegó a África en 1971 y a América en 1991. En la actualidad, el cólera es endémico en muchos países.
Aunque otros estudios refieren a entre cuatro y seis las pandemias de cólera en el siglo XIX, Pollizter, encargado de la OMS de redactar una monografía sobre el tema en 1959, refiere a seis: 1817, 1829, 1852, 1863, 1881-1896 y 1899-1923.

En agosto de 1817 la enfermedad se presentó en Calcuta con una virulencia mayor que la habitualmente descrita. Desde ahí se extendió rápidamente por toda Bengala, luego hacia toda la India, por el noreste, pasando por Vindhya Pradesh, Uttar Pradesh, Delhi, Punyab, alcanzando Surat y Bombay; por el sur, pasando por Hyderabad, Bangalore, Srirangapatna; y por Ganjam y Chennai. Desde ahí, alcanzó la isla de Madura. En diciembre de 1818, la pandemia llegó a Sri Lanka, comenzando en Trincomalee, y luego sumándose los puertos de Jaffna y Colombo en 1819, desde donde la enfermedad se extendió por toda la isla.

La pandemia llegó a Birmania y al antiguo reino de Siam en 1819. Bangkok fue alcanzado por la ruta marítima en 1820 y desde ahí la enfermedad, devastadora, se extendió por toda la región. Ese mismo año llegó a Malaca, Penang y Singapur. Las islas de Indonesia, Borneo y Filipinas también fueron alcanzadas este año. El año 1822, desde Java la enfermedad llegó a Japón.

China se vio afectada tempranamente (1817) por la vía terrestre, pero la enfermedad se extendió con gran intensidad después de 1820, cuando entró por los puertos de Cantón, Wenzhou y Ningbo. El norte de China fue afectado en 1821, destacando Pekín, y entre 1822 y 1824 la enfermedad alcanzó los territorios del centro de China.

El Oriente Medio y los países del golfo Pérsico fueron afectados desde 1819, apareciendo en la ciudad de Alepo, en Siria; luego, en 1821, entró a Omán por Mascate, y luego a Irak por Basora, afectando también la isla de Baréin. En Bagdad produjo una gran mortandad entre el ejército sirio, que estaba atacando la ciudad en esos momentos. El posterior avance de dicho ejército hacia el norte llevó la enfermedad a Tiflis (en la actual Georgia) y Astracán en Rusia entre los años 1822 y 1823. Llegó a Turquía por la ciudad de Alejandreta en 1823.

Finalmente, los lugares más alejados que fueron afectados por esta pandemia, fueron Mauricio a través de su puerto Port Louis, proveniente de Sri Lanka; y la isla de Zanzíbar en Tanzania.

La segunda pandemia comenzó en el año 1829 en Persia, Afganistán, Bujará (Uzbekistán) y Oremburgo (Rusia). Alcanzó luego Rasht (Irán) y Bakú (Azerbaiyán). Desde allí se desplegó por toda el área que se conoce como Oriente Próximo. Las autoridades rusas realizaron grandes esfuerzos, con cordones y cuarentenas, para detener el avance de la epidemia hacia el norte, sin embargo, en el otoño de 1830, el cólera llega a Moscú. En el año 1831, la enfermedad siguió avanzando hacia el norte y el oeste, alcanzando San Petersburgo y Arcángel, y desde ahí a Finlandia; llegó a Polonia por los soldados polacos que se encontraban en ese momento en un levantamiento contra el Imperio ruso, que siguió con una guerra hasta el año 1831. La emigración de soldados polacos hacia el oeste, expandió la enfermedad hacia el resto de Europa. Por la llegada de soldados enfermos, entró a Galicia (actual sector de Ucrania) y de ahí a Austria, llegando a Viena en agosto de 1831. En junio de ese año también había llegado a Hungría. Pese a los esfuerzos de las autoridades por evitar su llegada a Prusia, la enfermedad ingresó en dicho país desde Riga (de la actual Letonia) al puerto de Gdansk desde donde se extendió rápidamente, afectando Berlín y Hamburgo para el 1832.

A Inglaterra, dado el importante contacto comercial entre los puertos europeos y de la isla, el cólera llegó en junio de 1831, en Medway, al suroeste de Londres, a partir de enfermos que estaban en barcos en cuarentena provenientes de Riga. En octubre llegó a Sunderland y luego fueron apareciendo casos en Newcastle, Gateshead, Edimburgo, y, en febrero de 1832, en Londres. Luego, siguió extendiéndose por varias ciudades de la isla. Ese año se contabilizaron 14 796 casos de cólera con 5432 muertos.

Otros países europeos se fueron sumando a la pandemia: A Irlanda llegó en marzo de 1832 por Dublín; a Francia en marzo de 1832, por Calais y seguidamente en París; a Bélgica en la primavera, a través de las villas aledañas a Francia; a los Países Bajos en junio, por Scheveningen; a Noruega en el otoño, por Drammen, Moss y Oslo; a Portugal, en diciembre, por Douro y luego, en abril del año siguiente, llega a Lisboa; a España llega en agosto de 1833. Desde el puerto de Ceuta, en España, la enfermedad cruzó hacia el norte de África. En 1834 la enfermedad llega a Suecia.

En América, afectó primeramente a Canadá, por el puerto de Quebec en junio de 1832, desde donde se extendió rápidamente por el río San Lorenzo y sus afluentes; en Estados Unidos se presentó el 23 de junio en Nueva York, y el 5 de julio en Filadelfia. Desde ahí, recorrió el país pasando por las Montañas Rocosas hasta llegar a la costa Oeste del continente del norte. Se cree que llegó a Perú y Chile en 1832; a México y Cuba llegó en 1833; a Las Guayanas, Nicaragua y Guatemala en 1837.

La segunda pandemia presentó un decrecimiento en el año 1834 en Europa. Sin embargo, el año 1835, hubo focos de recrudecimiento en Francia (Marsella, Tolón y otras ciudades del sur del país), desde el sur de Francia llegó a Italia, donde se diseminó, llegando en el año 1837 a Malta. En 1836, desde el norte de Italia, la enfermedad pasó a Suiza por el Cantón del Tesino y se extendió por el Tirol. Desde ahí pasó a Baviera (y luego a Múnich en octubre de 1836). En el verano de 1837, la enfermedad volvió a recrudecer en Prusia, Hamburgo y Polonia, siendo los últimos embates de la primera oleada de esta pandemia en Europa.

Las tropas francesas en Argelia diseminaron la enfermedad por ese país. Entre 1835 y 1837, se extendió por Egipto, luego hacia el oeste a Libia (por Tripolitania) y Túnez; y por el sur a Sudán y Etiopía. Entre 1836 y 1837 reapareció en Somalia y Zanzíbar.

Al este de la India, (país dónde la enfermedad se mantuvo relativamente inactiva), se reportaron brotes en Indonesia y Filipinas hasta el 1830; en Japón reapareció en 1831; en Australia se presentó en 1832; en China, hubo un brote en Cantón en 1835; En Bengala, reapareció en 1837, desde donde se expandió hacia el este, hasta llegar a Afganistán en 1839. En 1840, desde Bengala, se trasladaron tropas hacia China y las Colonias del Estrecho, extendiendo la enfermedad a dichos territorios. Desde Cantón, la enfermedad se trasladó por el río Irawadi a Birmania, llegando a Rangún en 1842; desde China la enfermedad volvió a sus comienzos de la pandemia, extendiéndose por sus rutas comerciales desde Kasgar y Yarkand, a Kokand y Bujará en 1844. Por otro lado, desde Afganistán, dónde la enfermedad alcanzó a Kabul en 1844, se extendió a Pakistán, por Punyab y luego Karachi en 1845. Hacia India, por estar ruta, llegó a Delhi ese mismo año. A Rusia, la enfermedad retornó por Irán, a través de la ruta Mashhad - Teherán - Tabriz - Derbent.

En Bengala, el cólera recrudeció entre los años 1845 y 1846, avanzando por la ruta marítima hacia India, Madrás por el este y luego Bombay por el oeste, pasando por Sri Lanka. En mayo de 1846, llegó desde la India a Adén y Moca (en Yemen), y Yeda en Arabia Saudita. Luego se extendió hacia Omán. Desde Arabia, se extendió por toda Persia, y avanzó hacia el norte convirtiéndose en una nueva oleada de la enfermedad hacia Rusia, sumándose al foco que aún se mantenía latente en Derbent, en abril de 1847. La oleada se extendió por las costas del mar Caspio, afectando Astracán, subiendo luego por el río Volga. Hacia el oeste llegó a Tiflis (Georgia), y siguió extendiéndose en esa dirección por las costas del mar Negro; hacia el noroeste, avanzó por el Cáucaso al interior de Rusia. Por la cuenca del río Ural, la enfermedad llegó a Oremburgo, y de ahí se extendió por Siberia hasta llegar a Tobolsk en julio de 1847. En el verano, la enfermedad abarcó prácticamente toda Rusia, alcanzando Moscú en septiembre. Esta última oleada de la pandemia en Europa, culminó con la llegada por el norte a Riga el año 1848, desde donde alcanzó Noruega.

De esta forma, en el año 1848, la enfermedad estaba presente en Europa desde Noruega en el norte hasta la península balcánica por el sur; abarcaba Inglaterra, Escocia e Irlanda por el noroeste; y hasta España por el oeste. Ese mismo año, la enfermedad llegó a Estados Unidos. Por otro lado, recrudeció en Anatolia, Siria, Palestina y Persia. Afectaba también el norte de África.

La tercera pandemia, a diferencia de las dos primeras, no siguió un curso lineal, sino que respondió a la suma de recrudecimientos locales en diversas áreas, sumado a migraciones e importaciones sucesivas.

A partir de focos en India en 1852, recrudeció en Persia y Mesopotamia; paralelamente, una extensa oleada afectaba todo el norte de Europa, América del Norte, México y las Indias orientales.

En el año 1854, se mantenía en estas zonas, y avanzaba por Europa, por intermedio de las tropas francesas que participaban en la Guerra de Crimea, a Grecia y Turquía; en América, la enfermedad alcanzaba América del Sur por Colombia.

En 1855, sin dejar las zonas afectadas previamente, avanzó desde la India a Siria y Asia Menor por la ruta de Arabia. En África, apareció en Egipto y desde ahí avanzó a Sudán, Marruecos, y, por primera vez, afectó Cabo Verde. En Europa, avanzó a Italia, Austria y Suiza. En América, cesó en Estados Unidos, pero apareció en Venezuela y Brasil.

Entre los años 1856 y 1858, la enfermedad retrocedió en Europa, con excepción de focos en España y Portugal (inclusive Madeira).

Entre los años 1857 y 1859, la enfermedad, que ya había llegado tempranamente (1852) por Indonesia, recrudeció en China y Japón. En 1858 reapareció en Filipinas y en 1859 apareció en Corea.

La enfermedad fue descubierta por Filippo Pacini en el año 1854, y posteriormente Jaime Ferran i Clua elaboró la primera vacuna. La infección generalmente es benigna o asintomática, pero, a veces, puede ser grave. Aproximadamente una de cada 20 personas infectadas puede tener la enfermedad en estado grave, caracterizada por diarrea acuosa profusa, vómitos y entumecimiento de las piernas. En estas personas, la pérdida rápida de líquidos corporales lleva a la deshidratación y a la postración. Sin tratamiento adecuado, puede ocurrir la muerte en cuestión de algunas horas.

El cólera es endémico en más de 50 países y ha producido varias epidemias de alcance mundial. Desde 1817, siete pandemias de cólera se han extendido desde Asia al resto del mundo. La última de ellas ocurrió el año 1961 y afectó entre 3 y 5 millones de personas por año, muriendo alrededor de 120.000 personas.

En enero de 1991 surgió una epidemia de cólera en varios países del norte de América del Sur que se difundió rápidamente. El Brote de cólera en Haití de 2010 siguió al terremoto producido en enero de 2010.

El cólera ha sido poco frecuente en los países industrializados durante los últimos 100 años; no obstante, esta enfermedad aún es común en otras partes del mundo, incluyendo el subcontinente Indio, Sureste Asiático, Latinoamérica y el África Subsahariana.

Se presenta como epidemia donde existen condiciones sanitarias deficientes, hacinamiento, guerra e inanición. Áreas endémicas son: Asia, África, el Mediterráneo y más recientemente, América Central y del Sur. Un tipo de "Vibrio" ha estado asociado con los mariscos, especialmente ostras crudas. También son factores de riesgo residir en áreas endémicas o viajar por ellas, así como beber agua contaminada o no tratada.

Existen dos tipos de serotipos el O1 y el O139 y son aquellos que causan brotes epidémicos. El O1 causa la mayor parte de los brotes, mientras que el O139, que se identificó por vez primera en Bangladesh en 1992, está confinado al Asia Sudoriental. Las cepas distintas de la O1 y la O139 pueden causar diarrea leve, pero no dan origen a epidemias.
Los principales reservorios de V. cholerae son los seres humanos y las fuentes de agua salobre y los estuarios; a menudo hay una relación con la multiplicación de algas. Estudios recientes indican que el calentamiento del planeta crea un ambiente favorable para los vibriones.

Una persona puede adquirir cólera bebiendo líquido o comiendo alimentos contaminados con la bacteria del cólera. Durante una epidemia, la fuente de contaminación son generalmente las heces de una persona infectada. La enfermedad puede diseminarse rápidamente en áreas con tratamientos inadecuados de agua potable y aguas residuales. La bacteria del cólera también puede vivir en ríos salobres y aguas costeras.

Es poco común la transmisión del cólera directamente de una persona a otra; por lo tanto, el contacto casual con una persona infectada no constituye un riesgo para contraer la enfermedad.
El "V.cholerae" produce una potente toxina que, en las células de la mucosa intestinal, bloquea la GTPasa de la subunidad alfa de una proteína G, impidiendo que esta proteína se inactive. Al mantenerse activa, de igual manera mantiene activa a adenilil ciclasa que continúa produciendo AMPc (AMP cíclico) en forma sostenida, el cual se une a los canales de Cl. Estos canales se abren y el ion sale a la luz del intestino de forma masiva, arrastrando consigo a otros iones y provocando una excesiva secreción intestinal de agua con sodio, bicarbonato y potasio, que excede su capacidad de absorción.

Aparición brusca sin periodo de incubación (Farreras: periodo de 2-3 días que varía desde 5 h hasta 5 días) a diferencia de la salmonelosis.






Por todo lo anterior, nos encontramos ante un paciente que podría presentar uno o varios de los siguientes:


Excepto en sus formas más avanzadas se mantiene el estado de conciencia indemne. Cuando la pérdida de electrolitos es intensa pueden sobrevenir vómitos como consecuencia de la acidosis e intensos calambres musculares fruto de la hipopotasemia. En estos casos graves aparecen signos intensos de deshidratación, hipotensión y oliguria.

El cólera se sospecha frente a una diarrea muy acuosa, en gran volumen y alta frecuencia en zonas endémicas. Es un cuadro con poca inflamación.



Existen otras exploraciones que aunque tienen su importancia en el diagnóstico de epidemias no tiene relevancia clínica para un caso concreto:

La rehidratación agresiva es la medida más importante, con lo cual la mortalidad baja de más de un 50 % a menos de un 0,2 %.

Solución salina. Hay que dar una gran cantidad de sueros, las vías de administración son:

Estos sueros deberán contener sodio, cloro, potasio y bicarbonato dependiendo de lo que necesite en cada momento (se calcula en función de las pérdidas). Como fórmula de sueros orales preparada tenemos la limonada alcalina, pero si no tenemos eso a mano habrá que darle lo que sea (agua con limón, bebidas isotónicas e incluso carbonatadas)
(OMS: 1L de agua 2,6 g NaCl, 1,5 g KCl, 2,9 g citrato trisódico y 13,5 g glucosa),

El uso de antibióticos reduce la duración del cuadro diarreico en un 50 % y se recomienda para pacientes con diarrea moderada o severa.
Están indicados para erradicar la bacteria, pero, el manejo inicial del paciente está basado en la reposición enérgica de líquidos, ya que la deshidratación es la que puede llevar a la muerte del paciente. 

Reducen la duración de la diarrea, los requerimientos de líquidos y el periodo de excreción del vibrio. Se recomienda el uso de las tetraciclinas (como la oxitetraciclina) (500 mg/6h 3 días), las quinolonas (como el ciprofloxacino) y el trimetoprim y sulfametoxazol cotrimoxazol (320 mg/12h 3 días). Otra alternativa sería los nitrofuranos como la furazolidona (por ser como los anteriores antibióticos bactericidas y bacteriostáticos).



</doc>
<doc id="37961" url="https://es.wikipedia.org/wiki?curid=37961" title="Red óptica síncrona">
Red óptica síncrona

La red óptica sincronizada (en idioma inglés "Synchronous Optical Network", cuyo acrónimo es "SONET") es un estándar para el transporte de telecomunicaciones en redes de fibra óptica.

La decisión de la creación de SONET fue tomada por la E.C.S.A. (Exchange Carriers Standard Association) en los Estados Unidos para posibilitar la conexión normalizada de los sistemas de fibra óptica entre sí, aunque estos fueran de distinto fabricante. En las últimas etapas de desarrollo de SONET entró también el CCITT (Comité Consultivo Internacional Telefónico y Telegráfico), antecesor del actual UIT-T, de la UIT (Unión Internacional de Telecomunicaciones, sector de estandarización de telecomunicaciones) para que se pudiera desarrollar una norma que posibilitara la interconexión mediante fibra de las redes telefónicas a nivel mundial.
De esta etapa parte el desarrollo de la denominada Jerarquía Digital Síncrona, conocida popularmente como SDH (Synchronous Digital Hierarchy). A finales de los 90, se estima que los estándares SONET/SDH podrán proporcionar las infraestructuras de transporte para la red mundial de telecomunicaciones para las siguientes dos o tres décadas.
Aun cuando tienen puntos de compatibilidad, el estándar SONET prácticamente solo es aplicado en Estados Unidos y Canadá mientras que el SDH se aplica en el resto del mundo.

SONET define una tecnología para transportar muchas señales de diferentes capacidades a través de una jerarquía óptica síncrona y flexible. Esto se logra por medio de un esquema de multiplexado por interpolación de bytes. La interpolación de bytes simplifica la multiplexación y ofrece una administración de la red extremo a extremo.
El primer paso en el proceso de la multiplexación de SONET implica la generación de las señales del nivel inferior de la estructura de multiplexación. En SONET la señal básica la conocemos como señal de nivel 1 o también STS-1 (Synchronous Transport Signal level 1). Está formada por un conjunto de 810 bytes distribuidos en 9 filas de 90 bytes. Este conjunto es transmitido cada 125 microsegundos, correspondientes a la velocidad del canal telefónico básico de 64 kbit/s, por lo que la velocidad binaria de la señal STS-1 es 51,84 Mbit/s.

Figura 1.- "Estructura de trama de la señal STS-1"
Las señales de niveles más altos están formadas por la multiplexación de diversas señales de nivel 1 (STS-1), creando una familia de señales STS-N, donde la N indica el número de señales de nivel 1 que la componen. En la Tabla 1 se indican las denominaciones de las señales eléctricas y portadoras ópticas, así como sus velocidades y los puntos de coincidencia con los de la Jerarquía Digital Síncrona.

1.- Multiplexor terminal

Es el elemento que actúa como un concentrador de las señales DS-1 (1,544 Mbit/s) tributarias así como de otras señales derivadas de ésta y realiza la transformación de la señal eléctrica en óptica y viceversa. 
Dos multiplexores terminales unidos por una fibra con o sin un regenerador intermedio conforman el más simple de los enlaces de SONET.

2.- Regenerador

Necesitamos un regenerador cuando la distancia que separa a dos multiplexores terminales es muy grande y la señal óptica que se recibe es muy baja.
El reloj del regenerador se apaga cuando se recibe la señal y a su vez el regenerador reemplaza parte de la cabecera de la trama de la señal antes de volver a retransmitirla. La información de tráfico que se encuentra en la trama no se ve alterada.

3.- Multiplexor Add/Drop (ADM)

El multiplexor de extracción-inserción (ADM) permite extraer en un punto intermedio de una ruta parte del tráfico cursado y a su vez inyectar nuevo tráfico desde ese punto. En los puntos donde tengamos un ADM, solo aquellas señales que necesitemos serán descargadas o insertadas al flujo principal de datos. El resto de señales a las que no tenemos que acceder seguirá a través de la red.
Aunque los elementos de red son compatibles con el nivel OC-N, puede haber diferencias en el futuro entre distintos vendedores de distintos elementos. SONET no restringe la fabricación de los elementos de red. Por ejemplo, un vendedor puede ofrecer un ADM con acceso únicamente a señales DS-1, mientras que otro puede ofrecer acceso simultáneo a señales DS-1 (1,544 Mbit/s) y DS-3 (44,736 Mbit/s).

1.- Punto a punto
La configuración de red punto a punto está formada por dos multiplexores terminales, unidos por medio de una fibra óptica, en los extremos de la conexión y con la posibilidad de un regenerador en medio del enlace si éste hiciese falta.
En un futuro las conexiones punto a punto atravesarán la red en su totalidad y siempre se originarán y terminarán en un multiplexor.

2.- Punto a multipunto

Una arquitectura punto a multipunto incluye elementos de red ADM a lo largo de su recorrido. El ADM es el único elemento de red especialmente diseñado para esta tarea. Con esto se evitan las incomodas arquitecturas de red de demultiplexado, conectores en cruz (cross-connect), y luego volver a multiplexar. Se coloca el ADM a lo largo del enlace para facilitar el acceso a los canales en los puntos intermedios de la red.

3.- Red Hub

La arquitectura de red "hub" está preparada para los crecimientos inesperados y los cambios producidos en la red de una forma más sencilla que las redes punto a punto. Un "hub" concentra el tráfico en un punto central y distribuye las señales a varios circuitos.

4.- Arquitectura en anillo:

El elemento principal en una arquitectura de anillo (Figura 2) es el ADM. Se pueden colocar varios ADM en una configuración en anillo para tráfico bidireccional o unidireccional. La principal ventaja de la topología de anillo es su seguridad; si un cable de fibra se rompe o se corta, los multiplexores tienen la inteligencia necesaria para desviar el tráfico a través de otros nodos del anillo sin ninguna interrupción.
La demanda de servicios de seguridad, diversidad de rutas en las instalaciones de fibra, flexibilidad para cambiar servicios para alternar los nodos, así como la restauración automática en pocos segundos, han hecho de la arquitectura de anillo una topología muy popular en SONET.

Figura 2.- "Arquitectura en anillo"

La clave de SONET es que permite interfaces con fuentes asíncronas por lo que los equipos existentes pueden ser sustituidos o soportados por la red SONET. De esta forma las transiciones se pueden realizar gradualmente.

Aquí podemos ver las ventajas que presenta la SONET frente a otros sistemas:

Otras ventajas son: 


</doc>
<doc id="37971" url="https://es.wikipedia.org/wiki?curid=37971" title="Multiple access with collision avoidance">
Multiple access with collision avoidance

MACA (siglas en inglés de Multiple Access with Collision Avoidance) es un protocolo informático. La modificación incluida en este protocolo, respecto a CSMA/CD, es que ahora las estaciones, antes de transmitir, deben enviar una trama RTS (Request To Send). Dicha trama, indica la longitud del paquete de datos a enviar. El Tamaño de dicha trama es de 30 bytes.

Ante esto, el resto de estaciones actuarán de tal forma que, si “escuchan” un RTS, esperarán por el CTS (Clear to send) y, si “escuchan” un CTS, esperarán el tiempo necesario para que se transmita la longitud indicada en dicho CTS.
También existe el protocolo combinado de MACA con CSMA/CA consiste en utilizar MACA usando el CSMA/CA para enviar el RTS y el CTS.Este último es el más usuado en la actualidad en redes inalámbricas (802.11).



</doc>
<doc id="37980" url="https://es.wikipedia.org/wiki?curid=37980" title="Jerarquía digital síncrona">
Jerarquía digital síncrona

La jerarquía digital síncrona —abreviado como SDH, del inglés "Synchronous Digital Hierarchy"— es un conjunto de protocolos de transmisión de datos. Se puede considerar como la revolución de los sistemas de transmisión, como consecuencia de la utilización de la fibra óptica como medio de transmisión, así como de la necesidad de sistemas más flexibles y que soporten anchos de banda elevados. La jerarquía SDH se desarrolló en EE. UU. bajo el nombre de SONET o ANSI T1X1 y posteriormente el CCITT (Hoy UIT-T) en 1989 publicó una serie de recomendaciones donde quedaba definida con el nombre de SDH.

Uno de los objetivos de esta jerarquía estaba en el proceso de adaptación del sistema PDH (Plesiochronous Digital Hierarchy), ya que el nuevo sistema jerárquico se implantaría paulatinamente y debía convivir con la jerarquía plesiócrona instalada. Ésta es la razón por la que la ITU-T normalizó el proceso de transportar las antiguas tramas en la nueva. La trama básica de SDH es el STM-1 (Synchronous Transport Module level 1), con una velocidad de 155 Mbit/s.

Cada trama va encapsulada en un tipo especial de estructura denominado "contenedor". Una vez encapsulados se añaden cabeceras de control que identifican el contenido de la estructura (el contenedor) y el conjunto, después de un proceso de multiplexación, se integra dentro de la estructura STM-1. Los niveles superiores se forman a partir de multiplexar a nivel de byte varias estructuras STM-1, dando lugar a los niveles STM-4,STM-16, STM-64 y STM-256.

Las tramas contienen información de cada uno de los componentes de la red: "trayecto", "línea" y "sección", además de la información de usuario. Los datos son encapsulados en contenedores específicos para cada tipo de señal tributaria.<br>
A estos contenedores se les añade una información adicional denominada "tara de trayecto" ("Path overhead"), que consiste en una serie de bytes utilizados con fines de mantenimiento de red, y que dan lugar a la formación de los denominados contenedores virtuales (VC). 
El resultado de la multiplexación es una trama formada por 9 filas de 270 octetos cada una (270 columnas de 9 octetos). La transmisión se realiza bit a bit en el sentido de izquierda a derecha y de arriba abajo. La trama se transmite a razón de 8000 veces por segundo (cada trama se transmite en 125 μs). Por lo tanto, el régimen binario (Rb) para cada uno de los niveles es:

STM-1 = 8000 * (270 columnas * 9 filas * 8 bits)= 155 Mbit/s<br>
STM-4 = 4 * 8000 * (270 columnas * 9 filas * 8 bits)= 622 Mbit/s<br>
STM-16 = 16 * 8000 * (270 columnas * 9 filas * 8 bits)= 2.5 Gbit/s<br>
STM-64 = 64 * 8000 * (270 columnas * 9 filas * 8 bits)= 10 Gbit/s<br>
STM-256 = 256 * 8000 * (270 columnas * 9 filas * 8 bits)= 40 Gbit/s

De las 270 columnas que forman la trama STM-1, las 9 primeras forman la denominada "tara o cabecera" ("overhead"), independiente de la tara de trayecto de los contenedores virtuales antes mencionados, mientras que las 261 restantes constituyen la carga útil (Payload).
El SOH ("Section Overhead") se divide en dos partes: El R-SOH y el M-SOH. El primero de ellos(R-SOH) es utilizado para aplicaciones entre repetidores, los cuales están comprendidos por los bytes de las filas 1 a 3, en tanto que para el uso entre terminales de multiplexación (M-SOH) corresponden a los bytes de las filas 5 a 9. A continuación se detalla las funciones de cada uno de los bytes que componen el SOH.<br>
a) Señal de alineamiento de trama A1, A2:<br>
A1 y A2 son patrones fijos de sincronización de trama. A1 está dispuesto en 11110110 y A2 en 00101000.<br>
b) Traza de sección de regenerador J0:<br>
El uso de J0 está aún bajo estudio.<br>
c) Monitoreo de errores B1, B2:'<br>
Los errores de transmisión son monitoreados en las secciones de regenerador y multiplexor. B1 es para la sección de regenerador y B2 para la de multiplexor.<br>
d) Canal de servicio para Ingeniería E1, E2:<br>
El E1 es accesible en regeneradores y multiplexores, el E2 sólo en multiplexores. Cada circuito posee una capacidad de 64Kb/s.<br>
e) Canal de usuario F1:<br>
Este es un canal de datos de 64 Kb/s que puede utilizar cualquier operador de red para sus propósitos.<br>
f) Canal de comunicación de datos D1-3, D4-12:<br>
Estos bytes son asignados como canales de comunicación de datos para transmitir información hacia multiplexores y regeneradores y viceversa.<br>
g) Señalización de conmutación de protección automática K1, K2:<br>
El intercambio de información entre dos extremos en una sección de multiplexor se lleva a cabo a través de los bytes K1 y K2. Parte de K2 también se utiliza para enviar MS-RDI (indicación de defectos remotos en la sección de multiplexor) y MS-AIS (señal de indicación de alarmas en la sección de multiplexor).<br>
h) Estado de sincronización S1:<br>
El byte S1 comunica a la siguiente estación la calidad de la fuente de referencia de sincronización utilizada por el equipo.<br>
Los bits 1 al 4 del byte S1 están reservados para la calidad usada por operadores individuales. Los bits 5 al 8 pueden tomar los siguientes valores:<br>
0000 Calidad desconocida (red de sincronización existente)<br>
0001 Reservados<br>
0010 Señal generada por un equipo que está sincronizado a un reloj según la Rec. ITU-T G.811<br>
0011 Reservados<br>
0100 Señal generada por un equipo que está sincronizado a un reloj del tipo SSU-A<br>
0101 Reservados<br>
0110 Reservados<br>
0111 Reservados<br>
1000 Señal generada por un equipo que está sincronizado a un reloj del tipo SSU-B<br>
1001 Reservados<br>
1010 Reservados<br>
1011 Señal generada por un equipo que está sincronizado a un reloj según la Rec. ITU-T G.813 Option I (SEC)<br>
1100 Reservados<br>
1101 Reservados<br>
1110 Reservados<br>
1111 No utilizar la sincronización de esta señal<br>
i) Z1 y Z2 son bytes de reserva.<br>
""'j) M1 Byte de indicación de Error en la Sección de multiplexación Remota.

El POH ("Path OverHead") tiene como misión monitorizar la calidad e indicar el tipo de contenedor virtual que se tiene. Está compuesto por el VC (Contenedor Virtual) que es la entidad de carga útil que viaja sin cambios a lo largo de la red, además de algunos bytes que se agregan y se desempaquetan en los distintos puntos terminación del servicio de transporte. Los bytes que se agregan dependerán del tipo de contenedor virtual y se dividen en dos tipos "Higher-order Path Layer" y "Lower-order Path Layer". En la siguiente tabla se muestra los bytes correspondientes al "Higher-order Path Layer".

El segundo tipo de bytes que se agregan son los del tipo Lower-order Path Layer que corresponden a los VC-12. En la siguiente tabla se muestra el funcionamiento de cada uno de ellos.

Para considerarse un estándar internacional, las diversas interfaces de tasas de bit PDH existentes deben ser acomodadas en la estructura SDH. Esto se hace permitiendo diferentes interfaces para ser mapeadas en la trama SDH.
Esta multiplexación parte de la unidad básica de PDH que es el E1 (2 Mbit/s) para formar un STM-1. Se pueden transportar 63 señales PDH de 2 Mbit/s. A continuación se detallan los pasos para el mapeo de un STM-1 mediante un E1. 
Para realizar esta multiplexación se ejecutan los pasos anteriores de forma similar. Se pueden transmitir hasta 3 señales de 34Mbit/s.
Para multiplexar señales PDH es necesario primero adaptarlas a la velocidad SDH. Los pasos para realizar dicha multiplexación se dan en forma similar a los desarrollados en los puntos anteriores.
Un sistema síncrono se basa en el hecho de que cada reloj está en fase y frecuencia de sincronismo con el siguiente. En la práctica esto es imposible de lograr, por lo tanto, las desviaciones de fase y frecuencia ocurrirán. Dentro de una red la frecuencia del reloj se extrae de la señal de línea, sin embargo, las variaciones de fase pueden ocurrir a partir de la acumulación del jitter sobre la red. Las variaciones de interfaz de frecuencia en la red pueden ocurrir. La forma en que SDH supera este problema es usando punteros para apuntar a la dirección del principio del contenedor virtual dentro de la trama.
El valor del puntero inicial corresponde a la diferencia de fase entre llegada de la unidad tributaria y la unidad tributaria vacía dentro de la trama en el momento que el tributario es mapeado en el contenedor virtual. Si la fase varía entre los relojes de lectura y escritura de tal manera que los buffers de entrada de flujo de terminación digital muestran una tendencia de desbordamiento o de ejecutar vacío, un puntero de ajuste se producirá.
En la siguiente tabla se hace una breve descripción de los punteros utilizados para el mapeo de tramas STM-N.

Una justificación positiva del puntero se da cuando la frecuencia de entrada es menor que la de salida, por lo tanto se insertan bytes de relleno que no afectan a los datos. Los bytes de justificación siempre se insertan en la misma ubicación dentro de la trama.

Una justificación negativa del puntero se da cuando la frecuencia de entrada es mayor que la de salida, los bytes H# pueden llevar información real del VC4 sin afectar a los datos de la carga útil. Demasiado ajuste de punteros puede llegar a ocasionar jitter.

Las celdas ATM se asignan a los contenedores en diferentes velocidades de bits. Estas celdas ATM son mapeadas mediante la alineación de cada celda con la estructura de los contenedores virtuales o concatenados. Dado que la capacidad no pueda ser un múltiple integrador del largo de las celdas ATM (53 bytes), una celda se le permite cruzar el límite del contenedor de la trama. El campo de información de la celda ATM (48 bytes) está codificada antes de ser mapeada, para garantizar la delineación. Un flujo de celdas ATM con una velocidad de datos que puede ser mapeado es igual a la capacidad del payload del VC.
Desafortunadamente ATM no fue aceptado por el mercado como la solución para llevar a datos sobre los protocolos de SDH / SONET. Su inherente ineficiencia de ancho de banda, altos costos y la complejidad empujó a ATM a nichos de mercado específicos, tales como el transporte Frame Relay, acceso xDSL y a algunas aplicaciones militares y científicas.

Señales en tramas HDLC son mapeadas mediante la alineación de la estructura de los bytes de cada trama con la estructura byte del VC. El rango va desde 1,5 Mbit/s hasta varios Gbit/s utilizando las técnicas de concatenación. Las banderas (flags) 7EX HDLC se utilizan entre tramas para llenar el buffer, debido a la llegada discontinua de las señales de tramas HDLC. Las tramas HDLC son de longitud variable, una trama puede cruzar el límite del contenedor.

Para la sincronización en SDH se toman en cuenta las normas G.803 (Arquitectura de redes de transporte basadas en la jerarquía digital síncrona) y G.811 (Características de temporización de los relojes de referencia primarios) entre otras como la G.822, G.812, etc.
Sincronizar se refiere a que dos o más elementos, eventos u operaciones sean programados para que ocurran en un momento predefinido de tiempo o lugar. En ingeniería electrónica, en lógica digital y en transferencia de datos, la sincronización implica que el dispositivo utiliza una señal de reloj.
La red de sincronización es la red que es responsable de distribuir la información de
sincronización a elementos de red que tiene que funcionar síncronamente para satisfacer los requisitos de característica de deslizamiento de octetos de la Recomendación UIT-T G.822.<br>
El funcionamiento síncrono de los tipos de elementos de red, suele estar ordenado en una determinada zona geográfica, en la cual todos estos elementos están sincronizados con un "reloj maestro". La zona en la cual todos los elementos de red pertinentes (en funcionamiento normal) están sincronizados con un reloj maestro se denomina una "zona de sincronización”.<br>
El reloj maestro de una zona de sincronización debe cumplir los requisitos descritos en la Recomendación UIT-T G.811.

En la Recomendación UIT-T G.810 se identifican dos métodos fundamentales de sincronización de relojes nodales, a saber: sincronización principal-subordinado y sincronización mutua.
La sincronización principal-subordinado: Es un método adecuado para la sincronización de redes SDH; donde se utiliza una jerarquía de relojes en la que cada nivel jerárquico está sincronizado con referencia a un nivel superior. El nivel más alto de la jerarquía es el PRC. Las señales de referencia de reloj se distribuyen entre los niveles de la jerarquía por medio de una red de distribución que puede utilizar la infraestructura de la red de transporte. Los niveles jerárquicos son los siguientes:
La viabilidad de la sincronización mutua queda en estudio.<br>
La distribución de la temporización entre relojes de nodo jerárquico debe efectuarse empleando un método que evite el procesamiento de puntero intermedio.<br>
Todos los elementos en la red SDH se operan bajo un mismo reloj de frecuencia, suministrado por una fuente de señal llamada reloj de referencia primario (PRC). En la recomendación ITU-T G.811, se encuentran las especificaciones de rendimiento del PRC, cuya estabilidad y exactitud en frecuencia se hallan en el orden de ±10-11, posible gracias a un oscilador de cesio.

En la norma ITU-T G.803, se hace hincapié en la necesidad de que los relojes de SDH se ajusten al reloj de referencia primario (PRC, primary reference clock) y posean una buena característica de estabilidad a corto plazo, a fin de ajustarse a los objetivos de tasa genérica de deslizamientos de la Recomendación UIT-T G.822.<br>
Se señala además que, siempre que el reloj de SDH cumpla la plantilla de estabilidad a corto plazo, no existen limitaciones prácticas al número de elementos de tratamiento de punteros que pueden conectarse en cascada en una red SDH, para cumplir los requisitos de fluctuación de fase de salida de la cabida útil en una frontera SDH/PDH.<br>
“Los relojes de referencia primarios necesitan una fiabilidad muy alta y es probable que incluyan equipo repetido, a fin de asegurar la continuidad de salida. Sin embargo, toda discontinuidad de fase debida a operaciones internas en el reloj, no deberá producir más que un alargamiento o acortamiento de la anchura del intervalo de la señal de temporización y no provocar, en la salida del reloj, una discontinuidad de fase superior a 1/8 de UI a la salida del reloj”. ello se señala en ITU-T G.811.<br>
La calidad de funcionamiento del PRC no se especifica, por tanto, en puntos de referencia internos sino más bien en la interfaz externa del equipo. Las interfaces de salida especificadas para el equipo en el que puede estar contenido el PRC son:
La distribución de la señal de reloj se manifiesta a través de líneas de transmisión ordinarias como, en este caso, un sistema de transmisión SDH. Los elementos de red “intermedios”, tales como regeneradores, multiplexores de inserción y extracción, etc., son operados por medio de un “modo esclavo”, el cual utiliza un componente de señal de reloj extraído de la señal STM-N recibida.<br>
El deterioro en la señal de reloj, como la fluctuación acumulada durante la transmisión a través de una cadena de elementos de red y línea de transporte, se reduce con un equipo de reloj esclavo de alto rendimiento según especifica la recomendación G.812 para nodo de tránsito y para nodo local.<br>
Un elemento de red SDH tiene la capacidad de enviar una señal de reloj externa dirigida hacia el BITS (fuente integrada de temporización de construcción) para reducir el deterioro en la señal de reloj. El elemento de red intermedio utiliza directamente la señal de reloj extraída por sí mismo.<br>
Las señales de reloj necesarias para la operación del NE (Elemento de red) son producidas por un circuito de reloj que corre principalmente bajo el modo esclavo. Las fuentes de referencia disponibles son:<br>
- Entrada externa<br>
En este puerto normalmente se conecta o una señal de reloj externa proveniente de un reloj de referencia primario (G.811), o BITS (G.812 tránsito o local), o el reloj de un sistema de conmutación.<br>
- Señal de línea STM-N<br>
El componente de la señal de reloj extraída de una señal de línea puede ser utilizado como fuente de referencia, estando éste conectado hacia el este, hacia el oeste o hacia una dirección tributaria. Entonces, el byte S1 del SOH muestra el nivel de calidad del componente de reloj. Este, en cambio, muestra la señal de reloj que originalmente generó la señal de línea STM- N, siempre y cuando la señal STM-N pueda ser encontrada desde G.811 o G.812 T, L, u otro.<br>
- Señal PDH de 2 Mb/s en el tributario<br>
Dos de las señales tributarias de 2 Mbit/s pueden ser seleccionadas como fuentes de referencia. Este sería el caso si, por ejemplo, el sistema SDH fuese instalado en un área aislada con el reloj síncrono comunicado a través de una señal de 2 Mbit/s generada por un PRC, o cuando el sistema SDH es sincronizado a un reloj ESS (sistema de conmutación) en vez de PRC.<br>

Aparte de ser utilizado en modo de operación esclavo, el circuito de reloj del NE también puede funcionar como una fuente de reloj independiente, para la cual existen dos modos de operación:<br>

- Modo de retención<br>
Mientras el circuito de reloj opera en modo esclavo, todos los parámetros como frecuencia,
fase, etc. son memorizados. Cuando el circuito pierde contacto con la fuente de referencia, por alguna falla en la línea por ejemplo, esta información almacenada facilita el flujo de operación continua ininterrumpidamente. De este modo, se pueden evitar perturbaciones de transmisión causadas por cambios abruptos de frecuencia y de fase.

- Modo de operación libre<br>
El circuito de reloj que es básicamente un VCXO (oscilador controlado por voltaje), opera
libremente sin fuente de referencia. Este es una excelente opción para un área donde no haya una fuente de referencia de reloj disponible, y donde el sistema SDH se utilice de manera semejante al PDH.

Esta Recomendación especifica los parámetros de las interfaces ópticas para equipos y sistemas basados en la jerarquía digital síncrona para permitir la compatibilidad transversal (multivendedor) en secciones de cable elementales.<br>
También se pretende que estas especificaciones estén conformes con la Rec. UIT-T G.955 compatibilidad longitudinal de equipos de nivel jerárquico y aplicación comparables. La Recomendación se basa en el uso de una fibra óptica para cada dirección.
Mediante la adecuada combinación de transmisores y receptores pueden obtenerse balances de potencia para los sistemas de línea de fibra óptica optimizados, en términos de atenuación/dispersión y costes con respecto a las diversas aplicaciones. Sin embargo, para simplificar el desarrollo de los sistemas con compatibilidad transversal, conviene limitar el número de categorías de aplicaciones y los correspondientes conjuntos de especificaciones de interfaces ópticas para la normalización. Se contemplan tres amplias categorías de aplicación:<br>
Intracentrales: Correspondiente a distancias de interconexiones inferiores a 2 km aproximadamente.<br>
Intercentrales: A corta distancia, correspondiente a distancias de interconexión de 15 km aproximadamente.<br>
Intercentrales: A larga distancia, correspondiente a distancias de interconexión de 40 km aproximadamente en la ventana de 1310 nm y de 80 km aproximadamente en la ventana de 1550 nm.
Para proporcionar flexibilidad en la implementación de sistemas con compatibilidad transversal y hacer posible una futura utilización de multiplexación por división en longitud de onda (WDM, wavelength-division multiplexing), conviene admitir una gama lo más amplia posible de longitudes de onda de funcionamiento del sistema. La especificación de las gamas de longitud de onda de funcionamiento es afectada por las siguientes consideraciones generales: Tipo de fibra, las características de la fuente, la gama de atenuación del sistema y la dispersión del trayecto óptico.<br>
La gama de longitudes de onda de funcionamiento es la gama admisible máxima de longitudes de onda de la fuente. En esta gama, las longitudes de onda de la fuente pueden seleccionarse para diferentes degradaciones relacionadas con la fibra. El receptor debe tener la gama mínima de longitudes de onda de funcionamiento que corresponda a la gama máxima admisible de longitudes de onda de la fuente. Para las redes SDH que utilizan amplificadores de fibra óptica, podría ser necesario limitar la gama de longitudes de onda de funcionamiento.<br>
Las regiones de longitud de onda que permiten el funcionamiento del sistema son
parcialmente determinadas por los valores de longitud de onda de corte de la fibra o del cable de fibra. Para las fibras G.652 y G.653 estos valores se han elegido de tal forma que permitan el funcionamiento monomodo del cable de fibra para longitudes de onda de 1270 nm y superiores, si bien algunas administraciones permiten valores tan bajos como 1260 nm. Para los cables de fibra G.654, se han aceptado los valores de longitud de onda de corte para el funcionamiento monomodo en 1530 nm y superior.<br>
Las regiones de longitud de onda admisibles vienen definidas además por la atenuación de la fibra.<br>
Si bien la atenuación por dispersión intrínseca disminuye normalmente al aumentar la longitud de onda, puede aparecer la absorción OH-iónica alrededor de 1385 nm y, en menor medida, alrededor de 1245 nm. Por consiguiente, estas crestas de absorción y la longitud de onda de corte definen una región de longitudes de onda centrada alrededor de 1310 nm.<br>
Las fibras con dispersión no desplazada conformes a la Rec. UIT-T G.652 están optimizadas para su utilización en esta región de longitudes de onda. Para longitudes de onda mayores, la atenuación por flexión aparece para valores de 1600 nm o superiores y la absorción infrarroja aparece más allá de 1600 m.<br>
Por consiguiente, estas atenuaciones y la cresta de absorción de vapor de agua que aparece a 1385 nm definen una segunda región de longitudes de onda de funcionamiento alrededor de 1550 nm. La Rec. UIT-T G.654 para fibras con corte desplazado se limita únicamente a esta región. Sin embargo, las fibras G.652 y las fibras con dispersión desplazada G.653 pueden utilizarse en esta región.

Los dispositivos transmisores son:
La potencia inyectada media en el punto de referencia S es la potencia media de una secuencia de datos seudoaleatorios acoplada a la fibra mediante el transmisor. Se expresa como una gama para permitir una cierta optimización de los costes y tener en cuenta los márgenes de explotación en condiciones de funcionamiento normalizadas, las degradaciones del conector del transmisor, las tolerancias en las mediciones y los efectos de envejecimiento. Estos valores permiten determinar los valores de sensibilidad y el punto de sobrecarga para el receptor en el punto de referencia R.<br>
El convenio adoptado para el nivel lógico óptico es el siguiente:
El funcionamiento adecuado del sistema requiere la especificación de la sensibilidad mínima del receptor y del nivel de potencia de sobrecarga mínimo.
Para potenciar el sistema existen dos posibilidades:
Las funciones principales de las redes SDH las podemos integrar en dos grandes grupos:
Una red de transporte basada en la tecnología SDH puede descomponerse en redes de capa de transporte independientes con una asociación cliente servidor.
La arquitectura de la red de transporte estaba basada en los conceptos de estratificación y subdivisión dentro de cada capa.<br>
La arquitectura de las redes SDH está definida por la Recomendación G.803, en esta recomendación se define un modelo tridimensional.<br>
La capa de red son un conjunto de puntos de acceso similares y que pueden estar asociados para transferir información.<br>
La función de adaptación es el proceso mediante el cual se adapta una información de capa para ser transportada por la red de la capa servidora. La adaptación intercapas cuenta con los siguientes procesos:
La supervisión de la conexión se realiza a través de:<br>
Supervisión Intrínseca: <br>
Las conexiones de capa de trayecto pueden supervisarse de forma indirecta utilizando los datos disponibles intrínsecamente de la sección múltiplex o las capas del servidor del trayecto de orden superior, y calculando el estado aproximado de la conexión de trayecto del cliente a partir de los datos disponibles.<br>

Supervisión no intrusiva: <br>
La conexión puede supervisarse directamente mediante la información de tara pertinente en la sección de regeneración, la sección múltiplex, el trayecto de orden superior o el trayecto de orden inferior, calculándose a continuación el estado aproximado de la conexión a partir de la diferencia entre los estados supervisados en cada extremo de la conexión.<br>
Supervisión de Subcapa: <br>
Las conexiones pueden supervisarse de manera directa sobrescribiendo alguna parte de la capacidad de tara del camino original al comienzo de la conexión. En el caso de la SDH, la tara se ha definido a esos efect os en la capas de trayectos de orden superior e inferior. Cuando se aplica una conexión en cascada de la SDH, este método de supervisión se conoce como supervisión de la conexión en cascada.
Los eventos de fallos se detectan mediante la función de terminación de sección múltiplex (MST, Multiplex Section Termination) y la reconfiguración utiliza las funciones de conmutación de protección que se encuentran en la subcapa de protección de sección múltiplex. La reconfiguración resultante puede contemplar la conmutación de protección en elementos múltiples de la red SDH. La coordinación de esta conmutación en elementos múltiples de la red SDH se realiza mediante un protocolo de conmutación automática de protección (APS, Automatic Protection Switching).
MS-SP Ring (Multiplex Section-Shared Protection Ring): <br>
Se emplea solo la mitad de la capacidad en cada sección de multiplexación para cursar tráfico. Máximo 16 nodos. Distancia máxima total de la estructura de 1200 km. Tiempos de conmutación inferiores a 50ms. 
Ante un fallo: <br>
– Los Nodos adyacentes detectan el fallo realizan una operación de Bridge&Switch. <br>
– El resto de nodos realizan una operación de Full Pass-Through.<br>
– En situación de conmutación el tráfico circula siempre pasando por todos los nodos del anillo MS-SPRING. <br>

MS-DP Ring (Multiplex Section-Dedicated Protection Ring) :<br>
Cada sentido de una conexión bidireccional emplea un camino distinto siguiendo un sentido del anillo. El sentido contrario sería el backup. Un inconveniente es que cada conexión bidireccional consume BW en todo el anillo. Máximo 16 nodos (por limitaciones en señalización).<br>

SNCP Ring (Subnetwork Connection Protection Ring) : <br>
Empleada en un anillo. Cada conexión unidireccional emplea ambos caminos en el anillo (es un 1+1). No tiene la limitación de 16 nodos. Soporta el fallo de un nodo.

Las redes SDH actuales están construidas, básicamente, a partir de cuatro tipos distintos de equipos o elementos de red (ITU-T G.782): Regeneradores, Multiplexores Terminales, Multiplexores de Inserción y Extracción, y Distribuidores Multiplexores. Estos equipos pueden soportar una gran variedad de configuraciones en la red, incluso, un mismo equipo puede funcionar indistintamente en diversos modos, dependiendo de la funcionalidad requerida en el nodo donde se ubica. En la Figura 6 se muestra un diagrama de bloques de un elemento SDH genérico, sin considerar amplificadores o boosters opcionales.

Como su propio nombre indica regeneran la señal de reloj y la relación de amplitud de las señales digitales a su entrada, que han sido atenuadas y distorsionadas por la dispersión de la fibra óptica por la que viajan. Los regeneradores obtienen la señal de reloj a partir de la ristra de bits entrante.
Es un elemento que se utiliza en un enlace punto a punto. Implementara únicamente la terminación de línea y la función de multiplexar o desmutiplexar varios tributarios en una línea STM-N. En el elemento genérico de la Figura 8, el TM STM-4 dispondría de una única interfaz agregada óptica STM-4 (con transmisión y recepción) y, dependiendo de la configuración, de varias interfaces tributarias eléctricas (1,5 Mbit/s, 2 Mbit/s, 34 Mbit/s, 45 Mbit/s, 140 Mbit/s, STM-1) u ópticas (STM-1).
Se encargan de extraer o insertar señales tributarias plesiócronas o síncronas de cualquiera de las dos señales agregadas STM-N que recibe (una en cada sentido de transmisión), así como dejar paso a aquellas que se desee.
Aporta la flexibilidad a la red SDH

Permiten la interconexión sin bloqueo de señales a un nivel igual o inferior, entre cualquiera de sus puertos de entrada y de salida. Los DXCs admiten señales de acceso, tanto plesiócronas como sícronas, en diversos niveles.

Las señales de niveles más altos están formadas por la multiplexación de diversas señales de nivel 1 (STM-1), creando una familia de señales STM-N, donde la N indica el número de señales de nivel 1 que la componen. En la Tabla 1 se indican las denominaciones de las señales eléctricas y portadoras ópticas, así como sus velocidades y los puntos de coincidencia con los de SONET.

En la tabla anterior, el ancho de banda de carga es la velocidad de línea menos el ancho de banda de las línea y de sección. 

Hay que resaltar que la progresión de velocidad de datos comienza en 155 Mbit/s y aumenta en múltiplos de 4. La única excepción es OC-24, que está normalizado en ANSI T1.105, pero no es una velocidad SDH estándar de la ITU-T G.707. A veces se describen otras tasas como OC-9, OC-18, OC-36 y OC-96 y OC-1536, pero probablemente nunca han sido desplegados. Sin duda no son comunes y no son compatibles con las normas.

La siguiente velocidad de 160 GB/s OC-3072/STM-1024 no se ha normalizado todavía, debido al coste de transceptores de alta velocidad, al ser más baratos los múltiplex de longitudes de onda a 10 y 40 Gbit/s.

La SDH presenta una serie de ventajas respecto a la jerarquía digital plesiócrona (PDH).

Algunas de estas ventajas son:

En cuanto a las desventajas tenemos que:

Estándares ITU relacionados con SDH:


</doc>
<doc id="37983" url="https://es.wikipedia.org/wiki?curid=37983" title="Héctor de Jesús Ruiz">
Héctor de Jesús Ruiz

Héctor de Jesús Ruiz Cárdenas (n. 25 de diciembre de 1945) es un empresario mexicano que se desempeñó como presidente mundial de Advanced Micro Devices (AMD), la segunda empresa más grande del mundo en la fabricación de microprocesadores y circuitos integrados para productos electrónicos. 

Nació en la ciudad de Piedras Negras, Coahuila, hijo de un padre cuidador de ganado y una madre secretaria, el mayor de una familia compuesta por 4 mujeres y el, en sus inicios lustraba calzado en el zócalo de la ciudad, conoció a Olive Givin, misionero metodista estadounidense que vivía cerca y le dio empleo de mandadero.
Fue él quien le sugirió estudiar en Estados Unidos. Con apoyo del Club Rotario local, cruzaba día a día la frontera para estudiar high school en Eagle Pass. Batallaba con el inglés y pasaba largas horas vespertinas con maestros de química, física y geometría. Se graduó con los máximos honores e ingresó en la Universidad de Texas en Austin. Givin le pagó el primer año de estudios ahí cursó la licenciatura y la maestría en ingeniería eléctrica en la Universidad de Texas en Austin. Posteriormente cursó el Doctorado en Electrónica en la Universidad Rice (1973). Tras laborar brevemente en la empresa Texas Instruments comenzó a laborar para Motorola, donde alcanzó la presidencia del sector semiprocesadores antes de dirigir las operaciones de AMD.

En abril de 2003, Ruiz fue seleccionado por el Presidente George W. Bush para integrarse al Comité Consultivo de Negociaciones y Políticas Comerciales, la cual asesora directamente a la Cámara de Representantes de los Estados Unidos. 

Ruiz también se desempeña en el directorio de accionistas de la empresa Kodak Eastman. En el último tiempo declaró sus intenciones de convertirse en un nuevo proveedor de procesadores para Apple Computer aparte de afirmar que en un futuro la empresa de Steve Jobs se verá en la necesidad de comprar a la empresa.
Tras una severa crisis de la que AMD no logra salir pese al haber despedido al diez por ciento de su plantilla, Ruiz resigna su cargo de CEO de AMD el 18 de julio de 2008, pese a lo cual sigue ligado a la empresa como executive chairman.[1]

1 Hector Ruiz steps down as AMD CEO



</doc>
<doc id="37988" url="https://es.wikipedia.org/wiki?curid=37988" title="Unión Astronómica Internacional">
Unión Astronómica Internacional

La Unión Astronómica Internacional (UAI, en francés Union astronomique internationale o UAI, en inglés International Astronomical Union o IAU) es una agrupación de las diferentes sociedades astronómicas nacionales y constituye el órgano de decisión internacional en el campo de las definiciones de nombres de planetas y otros objetos celestes así como los estándares en astronomia. Fue creada en 1919 a partir de la unión de diferentes organismos como el "Bureau International de l'Heure", la "Carte du Ciel" y la "Solar Union". Su primer presidente fue Benjamin Baillaud. Su objetivo es promover y coordinar la cooperación internacional en la astronomía y la elaboración de las reglas de nomenclatura de los diferentes cuerpos celestes.

Los grupos de trabajo de la UAI también incluyen "El grupo de trabajo para la nomenclatura de sistemas planetarios" (WGPSN en inglés), que establece la nomenclatura para cuerpos planetarios y las convenciones de nomenclatura astronómica.

En el año 2015, la UAI tenía socios individuales, principalmente profesionales de la astronomia con el grado académico de doctor, y 74 socios nacionales, es decir, países afiliados a la UAI. El 84 % de sus miembros son masculinos y un 16 % de miembros femeninos. La presidenta actual de la UAI es Ewine F. van Dishoeck (2018-2021) y la astrónoma portuguesa Teresa Lago ejerce como Secretaria General de la institución para el mismo trienio.

La UAI coordina 37 comisiones y 85 grupos de trabajo sobre diferentes temas astronómicos agrupadas en 12 disciplinas principales.

La Asamblea General de la UAI se reúne cada tres años. Los encuentros incluyen:




</doc>
<doc id="37994" url="https://es.wikipedia.org/wiki?curid=37994" title="Canfranc">
Canfranc

Canfranc (en aragonés "Canfrán" o "Canfranc") es un municipio del Valle de Canfranc, comarca de la Jacetania, en la provincia de Huesca (Aragón, España).
Tiene una población de 540 habitantes (INE 2019).

Canfranc se encuentra a 1 040 msnm en el valle del Aragón o valle de Canfranc, a los pies del puerto del Somport, paso fronterizo por carretera entre Francia y España.
En la actualidad, el túnel internacional de Somport ha venido a sustituir al puerto, siendo una de las puntas del eje Somport-Sagunto.

El término municipal de Canfranc ocupa una superficie de 71,6 km². Consta de dos núcleos de población, Canfranc (1040 m) y Canfranc-Estación (1190 m), este segundo situado en el paraje denominado Los Arañones donde se encuentra la Estación Internacional de Canfranc.

Al norte del mismo se sitúan las estaciones de deportes de invierno de Candanchú y Astún, aunque pertenecen a los municipios de Aísa y Jaca respectivamente.

El municipio posee un clima de montaña, con temperaturas mínimas por debajo de 0 °C buena parte del año, persistiendo el riesgo de heladas hasta bien avanzada la primavera cuando todavía perdura la nieve.
La precipitación media anual es de 1900 mm y la temperatura media anual es de 8.7 °C.

En el término de Canfranc se encuentra el conjunto megalítico de la Rinconada de la Canal Roya, que comprende el dolmen La Rinconada, un túmulo y un círculo de piedras.
El túmulo de piedras posee un diámetro exterior de 17 m y su altura es de 2,5 m. Se halla formado por rocas de andesita.

Asimismo, existen dos dólmenes en la Explanada del Barranco de las Blancas; el dolmen oeste, de gran tamaño, conserva cinco losas laterales y la tapa, estando tumbada la losa que mira al sur, la cual podría presentar ventana.

Por el municipio transcurre principalmente el río Aragón, pero también existen ríos de tamaño considerable. Río de Canal Roya, barranco de Izas, Barranco de Ip y barranco de Aguaré. Ambos aumentan bastante el cauce del río Aragón. También cabe destacar que hay otros muchos ríos de menor importancia.

En el siglo XI, y a la vera del camino de Francia, nació Canfranc como pueblo fronterizo.
En medio de un valle profundo, con escasos recursos agrícolas, sus habitantes se dedicaron al comercio, basando su economía en las transacciones entre Aragón y el Bearn, incluida la acogida de viajeros y peregrinos ya que por aquí discurre el Camino de Santiago aragonés.
Se sabe que en 1095 existía una alberguería privada; es posible que posteriormente esta fuera pasada a Santa Cristina, dando lugar al Hospital de este nombre en Somport.
De acuerdo a algunos autores, su población estaría principalmente compuesta por francos y el topónimo, podría estar relacionado con dicho elemento.
O puent de pelegrins de Canfranc.

Los reyes de Aragón concedieron a Canfranc importantes concesiones debido a dos circunstancias: el ser punto fronterizo y la pobreza de sus tierras.
Así, en la segunda mitad del siglo XIV, Pedro IV concedió el llamado «privilegio del vino», mediante el cual los habitantes de Canfranc podían transportar vino blanco y tinto sin pagar el impuesto correspondiente.
Otro privilegio, otorgado por la reina María de Castilla y fechado en 1440, reconocía al municipio el derecho de «rota» y «porta», obligándose sus habitantes a mantener expedito, vigilar y defender el camino a la frontera, a cambio de la exención de impuestos y el cobro de los derechos de peaje y aduana.

Históricamente, las comunicaciones interfronterizas han marcado la evolución histórica del valle: el Somport —también llamado en tiempos el puerto de Canfranc— es el paso fronterizo menos abrupto y más transitado de todo el Pirineo central. Desde el viejo camino de herradura, transformado en 1876 en carretera, hasta la construcción del ferrocarril transfronterizo —inaugurado en 1928— y el reciente túnel de carretera, la historia de las comunicaciones a través del Somport está estrechamente ligada a la propia historia de Canfranc.

La antigua villa de Canfranc, pese a los devastadores incendios de 1617 y 1944, todavía conserva parte de su antiguo patrimonio monumental. Los restos del castillo, de origen medieval y ampliado en el siglo XVI, se conservan sobre una roca; la fachada de la Torre de Aznar Palacín del siglo XIV; la Iglesia parroquial de la Asunción, con sus cuatro retablos barrocos; y el conjunto monumental de la Trinidad fundado en el siglo XVI por Blasco de Les para atender a pobres y peregrinos.

La importancia estratégica y militar del valle de Canfranc, dada su cercanía a la frontera, fue la causa de la construcción de diversos fuertes y posiciones militares, entre los que destacan el Fuerte de Coll de Ladrones (siglo XVII – XIX) y la Torreta de los Fusileros (siglo XIX).

A mediados del siglo XIX, el geógrafo e historiador Pascual Madoz refiere que Canfranc contaba con 84 casas distribuidas en dos hileras, formando una calle y una pequeña plaza. Señala que sus cultivos principales eran trigo, avena y algo de azafrán, obteniéndose buenas patatas pero escasas verduras. También menciona la caza mayor —osos y corzos—, así como la pesca de truchas.

La llegada del ferrocarril revitalizó el pueblo, surgiendo un núcleo urbano en el paraje de Los Arañones, donde está emplazada la Estación Internacional. Las obras de perforación del túnel duraron varios lustros; la construcción de la línea férrea comenzó en 1882 y en 1908 se abrió el túnel, si bien el ferrocarril tardó varios años más en ser inaugurado (1928).
Tras el devastador incendio de abril de 1944, se trasladó el ayuntamiento y la capitalidad del municipio al nuevo núcleo urbano.
El viento huracanado, encajonado en el estrecho valle del Aragón, convirtió en terrible incendio la pequeña chispa que salió de la chimenea de un vecino.
Aquel siniestro destruyó la mayor parte de las 130 casas que entonces había, y éstas, salvo alguna excepción, no se reconstruyeron, pese a que se llevó a cabo una colecta nacional para recaudar fondos. En torno a dicha reconstrucción se han denunciado graves injusticias y arbitrariedades, pues finalmente tan sólo se recuperaron trece de las casas.

Canfranc consta de dos unidades de población, el pueblo originario, llamado Canfranc o Canfranc Pueblo con 77 habitantes (4 km al norte de Villanúa y a 16 km de Jaca), y Canfranc-Estación o Arañones, emplazada 4 km más al norte que cuenta con 463 habitantes.
Entre ambos sumaban 540 habitantes a comienzos de 2019 (INE).
Canfranc albergó una población superior a los 1 000 habitantes entre 1910 y 1970, año en el que se produce el cierre al tráfico internacional de la Estación de Canfranc, iniciándose la actual recesión demográfica.
De igual forma, la apertura de la misma en 1928, marcó el punto álgido de la población con 1 698 habitantes.
En los últimos tiempos, el desarrollo del turismo ha permitido una ligera recuperación.

Inaugurada el 18 de julio de 1928 por Alfonso XIII, la Estación Internacional vivió momentos de esplendor y otros oscuros y novelescos como los del famoso oro alemán en tiempos de la II Guerra Mundial. Para su construcción fue preciso realizar una gran obra de ingeniería forestal ya que las avenidas de nieve y desprendimientos de las laderas eran constantes dada la geografía tan abrupta que presenta y la intensa deforestación a la que se vio sometida en los siglos anteriores. Para corregir este problema se realizó lo que se considera uno de los mejores ejemplos de restauración hidrológico-forestal que se conocen. De esta manera se consiguió retener el terreno y restaurar la masa arbórea, lo que posibilitó la construcción del ferrocarril que comunicaría ambos lados del Pirineo.
El edificio está construido con materiales usuales de la arquitectura de principios de siglo XX como cristal, cemento y hierro, bajo diversas influencias arquitectónicas. En el exterior tiene grandes ventanales que se abren mediante arcos de medio punto. Está cubierto con pizarra, material utilizado en Aragón para las cubiertas.

El 27 de marzo de 1970 un tren de mercancías francés descarriló a la entrada del puente de L'Estanguet en Francia. Desde entonces el tráfico internacional ferroviario quedó interrumpido.

Junto al cauce del río Aragón, en el centro de la población, se alza la Iglesia de la Asunción.
La primitiva iglesia parroquial, construida bajo la advocación de Nuestra Señora en el siglo XII, fue entregada por el rey Pedro II al monasterio de Santa Cristina de Somport en 1202. El templo actual corresponde a varios períodos arquitectónicos; la capilla mayor fue probablemente construida en el siglo XVI.
El templo posee planta rectangular y cabecera cuadrada. Consta de tres naves, cubiertas con bóvedas de cañón, divididas en cinco tramos.
La fábrica exterior es de sillería en las esquinas y mampuesto en los lienzos de los muros, destacando la torre-campanario de planta octogonal.

En la salida de la villa hacia Jaca se pueden observar las ruinas de la Iglesia de la Trinidad.
Data de mediados del siglo XVI, cuando el rico comerciante don Blasco de Les decidió instituir una fundación religiosa y asistencial que incluía la iglesia. Con el paso de los años la fundación entró en una profunda decadencia, propiciando el abandono del templo en el siglo XIX.

Mucho más moderna es la Iglesia de Nuestra Señora del Pilar, diseñada por Miguel Fisac en 1965.
Constituye un complejo parroquial rural, compuesto por un templo y los servicios anexos de sacristía, despacho parroquial, archivo, salón parroquial y vivienda del párroco.
Considerada una joya del movimiento moderno, plasma en su arquitectura las nuevas tendencias surgidas tras el Concilio Vaticano II: «desornamentación», racionalidad compositiva y funcionalidad.
Su interior contiene varias imágenes de interés como un Cristo Crucificado en madera policromada del siglo XVI —procedente de la Iglesia parroquial de Tiermas—, una Virgen del Pilar en madera policromada del siglo XVIII —de la antigua iglesia de Los Arañones—, así como una Inmaculada Concepción del siglo XX.
Ha sido declarada Bien de Interés Cultural en 2007.

Canfranc-Pueblo, que es el núcleo original de población del municipio, mantiene trazas del pueblo caminero antiguo que fue sobre el Camino de Santiago Aragonés, y conserva parte de su antiguo patrimonio arquitectónico, aunque este fue devastado por sendos incendios en 1617 y 1944. Por ello este conjunto urbano mereció ser incluido en el catálogo de bienes inscritos como parte del sitio Caminos de Santiago de Compostela: Camino Francés y Caminos del Norte de España, dentro del Patrimonio de la Humanidad. 

Entre los edificios destacables se encuentra la Escuela de 1928 y reconstruida en 1948, los restos del castillo, de origen medieval y ampliado en el siglo XVI, se conservan sobre una roca; la fachada de la Torre de Aznar Palacín del siglo XIV; la Iglesia parroquial de la Asunción, con sus cuatro retablos barrocos; y el conjunto monumental de la Trinidad fundado en el siglo XVI por Blasco de Les para atender a pobres y peregrinos.

En la plaza del Ayuntamiento en Canfranc-Estación se encuentran en el mismo edificio, la Casa Consistorial y la Oficina de Turismo.

Otro lugar de interés es el Centro A Lurte, término en lengua aragonesa que significa «el alud», inaugurado en 2001. Es un centro de referencia de la nieve y los aludes, que cuenta con una exposición permanente sobre los diversos tipos de nieve y avalanchas.

Ya en época medieval existía una torre que defendía la entrada a Canfranc por el camino que venía de Francia.
Era un edificio de planta cuadrada que se alzaba sobre un resalte rocoso enfrente del molino.
A esta estructura se le añadieron, en épocas diversas, una torre, varios torreones y un patio de armas, cuyo conjunto constituía el Castillo de Canfranc.
Puente de los Peregrinos (Canfranc)
Desde el siglo XVII hasta mediados del siglo XVIII, su guarnición se ocupó de la vigilancia de la frontera y del control de los comerciantes que circulaban por el camino del Somport.
Mantuvo sus muros hasta 1928, cuando se abrió la nueva variante de la carretera por medio de su recinto, perviviendo tan sólo la fachada oriental.

La estratégica montaña de «Cot de Latrós» fue elegida en 1751 para levantar una nueva fortificación fronteriza que sustituyera al Castillo en cuanto a labores defensivas. Las obras, concluidas en 1758, dieron lugar al Fuerte de Coll de Ladrones aunque es posible que en el mismo lugar hubiera una fortaleza anterior fechada en 1592.
Funcionó hasta 1845 o 1850, momento en el que fue abandonada, para ser nuevamente rehecha en 1887.
Conserva un robusto y largo muro de mampostería que cubre parte de la ladera en un punto muy escarpado.

Otro edificio defensivo, la Torreta de los Fusileros, se construyó en 1876 tras la conclusión de la carretera de Zaragoza a Francia por Somport. Su fin era defender la nueva vía de comunicación. Es un edificio fortificado de tres plantas en piedra sillar, con planta rectangular de esquinas fuertemente redondeadas.
Posee un patio interior cubierto y cuenta con un foso de protección con puente levadizo.

De la llamada Torre Vieja o de la Espelunca, construida en el siglo XVI, apenas se conservan restos escasos. Las torres tenían la particularidad de estar situadas sobre el propio camino, que atravesaba bajo el edificio mediante pasadizo abovedado. Su construcción está documentada en 1592 y mantuvo su uso militar al menos hasta el siglo XVIII.

Ya en el siglo XX, se creó la denominada «línea P» («P» de Pirineos), conjunto defensivo construido entre los años 1944 y 1959 a lo largo de la vertiente pirenaica española.
Está formada por cientos de posiciones de hormigón armado (búnkeres) en las cabeceras de los valles fronterizos en previsión de una hipotética invasión desde Francia.








</doc>
<doc id="37997" url="https://es.wikipedia.org/wiki?curid=37997" title="IEEE 802.11">
IEEE 802.11

El estándar 802.11 es una familia de normas inalámbricas creada por el Instituto de Ingenieros Eléctricos y Electrónicos (IEEE). 802.11n es la forma más apropiada de llamar Wi-Fi, lanzada en 2009. Mejoró con respecto a versiones anteriores de Wi-Fi con múltiples radios, técnicas avanzadas de transmisión y recepción, y la opción de usar el espectro de 5 GHz. Todo implica una velocidad de datos de hasta 600 Mbps.

La familia 802.11 consta de una serie de técnicas de modulación semidúplex ("half duplex") por medio del aire que utilizan el mismo protocolo básico. Al estándar 802.11-1997 le siguió el 802.11b, que fue el primero aceptado ampliamente. Posteriormente surgirían versiones mejoradas: 802.11a, 802.11g, 802.11n y 802.11ac. Otras normas de la familia (c-f, h, j) son las modificaciones de servicio que se utilizan para extender el alcance actual de la norma existente, que también puede incluir correcciones de una especificación anterior. 

Las versiones 802.11b y 802.11g utilizan la banda ISM de 2,4 GHz, en Estados Unidos, por ejemplo, operan bajo las Reglas y Reglamentos de la Comisión Federal de Comunicaciones de los Estados Unidos. Debido a esta elección de la banda de frecuencia, los equipos 802.11b y 802.11g pueden sufrir interferencias con electrodomésticos tan comunes como el microondas o el horno o con dispositivos Bluetooth. Es por eso que deben controlar dicha susceptibilidad a las interferencias mediante métodos de señalización de espectro ensanchado por secuencia directa (DSSS) y de multiplexación por división de frecuencia ortogonal (OFDM), respectivamente. 

Por otro lado la versión 802.11a utiliza la banda U-NII de 5 GHz que, para gran parte del mundo, ofrece al menos 23 canales que no se superponen en lugar de la banda de frecuencia ISM de 2,4 GHz que ofrece solo tres canales que no se superponen. 802.11n puede utilizar la banda de 2,4 GHz o la de 5 GHz mientras que 802.11ac utiliza solo la banda de 5 GHz. El segmento del espectro de radiofrecuencia utilizado por la 802.11 varía de un país a otro. Las frecuencias utilizadas por los canales uno a seis de 802.11b y 802.11g caen dentro de la banda de radioaficionados de 2,4 GHz. Los operadores de radioaficionados con licencia pueden operar dispositivos 802.11b / g.


La versión original del estándar 802.11, del Instituto de Ingenieros Eléctricos y Electrónicos (IEEE), publicada en 1997, especifica dos velocidades de transmisión “teóricas” de 1 y 2 megabits por segundo (Mbit/s) que se transmiten por señales infrarrojas (IR). IR sigue siendo parte del estándar, aunque no hay implementaciones disponibles. Tuvo una revisión en 1999 con la intención de actualizarla, no obstante a día de hoy está obsoleta.

El estándar original también define el protocolo "múltiple acceso por detección de portadora evitando colisiones" ("carrier sense multiple access with collision avoidance", CSMA/CA) como método de acceso. Una parte importante de la velocidad de transmisión teórica se utiliza en las necesidades de esta codificación para mejorar la calidad de la transmisión bajo condiciones ambientales diversas, lo cual se tradujo en dificultades de interoperabilidad entre equipos de diferentes marcas. Estas y otras debilidades fueron corregidas en el estándar 802.11b, que fue el primero de esta familia en alcanzar amplia aceptación entre los consumidores.

La revisión 802.11a fue aprobada en 1999. Este estándar utiliza el mismo juego de protocolos de base que el estándar original, opera en la banda de 5 GHz y utiliza 52 subportadoras de multiplexación por división de frecuencias ortogonales ("Orthogonal Frequency-Division Multiplexing", OFDM) con una velocidad máxima de 54 Mbit/s, lo que lo hace un estándar práctico para redes inalámbricas con velocidades reales de aproximadamente 20 Mbit/s. La velocidad de datos se reduce a 48, 36, 24, 18, 12, 9 o 6 Mbit/s en caso necesario. Tiene un alcance de 20 km con radios especiales.
Dado que la banda de 2,4 GHz es muy utilizada hasta el punto de estar llena de gente, la utilización de la relativamente inusitada banda de 5 GHz da una ventaja significativa a 802.11a. Sin embargo, esta alta frecuencia portadora también presenta una desventaja: el intervalo global eficaz de 802.11a es menor que el de 802.11b / g. En teoría, las señales 802.11a son absorbidas más fácilmente por paredes y otros objetos sólidos en su trayectoria debido a su longitud de onda más pequeña, y, como resultado, no pueden penetrar hasta los de 802.11b. En la práctica, 802.11b normalmente tiene un rango más alto a bajas velocidades. 802.11a también sufre de interferencia, pero localmente puede haber menos señales para interferir, resultando en menos interferencia y mejor rendimiento.
Tiene 12 canales sin solapamiento, 8 para red inalámbrica y 4 para conexiones punto a punto. No puede interoperar con equipos del estándar 802.11b, excepto si se dispone de equipos que implementen ambos estándares.

La revisión 802.11b del estándar original fue ratificada en 1999.

802.11b tiene una velocidad máxima de transmisión de 11 Mbps y utiliza el mismo método de acceso definido en el estándar original CSMA/CA. El estándar 802.11b funciona en la banda de 2,4 GHz. Debido al espacio ocupado por la codificación del protocolo CSMA/CA, en la práctica, la velocidad máxima de transmisión con este estándar es de aproximadamente 5,9 Mbit/s sobre TCP y 7,1 Mbit/s sobre UDP.

Los productos que usan esta versión aparecieron en el mercado a principios del 2000, ya que 802.11b es una extensión directa de la técnica de modulación definida en la norma original. El aumento dramático del rendimiento de 802.11b y su reducido precio llevó a la rápida aceptación de 802.11b como la tecnología de LAN inalámbrica definitiva. 

Los dispositivos que utilizan 802.11b pueden experimentar interferencias con otros productos que funcionan en la banda de 2,4 GHz. 

Es menos usado que los primeros dos, por la implementación que este protocolo refleja. El protocolo ‘c’ es utilizado para la comunicación de dos redes distintas o de diferentes tipos, así como puede ser tanto conectar dos edificios distantes el uno con el otro, así como conectar dos redes de diferente tipo a través de una conexión inalámbrica. El protocolo ‘c’ es más utilizado diariamente, debido al costo que implica las largas distancias de instalación con fibra óptica, que aunque más fidedigna, resulta más costosa tanto en instrumentos monetarios como en tiempo de instalación.

"El estándar combinado 802.11c no ofrece ningún interés para el público general. Es solamente una versión modificada del estándar 802.11d que permite combinar el 802.11d con dispositivos compatibles 802.11 (en el nivel de enlace de datos capa 2 del modelo OSI)". Velocidad (teórica)- 600 Mbit/s
Velocidad (práctica) - 100 Mbit/s
Frecuencia - 2,4 Ghz y 5,4 Ghz
Ancho de banda - 20/40 MHz
Alcance - 820 metros
Año de implementación - 2009

Es un complemento del estándar 802.11 que está pensado para permitir el uso internacional de las redes 802.11 locales. Permite que distintos dispositivos intercambien información en rangos de frecuencia según lo que se permite en el país de origen del dispositivo móvil.

La especificación IEEE 802.11e ofrece un estándar inalámbrico que permite interoperar entre entornos públicos, de negocios y usuarios residenciales, con la capacidad añadida de resolver las necesidades de cada sector.
A diferencia de otras iniciativas de conectividad sin cables, esta puede considerarse como uno de los primeros estándares inalámbricos que permite trabajar en entornos domésticos y empresariales. La especificación añade, respecto de los estándares 802.11b y 802.11a, características QoS y de soporte multimedia, a la vez que mantiene compatibilidad con ellos. Estas prestaciones resultan fundamentales para las redes domésticas y para que los operadores y proveedores de servicios conformen ofertas avanzadas.
Incluye, asimismo, corrección de errores (FEC) y cubre las interfaces de adaptación de audio y vídeo con la finalidad de mejorar el control e integración en capas de aquellos mecanismos que se encarguen de gestionar redes de menor rango.
El sistema de gestión centralizado integrado en QoS evita la colisión y cuellos de botella, mejorando la capacidad de entrega en tiempo crítico de las cargas. 
Con el estándar 802.11, la tecnología IEEE 802.11 soporta tráfico en tiempo real en todo tipo de entornos y situaciones. Las aplicaciones en tiempo real pueden funcionar fiablemente gracias a la Calidad de Servicio (QoS) proporcionado por el 802.11e. El objetivo del nuevo estándar 802.11e fue introducir nuevos mecanismos a nivel de capa MAC para soportar los servicios que requieren garantías de Calidad de Servicio.
Para cumplir con su objetivo IEEE 802.11e introdujo un nuevo elemento llamado "Hybrid Coordination Function" (HCF) con dos tipos de acceso: 

En este estándar se definen cuatro categorías de acceso al medio (Ordenadas de menos a más prioritarias).
Para conseguir la diferenciación del tráfico se definen diferentes tiempos de acceso al medio y diferentes tamaños de la ventana de contención para cada una de las categorías.

Es una recomendación para proveedores de puntos de acceso que permite que los productos sean más compatibles. Utiliza el protocolo IAPP que le permite a un usuario itinerante cambiarse claramente de un punto de acceso a otro mientras está en movimiento sin importar qué marcas de puntos de acceso se usan en la infraestructura de la red. También se conoce a esta propiedad simplemente como itinerancia.

En junio de 2003, se ratificó un tercer estándar de modulación: 802.11g, que es la evolución de 802.11b. Este utiliza la banda de 2,4 Ghz (al igual que 802.11b) pero opera a una velocidad teórica máxima de 54 Mbit/s, que en promedio es de 22,0 Mbit/s de velocidad real de transferencia, similar a la del estándar 802.11a. Es compatible con el estándar b y utiliza las mismas frecuencias. Buena parte del proceso de diseño del nuevo estándar lo tomó el hacer compatibles ambos modelos. Sin embargo, en redes bajo el estándar g la presencia de nodos bajo el estándar b reduce significativamente la velocidad de transmisión.

Los equipos que trabajan bajo el estándar 802.11g llegaron al mercado muy rápidamente, incluso antes de su ratificación que fue dada aprox. el 20 de junio de 2003. Esto se debió en parte a que para construir equipos bajo este nuevo estándar se podían adaptar los ya diseñados para el estándar b.

Actualmente se venden equipos con esta especificación, con potencias de hasta medio vatio, que permite hacer comunicaciones de más de 50 km con antenas parabólicas o equipos de radio apropiados.

Existe una variante llamada 802.11g+ capaz de alcanzar los 108Mbps de tasa de transferencia. Generalmente solo funciona en equipos del mismo fabricante ya que utiliza protocolos propietarios.

802.11g tiene la ventaja de poder coexistir con los estándares 802.11a y 802.11b, esto debido a que puede operar con las Tecnologías RF DSSS y OFDM. Sin embargo, si se utiliza para implementar usuarios que trabajen con el estándar 802.11b, el rendimiento de la celda inalámbrica se verá afectado por ellos, permitiendo solo una velocidad de transmisión de 54 Mbps. Esta degradación se debe a que los clientes 802.11b no comprenden OFDM.

Suponiendo que se tiene un punto de acceso que trabaja con 802.11g, y actualmente se encuentran conectados un cliente con 802.11b y otro 802.11g, como el cliente 802.11b no comprende los mecanismos de envío de OFDM, el cual es utilizados por 802.11g, se presentarán colisiones, lo cual hará que la información sea reenviada, degradando aún más nuestro ancho de banda.

Suponiendo que el cliente 802.11b no se encuentra conectado actualmente, el Punto de acceso envía tramas que brindan información acerca del Punto de acceso y la celda inalámbrica. Sin el cliente 802.11b, en las tramas se verían la siguiente información:

ERP ("Extended Rate Physical") hace referencia a dispositivos que utilizan tasas de transferencia de datos extendidos, en otras palabras, NON_ERP hace referencia a 802.11b. Si fueran ERP, soportarían las altas tasas de transferencia que soportan 802.11g.

Cuando un cliente 802.11b se asocia con el AP (Punto de acceso), este último alerta al resto de la red acerca de la presencia de un cliente NON_ERP. Cambiando sus tramas de la siguiente forma:

Ahora que la celda inalámbrica sabe acerca del cliente 802.11b, la forma en la que se envía la información dentro de la celda cambia. Ahora cuando un cliente 802.11g quiere enviar una trama, debe advertir primero al cliente 802.11b enviándole un mensaje RTS (Request to Send) a una velocidad de 802.11b para que el cliente 802.11b pueda comprenderlo. El mensaje RTS es enviado en forma de unicast. El receptor 802.11b responde con un mensaje CTS (Clear to Send). 

Ahora que el canal está libre para enviar, el cliente 802.11g realiza el envío de su información a velocidades según su estándar. El cliente 802.11b percibe la información enviada por el cliente 802.11g como ruido.

La intervención de un cliente 802.11b en una red de tipo 802.11g, no se limita solamente a la celda del Punto de acceso en la que se encuentra conectado, si se encuentra trabajando en un ambiente con múltiples AP en Roaming, los AP en los que no se encuentra conectado el cliente 802.11b se transmitirán entre sí tramas con la siguiente información:

La trama anterior les dice que hay un cliente NON_ERP conectado en uno de los AP, sin embargo, al tenerse habilitado Roaming, es posible que este cliente 802.11b se conecte en alguno de ellos en cualquier momento, por lo cual deben utilizar los mecanismo de seguridad en toda la red inalámbrica, degradando de esta forma el rendimiento de toda la celda. Es por esto que los clientes deben conectarse preferentemente utilizando el estándar 802.11g. Wi-Fi (802.11b / g).

La especificación 802.11h es una modificación sobre el estándar 802.11 para WLAN desarrollado por el grupo de trabajo 11 del comité de estándares LAN/MAN del IEEE (IEEE 802) y que se hizo público en octubre de 2003. 802.11h intenta resolver problemas derivados de la coexistencia de las redes 802.11 con sistemas de radares o satélites.

El desarrollo del 802.11h sigue unas recomendaciones hechas por la Unión Internacional de Telecomunicaciones (ITU) que fueron motivadas principalmente a raíz de los requerimientos que la Oficina Europea de Radiocomunicaciones (ERO) estimó convenientes para minimizar el impacto de abrir la banda de 5 GHz, utilizada generalmente por sistemas militares, a aplicaciones ISM (ECC/DEC/(04)08).

Con el fin de respetar estos requerimientos, 802.11h proporciona a las redes 802.11a la capacidad de gestionar dinámicamente tanto la frecuencia, como la potencia de transmisión.

DFS ("Dynamic Frequency Selection") es una funcionalidad requerida por las WLAN que operan en la banda de 5 GHz con el fin de evitar interferencias co-canal con sistemas de radar y para asegurar una utilización uniforme de los canales disponibles.

TPC ("Transmitter Power Control") es una funcionalidad requerida por las WLAN que operan en la banda de 5 GHz para asegurar que se respetan las limitaciones de potencia transmitida que puede haber para diferentes canales en una determinada región, de manera que se minimiza la interferencia con sistemas de satélite.

Está dirigido a batir la vulnerabilidad actual en la seguridad para protocolos de autenticación y de codificación. El estándar abarca los protocolos 802.1x, TKIP (Protocolo de Claves Integra – Seguras – Temporales), y AES ("Advanced Encryption Standard", Estándar de Cifrado Avanzado). Se implementa en "Wi-Fi Protected Access" (WPA2). La norma fue ratificada el 24 de junio de 2004. 

Es equivalente al 802.11h, en la regulación de Japón. Fue diseñada especialmente para el mercado japonés y permite que la operación de LAN inalámbrica en la banda de 4,9 a 5 GHz se ajuste a las normas japonesas para la operación de radio para aplicaciones en interiores, exteriores y móviles. La enmienda se ha incorporado a la norma IEEE 802.11-2007 publicada.

Permite a los conmutadores y puntos de acceso inalámbricos calcular y valorar los recursos de radiofrecuencia de los clientes de una red WLAN, mejorando así su gestión. Está diseñado para ser implementado en software, para soportarlo el equipamiento WLAN solo requiere ser actualizado. Y, como es lógico, para que el estándar sea efectivo, han de ser compatibles tanto los clientes (adaptadores y tarjetas WLAN) como la infraestructura (puntos de acceso y conmutadores WLAN).

En enero de 2004, el IEEE anunció la formación de un grupo de trabajo 802.11 (Tgn) para desarrollar una nueva revisión del estándar 802.11. La velocidad real de transmisión podría llegar a los 600 Mbps (lo que significa que las velocidades teóricas de transmisión serían aun mayores), y debería ser hasta diez veces más rápida que una red bajo los estándares 802.11a y 802.11g, y unas cuarenta veces más rápida que una red bajo el estándar 802.11b. También se esperaba que el alcance de operación de las redes sea mayor con este nuevo estándar gracias a la tecnología MIMO ("Multiple Input – Multiple Output"), que permite utilizar varios canales a la vez para enviar y recibir datos gracias a la incorporación de varias antenas (3). Existen también otras propuestas alternativas que podrán ser consideradas. El estándar ya está redactado, y se viene implantando desde 2008. A principios de 2007 se aprobó el segundo boceto del estándar. Anteriormente ya había dispositivos adelantados al protocolo y que ofrecían de forma no oficial este estándar (con la promesa de actualizaciones para cumplir el estándar cuando el definitivo estuviera implantado). 
A diferencia de las otras versiones de Wi-Fi, 802.11n puede trabajar en dos bandas de frecuencias: 2,4 GHz (la que emplean 802.11b y 802.11g) y 5 GHz (la que usa 802.11a). Gracias a ello, 802.11n es compatible con dispositivos basados en todas las ediciones anteriores de Wi-Fi. Además, es útil que trabaje en la banda de 5 GHz, ya que está menos congestionada y en 802.11n permite alcanzar un mayor rendimiento.

El estándar 802.11n fue ratificado por la organización IEEE el 11 de septiembre de 2009 con una velocidad de 600 Mbps en capa física.

La mayoría de productos son de la especificación "b" o "g", sin embargo, ya se ha ratificado el estándar 802.11n que sube el límite teórico hasta los 600 Mbps. Actualmente ya existen varios productos que cumplen el estándar "N" con un máximo de 600 Mbps (80-100 estables).

El estándar 802.11n hace uso simultáneo de ambas bandas, 2,4 Ghz y 5 Ghz. Las redes que trabajan bajo los estándares 802.11b y 802.11g, tras la reciente ratificación del estándar, se empiezan a fabricar de forma masiva y es objeto de promociones por parte de los distintos Proveedores de servicios de Internet, de forma que la masificación de la citada tecnología parece estar en camino.
Todas las versiones de 802.11xx, aportan la ventaja de ser compatibles entre sí, de forma que el usuario no necesitará nada más que su adaptador wifi integrado, para poder conectarse a la red.

Sin duda esta es la principal ventaja que diferencia wifi de otras tecnologías propietarias, como LTE, UMTS y Wimax, las tres tecnologías mencionadas, únicamente están accesibles a los usuarios mediante la suscripción a los servicios de un operador que está autorizado para uso de espectro radioeléctrico, mediante concesión de ámbito nacional.

La mayor parte de los fabricantes ya incorpora a sus líneas de producción equipos wifi 802.11n, por este motivo la oferta ADSL, suele venir acompañada de wifi 802.11n en el mercado de usuario doméstico.

Se conoce que el futuro estándar sustituto de 802.11n será 802.11ac con tasas de transferencia superiores a 1 Gb/s.

Este estándar opera en el espectro de frecuencias de 5,90 GHz y de 6,20 GHz, especialmente indicado para automóviles. Será la base de las comunicaciones dedicadas de corto alcance (DSRC). La tecnología DSRC permitirá el intercambio de datos entre vehículos y entre automóviles e infraestructuras en carretera. Además agrega el wireless access in vehicular environments o WAVE (acceso inalámbrico en entornos vehiculares), un sistema de comunicación vehicular. Esta mejora es muy usada en la implementación de los Sistemas Inteligentes de Transporte (SIT). Esto incluye el intercambio de datos entre vehículos entre sí y entre vehículos y la infraestructura de las carreteras por las que circulan.

También se conoce como "Fast Basic Service Set Transition", y su principal característica es permitir a la red que establezca los protocolos de seguridad que identifican a un dispositivo en el nuevo punto de acceso antes de que abandone el actual y se pase a él. Esta función, que una vez enunciada parece obvia e indispensable en un sistema de datos inalámbricos, permite que la transición entre nodos demore menos de 50 milisegundos. Un lapso de tiempo de esa magnitud es lo suficientemente corto como para mantener una comunicación vía VoIP sin que haya cortes perceptibles.

Fue publicada en 2011. Y servirá para permitir la configuración remota de los dispositivos cliente. Esto permitirá una gestión de las estaciones de forma centralizada (similar a una red celular) o distribuida, a través de un mecanismo de capa de enlace de datos (capa 2). Esto incluye, por ejemplo, la capacidad de la red para supervisar, configurar y actualizar las estaciones cliente.
Además de la mejora de la gestión, las nuevas capacidades proporcionadas por el "11v" se desglosan en cuatro categorías:

Es un protocolo que hace parte de IEEE 802.11 basado en el protocolo 802.11i, sirve para proteger redes WLAN contra ataques sutiles en las tramas de gestión inalámbricas (WLAN).

Todavía no concluido. TGw está trabajando en mejorar la capa del control de acceso del medio de IEEE 802.11 para aumentar la seguridad de los protocolos de autenticación y codificación.

Las WLAN envían la información del sistema en tramas desprotegidas, que las hace vulnerables. Este estándar podrá proteger las redes contra la interrupción causada por los sistemas malévolos que crean peticiones desasociadas que parecen ser enviadas por el equipo válido.
Se intenta extender la protección que aporta el estándar 802.11i más allá de los datos hasta las tramas de gestión, responsables de las principales operaciones de una red. Estas extensiones tendrán interacciones con IEEE 802.11r e IEEE 802.11u.

IEEE 802.11ac (también conocido como WiFi 5 o WiFi Gigabit) es una mejora a la norma IEEE 802.11n, se ha desarrollado entre el año 2011 y el 2013, y finalmente aprobada en julio de 2014.

El estándar consiste en mejorar las tasas de transferencia hasta 433 Mbit/s por flujo de datos, consiguiendo teóricamente tasas de 1.3 Gbit/s empleando 3 antenas. Opera dentro de la banda de 5 GHz, amplía el ancho de banda hasta 160 MHz (40 MHz en las redes 802.11n), utiliza hasta 8 flujos MIMO e incluye modulación de alta densidad (256 QAM).

La IEEE 802.11ax, nombrada también como "Wi-Fi 6" o "Wi-Fi 6th Generation" por la Wi-Fi Alliance, está diseñado para operar en los espectros ya existentes de 2.4 GHz y 5 GHz. Introduce OFDMA para mejorar la eficiencia espectral global. Se esperaba que esta tecnología esté disponible a los usuarios en 2019, pero no fue sino hasta principios de 2020 que la FCC anunció la ampliación del espectro de uso a los 6 GHz y sin licencia, por lo que cualquier proveedor podrá usarlo sin coste. 

Los identificadores de canales, frecuencias centrales, y dominios reguladores para cada canal usado por 802.11b y 802.11g:

Los estándares 802.11b y 802.11g utilizan la banda de 2,4 GHz. En esta banda se definieron 11 canales utilizables por equipos wifi, que pueden configurarse de acuerdo a necesidades particulares. Sin embargo, los 11 canales no son completamente independientes (un canal se superpone y produce interferencias hasta un canal a 4 canales de distancia). El ancho de banda de la señal (22 MHz) es superior a la separación entre canales consecutivos (5 MHz), por eso se hace necesaria una separación de al menos 5 canales con el fin de evitar interferencias entre celdas adyacentes, ya que al utilizar canales con una separación de 5 canales entre ellos (y a la vez cada uno de estos con una separación de 5 MHz de su canal vecino) entonces se logra una separación final de 25 MHz, lo cual es mayor al ancho de banda que utiliza cada canal del estándar 802.11, el cual es de 22 MHz. Tradicionalmente se utilizan los canales 1, 6 y 11, aunque se ha documentado que el uso de los canales 1, 5, 9 y 13 (en dominios europeos) no es perjudicial para el rendimiento de la red.

Esta asignación de canales usualmente se hace solo en el Punto de acceso, pues los “clientes” automáticamente detectan el canal, salvo en los casos en que se forma una red “Ad-Hoc” o punto a punto cuando no existe Punto de acceso.

Los identificadores de canales, frecuencias centrales, y dominios reguladores para cada canal usado por IEEE 802.11a:

Pese a que el ensanchado de espectro y la modulación son diferentes, en la banda de 5GHz se mantiene un ancho de banda cercano a los 20MHz, de manera que el requerimiento de separación de 5 canales de la banda de 2,4GHz se mantiene.
En Europa, para evitar interferencias con comunicaciones por satélite y sistemas de radar existentes, es necesaria la implantación de un control dinámico de las frecuencias y un control automático de las potencias de transmisión; por ello las redes 802.11a deben incorporar las modificaciones del 802.11h.




</doc>
<doc id="37999" url="https://es.wikipedia.org/wiki?curid=37999" title="Conflicto árabe-israelí">
Conflicto árabe-israelí

El conflicto árabe-israelí (en árabe: الصراع العربي الإسرائيلي "Al-Sira'a Al'Arabi A'Israili"; en hebreo: הסכסוך הישראלי-ערבי "Ha'Sikhsukh Ha'Yisraeli-Aravi") se refiere a la tensión política y los conflictos armados entre el Estado de Israel y sus vecinos árabes, en particular los palestinos.

Su definición, historia y posibles soluciones son materia de permanente debate y los problemas que incluye varían con el tiempo. Al día de hoy, las principales cuestiones son la soberanía sobre la Franja de Gaza y Cisjordania, el estatus de la parte oriental de Jerusalén, de los Altos del Golán y de las Granjas de Shebaa, el destino de los asentamientos israelíes y de los refugiados palestinos, el reconocimiento de Israel y Palestina como Estados independientes, el derecho de ambos a existir y vivir en paz al abrigo de amenazas y actos de fuerza, así como la relación de Israel con Siria y el Líbano.

Actualmente Israel tiene tratados de paz vigentes con Egipto y Jordania que garantizan su convivencia. Así mismo, tiene tratados de alto el fuego firmados con el Líbano, Siria y Arabia Saudí que, si bien no reconocen la existencia de Israel, sí que han supuesto en la práctica un mecanismo eficaz para el cese de las hostilidades. También existe un complejo acuerdo provisional con Palestina, que supone el establecimiento de una especie de protectorado israelí sobre zona palestina y un alto el fuego parcial.

Durante más de quince siglos, el pueblo judío vivió dividido en varios países del mundo, especialmente en Europa, en lo que se conoce como la diáspora judía. La convivencia de estos con el resto de europeos no siempre fue fácil, y las persecuciones y pogromos, especialmente en la Europa del Este a finales del siglo XIX, fueron determinantes para la aparición y auge del sionismo político, que reclamaba un Estado propio para todas las comunidades judías dispersas por el mundo. Los sionistas culturales subrayaban la importancia que tenía convertir a Palestina en un centro para el crecimiento espiritual y cultural del pueblo judío. En la época en la que se fundó el sionismo, Palestina formaba parte del Imperio otomano y estaba habitada por árabes cristianos y musulmanes en su gran mayoría, así como por una pequeña comunidad de judíos religiosos que, aunque minoritaria, tenía una implantación significativa en la ciudad de Jerusalén y en sus alrededores.

En el año 1914, el Imperio otomano decidió entrar en la Primera Guerra Mundial de lado de las potencias centrales y el gobierno británico empezó a ver al movimiento sionista como un posible aliado en una guerra que parecía desarrollarse mal para los aliados. De manera paralela, agentes británicos como T. E. Lawrence alentaron rebeliones árabes contra el dominio otomano en Oriente Próximo bajo la promesa de futuros Estados independientes árabes. Hacia 1917, David Lloyd George y Arthur Balfour, primer ministro y secretario de exteriores respectivamente, buscaban alianzas que pudieran mejorar el curso de la guerra. Se consideró entonces que los judíos podrían ser doblemente útiles, ayudando a sostener el frente oriental y estimulando el esfuerzo bélico estadounidense. Fue así como se produjo el 2 de noviembre de 1917 la Declaración Balfour, por la que el Reino Unido se declaraba favorable a los planes sionistas de creación de un hogar nacional judío en Palestina. La victoria aliada y el hundimiento del Imperio otomano dejaría al gobierno británico con el control de Palestina en los siguientes treinta años, adoptando la forma oficial de Mandato de la recientemente creada Sociedad de Naciones.

Durante la década de 1920 el número de judíos en Palestina se incrementó notoriamente: en 1922 su número era de 83 790 sobre una población total de 752 048; en 1929 había 156 481 en una población total de 992 559, duplicando su población en siete años. La inmigración judía se canalizaba a través de la Organización Sionista Mundial, cuya figura principal era Jaim Weizmann, y vinculada con la Agencia Judía para Palestina, que ejercía como un gobierno para los judíos de Palestina, comprando tierra y construyendo escuelas, hospitales y asentamientos. La principal figura de la organización hacia la mitad de los años treinta era David Ben Gurión.La filosofía de Ben Gurión y sus colegas era la de "construir Sion" forjando una nación judía, es decir, asentar las bases para la futura creación de un Estado judío en Palestina. Los árabes no poseían instituciones similares a las que los judíos estaban desarrollando, debido al feudalismo que aún existía y que permitía a los clanes más poderosos dominar a la mayoría de la población, destacando los continuos enfrentamientos entre los Husseinis y Nashashibis.

Palestina estuvo relativamente tranquila entre 1922 y 1928, momento en que se desató la violencia en forma de enfrentamientos entre árabes y judíos y entre los propios árabes en la Barrera Oeste de Jerusalén. En agosto de 1929 estos enfrentamientos se saldaron con la matanza de Hebrón, de Safed y de otras comunidades judías palestinas en 1929. El resultado de estos incidentes fue la muerte de 133 judíos y 116 árabes, y una reinterpretación a la baja de la Declaración Balfour y de las aspiraciones sionistas: dos comisiones británicas, bajo el mando de Walter Shaw y John Hope-Simpson, intentaron redefinir la política británica en Palestina, identificando el miedo de los árabes ante la inmigración y la compra de tierras por parte judía como principal causa de las dificultades entre ambas comunidades. La recomendación de Hope-Simpson de que las características del territorio sólo admitirían 20.000 inmigrantes judíos más provocó el rechazo de los sionistas. Sin embargo, en febrero de 1931, el primer ministro británico Ramsay MacDonald escribió a Weizmann indicándole que su gobierno no tenía intención de prohibir la inmigración judía, debido principalmente a que la situación en Palestina parecía haberse calmado nuevamente. Sin embargo, esta calma relativa no duraría mucho tiempo: el desarrollo político europeo cambiaría por completo el conflicto árabe-israelí. El 30 de enero de 1933, Adolf Hitler llegó al poder en Alemania y en marzo ya había asegurado su dictadura.

El incremento del antisemitismo en Alemania y Rumanía hizo que un gran número de judíos se marchara de Europa, teniendo a Palestina como única opción debido a las restricciones migratorias de los Estados Unidos. En 1936, la población judía se había incrementado hasta los 370.483 sobre una población total de 1.336.518. La reacción árabe contra lo que ellos consideraban una transformación desagradable del país fue la Revuelta Árabe, que empezó el 15 de abril de 1936 con el asesinato de un judío cerca de Nablús. La escala de la revuelta dio lugar a un despliegue importante de fuerzas británicas, así como a la simpatía oficial de estas hacia la Haganá, la fuerza defensiva de la Agencia Judía. La Comisión Real Palestina, bajo mando de Lord Peel, fue encomendada con la labor de investigar las causas subyacentes de los disturbios y de recomendar una solución para lidiar con las "quejas legítimas" de árabes y judíos. Su máximo exponente, el profesor Reginald Coupland de la Universidad de Oxford, llegó a la conclusión de que existían en Palestina dos culturas claramente diferenciadas: una árabe de origen asiático y una judía de origen europeo. Considerando que dos culturas tan contrastadas no llegarían a convivir en un solo Estado, Coupland propuso como única solución la partición en dos Estados distintos. Coupland consiguió convencer a sus colegas de la Comisión e incluso a Weizmann, quien se convirtió en un defensor de la partición. Aun así, no todos los sionistas estaban a favor de la partición y los árabes se oponían frontalmente a ella.

Hacia finales de 1937 los británicos empezaron a abandonar su apoyo a la idea del hogar judío y a la partición del Mandato, puesto que buscaban asegurarse la simpatía árabe en la situación prebélica que preludiaba la Segunda Guerra Mundial. Una nueva declaración, conocida como Libro Blanco, fue patrocinada por Malcolm MacDonald, ministro británico de Colonias, lo que supuso un giro completo de la política británica en Palestina y el fin del compromiso con los judíos iniciado dos décadas antes mediante la Declaración Balfour. El Libro Blanco fue publicado semanas antes del comienzo de la Segunda Guerra Mundial y establecía que en el plazo de diez años Palestina se convertiría en un solo Estado independiente gobernado en común por árabes y judíos. Según el nuevo plan, la inmigración judía se limitaría a 75.000 personas en los siguientes cinco años y con el previo consentimiento árabe (lo que en la práctica suponía el cierre a la inmigración legal en vísperas del inicio de la guerra), de modo que los judíos mantendrían siempre un estatus minoritario debido a su menor peso demográfico.

A pesar de que muchos árabes se dieron cuenta de que la nueva declaración favorecía en gran medida sus aspiraciones, el gobierno egipcio y algunos de los principales líderes palestinos como el clérigo exiliado Amin al-Husayni la rechazaron por considerarla insuficiente. La alianza del líder palestino con el Tercer Reich, que incluyó el reclutamiento de una división de musulmanes bosnios para las SS, dañaría muy significativamente la causa palestina al verse asociada con el régimen nazi.

En el caso de los judíos, la nueva política del gobierno británico plasmada en el Libro Blanco, cuya vigencia se mantuvo durante la guerra, fue vista como un acto de delación, pese a lo cual mantuvieron su apoyo a Gran Bretaña en el inminente conflicto bélico. En noviembre de 1938, la "Reichskristallnacht", en la que los nazis dieron rienda suelta al terror de Estado contra los judíos, reveló las verdaderas intenciones del III Reich y provocó un crecimiento de la emigración judía.

El impedimento por parte de las autoridades británicas de la inmigración judía hacia Palestina (como puso de relieve el caso del barco SS "Struma") confirmó la creencia judía de que la protección podía ser alcanzada únicamente mediante la construcción de un Estado donde los judíos pudieran controlar su propio destino, motivo por el cual la Haganá empezó a comprar y a fabricar armas. Más problemáticas para los británicos fueron las actividades de otros dos grupos judíos clandestinos: el Irgun Zvai Leumi ("Organización Militar Nacional") y el Leh'i ("Luchadores por la Libertad de Israel"), que representaban la tradición de extrema derecha del sionismo, en conflicto con la Agencía Judía y el movimiento oficial.

En febrero de 1944, el Irgún, dirigido por un joven judío polaco, Menahem Begin, proclamó que los británicos habían traicionado al pueblo judío y declaró la guerra al Mandato. El Lehi había sido creado por otro judío polaco, Abraham Stern, cuyo rencor hacia los británicos hizo que simpatizara con los propios alemanes. El 6 de noviembre de 1944, sus miembros asesinaron a Lord Moyne, el ministro británico en Oriente Medio. Este hecho provocó la antipatía de Winston Churchill, amigo cercano de Moyne, quien había planeado desarrollar el Estado judío justo después de la guerra.

En aquel tiempo, gran parte de Oriente Medio estaba bajo control del Reino Unido, que tenía intereses en el Golfo Pérsico y bases aéreas en Irak. De los Estados limítrofes con Palestina, Líbano y Siria habían sido liberados del Mandato francés en 1943 y 1946 respectivamente. Egipto mantenía relaciones importantes con los británicos debido al tratado de 1936, cuyo elemento más importante era la zona del Canal de Suez. Transjordania se independizaría en 1946, pero siguió estrechamente vinculada a Gran Bretaña. En un momento que se iba a demostrar histórico para los árabes palestinos, estos carecían de las necesarias estructuras políticas y de liderazgo, incapaces de copiar la bien organizada estructura política de los judíos con la Agencia Judía. La represión británica de la revuelta de 1936-1939 había causado la muerte, el encarcelamiento o el exilio de la mayor parte de los líderes árabes palestinos, la incautación de importantes cantidades de armas y una sensación general de hastío bélico entre la población civil. En opinión de algunos autores, el mundo árabe en general, y el árabe palestino en particular, se encontraban en una condición de desventaja para resistir al desafío sionista que se avecinaba. En opinión de otros historiadores, como Joan B. Culla, el maximalismo de la posición árabe impidió aprovechar las oportunidades de que dispusieron en los distintos procesos negociadores, priorizando la expulsión de los judíos y los intereses propios de los nuevos Estados árabes vecinos de la zona (incluso la posibilidad de anexionarse la parte árabe de Palestina), por encima de los intereses de la población árabe palestina y del derecho reconocido a estos para disponer de su propio Estado.

El presidente estadounidense, Harry S. Truman, tenía cierta simpatía por la causa judía pero, en la práctica, Truman solamente dirigió su atención hacia Palestina después de un intento fallido de persuadir al Congreso de que permitiese a un gran número de judíos establecerse en los Estados Unidos. El 31 de agosto de 1946 pidió formalmente al gobierno británico que emitiese 100.000 certificados de inmigración, señalando que ""ningún otro problema es tan importante para quienes han conocido los horrores de los campos de concentración"". La respuesta británica fue negativa, señalando que en los campos europeos había muchas víctimas de Hitler y que los judíos no debían ponerse a la cabeza de la lista. El tono de la contestación británica mostraba hasta qué punto la actitud británica se había alejado de la simpatía pro-sionista de 1944, y se abría así el camino para la lucha de los judíos contra los británicos en el mandato de Palestina, cuyo atentado más famoso fue contra el cuartel general británico, alojado en el Hotel Rey David de Jerusalén, que causó 91 muertos y que, a la larga, condujo a los británicos fuera de Palestina y allanó el camino para la creación del Estado judío.

El 29 de noviembre de 1947, tras múltiples disputas diplomáticas, la Asamblea General de las Naciones Unidas aprobó el Plan de Partición de Palestina en dos Estados, uno árabe y otro judío, ni compactos ni homogéneos, divididos en tres respectivas porciones apenas unidas entre sí. El estado judío supondría un 55% del territorio del Mandato (14.100 km²), incluido el desierto del Néguev, y su población estaría formada por 500.000 judíos y 400.000 árabes palestinos. En ese momento, los judíos solo poseían el 7% de las tierras de Palestina. El estado árabe palestino tendría el 44% del territorio del Mandato (11.500 km²) y una minoría de unos 10.000 judíos. Jerusalén y su área circundante, incluida Belén, conformarían un "corpus separatum" de 700 km² bajo la administración del Consejo de Administración Fiduciaria de las Naciones Unidas. Además, este plan preveía la retirada del ejército británico del Mandato antes de agosto de 1948 y la fijación de las fronteras entre los dos Estados y en la propia Jerusalén.

Los judíos aceptaron el Plan propuesto, a pesar de no estar de acuerdo con los términos de un reparto que hacía indefendible y poco viable el territorio asignado, pero los árabes lo rechazaron de plano. El Alto Comité Árabe (el organismo de la dirigencia árabe-palestina) calificó de "absurdos, impracticables e injustos" tanto el reparto como la propuesta federal y, viendo perdido el terreno diplomático, amenazaron con la guerra para defender la Palestina árabe. En cualquier caso, por aquel entonces ya se estaba desarrollando una guerra civil en Palestina que consistió principalmente en ataques terroristas primero y en movimientos militares un tiempo después. Los distintos ataques terroristas de grupos judíos como el Lehi, el Irgún o la propia Haganá fueron propiciando un éxodo de la población árabe de Palestina a zonas que consideraban más seguras. A mediados de enero de 1948, los ataques de los diversos grupos judíos habían empujado a una quinta parte de la población de Jaffa (unas 15.000 personas) a huir a otros lugares. A finales de enero, se calcula que unos 20.000 árabes palestinos habían abandonado sus hogares en Haifa. 

Se sucedieron una serie de cruentos atentados, entre los que cabe destacar la bomba plantada por la Haganá en el Hotel Semiramis de Jerusalén, matando a 26 civiles entre los que estaba el cónsul general español en la ciudad, o el atentado contra el diario judío "Palestine Post", que dejó veinte víctimas civiles poco después. En plena guerra civil, dos hechos terminaron por derrumbar la moral de la población civil palestina y acrecentar aún más su huida. El 8 de abril murió accidentalmente Abdelkader al-Husayni, comandante del Ejército de la Santa Yihad y principal líder militar palestino. Al día siguiente, el 9 de abril, unos 120 hombres del Irgún y el Lehi perpetraron la masacre de Deir Yassin. El recuento de víctimas se fijó en 107 aldeanos muertos, entre los que se contaron multitud de mujeres, ancianos y niños. Las noticias de la masacre desataron el pánico aún más entre la población civil árabe palestina y provocaron su huida en masa. Una nueva masacre perpetrada por la Brigada Golani en Khirbet Nasser ed-Din, que dejó 22 aldeanos muertos, tuvo lugar el 12 de abril de 1948, un día antes de que un grupo de guerrilleros árabes palestinos emboscaran un convoy de camiones, ambulancias, autobuses y vehículos acorazados que se dirigían al enclave judío del Monte Scopus. Murieron 78 profesores, alumnos, enfermeras, doctores y soldados de la Haganá que los escoltaban.
Las ciudades de Tiberíades, Haifa y Jaffa fueron cayendo en manos judías durante las siguientes semanas, con duros bombardeos sobre zonas residenciales árabes que provocaron la huida en masa de la población civil árabe palestina. El 10 de mayo, las tropas judías tomaron Safad y expulsaron a sus miles de habitantes árabes palestinos. En Beisan, todos los árabes fueron expulsados de sus hogares y enviados a Nazaret o más allá del río Jordán. Cuando las tropas judías entraron en Acre, solo unos 3.000 de sus 13.400 habitantes habían permanecido en la ciudad.

En este contexto, el 15 de mayo de 1948 expiró el Mandato Británico de Palestina. Un día antes, los judíos proclamaron la independencia del Estado de Israel en su parte del territorio otorgada por el Plan de Partición de la ONU, debido a la festividad del sabbat. Esta declaración provocó como reacción inmediata la invasión de los ejércitos de la alianza árabe, dando así inicio a la guerra árabe-israelí de 1948.

Ben Gurion, que inauguró el cargo de primer ministro del Estado de Israel, aceptó la partición de Palestina en territorios israelíes y territorios palestinos que la ONU estableció en 1947. Pero tenía un viejo pensamiento de fondo: en carta a su mujer confió que 

Al día siguiente de la Declaración de independencia del Estado de Israel en el territorio asignado por el Plan de la ONU para la partición de Palestina de 1947, cinco Estados árabes vecinos (Líbano, Siria, Transjordania, Irak y Egipto), inconformes con dicho Plan y con la expulsión o huida de la población civil palestina que se estaba llevando a cabo por parte de las tropas israelíes, le declararon la guerra al naciente Estado de Israel e intentaron invadirlo bajo el mandato del general árabe Salim Abdala Kais.

En la guerra intermitente que tuvo lugar durante los siguientes 15 meses (interrumpida ocasionalmente por varias treguas promovidas por la ONU), Israel conquistó y se anexionó un 26% adicional del antiguo Mandato británico, mientras que Transjordania y Egipto ocuparon la parte restante destinada por la ONU al Estado árabe-palestino: Egipto ocupó la Franja de Gaza y Transjordania se anexionó Cisjordania y Jerusalén Este, refundando el país con el nombre de Jordania.

La guerra provocó cientos de miles de desplazados en ambos sentidos: más de dos tercios de la población civil árabe palestina (en torno a unas 750.000 personas) fueron obligados a desplazarse a la Franja de Gaza y a Cisjordania, así como a otros países árabes vecinos, como Líbano, Siria o Jordania, dando origen al problema de los refugiados palestinos que todavía hoy perdura. En la zona israelí quedaron 156.000 árabes palestinos, que adquirieron la nacionalidad israelí y que, en general, en el plano teórico, gozaron de los derechos plenos de ciudadanía a partir de 1950, incluyendo su incorporación al ejército en el caso de los drusos. Sin embargo, los árabes palestinos de Israel vivieron sujetos a la ley marcial hasta 1966, y en torno a la mitad de ellos -conocidos legalmente como presentes ausentes- sufrieron la expropiación de sus tierras y propiedades por parte del Estado de Israel, que las destinó a los nuevos inmigrantes judíos. Según la historiografía tradicional israelí, la salida de los árabes de su tierra se debió a que la dirigencia árabe instigó a la población árabe en Palestina a abandonar sus hogares para garantizar a las tropas árabes mayor libertad de movimiento. Sin embargo, las fuentes árabes y los nuevos historiadores israelíes, así como numerosos historiadores internacionales, afirman que no existen pruebas al respecto y señalan a los ataques del ejército y los grupos paramilitares judíos como fuente del éxodo masivo palestino.

De forma paralela, las comunidades judías que habitaban en países árabes (muchas de ellas desde antes de la expansión árabe y musulmana), se vieron obligadas a emigrar en los años siguientes. Solo durante la década de 1950, 608.200 judíos orientales, una cifra algo menor a la de refugiados palestinos, huyeron o fueron expulsados de territorios árabes y se refugiaron en Israel, donde obtuvieron la ciudadanía israelí gracias a la denominada Ley del Retorno; otros 290.800 refugiados judíos se establecieron en Francia o en los Estados Unidos (véase Éxodo judío de países árabes). Se trató de un fenómeno de intensidad variable según los países en los que aconteció, y que fue desde la confiscación de bienes y tierras en algunos países a la persecución directa de los judíos en otros. En cualquier caso, el resultado en la práctica fue la liquidación casi total de las comunidades judías en los países árabes.

El 11 de diciembre de 1948, en su resolución 194, la Asamblea General de las Naciones Unidas dictaminó ""que" "debe permitirse a los refugiados que deseen regresar a sus hogares y vivir en paz con sus vecinos, que lo hagan así lo antes posible, y que deberán pagarse indemnizaciones a título de compensación por los bienes de los que decidan no regresar a sus hogares y por todo bien perdido o dañado cuando, en virtud de los principios del derecho internacional o por razones de equidad, esta pérdida o este daño deba ser reparado por los Gobiernos o autoridades responsables"". De esta manera, reconoció el derecho al retorno de los refugiados palestinos y creó la Agencia de las Naciones Unidas para los Refugiados Palestinos (UNRWA en sus siglas inglesas) con la esperanza de un retorno inmediato, algo que sin embargo no sucedió. De hecho, ya durante la propia guerra de 1948, en el mismo momento en el que la población palestina estaba huyendo o siendo expulsada por el ejército israelí, Israel hizo pública su postura oficial de rechazo al retorno de los refugiados. Al prolongarse indefinidamente su condición de "refugiados", y quedar su suerte en manos de la ONU, nunca obtuvieron la nacionalidad de los países árabes que los acogieron (salvo en el caso de Jordania, que reconoció como nacionales a los palestinos cisjordanos y jerosolimitanos) y permanecieron en condiciones de desarraigo y precarización. Por su parte, los refugiados judíos, que no recibieron reconocimiento ni ayuda alguna por parte de la ONU, fueron integrados rápidamente en Israel y alojados en las viviendas que habían quedado vacías tras el éxodo de la población palestina.

Aunque los armisticios de 1949 supusieron la independencia de Israel, no significaron el final de las hostilidades entre este país y sus vecinos árabes. Durante toda la década de 1950 se sucedieron tanto ataques de fedayines palestinos apoyados principalmente por Egipto como ataques israelíes contra posiciones fronterizas jordanas, sirias y egipcias. En octubre de 1953, una unidad israelí comandada por Ariel Sharon llevó a cabo la masacre de Qibya en territorio palestino bajo control jordano, en la que asesinaron a 69 civiles, dos tercios de ellos mujeres y niños.. En diciembre de 1955, durante la Operación Kinneret, un ataque israelí contra posiciones sirias en la costa del Mar de Galilea causó la muerte a 54 soldados sirios. En febrero de 1955, sin mediar provocación previa, comandos israelíes mataron a 38 soldados egipcios en un ataque transfronterizo ocurrido en la Franja de Gaza, por entonces bajo control egipcio. Esto llevó al presidente egipcio, Gamal Abdel Nasser, a decretar el cierre del estrecho de Tirán para barcos y aviones procedentes de o dirigidos a Israel. Por su parte, Israel firmó una alianza con el Reino Unido y Francia para un ataque conjunto a Egipto, dado que ambas naciones estaban enfrentadas con Gamal Abdel Nasser por la nacionalización del Canal de Suez.

El 29 de octubre de 1956 se desencadenó la invasión conjunta de Egipto por parte de Israel, que ocupó la península del Sinaí, y Francia y Reino Unido, que enviaron paracaidistas a Puerto Said y el Canal de Suez. Aunque militarmente los aliados alcanzaron todos sus objetivos, la presión diplomática conjunta de la Unión Soviética y EE. UU. forzó a estos a retirarse, en lo que los países árabes consideraron una victoria política. Como consecuencia de esta guerra, la ONU decidió el despliegue de una fuerza de cascos azules entre Egipto e Israel; sin embargo, Israel se negó a permitir el acceso a las fuerzas de interposición de la ONU, conocida como UNEF, por lo que solo se pudo desplegar en la parte egipcia de la frontera. El acercamiento franco-israelí trajo también consigo el desarrollo de la energía nuclear israelí, que se materializó en 1958 en la creación de la central nuclear de Dimona. 

En este contexto, los árabes comenzaron a organizarse en diferentes asociaciones para resistir. La más importante fue la OLP (Organización para la liberación de Palestina), fundada en mayo de 1964 en Jerusalén con el apoyo de la Liga Árabe y a instancias del presidente egipcio Gamal Abdel Nasser, como organización palestina unificada.

En 1967 el líder egipcio Gamal Abdel Nasser pidió a las Naciones Unidas que retiraran a los Cascos Azules de Gaza, el Sinaí, y de las islas de Tirán y Sanafir (a la entrada del Golfo de Aqaba), solicitud que la ONU, por entonces presidida por U Thant, aceptó pese a que eso significaba renunciar a su papel de interposición. Egipto movilizó 80.000 soldados en el Sinaí y envió tropas a sus islas del golfo de Aqaba el 22 de mayo. Esto volvió a poner en peligro la salida de los barcos israelíes al Mar Rojo, y fue considerado un "casus belli" por parte del gobierno israelí. Egipto propuso llevar el cierre de los Estrechos de Tirán a una corte de arbitraje internacional. Ese mismo mes, Egipto, Siria e Irak firmaron un pacto de defensa mutua. El 5 de junio de 1967, ante la negativa egipcia de desbloquear el Golfo de Aqaba, Israel bombardeó la aviación egipcia situada en la península del Sinaí, dando comienzo de esta forma a la Guerra de los Seis Días.

En los 6 días que duró la guerra, Israel conquistó la Franja de Gaza, Cisjordania, Jerusalén Este, la península del Sinaí y los Altos del Golán (Siria). Salvo el caso de la península del Sinaí, que fue devuelta a Egipto en 1979 como consecuencia de los acuerdos de paz de Camp David, el resto de territorios siguen a día de hoy ocupados militarmente por Israel. 

La guerra ocasionó una segunda oleada de entre 300.000 y 400.000 refugiados palestinos, de los que casi un tercio se convirtieron en refugiados por segunda vez. La mayoría se exilió en Líbano, Jordania, Siria y los Estados del Golfo Pérsico.

En noviembre de 1967 Naciones Unidas adoptó la resolución 242, que ""afirma que el acatamiento de los principios de la Carta requiere que se establezca una paz justa y duradera en el Oriente Medio, la cual incluya la aplicación de los dos principios siguientes:" ""retiro de las fuerzas armadas israelíes de los territorios que ocuparon durante el reciente conflicto"" y ""terminación de todas las situaciones de beligerancia o alegaciones de su existencia, y respeto y reconocimiento de la soberanía, integridad territorial e independencia política de todos los Estados de la zona y de su derecho a vivir en paz dentro de fronteras seguras y reconocidas y libres de amenaza o actos de fuerza"". Además, pedía ""garantizar la libertad de navegación por las vías internacionales de navegación de la zona"" y ""lograr una solución justa del problema de los refugiados"", así como establecer zonas desmilitarizadas entre los Estados de la zona.

El texto de la resolución 242 es ambiguo en cuanto a si exige a Israel una retirada de todos los territorios ocupados en la guerra (según el texto de la versión francesa ""Retrait des forces armées israéliennes des territoires occupés lors du récent conflit"" y española: ""retiro de las fuerzas armadas israelíes de los territorios que ocuparon durante el reciente conflicto"") o de solo una parte de los territorios (según la versión inglesa: "Withdrawal of Israeli armed forces from territories occupied in the recent conflict"). La ausencia del artículo "the" ("los") de la versión inglesa del texto ha llevado a Israel a argumentar que la resolución solo le exige "la retirada de territorios ocupados en el reciente conflicto", y no "la retirada de los territorios ocupados en el reciente conflicto", como exigen las versiones española y francesa del mismo documento.

La OLP rechazó categóricamente la resolución por considerar que "pisotea los derechos de dos millones de palestinos", y exigió que Israel cumpliese su parte y se retirase de los territorios conquistados, cosa que no hizo y que marcaría el conflicto hasta la actualidad.

En los años siguientes a la guerra de 1967 se pasó a una guerra soterrada conocida como guerra de Desgaste. Israel mantuvo la ocupación militar de todos los territorios conquistados en la guerra de 1967, sometiendo a la población nativa a la ley marcial e incentivando los asentamientos de judíos en los territorios ocupados, en una clara violación de la Cuarta Convención de Ginebra, que en su artículo 49 prohíbe el traslado de población civil de la potencia ocupante al territorio ocupado.

En cuanto a Jerusalén Este, Israel se anexionó esta parte de la ciudad a la conclusión de la guerra y comenzó la demolición de barrios palestinos y la construcción de barrios judíos en la zona. Por su parte, Egipto multiplicó su hostigamiento militar contra Israel, que culminaría en la guerra de Yom Kipur en 1973, e intensificó su apoyo a los grupos armados palestinos que, a partir de 1968 (y con el apoyo de Siria al Frente Popular para la Liberación de Palestina), iniciaron una escalada terrorista internacional que incluyó secuestros de aviones comerciales y destrucción de aviones vacíos, atentados contra embajadas y diplomáticos de Israel, ataques a intereses de la comunidad judía en todo el mundo o atentados contra instalaciones de gas y petroleras. Esta escalada culminaría finalmente en la masacre de Múnich, perpetrada contra deportistas israelíes que competían en los Juegos Olímpicos de 1972. Israel se vengó contra los perpetradores mediante la Operación Cólera de Dios en los años siguientes.

La Guerra de Desgaste fue una guerra limitada entre Egipto e Israel que tuvo lugar durante los años 1968 a 1970. Fue iniciada por Egipto en un intento de recobrar el Sinaí tras la negativa israelí de abandonar los territorios ocupados en la Guerra de los Seis Días, de acuerdo con lo exigido en la resolución 242. La Guerra de Desgaste supuso un importante número de bajas en el ejército israelí (más del doble de muertes que la Guerra de los Seis Días) y en el egipcio. La guerra supuso también la intervención directa de la Unión Soviética en el conflicto árabe israelí y terminó con un alto el fuego firmado entre los países en 1970 que dejaba las líneas de armisticio en el mismo lugar en el que estaban cuando la guerra comenzó.

La Guerra de Yom Kipur, Guerra del Ramadán o Guerra de Octubre, fue un conflicto armado entre Israel y los países árabes de Egipto y Siria que tuvo lugar durante octubre de 1973. Egipto y Siria iniciaron el conflicto para recuperar los territorios que Israel ocupaba desde la Guerra de los Seis Días de 1967. Ambas partes sufrieron graves pérdidas, aunque Israel mantuvo los territorios conquistados.

El 6 de octubre de 1973, día de la festividad judía del Yom Kipur, Egipto y Siria lanzaron su ataque contra Israel. La fecha había sido escogida con cuidado ya que la mayoría de la población israelí estaba en sus casas ayunando, lo que hacía su posición defensiva más vulnerable. El ejército egipcio cruzó rápidamente el canal de Suez superando rápidamente las defensas israelíes. Al mismo tiempo, las fuerzas sirias avanzaron en los Altos del Golán. Una vez recuperada la península del Sinaí, Anwar el Sadat, presidente de Egipto, decidió parar el frente egipcio dando por buena la recuperación de su antiguo territorio. Esto permitió a Israel concentrar todas sus fuerzas en el frente norte y recuperar la iniciativa en una guerra en la que por primera vez vio como fue superado por sus enemigos árabes. Esta decisión tomada por el presidente egipcio, junto con la posterior firma de un tratado de paz con Israel, fue considerada una traición en el mundo árabe y motivaría, años más tarde, su asesinato en un desfile militar.

Superado el impacto del golpe militar y el alto número de bajas, a mediados de octubre Israel había movilizado a sus tropas y había lanzado una serie de contraataques en ambos frentes. Desplazó a los sirios de los Altos del Golán, invadió el propio país y amenazó la capital, Damasco, con la artillería, situando un grueso de tropas a 100 km; al mismo tiempo, avanzaba en la contraofensiva del Sinaí, haciendo retroceder a los egipcios más allá de sus fronteras y cruzando el Canal de Suez, situando unidades blindadas a 40 km de El Cairo.

Los países árabes, ante esta realidad, decidieron iniciar una guerra económica y embargaron el petróleo de los países que ayudaban a Israel, al mismo tiempo que reducían las ventas con el propósito de lograr un aumento de los precios. Su efecto, que pasó a la historia como la crisis del petróleo de 1973, fue una desestabilización de la economía internacional, que presionó a los EE. UU. y la URSS a alcanzar un acuerdo a través de la ONU. Tras la resolución de la ONU de 22 de octubre, se llegó a un alto el fuego el 25. Egipto comenzó a alejarse de las tesis soviéticas y acercarse a los Estados Unidos, mientras que Siria mantuvo sus posiciones vinculadas a la URSS. Ambas partes se consideraron vencedoras. A partir de este momento, Israel no confiará más en una seguridad estática, pero la aproximación de Egipto al mundo occidental favorecerá los acuerdos de Camp David tiempo después.

La gran sensación de vulnerabilidad que causó la ofensiva egipcio-siria en la guerra del Yom Kipur empujó a Israel a comenzar a negociar unilateralmente con Egipto una paz separada que implicase la devolución de los territorios ocupados (la península del Sinaí) a cambio de un acuerdo de paz permanente y el establecimiento de relaciones colaterales. El 17 de septiembre de 1978, el presidente egipcio Anwar el-Sadat y el primer ministro israelí Menachem Begin firmaron los Acuerdos de Paz de Camp David en presencia del presidente de los Estados Unidos, Jimmy Carter. Este acuerdo supuso el primer tratado de paz de Israel con un país árabe y la aplicación por primera vez en la historia de Israel de la doctrina de paz por territorios, establecida en la resolución 242 de la ONU en 1967. 

El acuerdo de paz trajo importantes ventajas a ambos países, que tuvieron que realizar importantes renuncias para llevarlo a cabo. Israel consiguió un tratado de paz definitivo con la mayor potencia árabe de la región que, una vez fuera del tablero, reducía enormemente el riesgo vital del Estado de Israel. En cambio, tuvo que devolver el territorio conquistado en 1967, incluidos varios asentamientos establecidos al norte de la península del Sinaí, lo que supuso una importante fractura política dentro del país. Del lado egipcio, el tratado de paz significó la recuperación de todo el territorio nacional perdido en la Guerra de los Seis Días (la Franja de Gaza, que también había sido ocupada por Israel en dicho conflicto, no era legalmente parte de Egipto). Sin embargo, Egipto se alejó abruptamente de la "doctrina de los tres noes", establecida por la Liga Árabe en la Conferencia de Jartum en 1967, que declaraba "no a la paz con Israel, no al reconocimiento de Israel y no a las negociaciones con Israel". Esto le supuso numerosas acusaciones de traición y un importante aislamiento en la comunidad árabe durante años. Anwar el-Sadat fue especialmente señalado como traidor y murió asesinado por uno de sus guardaespaldas en El Cairo el 6 de octubre de 1981. 

Tras el Septiembre Negro de 1970, miles de guerrilleros palestinos son expulsados de Jordania y la OLP decide establecer sus bases en el Líbano, desde donde comenzaron a realizar incursiones en territorio israelí para provocar atentados, manteniendo enfrentamientos directos con las fuerzas israelíes a lo largo de la frontera sur del Líbano. En marzo de 1978, después de que un comando palestino causase la muerte de 35 civiles israelíes en un autobús, el gobierno de Menahem Begin ordena a tres brigadas del Tsahal que invadiesen unos 1000 kilómetros cuadrados del sur del Líbano, hasta el río Litani, con el objetivo de acabar con las bases de los "fedayín". Antes de retirarse, tres meses después, Israel establece una "zona de seguridad" de 10 kilómetros de ancho ´sobre territorio con una población mayoriteriamente cristiano-maronita, y la deja en manos de una milicia aliada llamada Ejército del Líbano Libre (después Ejército del Sur del Líbano). A su vez, la ONU despliega una fuerza de interposición (UNIFIL) entre la "zona de seguridad" y el río Litani con la misión de velar por la desmilitarización del área. Entre 1979 y 1981, la comunidad cristiana, a través de las Falanges libanesas, establece una alianza estratégica con Israel, una vez rota la que mantenían con Siria hasta entonces.
En junio de 1982 junto al recrudecimiento de los incidentes armados en la frontera o dentro de Israel, se produce un atentado del grupo palestino de Abu Nidal contra Shlomo Argov, el embajador israelí en Londres. Pese a que Abu Nidal era un enemigo acérrimo de Yaser Arafat y de la OLP, Israel tomó este atentado como un "casus belli" para la invasión de el Líbano, en una operación de grandes proporciones que denominará "Operación Paz para Galilea". Tres días después del atentado, el 6 de junio, un impresionante despliegue del Tsahal formado por casi 100.000 soldados (equivalente a ocho divisiones) y 1.500 tanques, apoyados por la aviación y la marina, superaron la "zona de seguridad" y las fuerzas de la UNIFIL y profundizaron en territorio libanés. Aunque la idea declarada por el entonces ministro de Defensa, Ariel Sharón, era no superar 40 kilómetros, las fuerzas del Tsahal llegan hasta la periferia de Beirut y la carretera que la une con Damasco. El propio Sharon y Rafael Eitan señalarían con posterioridad que, pese a lo que habían afirmado públicamente en dicho momento, "el Gabinete israelí sabía por adelantado que el alcance de la operación no se limitaría a cuarenta kilómetros". Los palestinos ofrecieron una resistencia muy desigual y Siria aprovechó para reforzar sus propias unidades a la zona, añadiendo 30.000 soldados y 400 tanques de refuerzo. Israel destruyó el sistema de misiles antiaéreos sirio desplegado en la Bekaa libanesa y derribó 29 de los 100 aviones cazabombarderos que Siria había enviado, en lo que supuso la mayor batalla aérea desde la Guerra de Corea. El 11 de junio entró en vigor un alto el fuego impuesto por EE.UU. y exigido también por la URSS. Con la mediación estadounidense, comenzó la evacuación de casi 15.000 combatientes y burócratas de la OLP y también de algunos de los soldados sirios. Portaban su armamento ligero, en señal de capitulación honrosa: los sirios regresaron a su país por vía terrestre, y los palestinos fueron transportados a Chipre, desde donde se dispersaron por varios países (como Argelia, Yemen, Irak, Jordania o Sudán). La cúpula de la OLP, con Arafat a la cabeza, estableció su nuevo cuartel general en Túnez.

Unos días después, el Parlamento libanés, ante la nueva correlación de fuerzas, eligió como al cristiano-maronita Bashir Gemayel, sustituyendo al también maronita Elias Sarkis, que había agotado su mandato. Sin embargo, antes de tomar posesión, Gemayel fue asesinado por un agente sirio, junto con otras 29 personas que se encontraban en ese momento en el cuartel general de las Falanges Libanesas en Beirut. En venganza por el asesinato, las Falanges Libanesas entraron en los campamentos de refugiados palestinos de Sabra y Chatila, provocando una masacre civil en la que murieron entre 460 y 3500 civiles palestinos. Una comisión judicial israelí del más alto nivel –la Comisión Kahan– investigó lo sucedido. Señaló a los cristianos falangistas como autores materiales de las muertes, pero imputaba a Israel una «responsabilidad indirecta» por no haberla evitado, al haberse producido los hechos bajo el control militar israelí de la ciudad. Este veredicto provocó un hondo impacto en la opinión internacional y en la propia Israel, que destituyó de su cargo al entonces ministro de Defensa, Ariel Sharon, si bien permaneció en el gabinete como ministro sin cartera. En mayo de 1983, Israel y Líbano alcanzaron un acuerdo para la retirada de las tropas israelíes. Sin embargo, el tratado de paz no llegó a ser ratificado y, en marzo de 1984, bajo presión siria, Líbano canceló el acuerdo. Ante el goteo de bajas israelíes y los constantes atentados chiíes (con un promedio de 100 al mes), Israel inició su repliegue unilateral y progresivo en 1985, dejando de nuevo la llamada "zona de seguridad" (unos 850 kilómetros cuadrados) en manos del cristiano-libanés (y pro-israelí) Ejército del Sur del Líbano, con una presencia menor de tropas del Tsahal.

Finalmente, en mayo de 2000, Ehud Barak, primer ministro israelí, cumplió su promesa electoral de retirar todas las tropas israelíes del sur del Líbano, en cumplimiento de la resolución 425 del Consejo de Seguridad de la ONU, que había sido aprobada 22 años antes. La ONU verificó "in situ" la retirada israelí al sur de la frontera internacional. Las granjas de Shebaa, un pequeño terreno de 20 kilómetros cuadrados en la falda del monte Hermón que el ejército israelí tomó a los sirios en 1967, y que Beirut ha reclamado desde mucho antes como propio, le sirvió a Hezbolá (la milicia chií proiraní) como pretexto para mantener su hostigamiento armado contra Israel y para no aceptar la resolución 1559 del Consejo de Seguridad de la ONU, que la obligaba a desarmarse y dejar el control de la frontera en manos del ejército libanés.

El estatus de Jerusalén sigue siendo uno de los puntos de disputa claves del conflicto árabe-israelí. Israel siempre ha reclamado Jerusalén como capital religiosa y civil del pueblo judío. Los árabes, que la controlaron durante unos 700 años (638-1513), o los turcos-otomanos, que la gobernaron durante otros 400 (1513-1917), o los jordanos, que la ocuparon durante 19 años más (1948-1967), nunca le procuraron ningún estatus especial de capitalidad nacional. La resolución 181 de la Asamblea General de las Naciones Unidas estableció la partición del Mandato británico de Palestina en un Estado árabe y otro judío, pero dejó la zona de Jerusalén y Belén como un "corpus separatum" que habría de estar administrado por las Naciones Unidas, algo que detalló con posterioridad en su resolución 303. Sin embargo, la ciudad quedó dividida en dos partes tras la guerra árabe-israelí de 1948. La parte occidental de Jerusalén fue proclamada capital de Israel en 1950, movimiento que no ha sido reconocido desde entonces por ningún país del mundo salvo Estados Unidos. El llamado Jerusalén Este, que incluía la Ciudad Vieja, quedó bajo control jordano. Durante la Guerra de los Seis Días de 1967, Israel conquistó el Muro de las Lamentaciones de la Ciudad Vieja, así como el resto de la parte oriental de la ciudad que había estado bajo control jordano desde 1949, y unificó administrativamente el municipio.

En 1980, Israel promulgó la Ley de Jerusalén que declaraba a la ciudad, incluida la parte oriental y una amplia zona periférica, como «capital eterna e indivisible» del Estado de Israel. La ONU replicó con la resolución 478, que invalidaba dicha declaración de capitalidad, y aconsejó a sus miembros a que situasen sus embajadas en Tel Aviv. Solo Costa Rica y El Salvador mantuvieron, hasta agosto del 2006, sus embajadas en Jerusalén, y ambas las trasladaron a Tel Aviv tras esta fecha. En cambio, Estados Unidos trasladó su embajada a Jerusalén el 14 de mayo de 2018. Ningún país del mundo ha reconocido expresamente la soberanía israelí sobre Jerusalén Este; el reconocimiento estadounidense de la soberanía israelí sobre Jerusalén realizado en 2017 sólo se hizo de forma nominal y no establecía ninguna posición sobre fronteras dentro de la ciudad ni, por tanto, sobre Jerusalén Este.

Por su parte, la Autoridad Nacional Palestina reclama la parte oriental de Jerusalén ("Al-Quds") como la capital del futuro Estado palestino. Desde 1964, fecha en que nació la OLP, hasta 1967, fecha en que Israel conquistó Jerusalén oriental, la OLP no planteó la cuestión de la capitalidad. En 2002, esta demanda fue ratificada por una ley firmada por Yaser Arafat.. 

La ONU sigue manteniendo que el estatus de Jerusalén es el de una ciudad internacional cuya soberanía debe ser resuelta en futuras negociaciones entre israelíes y palestinos, por lo que cataloga de ocupación ilegal el control israelí sobre Jerusalén Este. En el año 2000, Yasir Arafat rechazó una propuesta de paz del primer ministro Ehud Barak que, entre otros muchos puntos, incluía dejar bajo soberanía palestina el Barrio Musulmán y Cristiano de la ciudad.

Los palestinos que habitan Jerusalén poseen un documento israelí que les permite moverse por Israel, pero no tienen derecho al voto en las elecciones nacionales salvo que opten por la nacionalidad israelí.

Desde la conquista de Jerusalén Este y el resto de Cisjordania en 1967, Israel ha expropiado y comprado terrenos para crear nuevos asentamientos judíos en Jerusalén Este con la intención de consolidar la presencia judía en todo el municipio.

El 9 de diciembre de 1987, un vehículo israelí se vio involucrado en un accidente en Gaza, en el que murieron cuatro palestinos. Después de 20 años de ocupación militar israelí, los palestinos comenzaron a desafiar a las tropas israelíes.

La Intifada organizó la agrupación de todos los sectores palestinos bajo un liderazgo central, politizando a toda la sociedad palestina.

En 1991 se realizó la Conferencia de la Paz en Madrid, con la participación de Líbano, Siria, Israel, Egipto y una delegación palestino-jordana. En esta conferencia se logró acordar la realización de negociaciones.

En septiembre de 1993 los palestinos reconocieron el Estado de Israel y los israelíes reconocieron la Autoridad Nacional Palestina firmando los tratados de Oslo que preveían un repliegue de Israel y el establecimiento de un Estado Palestino.

Los tratados de Oslo preveían devolver a los palestinos la mayor parte del territorio ocupado en 1967, en la Guerra de los Seis Días. Sin embargo, mantenía la soberanía israelí sobre un gran número de asentamientos judíos dispersados por este territorio y habitados en su mayoría por sionistas.

Por otro lado, los conflictos por la soberanía de Jerusalén (que ambos reclamaban como capital de sus Estados) seguía sin ser resuelta.

La Autoridad Palestina aceptó el tratado y se establecieron ocho áreas autónomas alrededor de las ciudades palestinas más importantes. No obstante, Israel continuó el establecimiento ilegal de colonos judíos en los territorios que deberían haber sido devueltos, mientras que las distintas organizaciones armadas palestinas continuaron con sus ataques terroristas contra la población civil israelí.

El entonces líder de la oposición israelí, Ariel Sharón, visitó la zona exterior del recinto de la Cúpula de la Roca y la Mezquita de Al-Aqsa, en septiembre de 2000, en pleno debate sobre el futuro de Jerusalén pero con el permiso del jefe de la seguridad palestina en Cisjordania, lo que provocó algunos incidentes y choques con palestinos, pues fue visto por la población palestina como una gravísima provocación. Al día siguiente, en la plegaria del viernes, con la tensión entre ambas poblaciones en aumento, cientos de jóvenes musulmanes apedrearon desde la Explanada de las Mezquitas a los judíos congregados ante el Muro. La policía israelí reaccionó, extendiéndose los incidentes por toda la parte árabe de Jerusalén. Se ha venido sosteniendo que la Segunda Intifada se inició a raíz de estos hechos, aunque una comisión al efecto, la llamada Comisión Mitchell descartó esta posibilidad, asegurando que la violencia palestina hubiese estallado de cualquier forma como producto de la negativa de Arafat de aceptar las propuestas israelíes de Camp David. En concreto, Bill Clinton y Ehud Barak, entonces primer ministro de Israel, propusieron una serie de concesiones que no fueron aceptadas por el "rais" palestino, al no contemplarse el derecho de retorno de los refugiados palestinos, derecho reconocido por la ONU en 1948 y principal motivo del nacimiento de la OLP. La negativa de Arafat provocó la reacción de la población palestina de los territorios ocupados en respuesta a la propuesta de ambos presidentes.

Como respuesta a este ataque, y al cada vez más deteriorado y empantanado proceso de paz, Israel ocupa de nuevo algunos de los territorios que había liberado durante horas o semanas. En esta intifada, se comienza a generalizar el uso de las bombas suicidas. Los blancos de estos ataques suicidas fueron lugares frecuentados por los civiles israelíes como centros comerciales, restaurantes y las redes de transporte público.

En respuesta a los ataques suicidas de las organizaciones armadas palestinas, las autoridades israelíes pusieron en práctica los asesinatos extrajudiciales contra dirigentes palestinos vinculados a actividades terroristas. Estas muertes son conocidas por los israelíes como "asesinatos selectivos", un eufemismo popularizado por algunos medios de comunicación, y que, en opinión de sus críticos, constituyen una violación de la Convención de Ginebra, que señala en su punto 1d que este tipo de crímenes "están y se mantendrán prohibidos en cualquier tiempo y lugar las ejecuciones, sin previo juicio de una corte oficialmente constituida y asumiendo todas las garantías judiciales reconocidas como indispensables en los países civilizados". Este artículo se aplica a toda persona que "no tome parte activa en las hostilidades, incluyendo miembros de fuerzas armadas que hayan abandonado sus armas" y aquellas personas "fuera de combate por enfermedad, heridas, detención o cualquier otra causa". Sin embargo, Israel arguye que los objetivos seleccionados y abatidos son "parte activa en las hostilidades", ya que son los planificadores o instigadores de actividades terroristas dentro del territorio israelí.

En 2006, la situación es ambivalente: por un lado se ha completado el Plan de retirada unilateral israelí de la Franja de Gaza, lo cual, lejos de calmar la situación, fue aprovechado por los árabes como punto estratégico para los ataques terroristas con cohetes Qassam contra las poblaciones fronterizas israelíes como Sederot. Por otro lado, Israel mantiene el control fronterizo, lo que dificulta los viajes al exterior de los palestinos, y vigila de forma estricta el movimiento entre las ciudades palestinas (hay desplegados más de 90 puntos de control en las carreteras). Los palestinos no residentes en Jerusalén tampoco pueden ingresar en la ciudad.

En julio de 2006, un grupo de combatientes de Hezbolá cruzó la frontera de Líbano hacia Israel, atacó y tomó a dos soldados israelíes como rehenes. En una fallida incursión de rescate, ocho soldados israelíes murieron en una emboscada organizada por Hezbolá. Este grupo exigió la liberación de los presos libaneses en Israel a cambio de la liberación de los soldados capturados, algo a lo que Israel se negó. Se desencadenó así la Guerra del Líbano de 2006, que causó gran destrucción en el Líbano. Un alto el fuego patrocinado por la ONU (Resolución 1701) entró en vigor el 14 de agosto de 2006, terminando oficialmente el conflicto. El conflicto causó la muerte de más de mil libaneses y más de 150 israelíes, dañó enormemente las infraestructuras civiles libanesas y desplazó a aproximadamente un millón de libaneses y a entre 300.000 y 500.000 israelíes, aunque la mayoría pudo regresar a sus hogares. Después del alto el fuego, algunas partes del sur del Líbano permanecieron inhabitables debido a las bombas de racimo israelíes sin explotar. Se desplegó la Fuerza Provisional de las Naciones Unidas para el Líbano.

A finales de diciembre de 2008 finalizó la tregua entre Hamás y el ejército israelí en la Franja de Gaza. Israel lanzó su primer ataque en la Franja de Gaza el 27 de diciembre de 2008, como represalia contra Hamas por el lanzamiento de cohetes desde Gaza hacia Israel. El alto al fuego se decretó el 18 de enero de 2009, cuando Israel y Hamas declararon un cese de las operaciones militares. A pesar de esto, proyectiles y cohetes continuaron siendo disparados desde Gaza hacia los centros de población civil israelí, mientras que las Fuerzas de Defensa de Israel continuaron con sus ataques sobre la Franja de Gaza.

Con el nombre de ataque a la flotilla de Gaza se conoce popularmente un violento incidente ocurrido el 31 de mayo de 2010, cuando la Marina de Israel abordó una flotilla de seis embarcaciones de la organización pro-palestina Free Gaza (denominada «Flota de la Libertad») en aguas internacionales del mar Mediterráneo. Los comandos israelíes mataron a nueve activistas e hirieron a más de una treintena de personas. Uno de los heridos, que estuvo en coma durante cuatro años, falleció en mayo de 2014, lo que elevó la cifra de víctimas mortales a 10.

El 13 de noviembre caen dos obuses sirios en los altos del Golán en el norte de Israel además de La actual escalada -iniciada el sábado 10 de noviembre por la tarde con el misil palestino contra un jeep que patrullaba en la frontera con Gaza (cuatro soldados heridos) y los 120 misiles y cohetes contra el sur del país disparados por el grupo islamista Hamas y otras facciones armadas de Gaza, en respuesta a esos ataques, el 14 de noviembre de 2012 Israel declara la guerra a la Franja de Gaza, lanzando una ofensiva con misiles y bombardeos a ciudades de Gaza, dejando seis muertos, entre ellos el comandante Azedín Al Qasem, y decenas de heridos.

En pleno conflicto, el día 23 de octubre el emir de Catar Hamad bin Jalifa al Thani realiza una visita relámpago a la zona de Gaza, la primera de un jefe de Estado desde que en el año 2007 Hamás tomara el poder. Esta visita inquieta especialmente a las autoridades israelís que ven en el emir una fuente de financiación para el Gobierno de Hamás. La respuesta de Israel no se hace esperar, realizando ataques selectivos desde el día 24 de octubre, hasta el día 10 de noviembre.

Tras algunos días de relativa tranquilidad Israel lanza su ataque más hiriente: el día 14 de noviembre asesina a Ahmed Yabari, miembro del Gobierno de Hamás y máximo responsable militar de Hamás. De esta forma muere uno de los mandatarios más influyentes de Hamás, convirtiéndose en el asesinato de un político más importante desde el año 2008. Este fue el punto de inflexión que ocasionó la escalada de violencia, al considerarse a partir de ese momento como un acto de guerra, con todas sus consecuencias.

Hamás suscita ciertas simpatías en su entorno, como por ejemplo la del primer ministro egipcio Hisham Kandil, que visita al día siguiente la Franja de Gaza, manifestando su apoyo al Gobierno de Hamás. Este movimiento estratégico es especialmente significativo y clarifica la posición egipcia con respecto al conflicto palestino tras la revolución en el marco de la “Primavera Árabe”. La alianza egipcia-palestina inquieta especialmente a las autoridades israelís, ya que Egipto sí cuenta con un ejército profesional y una considerable potencia de fuego. De la misma manera se airean los recuerdos de la guerra de Yom Kipur en 1973, guerra relámpago en la que las tropas egipcias y sirias invadieron parte del territorio israelí adquirido en el marco de la Guerra de los Seis Días en 1967 (En este conflicto, Israel realizó una contraofensiva implacable, llegando a invadir territorio egipcio. Las tropas israelíes se detuvieron a 100 km de El Cairo tras firmarse el alto el fuego).

En este contexto, y hasta la firma de un alto el fuego el 21 de noviembre de 2012, se contabilizaron innumerables ataques desde ambos lados. Cabe destacar el impacto de dos cohetes palestinos en Jerusalén, que no recibía impactos desde 1991, y tres en Tel Aviv y su área metropolitana, dos ciudades que gozaban de cierta seguridad ante ataques palestinos. Los cohetes no produjeron daños materiales ni humanos en Jerusalén, al caer en una zona yerma de las afueras. En Tel Aviv, el primero no causó ningún daño, el segundo fue interceptado por el sistema Iron Dome y el tercero causó daños en un edificio residencial de las afueras dejando dos heridos leves.

Según datos publicados por la Organización Mundial de la Salud el 22 de noviembre, el conflicto arrojó un balance de 165 muertos palestinos, de los cuales 42 (26%) son niños, y 1.269 heridos, de los que 431 (34%) son niños. Según declaración del secretario general de la ONU el 21 de noviembre, del lado israelí se contabilizaron cuatro muertos y 219 heridos, la mayoría civiles, tres de ellos de gravedad. Un soldado israelí murió y 16 resultaron heridos, uno de gravedad que falleció posteriormente.

Desde 2012 ha habido algunos enfrentamientos en los Altos del Golán entre Israel y Siria, en el marco de la Guerra Civil Siria. Sin embargo, esta escalada de tensiones llegó a su punto álgido el 5 de mayo de 2013, cuando se especuló que Israel bombardeó un cargamento de armas iraníes en Siria con destino al Hezbolá en Líbano.

La guerra subsidiaria irano-israelí es un conflicto que tiene como principal base la lucha política entre la República Islámica de Irán y el Estado de Israel debido a que desde el ascenso del republicanismo teocrático del ayatolá Ruhollah Jomeini en Irán ésta se ha vuelto enemiga del estado judío. Israel también ha tratado de debilitar y eliminar los aliados políticos regionales como Hezbolá en el Líbano, y a Hamás y a la Yihad Islámica Palestina en Palestina. 

Este conflicto se agravó desde que a principios del siglo XXI se hizo evidente el Programa nuclear de Irán, algo que para las autoridades israelís es inaceptable porque en su opinión un Irán con armas nucleares sería una "amenaza existencial para Israel". Israel ha boicoteado de diversas maneras el desarrollo del programa nuclear iraní (asesinato de científicos nucleares iraníes, guerra cibernética...). Israel también culpa a Irán de estar detrás de atentados en otros países como el atentado a la embajada de Israel en Argentina en 1992 o el atentado a la AMIA de 1994.

Tras las revueltas de la Primavera árabe en 2011 y el desencadenamiento de la Guerra Civil Siria Irán ha apoyado al gobierno sirio de Bashar al-Ásad y ha enviado tropas (Guardia Revolucionaria) a ese país.

El 29 de noviembre de 1947 la Asamblea General de las Naciones Unidas, reunida en Nueva York, aprobó la Resolución 181, la cual recomendaba un plan para resolver el conflicto entre judíos y árabes en la región de Palestina, que se encontraba en esos momentos bajo administración británica. El plan de la ONU proponía dividir la parte occidental del Mandato en dos Estados, uno judío y otro árabe, con un área, que incluía Jerusalén y Belén, bajo control internacional. La incapacidad del gobierno británico para llevar a cabo este plan, junto con la negativa de los países árabes de la región a aceptarlo, tuvo como consecuencia la guerra árabe-israelí de 1948.

El 30 de abril de 2003 se presenta al Gobierno de Israel y a la Autoridad Palestina, una hoja de ruta elaborada por el Cuarteto (Estados Unidos, Unión Europea, Rusia y las Naciones Unidas) para lograr la paz entre Israel y Palestina teniendo como plazo máximo 2005. Texto completo

En la Fase I de esta Hoja de Ruta, se establecen las bases necesarias para la iniciación de un proceso paulatino de paz efectiva entre Israel y Palestina: el fin al terror y la violencia, normalización de la vida de los palestinos y creación de instituciones palestinas.

En la Fase II se establecen las bases para la retirada paulatina de las fuerzas israelíes de los territorios ocupados desde 2000, la congelación de la política de asentamientos israelíes, la continuación del desmantelamiento de las organizaciones terroristas y la consolidación de instituciones palestinas.

En la Fase III se sientan las bases para un estatuto permanente y el fin definitivo del conflicto israelí-palestino.

El 19 de noviembre, al ver la carencia de apoyo de la Hoja de Ruta entre las dos partes afectadas, las Naciones Unidas sacan la resolución 1515 en la que hacen suya la Hoja de Ruta e instan a las partes a la colaboración en la búsqueda de una solución pacífica al conflicto israelí-palestino.

Las Resoluciones más importantes en el conflicto árabe-israelí son:


En vidas humanas, las estimaciones oscilan desde los 51.000 fallecidos (35.000 árabes y 16.000 israelíes desde 1950 hasta 2007, hasta los 92.000 fallecidos (74.000 militares y 18.000 civiles) entre 1945 y 1995.

Un informe del "Strategic Foresight Group" (un "think tank" radicado en la India) ha estimado el coste de oportunidad del conflicto en Oriente Medio en unos 12 billones de dólares estadounidenses entre 1991 y 2010. El informe calcula el hipotético PIB de los países de Oriente Medio comparando el actual PIB con el PIB potencial en tiempos de paz. El correspondiente a Israel sería de un billón de dólares, mientras que a Iraq y a Arabia Saudita les corresponderían 2,2 y 4,5 billones, respectivamente. En otras palabras, si hubiera existido paz y cooperación entre Israel y los países árabes desde 1991, un ciudadano medio israelí podría haber obtenido una renta anual de 44.000 dólares, en lugar de 23.000 en el año 2010.













Existe una amplia filmografía sobre conflicto árabe-israelí que cubre prácticamente toda su historia. Sobre los conflictos intercomunitarios propios de la época del Mandato británico de Palestina, la serie británica "The Promise" aborda el nacimiento de la Haganá y su lucha contra las tropas británicas, así como el comienzo de la Nakba y, en especial, el bombardeo de la población civil palestina en la ciudad de Haifa. La guerra árabe-israelí de 1948 ha sido abordada por diversas películas estadounidenses, casi siempre aportando el punto de vista israelí, como en el caso de "Éxodo", protagonizada por Paul Newman, o "La Sombra de un Gigante", con Kirk Douglas como protagonista. "Miral", del estadounidense Julian Schnabel, aborda la expulsión y huida de la población palestina ante el empuje del ejército israelí en 1948, conocida como la Nakba. La miniserie "El Espía" narra la vida del espía israelí Eli Cohen, cuyo trabajo en Siria ayudó en gran medida a Israel durante la Guerra de los Seis Días de 1967, conflicto que forma el telón de fondo de "Avanti Popolo", película israelí que narra las odiseas de soldados egipcios e israelíes perdidos en el desierto del Sinaí. La película "Múnich" sigue al comando del Mosad que buscó y asesinó a los milicianos palestinos que atentaron contra la expedición israelí en los Juegos Olímpicos de Munich en 1972. El periodo de entreguerras de 1967 a 1973 queda retratado en la película estadounidense "The Angel", que muestra la vida de Ashraf Marwan, yerno de Gamal Abdel Nasser y espía (o agente doble) israelí. La guerra civil libanesa y sus consiguientes desastres humanitarios aparecen reflejados en películas como "Incendies", de Denis Villeneuve, o en las libanesas "West Beirut y El Insulto", que también describe la actual situación de los refugiados palestinos en el Líbano. La invasión israelí del Líbano (1982) y la consiguiente masacre de refugiados palestinos en los campamentos de Sabra y Shatila aparece descrita en la película de animación israelí "Vals con Bashir," mientras que la serie israelí "Hatufim" (en la que se basó la estadounidense "Homeland") describe la vida de varios prisioneros de guerra israelíes capturados en la Guerra de Líbano y liberados 17 años después. El asesinato de Isaac Rabin por parte de Yigal Amir es el tema de la película israelí "Incitement" (incitación), mientras que la retirada israelí del Líbano (2000) es el telón de fondo de la también israelí "Beaufort". La Segunda Intifada palestina ha producido películas como "Paradise Now," de Hany Abu-Assad, que critica el modus operandi de los suicidas palestinos contra Israel. La situación de la Franja de Gaza desde que Israel impuso un bloqueo por tierra, mar y aire en 2007 ha sido reflejada en películas como "Una botella en el mar de Gaza", de Thierry Benisti o "El ídolo", de Hany Abu-Assad. Más genéricamente, la ocupación israelí de Palestina es la protagonista en películas como "Los Limoneros", del director israelí Eran Riklis; "Foxtrot", del israelí Samuel Maoz; "La sal de este mar", de la directora palestina Annemarie Jacir; "Omar", del director palestino Hany Abu-Assad; "Inch'Allah", de la directora francesa Anaïs Barbeau-Lavalette; "El Atentado", del libanés Ziad Doueiri, o en las series israelíes "Our Boys", de Hagai Levi, y "Fauda", de Lior Raz y Avi Issacharoff, así como en la británica "The Honourable Woman". Por último, la situación de los israelíes de origen palestino se refleja en películas como "Boda en Galilea", "Wajib" o "El Hijo del Otro". 

El conflicto árabe-israelí también ha sido abundantemente descrito en el género del documental, como , que documenta el éxodo de unos 750.000 palestinos durante la guerra árabe-israelí de 1948. "The Oslo Diaries" refleja las negociaciones secretas de paz que acabaron desencadenando los Acuerdos de Oslo. Otros documentales relacionados con el conflicto árabe israelí son "Promesas," que muestra el punto de vista de cuatro chicos israelíes y tres palestinos que viven a veinte minutos de distancia entre sí, pero crecen en universos completamente alejados; "5 cámaras rotas", del palestino Emad Burnat, que describe las protestas contra el muro de separación israelí en la aldea palestina de Bil'in; "Nacido en Gaza", que describe las consecuencias psicológicas y físicas en un grupo de niños a consecuencia de los bombardeos israelíes en la Guerra de Gaza de 2014; "Yenín, Yenín", del palestino Mohammad Bakri, que describe los hechos que rodearon la Batalla de Yenín en 2002, durante la Segunda Intifada palestina; o "Occupation 101", que analiza los aspectos legales y económicos de la ocupación israelí.





</doc>
<doc id="38011" url="https://es.wikipedia.org/wiki?curid=38011" title="Zona de ablación de un glaciar">
Zona de ablación de un glaciar

La zona de ablación de un glaciar es el área donde se pierde el hielo y la nieve. Comprende la fusión, la evaporación y el desprendimiento de grandes masas de hielo cuando el glaciar desciende a niveles inferiores donde la morrena recubre una superficie en curso de adelgazamiento del glaciar o en de un glaciar muerto.





</doc>
<doc id="38012" url="https://es.wikipedia.org/wiki?curid=38012" title="Glaciar de piedemonte">
Glaciar de piedemonte

Un glaciar de piedemonte es un glaciar que se forma cuando uno o más glaciares de valle abandonan una zona montañosa y se desparrama por una tierra baja cercana. Son lóbulos de hielo en zonas donde un glaciar ha abandonado un valle y se extiende a una llanura adyacente

Un ejemplo de glaciar de piedemonte es el glaciar Malaspina en Alaska, Estados Unidos.


</doc>
<doc id="38018" url="https://es.wikipedia.org/wiki?curid=38018" title="Diaclasa">
Diaclasa

Una diaclasa (del griego «διά» "dia", a través de, y "klasis", rotura) es una fractura en las rocas que no va acompañada de deslizamiento de los bloques que determina, excepto una mínima separación transversal. Se distinguen así de las fallas, fracturas en las que sí hay deslizamiento de los bloques. Son estructuras de deformación frágil de las rocas muy abundantes en la naturaleza.

La orientación de una diaclasa, como la de otras estructuras geológicas, se describe mediante dos parámetros: 
Las diaclasas no tienen por qué ser en general planas, ni responder a ninguna geométrica regular, así que los parámetros indicados pueden variar de un punto a otro.

Las diaclasas no suelen aparecer aisladas, sino asociadas a fallas y a pliegues. Cuando, como suele ocurrir, existen dos o más conjuntos de diaclasas, se habla de un sistema de diaclasas o ""joint system"". Los más sencillos son:


Para poder discriminar entre diaclasas de compresión y de distensión hay que estudiar los ejes principales de la deformación local o regional, pues las diaclasas en sí mismas no aportan información suficiente (estrías o desplazamiento). En el caso de diaclasas de extensión la dirección de la familia más notoria suele ser perpendicular a la dirección de la extensión y en las de compresión la dirección de la bisectriz del ángulo agudo de la intersección de diaclasas.

La formación de las diaclasas obedece a muy diversas causas, incluyendo fuerzas dirigidas como las que provocan el fallamiento o plegamiento del terreno.
Una de las causas más frecuentes de diaclasamiento es la disminución del volumen del material (aumento de la densidad), que a su vez se puede producir por distintos motivos:



</doc>
<doc id="38021" url="https://es.wikipedia.org/wiki?curid=38021" title="Graderío glaciar">
Graderío glaciar

Un graderío glaciar se caracteriza por la presencia en el lecho de un valle de escalones rocosos de fuerte pendiente («umbrales»), que marcan las distintas alturas a las que se encontraba el hielo y otras partes menos inclinadas («ombligos»). Estos cambios de pendiente se deben a las diferencias de intensidad de la erosión glaciar, durante las épocas de avance y retroceso de los glaciares. La composición y el grado de plegamiento de las rocas que conforman el terreno también influyen en la formación de graderíos.


</doc>
<doc id="38024" url="https://es.wikipedia.org/wiki?curid=38024" title="Difluencia glaciar">
Difluencia glaciar

La difluencia es la división de un glaciar o cualquier curso de agua en brazos ("difluentes") que no vuelven a unirse. 

La lengua de un glaciar con excesivo aporte de hielo puede desbordar las aperturas de las brechas glaciares en las cuencas de una zona extensa, haciendo que muchos valles se llenen de hielo. La difluencia de los ríos es común en los deltas. Se debe generalmente a la disminución de la pendiente, pero en otros casos es consecuencia del aumento de la carga transportada por las aguas.

Un ejemplo de difluencia glaciar lo constituyen las brechas glaciares de Snowdon, al norte del País de Gales.


</doc>
<doc id="38025" url="https://es.wikipedia.org/wiki?curid=38025" title="Vritrá">
Vritrá

En la religión védica (previa al hinduismo), Vritrá es un asura (demonio) con forma de serpiente o dragón, personificación de la sequía y enemigo del dios Indra.
En los textos "Vedas" también era conocido como Aji (‘serpiente’). Era hermano de Valá.
Como un dragón bloqueó el curso de los ríos védicos y fue muerto heroicamente por Indra.



Según el "Rig-veda" (siglo XIV a. C.), Vritrá mantuvo cautivas las aguas del mundo hasta que fue asesinado por Indra, quien destruyó las noventa y nueve fortalezas de Vritrá (aunque a veces estas fortalezas se atribuyen a Shambara) antes de liberar a los ríos represados.
El combate comenzó poco después del nacimiento de Indra, quien había bebido una gran cantidad de soma en casa del sabio Tuashtri para empoderarlo antes de enfrentarse a Vritrá.
Tuashtri creó el rayo "(vashra)" para Indra, y el dios Visnú, cuando Indra se lo pidió, hizo espacio para la batalla, dando los tres grandes pasos por los que se hizo famoso.

Durante la batalla, Vritrá le rompió ambas mandíbulas a Indra, pero finalmente fue derribado por éste. Al caer, terminó de aplastar sus fortalezas, que ya Indra había hecho añicos.

Por esta hazaña, Indra fue conocido como Vritraján (‘asesino de Vritrá’) y también como ‘asesino del primogénito de los dragones’.
Después Indra atacó con su rayo y derrotó a la madre de Vritrá, Danu (que era también la madre de toda la raza danavá de asuras).

En una de las versiones de la leyenda, Indra convenció a tres devas —Váruna, Soma y Agní— para que lo ayudaran en su lucha contra Vritrá. Antes ellos habían estado del lado del asura, a quien llamaban «Padre».

En un verso de un himno rigvédico que elogia a la diosa Sárasuati, a ésta se le atribuye el asesinato de Vritrá. Esta mención no ocurre en ninguna otra parte.

En una confusa modificación posterior del mito, Vritrá habría sido creado por Tuashtri para vengar la muerte de su hijo Trisiras o Vishua Rupa, que había sido asesinado por Indra.
Vritrá ganó la batalla y se tragó Indra, pero los demás dioses lo obligaron a vomitarlo.
La batalla continuó e Indra se vio forzado a huir.

Visnú y los rishís (sabios ermitaños) negociaron una tregua: Indra prometió que no atacaría a Vritrá con cualquier cosa hecha de metal, madera o piedra, ni nada que fuera seco o húmedo, o durante el día o la noche.
Entonces Indra utilizó espuma extraída de las olas del océano (en la que el omnipenetrante Visnú se había introducido para convertirla en arma) y lo mató en el crepúsculo.

El "Bhágavata-purana" (siglo XI d. C. aprox.) convierte a Vritrá en un bhakta (devoto) del dios Visnú y por lo tanto más avanzado espiritualmente que el materialista Indra. Esa leyenda dice así:

Vritrá (que en esta versión es un brahmán) se convirtió en el líder de los asuras (que en los "Puranas" se consideran como inherentemente demoníacos, a diferencia de la antiquísima versión védica que les permitía ser dioses o demonios).
Vritrá luchó durante miles de años contra los dioses devas.
Al final, los asuras ganaron y los devas se retiraron.
Dirigidos por Indra, se acercaron al dios Visnú en busca de ayuda.
Éste les reveló que Vritrá no podría ser destruido por medios ordinarios, sino solo con una lanza hecha con la columna vertebral de un sabio.
Cuando los dioses revelaron sus dudas acerca de que ningún asceta se suicidaría para ayudarlos, Visnú los envió con el sabio rishí Dadichi.
Los huesos de éste eran indestructibles porque años atrás, cuando los devás le pidieron que guardara sus armas de hierro (una metalurgia desconocida en la India en esa época), Dadichi involuntariamente las había arruinado al mantenerlas almacenadas en agua del Ganges (que él creía que les daría brillo y energía). Las armas se habían oxidado y disuelto en el agua, y Dadichi bebió de esa agua para absorber la energía de las armas.
Cuando fue interpelado por los dioses, Dadhichi con gusto renunció a su cuerpo para la causa de los devas, indicando que ese era un mejor uso de sus huesos que pudrirse en la tierra.
El sabio se suicidó, los animales comieron su carne e Indra recogió su columna y creó el vashra aiudha.
Cuando atacó nuevamente a Vritrá, la batalla se prolongó durante 360 días antes de que el brahmán asura exhaló su último suspiro.

En ambas versiones (ya sea por matar al brahmán Trisiras o al brahmán Vritrá), Indra terminó siendo perseguido por la personificación antropomórfica del brahmanicidio "(brāhmana-jatia)".
Indra se escondió por su pecado, y Najusha fue invitado a ocupar su lugar como rey del cielo.

La versión puránica de que Indra pudo matar a Vritrá únicamente cuando se cumplieran ciertas condiciones podría provenir del "Ramaiana" (cuya composición podría datarse en la misma época), en donde los dioses no podían matar al demonio Ravana a causa de una bendición que impedía que él fuera muerto por cualquier ser humano. En ese caso Rama pudo matarlo porque aprovechó el vacío legal: él no era un ser humano sino un dios encarnado.

Quizá sea anterior una leyenda también puránica más similar, la de Nara Simja (‘hombre-león’, avatar de Visnú).
Allí Jirania-Kashipú, siendo un rey asura, utilizó la misma estratagema que sus antepasados, obtuvo una bendición del dios Brahmá que le impedía morir de día o de noche, por ser humano o bestia, dentro o fuera de una casa, y por ningún arma creada por el hombre.
Para matar a este asura sin contradecir la bendición de su querido Brahmá, Visnú encarnó como una mezcla de hombre y león (no era ser humano ni bestia).
Narasimja utilizó sus uñas (ningún arma creada por el hombre), colocó a Jirania-Kashipú en la puerta (no era el interior ni el exterior) y le arrancó los intestinos en el crepúsculo (no era de día ni de noche).




</doc>
<doc id="38031" url="https://es.wikipedia.org/wiki?curid=38031" title="Indra">
Indra

En la mitología hinduista, Indra es el rey de los dioses o devas y señor del Cielo y dios principal de la religión védica (previa al hinduismo) en la India.

Aparece como héroe, deidad y figura central en el libro "Rig-veda" (mediados del II milenio a. C.).
Es considerado el dios de la guerra, la atmósfera, el cielo visible, la tormenta y el rayo, que era representado como una espada con ondulaciones (como un rayo).

Posteriormente, en el hinduismo, se convirtió en el rey de todos los semidioses (dioses inferiores) y fue superado por los dioses Brahmá, Vishnú y Shivá.

Su arma es el relámpago "(vashra)".
Su "vajana" ("vahana:" ‘vehículo, montura’) es el elefante Airavata, que representa la nube de la cual Indra hace descargar su lluvia.

Entre otras cosas es el dios regente de la pupila del ojo derecho (mientras que la del izquierdo es representada por su esposa, la diosa Indrānī) o Sachi.

Su piel es blanca o amarillenta y su cuerpo está cubierto de ojos con párpados que le permiten ver todo lo que sucede en el mundo.

En realidad esos ojos fueron una maldición-bendición de sabio Gótama.
Indra había seducido a la esposa del sabio, Ajalia (Ahalya).
Al enterarse el asceta del adulterio, hizo que el cuerpo de Indra se llenara de decenas de vulvas.
Indra hizo penitencias para pedir perdón, y el sabio terminó accediendo a convertir las vulvas en ojos.

En las escrituras hinduistas, Indra es un dios temeroso de perder su puesto como dios principal.
Por eso, cuando se entera de que algún humano (como Vishuámitra) realiza muchas austeridades para ganar karma que le permita ascender en una siguiente encarnación y ocupar el puesto de Indra, este envía a las prostitutas celestiales, las "apsaras" (como Urvashí, Rambhá o Menaká) para que lo seduzcan y le hagan perder todo avance místico.

Se puede clasificar a Indra como una deidad afín a otros dioses indoeuropeos, como Thor, Perun, Zeus, Jupiter y el dios hitita-anatolio Tarhun, también con otros dioses de las bebidas alcohólicas como Dionisos. El nombre de Indra también se menciona entre los dioses de los mitanni, un pueblo hurrita que gobernó el norte de Siria entre el 1500 y el 1270 a. C.

En los versos del "Rig-veda" (mediados del II milenio a. C.) se dice:

Indra, con Váruna y Mitra, es uno de los Aditias, los dioses principales del 'Rig-veda' (además del dios del fuego Agní y de los Ashvins). Él se deleita en el consumo de la droga soma, y el mito védico central es su heroica victoria sobre el asura Vritrá, liberando los ríos, o, alternativamente, su destrucción del asura Valá, un demonio con forma de caverna en la montaña, donde los Panis habían encerrado a las vacas y a Ushas. Indra es el dios de la guerra, rompiendo las fortalezas de piedra de los Dasius, e invocado por los combatientes de ambos bandos en la batalla de los Diez Reyes.

El "Rig-veda" frecuentemente se refiere a él como Shakrá (‘poderoso’). En el período védico (entre los siglos XV y VII a. C., se suponía que el número de dioses era de 33, y que Indra era su señor (Traias-triṁśa-pati). El "Brijad-araniaka-upanishad" enumera los dioses como los ocho Vasus, los once Rudras, los doce Aditias, Indra y Prayapati Brahmá). Indra también es mencionado bajo el nombre de Vasavá (‘señor de los dioses Vasus’).

En la edad del "Vedanta" (hacia el siglo III a. C.), Indra se convirtió en el prototipo de todos los dioses y por lo tanto como rey que podía llamarse Mānavendra (Mánava-Indra, ‘señor de los hombres’). El dios Rama (el héroe del "Ramaiana") fue citado bajo el nombre de Raghavendra (Rághava-Indra, ‘señor de los Rághavas’). Por lo tanto el Indra original fue llamado también Devendra (Devá-Indra, señor de los dioses). Sin embargo, los nombres Shakrá y Vasavá fueron utilizados exclusivamente por el Indra original. Aunque los textos modernos por lo general se adhieren al nombre de Indra, los textos hinduistas tradicionales (los "Vedas", las Epopeyas y los "Puranas") usan Indra, Śakrá y Vasavá indistintamente y con la misma frecuencia.

Indra está casado con Indrani (cuyo padre, Puloman, él había matado). Fue padre —con distintas mujeres— de Áryuna, Yaianta, Midhusa, Nilambara, Kamla, Bhus y Rishabha, entre otros. Indra es uno de los hermanos del dios del Sol Suria. Indra mató a los hijos de Diti, así que ella esperaba un hijo que fuera más poderoso que Indra y le vengara. Para ello se mantuvo embarazada durante un siglo, mediante la práctica de la magia. Pero cuando Indra la descubrió, le arrojó su rayo, que destrozó el feto en 7 (o 49) partes. Cada parte se regeneró como un individuo, y se convirtieron en los Maruts, un grupo de dioses de la tormenta menos potentes que Indra.



</doc>
<doc id="38032" url="https://es.wikipedia.org/wiki?curid=38032" title="Drumlin">
Drumlin

Un drumlin (derivado de la palabra gaélica "druim", colina redondeada o montículo, de la que hay constancia de su uso en 1833) es una forma de relieve de origen glacial, un pequeño montículo de laderas lisas, de forma aerodinámica, formado frecuentemente por debajo de hielo glaciar en movimiento. Su forma, con un extremo más afilado que otro, se debe al modo en el que el glaciar discurrió por él o a su alrededor. Son colinas bajas, de forma dómica o de cuchara invertida, alargada en la dirección del movimiento del hielo, con la pendiente más suave apuntando en la dirección hacia la cual el hielo se desplazaba. Están formados por acumulación de sedimentos glaciares como tills y morrenas.

Drumlins y los grupos de drumlins son geoformas de origen glaciar ampliamente estudiadas que se componen principalmente de till glacial. Los geólogos han propuesto varias teorías acerca de su origen. Se forman a corta distancia dentro del hielo glaciar y registran la dirección final de movimiento del hielo antes de su retroceso. Los drumlins se presentan en formas simétricas, parabólicas y transversalmente asimétricas. Los dumlins se encuentran comúnmente junto a otras geoformas glaciales con las cuales están relacionadas en una escala regional. Los patrones a gran escala de los drumlins sugieren una formación relacionada con valles en túnel, eskeres, scours y rocas del basamento expuesta por erosión (scalloping y sichelwannen).



</doc>
